{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c199d9",
   "metadata": {},
   "source": [
    "# Домашнее задание к лекции \"Основы веб-скрапинга\"\n",
    "## Обязательная часть\n",
    "\n",
    "Вам необходимо написать функцию, которая будет основана на поиске по сайту habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
    "\n",
    "<дата> - <заголовок> - <ссылка на материал>\n",
    "\n",
    "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка.\n",
    "\n",
    "# Дополнительная часть (необязательная)\n",
    "\n",
    "Функция из обязательной части задания должна быть расширена следующим образом:\n",
    "\n",
    "- кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
    "- в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
    "\n",
    "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "998e3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import NaN\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34590115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16 января весь Слёрм будет праздновать: стартует юбилейный 5 поток Python для инженеров. Не надо оваций, просто присоединяйтесь :)Несмотря на то, что видеокурс по Пайтону доступен всегда, у потока в разы больше преимуществ. Одно из них — итоговый проект, на котором вы сможете решить свою реальную рабочую задачу и подтвердить свои навыки. Вот, что рассказывает наш студент: «Я автоматизировал рабочий проект и смог сэкономить порядка 2 часов на задачах по переводу с деплоя на удобный helmfile с помощью скриптов. Скрипт готовит все файлы и использует шаблонизацию. Таким образом все то, что раньше делалось руками, теперь автоматизировано».Где используется Python?Python — мастхэв для любого бизнеса, где автоматизируют с головой. Ниже несколько примеров, доказывающих это.Senior DevOps Engineer в Chartboost Артем Чекунов рассказывает об инструменте для анализа и отключения холостых ресурсов.«Компания активно пользуется облаком и тратит 2 миллиона рублей в месяц на инфраструктуру в AWS. Можно написать инструмент на Python, который будет анализировать и отключать «холостые ресурсы». Это позволит сократить расходы на 40-50% или около 12 миллионов рублей в год».DevOps инженер в X5 Retail Group Данил Бахаев делится историей про автоматизацию изменения конфигурации сервера.«На прошлой работе автоматизировал рутинные задачи для саппорта. Раньше приходил клиент и писал запрос на изменение конфигурации: увеличить диск на гипервизоре, например. Специалист менял конфигурацию руками, затем вносил изменения в биллинг. Тратили в среднем 5 минут на одного клиента, подобных задач было до 10 в день. После автоматизации процесса с помощью Python время на задачу сократилось до 15 секунд. Экономия в месяц — примерно 23 часа. В качестве бонуса увеличилась лояльность клиентов, ведь теперь их запросы обрабатывались в считанные секунды».Посмотреть больше кейсов на сайтеО 5 потоке курса Python для инженеровСтартуем 16 января, учиться будем почти 3 месяца. В потоке ждут 11 модулей и 8 девопс-задач, по каждой из которых вы получите подробное код-ревью. Занятия проходят в очень активном темпе, зато уже к 5 модулю вы будете легко понимать код на курсе и даже во внешних библиотеках.Оценить программу, задать вопрос и оставить заявку на обучение можно здесь: https://slurm.club/3Pq2fLi    \n",
      " \n",
      "\n",
      "Астрономы из Лейденской обсерватории опубликовали материал, посвященный работе своих коллег. Они подсчитали, что в ходе онлайн-конференции Европейского астрономического общества в атмосферу поступило в три тысячи раз меньше углекислого газа, чем во время очной конференции годом ранее. Их коллеги утверждают, что язык программирования Python, который часто используется в их работе, требует очень много электроэнергии. \n",
      "\n",
      "Лео Буртшер из Лейденской обсерватории, один из организаторов онлайн-конференции в 2020 году и автор статьи, отметил: «Конечно, мы ожидали, что в онлайне выбросы CO2 будут ниже. Но тот факт, что разница была огромной, стал неожиданностью».\n",
      "\n",
      "Буртшер и его соавторы предполагают, что сочетание онлайн-лекций с региональными оффлайн-встречами может стать хорошей альтернативой. \n",
      "\n",
      "Статья о более экономном использовании компьютеров принадлежит профессору вычислительной астрофизики Саймону Портегису Цварту. Он предлагает выполнять повседневную работу, например, писать электронные письма и тексты, на простом ноутбуке. \n",
      "\n",
      "Кроме того, ученый считает, что суперкомпьютер не нужно использовать на полную мощность. Для расчетов и моделирования же можно использовать специальные компьютеры с аппаратным обеспечением на базе видеокарт. \n",
      "\n",
      "Автор подсчитал, что объем углеродных выбросов при работе мощной рабочей станции сопоставим со средним объемом выбросов на душу населения в мире. При работе с одним ядром суперкомпьютер производит меньше углерода, чем рабочая станция. Однако, чем больше ядер, тем лучше производительность за счет производства большего количества углерода. При запуске миллиона ядер выбросы намного превосходят авиаперелеты и приближаются к углеродному следу от запуска ракеты в космос. Оптимальное сочетание производительности и выбросов достигается для ~1000 ядер, после чего суперкомпьютер начинает производить больше углерода, чем рабочая станция.\n",
      "\n",
      "\n",
      "\n",
      "Наконец, автор призвал воздержаться от использования Python, если речь не идет о больших вычислениях. Python (и в меньшей степени Java) требуют значительно больше времени на запуск и производит больше CO2, чем C ++ или Fortran.\n",
      "\n",
      "\n",
      "\n",
      "Многим астрономам этот призыв не понравится, считает Цварт. По его словам, Python удобен для пользователя, и существует множество коллекций бесплатных фрагментов кода, которые астрономы копируют в свои программы. Поэтому он призвал разрабатывать курсы программирования для студентов, где меньше внимания уделялось бы Python и больше — тем языкам, которые намного эффективнее работают с процессором компьютера. См. также: \n",
      "\n",
      "\n",
      "«В атмосфере Венеры нашли фосфин — возможный признак жизни»\n",
      "«Астрономия, big data и облака — как технологии помогают изучать Вселенную»\n",
      "«Быстрое обнаружение сверхновых с помощью нейронных сетей»\n",
      "    \n",
      " \n",
      "\n",
      "14 января 2022 года разработчик jokteur на Github предложил устроить коммунистическую революцию в Python, объединив все классы и сделать их равными изначально. Причина нестандартной эскалации — невозможность отмены класса в Python, не сломав экспрессию языка. По мнению, разработчика, единственный выход в этой ситуации — использовать модуль, чтобы объединить все классы и инициировать глобальную коммунистическую революцию.\n",
      "\n",
      "Фактически разработчик написал, что его задумка может повторить в Python, как он пояснил, действия «одного человека, который однажды сказал отменить классы или что-то в этом роде».\n",
      "\n",
      "Разработчик хочет, чтобы с его модулем элита, например, __builtins__, имела такие же права, как обычные работники типа leftwards_arrow_with_hook и наоборот. \n",
      "\n",
      "После использования модуля для устройства коммунистической революции в Python объявленные позже классы могут быть равными всем остальным. Классы, объявленные после communism.revolution(), не будут следовать правилу коммунизма.\n",
      "\n",
      "Разработчик подытожил, что теперь всем кто хочет, может быть доступен коммунизм, в Python.\n",
      "\n",
      ":)    \n",
      " 24-26 июня пройдёт онлайн-интенсив для инженеров и разработчиков с опытом в Python. Вы научитесь создавать скелет веб-сервиса с фреймворком FastAPI, разберётесь в видах тестирования и поймёте, как писать под Ansible. Вы получите реальный опыт разработки и к третьему дню интенсива создадите полноценный цифровой проект коммерческого уровня.На кого рассчитан интенсивИнженеров, которые знакомы с Python, но хотят больше погрузиться в программирование и применять свои знания на стыке dev и ops.Разработчиков, которые уже знают Python, но хотят лучше освоить этот язык, чтобы получить большую самостоятельность и реализовывать новые практики в компании собственными силами.Почему стоит пойти8 часов теории и 16 часов практики Не будем отвлекаться на абстрактные вещи, а рассмотрим только то, что пригодится в реальной жизни. Сразу после интенсива вы сможете интегрировать полученные знания в свою рабочую область.AMA-сессии со спикером за утренним кофеСпикеры ответят на ваши вопросы по пройденному материалу или разберут конкретные кейсы. AMA-сессии помогут глубже погрузиться в тему и избежать ошибок.Усиление экспертизы и профессиональный рост Вы выйдите за рамки скриптового программирования на Python и начнёте разбираться в конструкциях, типах данных и объектно-ориентированном программировании. Станете более самостоятельными в работе и сможете решать больше задач.Soft skills для эффективного взаимодействия в командеПрокачаем не только технические навыки. Вы сможете эффективнее взаимодействовать с командой и менеджерами и поймёте, как привносить улучшения в проекты собственными силами, экономы ресурсы и время компании. ИнтерактивностьДля участников интенсива будет создан закрытый чат. В нём вы также сможете пообщаться со спикерами, получить помощь куратора и найти единомышленников. Чат останется и после окончания интенсива.Программа интенсиваДень 1. Разберёмся с теорией по проектированию API. Создадим скелет приложения с CRUD операциями. Интегрируем приложение с СУБДДень 2. Создадим задачи с отложенным выполнением. Добавим общепринятые способы авторизации. Автоматически протестируем приложение.День 3. Рассмотрим, как использовать Python для написания плагинов к другим приложениям на примере Ansible. Поговорим об эффективном взаимодействии внутри команды. Проведём большую AMA-сессию со спикерами и Слёрмом, подведём итоги трёх дней.Получить полную программуСпикерыДенис Наумов — Techlead, Data Engineer в Skyeng / ex Слёрм, ISPsystem.Более 5 лет в анализе данных и разработке на PythonВ качестве DataOps развивает аналитические инфраструктуры и управляет потоками данныхСтроит системы реагирования на триггерные события во взаимодействии пользователя с продуктамиОтвечал за CI/CD аналитических сервисов и ML моделейРазрабатывал крупные модули в В2В продуктов7 лет работы тренером и координатором международной организации «Коллегия им. Теодора Хойсса» в Германии  Ольга Скобина — Специальный спикер, директор экосистемы «Слёрм».Эксперт в проектном менеджменте и гибком управлении (Agile, методология SCRUM) Бизнес-тренер по командному взаимодействию, конфликтному менеджменту, эмоциональному интеллекту Узнать больше об интенсиве и записаться: https://slurm.club/39wv0oV    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Рутинная работа уходит в прошлое: сегодня всем — тестировщикам, девопсам, сисадминам — нужны навыки автоматизации рабочих процессов. И знание Python может отлично в этом помочь.Согласно нашей обратной связи, многие боятся учить языки программирования, потому что страшно, сложно, непонятно и еще миллион других причин. Мы считаем, ничего подобного. Спикер и автор курса Python для инженеров, техлид и дата-инженер Skyeng Денис Наумов утверждает, что:Синтаксис языка задизайнен таким образом, что его можно читать практически как книгу. Пайтон простой и понимается естественным образом.Богатейшая экосистема: язык как будто создан, чтобы удобно писать приложения и предостерегать от ошибок, оставаясь при этом гибким и читаемым.Наши студенты поняли это на практике, разработав собственные итоговые проекты, благодаря которым смогли неслабо упростить рабочие процессы.Максим Дубакин рассказал о рабочем проекте на Python, который заавтоматизировал повторяющиеся задачи по переводу с деплоя bash-скриптами на helmfile и уменьшил времязатраты на ~ 2 часа. Максим давно увлекается программированием, но сомневался в том, что может написать нечто реально полезное для команды. Поэтому пришел к Пайтону, и он однозначно придал ему уверенности в своих силах.Денис Алексеев поделился проектом, который помогает упростить экспорт статистики из соцсетей через Prometeus и Grafana и поставить этот процесс на рельсы автоматизации. Денис более 10 лет занимается программированием, но постоянно изучает новое. В какой-то момент возникла необходимость читать скрипты на Python для понимания E2E тестов. Изучение языка круто упростило взаимодействие с командой тестирования.Наш курс не только для инженеров (хоть из названия это и кажется так😁). Он для всех, кто по работе сталкивается с рутинными задачами и хочет научиться автоматизировать Docker, GitLab, Prometheus, K8S и др.Ближайший поток курса стартует 16 января, присоединиться к нему можно до 23 января. Если за первые две недели поймете, что курс вам не подходит — вернем деньги. А еще если хочется сэкономить, можно попробовать решить тест из 5 вопросов по основам Python и получить за это скидку 20%.Посмотреть программу и занять место: https://slurm.club/3GwviZH    \n",
      " Думаете, написать свою первую программу на Python сложно? Всё гораздо проще и быстрее, чем кажется. Приходите на наш открытый урок и убедитесь в этом лично.Всего за час вы научитесь создавать ботов на Python и овладеете востребованным навыком. Будет полезно тем, кто уже в IT, но хочет усилить экспертизу и получить новые возможности для развития в профессии. В теорию и практику вы погрузитесь вместе с Леонидом Крутовским — Senior software engineer. За 1 час Леонид на языке Python напишет бота и покажет, как добавлять к нему разный функционал. А ещё расскажет, где держать бота, чтобы он работал постоянно.Начинаем 24 марта в 17:00 по мск.Узнать больше и зарегистрироваться    \n",
      " 27 апреля в 18:00 собираем питонистов на YouTube-канале «Технократии». Будем обсуждать Redis, FastAPI, SQLAlchemy и asynсio. Среди гостей Павель Мальцев из сервиса Speechki, а также Никита Архипов из Технократии. Также организуем круглый стол, за которым поговорим на тему «FastAPI vs Django. Останется только один?». Программа митапа:18:00 — Павел Мальцев. CTO, SpeechkiДоклад: «Redis, сова и глобус»Все мы привыкли, что Redis простой инструмент, который прекрасно  подходит для кеширования в формате Key-Value. Однако в нём есть более  весёлые структуры данных, которые могут сделать нашу жизнь веселее и  приятнее.Залезем в кроличью нору и найдём в документации списки, хеши,  множества и другие структуры. Покажу несколько кейсов из реального  продакшена, где эти структуры полезны и как использовать их особенности  себе на пользу. Строим свой велосипед на двух колёсах.18:45 — Никита Архипов. Python-разработчик, Технократия.Доклад: «Проблемы и практики FastAPI, SQLAlchemy и asynсio». Поговорим о том, какие проблемы могут возникнуть со стеком FastAPI, SQLAlchemy и asynсio и обсудим их решения.19:30 — Круглый столТема: «FastAPI vs Django. Останется только один?».Поговорим на горячую тему. Сможет ли FastAPI завоевать рынок и  получится ли откусить кусок от пользователей Django? Обсудим за круглым  столом. А оценивать доклады и ломать копья за круглым столом будут следующие эксперты:- Альбина Альмухаметова, Python lead, Технократия- Дарья Плотникова, Python-разработчица, СберМаркетРегистрация здесь: https://technomeets.timepad.ru/event/2007172/Стрим будет тут: Также подписывайтесь на наш телеграм-канал «Голос Технократии». Каждое утро мы публикуем новостной дайджест из мира ИТ, а по вечерам делимся интересными и полезными мастридами.    \n",
      " 29 августа мы запускаем 4 поток курса «Python для инженеров». Это большая ответственность, потому что на этот раз к нам придут специалисты, которые хотели обучиться Пайтону в 22-м, но все время откладывали.Четвертый поток станет последним в этом году. Но не только он: вас ждет еще кое-что последнее — подробнее расскажем в конце статьи.«Python для инженеров» — это 3-месячный видеокурс для инженеров инфраструктуры, девопсов и сисадминов, которые хотят автоматизировать рутинные задачи и повысить эффективность внутренних процессов.Будут реальные кейсы, live-coding, практические домашние задания, подробное ревью кода и душевные AMA-сессии со спикерами. На выходе вы не просто научитесь мастерски автоматизировать работу с Докером, Ансибл, Кубером и Прометеусом — вы станете разговаривать с разработчиками на одном языке.Вот 5 причин, почему присоединиться к курсу точно стоитМногие используемые продукты (например, Ансибл) написаны на Пайтоне. Их надо уметь читать и иногда под них писать. Хорошо, когда специалист эксплуатирует продукт и понимает, что он делает и зачем.Писать на Баше сложнее, чем на Пайтоне: он используется для последовательного запуска, у него усложненный синтаксис, и нет инструментов для работы с массивом данных. С Пайтоном все в разы проще.Системы с каждым годом становятся сложнее: инструментов, которых раньше было достаточно, теперь начинает не хватать. Пайтон легко интегрировать почти с любым инструментарием, а с Башем в этом тяжело и больно.Пайтон не дает много свободы — с этой точки зрения для инженера он проще. А еще с Пайтоном можно в режиме реального времени увидеть, как работают процессы, и внести изменения при необходимости.Девопсам нужно уметь писать стабильный, читаемый и поддерживаемый код. Ведь с помощью программирования можно реализовать не только бизнес логику, но и инфраструктурную логику.Смотреть примеры того, что можно автоматизировать с Пайтон: https://slurm.club/3d32ahnЧто еще станет последним на 4 потоке курса «Python для инженеров»?Мы в последний раз запускаем тариф Standart, в который входит доступ к АМА-сессиям. В следующих потоках на тарифе Standart останется только доступ к видеоурокам на 2 года.Наш любимый тариф — Premium. На нем в дополнение к видеоурокам и АМА-сессиям вы общаетесь с кураторами и ревьюерами в чате потока, а домашние задания гарантированно проверяются при сдаче в срок: 3 недели от открытия темы.​​Ближе к концу курса каждый студент начнет писать собственный проект. Тему можно будет выбрать: или одну из предложенных нами, или реальный кейс из вашей работы. Вы сможете опубликовать свой проект на гитхабе и использовать в портфолио. Ревью и проверку мы вам обеспечим.Посмотреть программу и входные требования, а также зарегистрироваться на курс «Python для инженеров» можно здесь: https://slurm.club/3d32ahnУвидимся в четвертом потоке!    \n",
      " Как джуну выделиться на фоне таких же новичков и получить оффер в технологически развитую компанию?Ответ прост: освоить асинхронное программирование (АП). Оно сложнее последовательного, поэтому его часто обходят стороной на базовых курсах по программированию. Однако без асинхронного программирования не получится решать сложные задачи, писать многопоточный и нагруженный код и работать с микросервисной архитектурой.У нашего партнёра KTS есть курс «Асинхронное программирование на Python для джуниор-разработчиков».На курсе:Вы научитесь мыслить нелинейно и сможете продумывать более сложные архитектуры приложений.Получите опыт работы с микросервисами.Узнаете лучшие практики написания асинхронных приложений на Python.Освоите стандартную python-библиотеку Asyncio и асинхронный веб-фреймворк Aiohttp, научитесь асинхронно работать с базами данных Postgres, Mongo, RabbitMQ.А ещё напишете сервер на aiohttp, асинхронного чат-бота и event loop. Эти проекты можно разместить в GitLab и приложить к резюме.Кому подойдёт курс?Курс для знающих Python: джунов, джунов+ и мидлов, которые пока не познакомились с модулем Asyncio. Ребятам, которые только присматриваются к разработке и не знают основ, на курсе будет очень тяжело. А суперсильным сеньорам-техлидам-архитекторам, скорее всего, будет скучно. (Зато сеньоры могут порекомендовать этот курс своим джунам.)Старт 13 октября.Посмотреть программу: https://slurm.club/3E0kK5j    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Полгода назад мы анонсировали первый поток курса Python для инженеров. На него записалось более 100 человек. Признаём, что были недочеты, но мы гибко вносили улучшения прямо по ходу курса. Вот несколько примеров:Спикер обещал всем ревью, но не успевал качественно ответить всем желающим. Мы сразу же пригласили дополнительных экспертов.На втором потоке у нас уже на старте, помимо спикера, будут эксперты Python,  чтобы никого не оставить без ревью.У части инженеров не было вообще опыта программирования, им требовалось больше времени на теорию и домашки.Мы добавили дополнительные теоретические материалы в курс и продлили сроки на домашние работы.Не очень понравился формат стримов, когда спикер в реальном времени писал на Python небольшой модуль: было долго и не динамичноПереупаковали материал: добавили таймкоды, паузы, дополнительные ссылки, стало удобнее смотреть. Сегодня анонсируем второй поток. Приглашаем всех инженеров и администраторов, которые хотят научиться писать модули автоматизации для Ansible, Docker, Kubernetes, Gitlab, Prometheus и др.Стартуем 11 октября. Курс длится 2,5 месяца. Раз в неделю открываем теорию и даем домашнее задание, потом проводим ревью кода, даем обратную связь. Два раза за курс будут АМА-сессии со спикером, чтобы обсудить процесс обучения и ответить на вопросы.Ближе к концу курса каждый студент начнет писать собственный проект. Темы на выбор: или одну из предложенных нами, или реальный кейс из вашей работы. Проект можно будет опубликовать на гитхабе и использовать в портфолио. Ревью и проверку мы вам обеспечим.Прочитать подробнее о курсе  и оставить заявку можно по ссылке https://slurm.club/3zLhNjn    \n",
      " 14 декабря в Нетологии пройдёт вебинар, посвященный анализу данных и тому, как использовать Python для работы с данными. Это вводное занятие для специалистов, которые хотят расширить знания: аналитиков, продакт-менеджеров или разработчиков, которые изучали другие языки программирования.Python помогает решать сложные аналитические задачи, автоматизировать рутинную работу, обрабатывать большие объёмы информации без администрирования и баз данных. На вебинаре разберём, зачем аналитику и продакту этот язык программирования и сколько времени может уйти на его изучение. Более подробный курс «Python для анализа данных» начнётся чуть позже, 20 декабря.Записаться на вебинар можно здесь.    \n",
      " В блоге Microsoft сообщили о нововведениях плагина для работы с Python в Visual Studio Code. Обновление включает в себя функцию сворачивания блоков кода, смарт селекторы, улучшенный список интерпретаторов и исправления для пользователей дистрибутива Anaconda. Подробный журнал изменений доступен в репозитории проекта на GitHub.смарт селекторы — теперь система лучше понимает структуру проекта и помогает выбирать фрагменты кода с помощью меньшего количества нажатий клавиш. Работа функции основана на языковом сервере Pylance, который отвечает за определение диапазонов выбора;сворачивание блоков кода — раньше блоки кода определялись только по отступам, что было не всегда удобно и пользователи жаловались на некорректное срабатывание функции. Теперь Pylance также обеспечивает правильную работу сворачивания;список интерпретаторов Python теперь сгруппирован по категориям, что облегчает поиск;исправили некоторые ошибки связанные с дистрибутивом Anaconda. Теперь все инструменты и файлы среды conda запускаются с помощью conda run. Также разработчики Visial Studio Code сообщили, что если путь файла содержит специальные символы, то он может некорректно открываться в редакторе. В таких случаях рекомендуют открывать VS Code с помощью команды code в терминале активной среды conda. Команда Anaconda уже работает над исправлением этой ошибки и скоро выпустит обновление.Помимо больших функций добавили и минорные обновления:улучшили интерфейс быстрого выбора папки в больших проектах;отказались от поддержки Python 3.6, так как в декабре версия прекратила свое существование.    \n",
      " Сделали для вас подборку свежих статей и выгодных акций. А ещё подготовили тест на уровень кунг-фу по Python и важную информацию про даты. Всё это сегодня в дайджесте.Серия статей про Apache Kafka1. Apache Kafka и RabbitMQ: в чём разница и что лучше изучать?Сравниваем два самых популярных брокера сообщений и рассказываем, для каких задач лучше изучать Apache Kafka, а для каких — RabbitMQ.2. Обработка ошибок в приложениях Apache KafkaПриложения Apache Kafka распределены по нескольким контейнерам и машинам, а в распределённых системах всегда что-то может пойти не так. В статье говорим о том, как обрабатывать ошибки и настраивать ретраи (повторные попытки) в приложениях потоковой обработки событий.3. Полезные инструменты для разработчиков Apache KafkaKafka лежит в основе большой экосистемы, куда входят инструменты для разработчиков, с ними работа с Kafka станет проще и приятнее.Статья про развитие личного брендаСтатья Марселя Ибраева про развитие личного бренда через преподавание на курсах и выступления на конференциях. Как и зачем становиться спикером и какие от этого можно получить плюшки. Акции1. Изменились даты интенсива по Python  «Работа с API и фреймворками». Он состоится 24–26 июня. До 13 июня интенсив можно приобрести за 40 000 ₽, после 13 июня — за 50 000 ₽.  2. До 31 мая на видеокурс или поток «Golang для инженеров» скидка 10% по промокоду GO-3_10. Старт третьего потока — 4 июля. На курсе вы научитесь создавать свой API сервер с помощью Golang, запускать контейнеры, взаимодействовать с Docker из Go, работать с кастомными операторами и многим другим приятным и полезным штукам.Мини-тест по PythonМы подготовили мини-тест, чтобы проверить уровень вашего кунг-фу по Python.Вакансии и предложенияВакансии понятные — на русском языке, без кота в мешке в телеграм-канале «Слёрм Карьера». С указанием зп и компании. Никогда не знаешь, где найдешь работу своей мечты.Хотите стать спикером Слёрма? Если у вас есть желание преподавать или консультировать, заполните, пожалуйста, короткую форму. Мы ищем по конкретному стеку, но готовы создать курс, если у вас есть предложения.На сегодня всё. Отличной недели!    \n",
      " Через два дня стартует курс по асинхронному программированию на Python от нашего партнёра KTS. Его можно пройти за полтора месяца, если заниматься 8–16 часов в неделю.Этот курс — маст хев для тех, кто хочет прокачать харды и стать специалистом, который не боится сложных задач. Асинхронное программирование используется для высоко­нагруженных проектов и микросервисов, его спрашивают на собеседованиях в технологически развитых компаниях.Если вы уже пишете на Python, но пока не изучили модуль Asyncio, приглашаем на курс:7 модулей с видеоуроками — чтобы шаг за шагом освоить тему;домашние задания с автопроверкой — чтобы закрепить знания на практике;онлайн-разборы ДЗ или менторская поддержка — чтобы ещё лучше разобраться в асинхронном подходе.Чему вы научитесьВы поймёте, как работает асинхронное программи­рование и где его лучше применять.Узнаете best practices написания асинхронных приложений на Python.Научитесь асинхронно работать с базами данных Postgres, Mongo, RabbitMQ.Напишите асинхронного телеграм-бота и свой Event Loop.Погружение в асинхронное программирование начнётся уже 13 октября.Посмотреть красивый лендинг и решить, надо ли оно: https://slurm.club/3fNdH66    \n",
      " VK Образование запускает набор на бесплатные образовательные программы для студентов старших курсов и выпускников российских вузов. Вы сможете освоить ручное тестирование и программирование на Python, усилить на практике необходимые для карьеры в IT навыки. Лучшие студенты смогут присоединиться к команде VK в качестве стажёров или сотрудников.  Важное место в обучении отведено практической части — учащиеся будут работать над реальными задачами проектов VK. Преподают на курсах эксперты компании.Обучение бесплатное, но для зачисления на курс нужно пройти тестирование, оно начнётся 21 февраля. Обучение с 7 марта, продлится три месяца. Все занятия будут проходить в онлайн-формате — можно совмещать с учёбой или работой.На курсе по ручному тестированию вы научитесь тестировать мобильные, веб- и десктопные приложения. Познакомитесь со снифферами и платформой Postman, узнаете, как составлять тест-кейсы и чек-листы. Для поступления понадобятся понимание основ клиент-серверного взаимодействия и базовые навыки использования ОС. Также вы получите доступ к сообществу VK Testers. Заявку на обучение можно подать на сайте до 20 февраля.На курсе по углублённому Python вы узнаете о внутреннем устройстве одного из самых популярных языков программирования, научитесь эффективно использовать его стандартную библиотеку для решения  практических задач. Чтобы успешно пройти отбор, необходимы базовые знания Python. Заявку на обучение можно подать на сайте до 20 февраля.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23 сентября в 16:30 в рамках цикла открытых технических семинаров компании Xperience AI с докладом выступит Григорий Серебряков (CTO, Xperience AI). Тема доклада - The Python packaging.\"The Python packaging has a reputation of a somewhat black box from the early days. There are many unknown parts, and people mostly get by with just copying other projects, build configurations, and roll with them.” (c) Bernat Gabor - the maintainer of both the virtualenv and tox project, occasionally contributing to both setuptools and pip.Попытаемся исправить эту ситуацию и разобраться, как устроено построение пакетов в питоне (а точнее, как оно было устроено раньше и как предлагается делать сейчас). А также, рассмотрим более сложный случай - построение пакетов с нативными модулями (С++/CUDA).Технические вебинары компании Xperience AI проходят регулярно в свободном доступе. Чтобы получить приглашение на доклад Григория Серебрякова, а также получать приглашения на будущие выступления, оставьте свой адрес здесь. Часть лекций появляется на youtube-канале Xperience AI.    \n",
      " \n",
      "Расположение векторного регистрового файла (жёлтый) и целочисленного регистрового файла (красный) на процессорах Skylake. Блок векторных регистров AVX-512 зашит в жёлтой области\n",
      "\n",
      "Компания Intel представила патч к библиотеке Numpy с оптимизациями, которые используют фирменный набор инструкций AVX-512.\n",
      "\n",
      "Numpy — библиотека с открытым исходным кодом для языка программирования Python c поддержкой многомерных массивов (включая матрицы) и высокоуровневых математических функций. Numpy активно используется сообществом Python для реализации вычислительных алгоритмов — и эту библиотеку можно рассматривать в каком-то смысле как свободную альтернативу Matlab.\n",
      "\n",
      "В комментариях к патчу в файле doc/release/upcoming_changes/19478.performance.rst написано, что векторизация модуля umath с использованием AVX-512 означает применение библиотеки Intel Short Vector Math Library (SVML) для векторизации 18 функций umath (exp2, log2, log10, expm1, log1p, cbrt, sin, cos, tan, arcsin, arccos, arctan, sinh, cosh, tanh, arcsinh, arccosh, arctanh) как одинарной, так и двойной точности. \n",
      "\n",
      "«В настоящее время это изменение включено только для пользователей Linux и на процессорах с набором инструкций AVX-512. Оно обеспечивает среднее ускорение в 32x и 14x для функций одинарной и двойной точности соответственно», — сказано в комментарии.\n",
      "\n",
      "\n",
      "\n",
      "На самом деле первоначальный код AVX-512 для Numpy компания Intel выложила ещё летом. На прошлой неделе его, наконец, приняли. Как написано в комментарии выше, он основан на библиотеке Intel Short Vector Math Library (SVML). Судя по всему, основная часть кода взята оттуда. Intel также разрабатывает версию Numpy на базе SVML в качестве отдельного форка.\n",
      "\n",
      "Первоначальная реализация AVX-512 предлагала оптимизированные версии 44 математических функций — практически всех основных математических функций Numpy, причём в режиме одинарной, так и двойной точности. Однако в финальной версии к слиянию приняли код только для 18 функций, перечисленных выше.\n",
      "\n",
      "Инженеры Intel проверили и обнаружили, что даже на старых процессорах Intel Skylake X такая оптимизация Numpy даёт ускорение до 55х в отдельных функциях. Среднее ускорение составило 14x для двойной точности и 32x для одинарной.\n",
      "\n",
      "Cтоль грандиозное ускорение достигается не только благодаря инструкциям AVX-512, но во многом благодаря оптимизированному коду SVML. К сожалению, Intel не открывает полностью исходный код этой библиотеки, иначе его бы уже давно добавили в Numpy.\n",
      "\n",
      "Справка. Набор инструкций AVX-512 расширяет систему команд AVX до векторов длиной 512 бит при помощи кодировки с префиксом EVEX. Расширение AVX-512 вводит 32 векторных регистра (ZMM), каждый по 512 бит, 8 регистров масок, 512-разрядные упакованные форматы для целых и дробных чисел и операции над ними, тонкое управление режимами округления (позволяет переопределить глобальные настройки), операции broadcast (рассылка информации из одного элемента регистра в другие), подавление ошибок в операциях с дробными числами, операции gather/scatter (сборка и рассылка элементов векторного регистра в/из нескольких адресов памяти), быстрые математические операции, компактное кодирование больших смещений. \n",
      "\n",
      "Блок AVX-512 реализован в следующих процессорах: Intel Xeon Phi x200 и x205 (в сопроцессорах Knights Landing и Knights Mill), а также в семействах Skylake-SP, Skylake-X, Cannon Lake, Cascade Lake, Cooper Lake, Ice Lake, Rocket Lake, Tiger Lake, Sapphire Rapids.\n",
      "Любопытно, что Intel убрала блок инструкций AVX-512 из новых процессоров 12-го поколения Alder Lake. Возможно потому, что он занимает очень много места на микросхеме (см. фотографию вверху). А вот компания AMD, наоборот, по слухам, собирается добавить блок AVX-512 в будущие процессоры Zen 4. Вероятно, более продвинутый техпроцесс с уменьшенным размером узлов позволяет это сделать.\n",
      "\n",
      "Так что нынешний коммит Intel в библиотеку Numpy — это своеобразный подарок AMD на будущее.    \n",
      " Привет, Хабр! Если вы хотите проверить свои силы в Python-разработке, плюс вам близка тема контента – есть интересная возможность. Дело в том, что аудиосервис СберЗвук проводит свой хакатон с общим призовым фондом 500 000 рублей.  С 30 по 31 октября пройдут SberZvuk Tech Days в гибридном формате: 100 человек получат возможность посоревноваться в Москве в офлайне и 50 участников со всей России смогут принять участие онлайн. Больше подробностей и ссылка для регистрации – под катом.Цель хакатонаКонтент, который создают пользователи в интернете (UGC), может нарушать авторские права, правила компаний и законодательство. Подобные нарушения отрицательно сказываются на репутации бренда в онлайне. Основная задача хакатона — разработать UGC-фильтр с возможностью автоматической фильтрации пользовательского видеоконтента на предмет соответствия требованиям и правилам. Благодаря общению с менторами от компании, ваши проекты получат качественную экспертную оценку и шанс стать реальным механизмом фильтрации контента в СберЗвуке.Условия участия К участию приглашаются python-разработчики middle и senior уровня. Помимо основного призового фонда в 500 000 рублей участники получат:крутой фирменный мерч для всех, кто пройдёт в финал;возможность валидировать своё решение вместе с другими участниками;кофе-брейки, обеды на офлайн-площадке и промокоды на доставку еды для всех онлайн-участников, которые пройдут отбор на хакатон;нетворкинг, благодаря которому участники смогут ближе познакомиться с аудиосервисом СберЗвука, задать вопросы экспертам компании, оценить применимость и жизнеспособность своего решения.«Мы проводим SberZvuk Tech Days, чтобы показать задачи, с которыми работает команда СберЗвука. Если вы хотите стать частью нашей команды — регистрируйтесь на хакатон, будет интересно!».А где регистрироваться?Зарегистрироваться и собрать команду нужно до 24 октября. Регистрация доступна вот по этой ссылке.Сделаем виртуальный мир безопаснее!    \n",
      " В официальном стороннем репозитории Python были обнаружены вредоносные пакеты. Данные зловреды крадут ключи доступа к облакам Amazon Web Services и отправляют их на общедоступный ресурс. Вредоностные пакеты обнаружили исследователи кибербезопасности из компании Sonatype в количестве пяти штук: loglib-modules, pyg-modules, pygrata, pygrata-utils и hkg-sol-utils. Пакеты были проанализированы, и в них нашли код, который считывает и отправляет конфиденциальные данные пользователей. Эксперты считают, что первые два пакета нацелены на пользователей официальных библиотек loglib и pyg, а вот pygrata-utils нацелен на библиотеку loglib-modules. На что нацелены остальные два пакета — неизвестно.Исследователи также обнаружили, что все украденные данные размещаются без шифрования в файле с расширением *.txt на ресурсе в открытом доступе. Специалисты компании Sonatype не знают, сделали это злоумышленники по ошибке или с демонстрационными целями. Информация о вредоносах была передана в PyPI, и они вскоре исчезли из репозитория, как и ресурс, на который отправлялись украденные данные.    \n",
      " Привет!3 октября начинается 3-й поток нашего флагманского курса — «Асинхронное программирование на Python».Асинхронное программирование используется для высоко­нагруженных проектов и микросервисов. Его спрашивают на собеседованиях в технологически развитых компаниях, и оно открывает дорогу к работе в интересных проектах.В Python асинхронный подход реализуется через Asyncio, который мы рассматриваем в этом курсе. Каждый модуль — это полноценный проект, куда вы дописываете нужную функциональность. Дальше тесты автоматически проверяют взаимодействие всех компонентов проекта. Если ваше решение работает, вы успешно прошли модуль. 🐍 На курсепознакомитесь с теорией асинхронностиузнаете, чем она отличается от параллельного выполнения потоков и процессовнаучитесь использовать асинхронный веб-фреймворк в Pythonначнете работать с БД через асинхронные коннекторынапишите чат-ботаразберетесь, как написать собственный event loop🐍 Кому подойдетВыпускникам курсов по программированиюДжуниор-разработчикам на PythonМидлам, которые пока не изучили Asyncio🐍 ДЗВ ходе курса вы выполните 6 домашних заданий. ДЗ решаются на учебном виртуальном сервере с настроенным окружением — ничего не нужно искать, скачивать и устанавливать на свой компьютер.Каждое домашнее задание подразумевает дописывание полноценного проекта. Вы не пишете обособленный кусок кода, а вписываете свой код в готовый проект. Дальше тесты автоматически проверяют взаимодействие всех компонентов проекта. Если ваше решение верное — оно пройдёт тестирование.Такой подход к домашним заданиям тренирует насмотренность: вы видите, как устроен проект, как выглядит код, куда вы дописываете своё решение.Ждем вас на курсе и желаем успехов в обучении!Посмотреть программу и записатьсяПодключайтесь к ТГ-боту Слёрмику, и он поделится полезными материалами по теме. А ещё будет сообщать о наших бесплатных вебинарах. Там же вы в любой момент сможете купить курс.ПодключитьсяКстати о вебинарах: уже в этот четверг мы проведём первый из 2 вебинаров для погружения в тему, «Пишем Websocket-сервер для геолокации на asyncio»Зарегистрироваться    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Митап завершился, а значит пора поделиться его докладами. В этот раз спикеры из Раффайзенбанка, VK и Nordcurrent рассказали о будущем Python, тестировании неизвестного науке API и Deep Learning на примере игровых данных. Добавляйте видео в закладки, и пишите комментарии. Куда двигается Python в 2022 году?Денис Аникин, Team lead/Community lead Райффайзенбанк.Большинство разработчиков заняты работой и им некогда читать новости о языке, копаться в стенограммах и смотреть часы докладов. Я сделаю это за них.Знание трендов позволит больше узнать о вещах, которые не столь часто обсуждаются в среде разработчиков. Кому полезно: каждому, кто активен в достаточной мере, чтобы слушать митапы и конференции.Тестирование АПИ при помощи DjangoАлексей Шарыпов, Lead developer VK.История написания сервиса для тестирования черного ящика без тестов и документации, написанном на малоизвестном языке. Сейчас пишут много микросервисов с оркестраторами, и многие разработчики сталкиваются с отсутствием документации и тестов. Кому полезно: тем, кому нужно тестировать микросервисы, проксирующие запросы с изменениями или без.Распознаем интересные моменты в матчах Dota 2Дмитрий Савостьянов, Senior Machine Learning Engineer Nordcurrent.Доклад можно воспринимать как обзор классических методов Deep Learning в приложении к понятной задаче, базирующейся на игровых данных. Познакомимся со структурой данных Dota 2, методом кластеризации DBSCAN, библиотекой yt_dlp для работы с YouTube, моделью распознавания текстов на картинках TrOCR, языковой моделью BERT.Кому полезно: всем, кому интересен ML, DL, Dota 2, Data Science.Теперь следить за митапами Evrone стало удобнее. В Telegram-канале Evrone meetups мы выкладываем анонсы с подробными описаниями докладов, а также студийные записи после мероприятий. А ещё, у нас можно выступить, мы поможем оформить вашу экспертизу в яркое выступление. Подписывайтесь и пишите @andrew_aquariuss, чтобы узнать подробности.    \n",
      " 9 декабря в 16:30 Виктор Бебнев (Xperience AI) прочитает открытый семинар на тему «FastAPI, или как быстро добавить REST API для вашего проекта на Python». Во время семинара:Поговорим об основных фичах.Сравним с конкурентами.Посмотрим как тестировать.Запустим игрушечный пример. Зарегистрироваться на семинар можно по ссылке. Участие бесплатное.Вы можете посмотреть часть видео с прошедших семинаров в открытом доступе на нашем YouTube-канале.     \n",
      " В конце марта Мстислав Казаков, руководитель практики Python ГК Юзтех, провёл внешний Usetech Meetup на тему «Как писать Gitlab CI файлы которые легко понимать, расширять и поддерживать».  С ростом проекта и увеличением количества автоматизированных операций содержимое Gitlab CI файла превращается в спагетти-код. На примере демо проекта Мстислав поделился подходами, которые помогли ему решить эти проблемы. Спойлер: проблема решалась при помощи include, reference, rules и манипуляций с Docker.Демо проект (или эталонный проект) – это монорепозиторий (представлен на изображении ниже). Он содержит 10 сервисов, монолит с беком и фронтом и несколько внутренних библиотек. Если говорить о тестовых средах, обновлениях, релизных циклах, и т.д., то их 3. В первой среде находится актуальная develop версия проекта, во второй среде тестируется release candidate, а третья среда предназначена для end-to-end тестов.На этом примере Мстислав показал:— Include в Gitlab CI: разбил большое на малое.— Reference в Gitlab CI: в чём разница между алиасами? Можно ли использовать reference в правилах?— Rules в Gitlab CI: замена устаревшим only/except. Рассмотрел, как простые правила, так и «сборные».— Билдим и деплоим всё единообразно: вынес нюансы билда и деплоя сервисов из CICD процессов.— Env vars из настроек Gitlab CI: как облегчить управления большим количеством переменных.— Всем джобам единый образ! Как экономить время и строчки кода.— Gitlab раннеры: как это сделано и почему это полезно знать? Смотрите видео и оставляйте обратную связь в виде вопросов и комментариев.      \n",
      " Как людей учат разработке? Вовлекают, дают интересные задачи, поддерживают комфортный темп. А потом свежеобученный айтишник убивается о рабочую реальность.Мы подружились с KTS, у которых есть бесплатный курс «войти в айти», и он похож на работу: минимум 8 часов в неделю, первый этап построен на автопроверке заданий, и только те, кто успел сдать все задания вовремя, начинают общаться с ментором.Хорошая новость: кто пройдёт курс до конца, получит приглашение на стажировку.Плохая новость: до конца дойдёт меньше 10% стартовавших.Хорошая новость: кто не дошёл до конца, поймёт, что жизнь разработчика — это не только зарплата 500к/наносек., но и сложная напряжённая работа.Если у вас есть друг, который хочет «войти в айти», и вы его ещё не отговорили, посмотрите на курс «Начинающий Backend-разработчик на Python». Он бесплатный, он с шансом стажировки, и он даёт попробовать работу разработчика на вкус.Если друг дойдёт до конца, значит, айти — действительно его призвание.Регистрация тут: https://slurm.club/3OxQS1N    \n",
      " Привет, Хабр!Меня зовут Тимофей, я Python Engineer с опытом в 3+ года.ТимофейPython Engineer в компании Fenion GmbHНедавно я опубликовал статью на Хабре о том, как я докатился до такой жизни и стал разработчиком на Python. Там я упоминал некоторые материалы, которые помогали мне в изучении. За последнее время нашлись люди, которые спрашивали у меня, как лучше вкатиться в веб‑разработку, изучив Python: с чего начать, в каком порядке и какие материалы использовать и т. д.Я бы не назвал данную статью полноценным роадмэпом в том понимании, в котором это слово обычно используют. В статье нет сроков изучения, отсутствует подробный план, затрагивающей все темы и подтемы. Да и зачем нужен еще один роадмэп «под копирку», когда в сети и так хватает качественных предложений.Скорее, я хотел бы составить абстрактный план обучения, который сможет задать общий ориентир, а также будет коррелировать с роадмэпом вроде roadmap.sh. Кроме этого, я смогу дать несколько полезных советов из своего опыта, а также составлю списки полезных ресурсов для изучения языка, сопутствующих инструментов и технологий, которые в свое время помогли или же помогают мне самому.Также хотел отметить, что все написанное в данной статье является моим субъективным взглядом, основанным на личном опыте. Я не являюсь истиной в последней инстанции, поэтому, в конечном счете, решение следовать данному плану или нет, конечно же, остается за вами. В любом случае, я надеюсь, что данная статься окажется полезной и вы сможете подчеркнуть что‑нибудь, что поможет вам в изучении языка Python и веб‑разработки в целом.Что такое язык Python и где он используетсяPython — это высокоуровневый интерпретируемый язык программирования, который был впервые выпущен в 1991 году(Не все знают, но язык даже старше Java!). Он известен своей простотой, удобочитаемостью и удобством использования, что делает его популярным выбором для широкого спектра приложений, от веб‑разработки до научных вычислений.Python может быть использован для многих целей, включая:Веб‑разработка: Python широко используется для создания серверной части веб‑приложений с использованием таких фреймворков, как Django, Flask или FastAPI.Анализ данных и машинное обучение: Python стал фактическим языком для анализа данных, машинного обучения и искусственного интеллекта благодаря своему богатому набору библиотек, таких как NumPy, Pandas и Scikit‑learn.Написание скриптов: Простота и удобство использования Python делают его популярным выбором для написания сценариев для автоматизации задач, таких как системное администрирование и сетевое программирование.Разработка десктопных приложений: Python также можно использовать для создания настольных приложений с использованием библиотек GUI, таких как PyQt.Разработка игр. Конечно, ААА‑проектов на питоне не делают, что что‑то простое — вполне. На YouTube есть интересный канал, на котором автор периодически выкладывает свои Pygame‑проекты.Образование: Простой в освоении синтаксис Python и широкий спектр приложений делают его популярным выбором для вводных курсов в программирование в университетах и школах.В целом, Python — это универсальный язык, который может использоваться для широкого спектра целей, от простых скриптов до сложных приложений.Важное предисловие и некоторые советыУ меня для вас хорошая новость: любой человек может достичь хорошего уровня владения Python или же любым другим языком, благодаря правильному подходу, а именно синергией заинтересованности, мотивации и дисциплины.Могу предположить, что некоторые люди пытаются попасть в ИТ в первую очередь из‑за денег. Не будем лукавить: денежная мотивация занимает далеко не последнее место и это вполне естественно: труд должен соответствующе оплачиваться. На одной лишь мотивации, а особенно на денежной, в принципе долго выезжать не получится. В данной сфере нужно очень много учиться. Причем делать это нужно постоянно. А какой нормальный человек сможет тратить огромное количество своего драгоценного времени на изучение того, что ему не интересно? Естественно, бывают разные программисты, я даже лично знаю некоторых, кто с трудом ориентируется в базовом синтаксисе языка, хотя уже несколько месяцев работают в ИТ‑компании. Мы же хотим стать хорошими специалистами, так?Будучи новичком, я не сразу обрел истинный интерес к языку. На своих первых курсах мы решали математические задачки при помощи этого языка, и скажу вам так: это меня совсем не заинтересовало. Прошел достаточно продолжительный период времени(около года), прежде чем я вернулся к языку. Я вспоминал основы, после чего начал изучать фреймворк Django и сопутствующие технологии прямо во время процесса разработки.Перейдя на любую вакансию новичок вполне может ужаснуться от списка технических требований к кандидатам: уверенное знание языка, опыт работы с N количеством фреймворков и библиотек, SQL, REST API, GIT, Docker. И данный список может расширяться в зависимости от конкретной вакансии. Как же все выучить, работать эффективно, не потерять интерес и не сойти с ума?Для ответа на этот вопрос, оставлю некоторые общие рекомендации:Выберите удобную IDE. Процесс разработки в первую очередь должен быть комфортным для вас самих. Фактически, на рынке есть лучшая IDE для разработки на Python — это Pycharm. К сожалению, из‑за политики JetBrains их продукты больше не доступны для покупки аккаунтам из Беларуси и России. Альтернативой является VS Code, достаточно шустрый редактор кода, который благодаря плагинам оставляет большинство плюшек Pycharm.Обязательно делайте перерывы: дайте вашему мозгу шанс обработать и усвоить полученную информацию. Вы можете удивиться, но эта штука практически всегда работает. Если вы видите, что у вас долго что‑то не получается — просто отдохните и отвлекитесь. Но данный отдых должен быть, в первую очередь, отдыхом для мозга. Лучшим отвлечением лично для меня является прогулка. Зачастую, после такого перерыва вы сможете по‑другому взглянуть на вашу задачу, у вас появятся новые мысли и вы увидите другие пути решения.Выработайте привычку заниматься каждый день. Теория + много практики. Теоретические знания — это безусловно хорошо, но без должного количества практики они ничего не значат. Часто бывает такое, что в теории вам все кажется очевидным и понятным, но как дело касается решения вы обнаруживаете, что сильно путаетесь и голова будто бы не работает. Это нормально! Пытайтесь, пока не получится.Не прыгайте между темами. Обучайтесь поэтапно и в комфортном для вас темпе. Никто не выиграет от того, что вы с утра до вечера будете сидеть за книгами и задачами, после чего сильно перегорите. Перегорание — это в целом распространенная проблема у программистов(и не только). Во время обучения и работы я тоже перегорал и знаю, что это такое. Но в итоге я нашел комфортный для себя темп и определил некоторые рабочие правила, вроде периодического отдыха.Не злоупотребляйте копипастом кода. Особенно это касается проектов. Во всех курсах авторы закрепляют теорию практикой. В роли такой практики выступает создание проекта. Старайтесь думать сами. Это не значит, что вы не должны слушать автора, напротив, изучайте курс внимательно. Я говорю о том, что вы не должны слепо переписывать код и ждать чуда. Пытайтесь решить задачу сами, экспериментируйте, думайте над задачей вместе с автором.Если вам нужно решить большую или же непонятную задачу, декомпозируйте ее на более мелкие подзадачи. Это работает. Так вы сможете намного быстрее найти нужное решение.Присоединяйтесь к сообществу: присоединение к сообществу изучающих Python и разработчиков может быть отличным способом сохранять мотивацию и учиться у других.Создавайте пет‑проекты. Создание пет‑проектов — очень важный аспект в обучении, а также способ применить полученные знания и получить практический опыт, который можно будет добавить в резюме. Начните с простых проектов, таких как калькулятор или текстовая игра, и постепенно переходите к более сложным.Итак, после небольшого вступления вы определились с мотивацией и ознакомились со списком общих рекомендаций. Можем переходить к самим шагам.Шаг 1. Изучение основ языка + GITНачните с основ Python, таких как типы данных, переменные, циклы, функции и модули. Для начало следует понять основы, уверенно ориентироваться в языке. Да, в какой‑то момент это может наскучивать, но без этих основ вы не сможете двигаться дальше.Также хотел вынести сюда GIT — систему контроля версий. Данная технология обязательна к изучению всем разработчикам, т.к. во‑первых, это удобно, а во‑вторых, она используется повсеместно. Для начала вам понадобятся несколько базовых команд, вроде:git init # Инициализация репозитория\n",
      "git add # добавление файлов в staging area\n",
      "git commit # снимок текущих изменений проекта\n",
      "git pull # стягиваем последние изменения с репозитория\n",
      "git push # заливаем изменения после commit'аШаг 2. Объектно-ориентированное программированиеPython — это объектно‑ориентированный язык программирования, здесь все является объектом. Следовательно, важно изучить объектно‑ориентированные концепции, такие как классы, объекты, наследование, полиморфизм и инкапсуляция. Концепции ООП практически всегда спрашивают на собеседованиях, поэтому вы должны хорошо в них ориентироваться. Помимо этого вы постоянно будете сталкиваться с классами в своих повседневных задачах.Шаг 3. АлгоритмыДля того, чтобы уметь решать поставленные задачи, нужно мыслить алгоритмически. А как это сделать? Правильно, практиковаться в решении различных задач. Книга «Грокаем алгоритмы» — это просто находка для знакомства с ними. В книге доступнейшим языком рассказывается о различных базовых алгоритмах и понятиях, вроде времени выполнения и О‑большого. Параллельно с изучением данной книги, особенно рекомендую решать разные задачки на codewars или leetcode.Материалы для изучения. Шаг 1 - Шаг 3Хотел бы сразу сказать, что я не говорю о том, что все материалы ниже обязательны к изучению. Я лишь оставляю список качественных и полезных на мой взгляд материалов, которые помогли мне и, возможно, помогут вам в процессе обучения.Знакомство с Python. Дэн Бейдер.Пытаетесь найти что‑нибудь для начинающих о языке Python в интернете? Не можете решить, с чего начать? Как структурировать это море информации? В каком порядке изучать? Если вы задаетесь подобными вопросами, потому что хотите заложить фундамент будущей карьеры питониста — эта книга для вас! Вместо скучного перечисления возможностей языка авторы рассказывают, как сочетать разные структурные элементы Python, чтобы сразу создавать скрипты и приложения. Книга построена по принципу 80/20: большую часть полезной информации можно усвоить, изучив несколько критически важных концепций. Освоив самые популярные команды и приемы, вы сразу сосредоточитесь на решении реальных повседневных задач.Вот что издатель пишет в описании книги, с чем я вполне соглашусь. От себя хотел добавить, что уже успел посмотреть книгу  и оставила она только положительные впечатления. Дэн Бейдер в принципе считается одним из лучших авторов по тематике Python. Отличительной особенностью его работ является понятный стиль изложения. И данная книга не является исключением, она отлично подойдет для новичков. «Изучаем Python». Марк ЛутцМногие питонисты считают данную книгу мастхэвом. И не зря, ведь данная работа уже считается классикой. Очень малое количество книг могут сравниться с Лутцом по глубине и подробности в описании своего предмета. Освоив материал книги, вы совершенно точно сможете решать с помощью Python самые разные задачи. Недостатком является форма изложения, которая зайдет далеко не всем, а также размер книги — около 1200 страниц.Укус питона. Swaroop C.HКнига является противоположностью учебнику Лутца. Она маленькая(около 164 страниц), совсем не является полноформатным учебником и содержит в себе самые основы в формате «записок» автора. Книгу вполне можно изучать вместе с каким‑нибудь курсом, например из пункта 4.Грокаем алгоритмы. Иллюстрированное пособие для программистов и любопытствующих. Адитья БхаргаваПринято считать, что программирование — это очень сложно. Особенно если раз за разом наступать на одни и те же грабли, пытаться сделать по‑своему то, что уже и так было придумано до нас. Ведь практически для любой задачи есть готовый алгоритм решения, осталось только найти его и правильно использовать.В книге «Грокаем алгоритмы» Адитья Бхаргава не просто показывает примеры таких решений с детальными иллюстрациями, но и учит читателя самостоятельно находить их в дальнейшем. Читатель знакомится с понятиями бинарного поиска, массивами, связанными списками, структурами данных, рекурсией.Книга рассчитана на тех, кто уже знаком с основными азами программирования и интересуется алгоритмическими решениями. Автор старается доносить информацию понятным даже новичку языком, иллюстрирует все основные моменты.Вся серия курсов «Поколение Python» на степике. Очень хорошие курсы с большим количеством задач, которые познакомят вас с основными типами данных, циклами, условиями и т. д.Руководство по языку Python. Метанит. Вполне себе хорошее базовое руководство по языку. Рассматриваются все базовые темы, включая ООП. Сам когда‑то пользовался, было полезно.Git за полчаса: руководство для начинающих. Отличная статья, которая научит вас основам работы с системой контроля версий.Гуглите. Вы обучаетесь только тогда, когда сталкиваетесь с трудностями. Поиск решения задачи сильно прокачивает ваши скиллы. Ищите ответы на форумах, в статьях, видео на YouTube и т. д. Только путем проб и ошибок можно по‑настоящему понять изучаемую тему.Шаг 3. Изучение основ SQL, работа с ORMРабота с базой данный — неотъемлемая часть работы любого backend‑разработчика. На практике, вы будете работать с БД постоянно. Нет, на данном этапе вам не обязательно знать как руками написать JOIN запрос, в котором будут находится данные из 10 разных таблиц. Для начала, изучите основы, вроде: что такое таблицы, поля, связи и запросы. Остальное за вас сделает ORM. Да, практически во всех современных фреймворках есть наличие ORM(Object‑Relational Mapping). Это специальный инструмент, который позволяет разработчикам взаимодействовать с базами данных более объектно‑ориентированным образом. Вместо написания необработанных SQL‑запросов для взаимодействия с базой данных разработчики могут определять классы, представляющие таблицы базы данных, и использовать эти классы для запроса данных к базе и манипулирования ими.При помощи ORM вы можете делать все то же самое, что и в самом SQL: создавать таблицы, обновлять их, использовать индексы, писать различные запросы и т. д. Например, в Django используется удобная Django ORM. Сравните сами:Допустим, у нас есть есть три таблицы: Students, Grade и Marks. Нам нужно вывести всех учащихся 7 класса, у которых оценки выше 4. Примерно так бы выглядел SQL‑запрос, удовлетворяющий нашей задаче:SELECT s.*\n",
      "FROM Students s\n",
      "INNER JOIN Grades g ON s.grade_id = g.id\n",
      "INNER JOIN Marks m ON s.id = m.student_id\n",
      "WHERE g.grade = 7 AND m.mark > 4;А вот так в Django ORM:from django.db.models import Q\n",
      "from myapp.models import Students\n",
      "\n",
      "students = Students.objects.filter(\n",
      "    Q(grade__grade=7) &\n",
      "    Q(marks__mark__gt=4)\n",
      ").distinct()Вот еще пример:Допустим, у нас есть две таблицы: Book и Author. Мы хотим вывести все книги, автором которых является Стивен Кинг. SQL-запрос:SELECT b.*\n",
      "FROM Books b\n",
      "JOIN Author a ON b.author_id = a.id\n",
      "WHERE a.name = 'Steven King';Django ORM:from myapp.models import Book\n",
      "\n",
      "Book.objects.filter(author__name='Steven King')Материалы для изученияРазбираем SQL на примере PostgreSQL — SELECT, JOIN, GROUP, HAVING, Coalesce и др. YouTube канал Диджитализируй!Django ORM и его самые популярные фичи. Статья.Официальная документация Django ORM.Шаг 4. Изучение верстки на базовом уровнеСегодня без верстки никуда. Ее должен знать каждый, в том и числе и backend‑инженер. То же самое я бы мог сказать и о JavaScript, но на данном этапе изучать его необязательно. Вы всегда сможете вернуться к нему позже.Без знаний верстки вы не поймете как работает ваше приложение. Вы даже не сможете нормально отобразить данные на странице. Если я вас не убедил, то вот некоторые из причин, почему вам следует изучить верстку:Понимание того, как работает Web. Backend‑инженеры обычно работают на стороне сервера, но они также должны хорошо понимать, как работает клиентская часть (т. е. браузер). HTML и CSS являются основополагающими языками Интернета и необходимы для создания веб‑страниц. Знание того, как работать с HTML и CSS, помогает бэкенд‑инженерам понять весь процесс веб‑разработки.Лучшее кооперирование с frontend‑разработчиками. Backend‑разработчики часто сотрудничают с frontend‑инженерами для создания веб‑приложений. Знание HTML и CSS облегчает им общение с разработчиками интерфейсов и понимание кода, над которым они работают. Это может привести к более эффективному сотрудничеству.Отладка и устранение неполадок. Инженеры серверной части отвечают за обеспечение правильной работы веб‑приложений. Если есть проблемы с интерфейсом, такие как неработающий макет или смещенный элемент, знание того, как работать с HTML и CSS, может помочь backend‑разработчику быстрее диагностировать и устранять проблему.Быстрое прототипирование. Backend‑инженерам часто приходится создавать быстрые прототипы или макеты веб‑страниц, чтобы протестировать их функциональность. Знание HTML и CSS позволяет им быстро и легко создавать простые веб‑страницы, не полагаясь на разработчика внешнего интерфейса.Больше возможностей для трудоустройства: На современном рынке многие вакансии для бэкенд‑инженеров также требуют знания интерфейсных технологий, таких как HTML, CSS и JavaScript. Обладание этими навыками может сделать backend‑инженеров более востребованными на рынке и открыть больше возможностей для трудоустройстваМатериалы для изучения Шаг 4YouTube канал Евгения Андриканича «Фрилансер по жизни». Отличнейший канал, благодаря которому я также в свое время изучал верстку. Автор доступным языком объясняет различные концепции и штуки из верстки и фронтенда в целом. На канале очень много практики, которую Женя буквально «разжевывает» для зрителя.Ресурс W3 Schools. Полезный сайт, на котором содержится куча полезный информации и практических примеров по всем html‑тегам, css‑селекторам и т. д.В целом, после знакомства с основными концепциями, я бы посоветовал набрать понравившихся макетов из тематических тг‑каналов, вроде этого и тупо верстать их. После нескольких таких макетов вы будете чувствовать себя уверенно и поймете, что верстка — это просто.Шаг 5. Изучение фреймворкаПосле того, как вы изучите базовые конструкции Python, а также работу с SQL, вы можете переходить к изучению фреймворка. Во время изучения, параллельно стоит изучить как работает веб в целом: как сервер взаимодействует с клиентом, как осуществляется сохранение данных на сервере, какие HTTP‑запросы существуют и т. д. Да, это займет у вас какое‑то время, но эти знания являются обязательными.На данный момент на Python существует три самых популярных решения для разработки веб‑приложений: Django, Flask и FastAPI. Какой из них выбрать — решать вам. В этой статье автор проводит понятное сравнение всех трех фреймворков. В свое время я начинал с Django: меня подкупила популярность, наличие огромного множества библиотек, широкий функционал прямо из коробки, большое количество обучающего материала, а также большое комьюнити.Для понимания того, как устроен фреймворк, вы также должны знать о существовании такого понятия, как паттерны разработки. Шаблон проектирования или паттерн в разработке программного обеспечения — это повторяемая архитектурная конструкция, представляющая собой решение проблемы проектирования в рамках некоторого часто возникающего контекста. На самом деле существует огромное количество таких паттернов и подходов. Вам не нужно знать их все, но зачастую фреймворк использует тот или иной архитектурный паттерн, следовательно, вам стоит знать как он устроен.Шаг 6. Создание первого проектаКак я упоминал выше, в свое время я изучал Django прямо во время разработки своего первого веб‑приложения, а именно книжного интернет‑магазина. Если вы будете делать также, то таким образом сможете сразу подкреплять свои теоретические знания полезной практикой. Не стоит бросаться в крайности. Не нужно 10 часов подряд изучать сухую теорию и никак не использовать ее на практике. Но, также, в свою очередь, для начала нужно изучить основные концепции и инструменты фреймворка, вроде моделей, представлений, контроллеров и т. д., а также их взаимодействие между собой. Итоговый совет такой: изучите основную концепцию инструмента: базовые вещи и из чего он состоит, а после следуйте примерному плану: изучили тему — внедрили функционал в свое приложение.Как только вы подойдете к созданию пет‑проекта, у вас, как и у большинства новичков, сразу возникнет вопрос: «А какой вообще проект делать?». Тут все просто: не нужно придумывать велосипед. Для начала вы можете делать проект с курса или книги, которые вы изучаете. После этого вам нужно будет написать что‑то свое. Подумайте. Может быть у вас есть хобби? Может быть вы любите читать книги и хотите создать интернет‑магазин по их продаже. А может вы любите играть в футбол и хотите создать сайт о спортивных мероприятиях, матчах, командах и т. д. Выбор тут и вправду огромный и он ограничивается лишь вашей фантазией.Шаг 7. Работа с APIВам так или иначе придется научиться работать с API (Application programming interface) — интерфейсом, при помощи которого один сайт может взаимодействовать с другим.Грубо говоря API — это набор операций и функций, доступный внешним клиентам.Задачи backend‑разработчика так или иначе сводятся к созданию или работе с API‑интерфейсом, который далее будет использоваться frontend‑разработчиком.В Django для проектирования API используется библиотека Django Rest Framework, которая предоставляет все нужные инструменты для создания API прямо из коробки.Попробуйте поработать с API разных платформ: например, с API площадки Steam, при помощи которой вы можете получить данные о всех размещенных на ней играх.Материалы для изученияQuickstart по Django Rest Framework. Официальная документация.Официальная документация по Django.Django3 by example. Antonio MeleНовая обновленная версия книги Django2 by example. В книге автор разбирает создание различных проектов на Django: начиная от просто блога, заканчивая интернет‑магазином и образовательной платформойTwo Scoops of Django3. Daniel Roy Greenfeld, Audrey GreenfeldВ этой книге авторы познакомят вас с различными советами, хитростями, шаблонами, фрагментами кода и техниками, которым они научились за годы разработки на Django. Стоит отметить, что книга является обновлением и дополнением предыдущих изданий.Django for APIs: Build web APIs with Python & DjangoDjango for APIs — это основанное на разработке проекта руководство по созданию современных веб‑API с использованием Django & Django REST Framework. Он подходит для начинающих, которые никогда раньше не создавали API, а также для профессиональных программистов, ищущих быстрое знакомство с основами Django и лучшими практиками. Стоит отметить, что материалы книги используют современные версии фреймворка: Django 4.0 и Rest Framework 3.13YouTube канал Django School. Автор на практике понятным языком объясняет как работать с Django. На канале есть несколько плейлистов по полноценной пошаговой разработке веб‑приложений, включая сайт по поиску фильмов.Прочие инструменты и технологииЯ бы мог еще долго перечислять различные инструменты и технологии, которые могут понадобиться вам в работе. Например Docker или принцип работы веб‑серверов, вроде Nginx. Для того, чтобы понять, что нужно изучить в дополнение к базовым знаниям, вам обязательно нужно ходить по собеседованиям в ИТ‑компании. Так вы сможете понять свой текущий уровень и что вам нужно подтянуть, а также увидеть то, чего от вас ждет рынок в плане хард‑скиллов.ИтогиВ данной статье я описал примерный список необходимых знаний для старта в разработке веб‑приложений на Python. Да, объективно он далеко не маленький. Любое обучение, а особенно осознанное — это в принципе дело не быстрое. Но если вы добросовестно изучите все темы, описанные выше, то получите необходимый фундамент, который поможет вам в дальнейшем.Также, хотел еще раз отметить, что у вас не должно быть цели изучить все эти темы за месяц. Обучайтесь в комфортном для вас темпе, главное — регулярность. Кто‑то сможет осилить данный объем материала за условных 6 месяцев, а кто‑то за 12. В этом нет ничего плохого. Главное, чтобы обучение проходило качественно и приносило вам реальную пользу и результат.Спасибо за внимание! Если статья оказалась для вас полезной и интересной, то можете подписаться на мой телеграм‑канал, где я рассказываю о разработке на Python и об ИТ в целом на основе своего рабочего опыта.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Начиная с 23-й версии Платформы nanoCAD пользователю доступны команды PY и -PY, которые позволяют выполнять в Платформе скрипты на языке Python.🔹  Команда PY вызывает диалоговое окно, в котором следует указать заранее подготовленный скрипт на Python, и выполняет выбранный скрипт.🔹  Команда -PY запрашивает путь к скрипту в виде строки и выполняет указанный скрипт.Подготовка средыДля выполнения скриптов Python на Платформе nanoCAD требуется Python 3 и расширение Python.AXScript.2 (Active Scripting). Чтобы использовать Python в работе, нужно, располагая правами Администратора, выполнить следующие шаги.1. Установить Python 3. Для установки следует использовать внешний источник: www.python.org/downloads/windows (в этой статье рассматривается версия 3.10.4).Примечание. На первом шаге установки для включения команд Python в переменную PATH включите Add Python 3.10 to PATH (рис. 1).Рис. 12. Установить расширение Active Scripting для Python.При установке Python Win32 extentions поддержка Python Active Scripting (win32com.axscript) будет зарегистрирована автоматически.Если при установке Python путь был добавлен в переменную PATH, то достаточно воспользоваться в консоли командой python -m pip install --upgrade pywin32илиpip install pywin32 --upgradeПримечание. В случае возникновения ошибки (например, «Модуль не найден») можно использовать команду для ручной регистрации расширения:python Scripts/pywin32_postinstall.py -installПри загрузке nanoCAD выполняется проверка, присутствует ли расширение «Python.AXScript.2» в операционной системе. Если проверка не пройдена, Платформа при попытке выполнить скрипт уведомит о недостающем расширении Python сообщением вида «Unable to create scripting engine for „Python.AXScript.2“».Выполнение скриптов PythonТребуемые параметры запуска:Платформа nanoCAD 23;Python for Windows 3.10.4;Pywin32 – Release 304.Запустите Платформу nanoCAD и убедитесь, что команды PY (рис. 2) и -PY теперь находятся в списке доступных.Рис. 2Листинг MyPythonScript.py:doc = ThisDrawing;\n",
      "\n",
      "ut = doc.Utility;\n",
      "\n",
      "ut.Prompt(\"Привет, Python в nanoCAD!\");Результат выполнения представлен на рис. 3.Рис. 3Пример использования команды -PY для скрипта MyPythonScript.py показан на рис. 4, а результат – на рис. 5.Рис. 4Рис. 5Также можно воспользоваться встроенным редактором скриптов на Платформе nanoCAD: Сервис →  Скрипты →  Редактор скриптов (рис. 6).Рис. 6Примечание. Перед запуском скрипт должен быть сохранен на компьютере.Сергей Евсеев,специалист группы поддержки APIООО «Нанософт разработка»    \n",
      " Предлагаю вашему вниманю пересказ замечательной статьи автора Jinja2, Werkzeug и Flask, соавтора Sphinx и Pygments Армина Ронахера. Я получил огромное удовольствие разбирая исходные коды его творений и очень многое для себя почерпнул. Армин пишет отличные фреймворки, и как никто другой может разъяснить, чем чреват переход с Python 2 на Python 3 и почему его не так легко осуществить.\n",
      "\n",
      "\n",
      "\n",
      "Мысли о Python 3\n",
      "В последнее время меня часто посещали мысли о состоянии, в котором находится Python 3. Хоть и не с первого взгляда, я полюбил Python и более чем доволен курсом, которым он идёт. Десять лет моя жизнь проходит вместе с Python. И пока это бОльшая часть моей жизни.\n",
      "\n",
      "Заранее предупреждаю: это очень личная статья. Я насчитал сотню экземпляров одной заглавной буквы в этом тексте.\n",
      "\n",
      "Это потому, что я очень признателен всем возможностям, полученным за последние два года: возможностям путешествовать по миру, общаться с людьми и разделять дух сотрудничества, который позволяет свободно распространяемым проектам, таким как Python, поощрять инновации и радовать людей. У Python замечательное сообщество, о чём я частенько забываю сказать вслух.\n",
      "\n",
      "А с другой стороны, хоть я и люблю Python и люблю обсуждать пути и решения, я не связан с проектом какими-либо обязательствами, несмотря на преданность ему. Когда я посещаю совещания о языке, то сразу понимаю, почему мои предложения воспринимаются в штыки, а сам я считаюсь ещё той занозой. «Постоянно жалуется и ничего не делает». Есть столько всего, чего бы мне хотелось видеть в Python, но в конце концов, я его пользователь, а не разработчик.\n",
      "\n",
      "Когда вы будете читать мои комментарии о Python 3, учитывая что его первая версия уже выпущена, у вас сложится впечатление, что я его ненавижу и вообще не хочу на него переходить. Ещё как хочу, но только не в той форме, в которой он сейчас находится.\n",
      "\n",
      "Учитывая мой опыт того, что люди ссылаются на статьи спустя много времени после того как они были написаны, позвольте вначале разъяснить ситуацию с Python 3 на момент написания: вышла версия 3.2, следующая версия 3.3 и нет никаких планов когда-либо выпустить Python 2.8. Более того, существует PEP, в котором чёрным по белому написано: релизу не быть. Прекрасно развиваясь, PyPy остаётся проектом, архитектура которого настолько отдалена от всего остального, что его ещё долго никто не будет воспринимать всерьёз. Во многом PyPy делает вещи которые «я бы не сделал» и это мне кажется удивительным.\n",
      "\n",
      "Почему мы используем Python?\n",
      "Почему же мы используем Python? Мне кажется, что это очень правильный вопрос, который мы редко себе задаём. У Python есть куча изъянов, но я им всё-равно пользуюсь. На вечеринке, в последний день конференции PyCodeConf этого года, я успел многое обсудить с Ником Кофланом. Мы были подшофе и благодаря этому дискуссия получилась очень искренней. Мы согласились признать факт того, что Python не идеален, как язык, что над некоторыми изъянами продолжается работа и что при внимательном рассмотрении, некоторым из них нет оправданий. Был рассмотрен PEP о «yield from» как пример развития сомнительного дизайна (coroutine как generator) для придачи ему более-менее рабочего вида. Но даже с изменениями принятыми в «yield from», всё это очень далеко от удобства greenlet'ов.\n",
      "\n",
      "Этот разговор был продолжением услышанного на лекции «Предвзятое мнение о языках программирования», которую читал Гери Бернард в тот же памятный день конференции. Мы пришли к общему мнению о том, что у блоков Ruby восхитительный дизайн, но по многим причинам он не прижился бы в Python (в его текущем состоянии).\n",
      "\n",
      "Лично я не думаю, что мы используем Python потому, что это совершенный и безупречный язык. Более того, если вы вернётесь в прошлое и присмотритесь к ранним версиям Python то увидите, что он очень и очень уродлив. Не стоит удивляться тому, что в свои ранние годы Python оставался никем незамеченным.\n",
      "\n",
      "Мне кажется, что размах полученный Python с тех пор, можно считать большим чудом. И вот почему, как мне кажется, мы используем Python: эволюция этого языка была очень плавной, а воплощённые идеи — верными. Ранний Python был ужасен, в нём отсутствовала концепция итераторов и более того, для итерации по словарю приходилось создавать промежуточный список всех его ключей. В какой-то момент исключения были строками, методы строки были не методами а функциями из одноимённого модуля (string). Синтаксис перехвата исключений мучает нас во всех ипостасях языка Python 2, а Юникод появился слишком поздно и частично — никогда.\n",
      "\n",
      "Однако, в нём было и много чего хорошего. Пускай и небезупречная, идея о модулях с их собственными пространствами имён была восхитительной. Структура языка основанная на мультиметодах* до сих пор во многом не имеет себе равных. Мы ежедневно выигрываем от этого решения, хоть и не отдаём себе в этом отчёта. Этот язык всегда честно делал свою работу и не скрывал происходящее в интерпретаторе (traceback'и, кадры стека, опкоды, кодовые объекты, ast и т.д.), что вкупе с динамической структурой позволяет разработчику быстро произвести отладку и решить проблемы с лёгкостью недостижимой в других языках.\n",
      "\n",
      "Часто подвергался критике и синтаксис основанный на отступах, но видя, сколько новых языков внедряют этот подход (на ум приходят HAML, CoffeeScript и многие другие) доказывает, что он получил признание.\n",
      "\n",
      "Даже тогда, когда я не соглашаюсь с тем, как Реймонд* пишет что-то новое для стандартной библиотеки, качество его модулей не вызывает ни малейшего сомнения и это одна из основных причин, по которым я использую Python. Не могу представить себе работу с Python без доступа к модулю collections или itertools.\n",
      "\n",
      "Но настоящей причиной, по которой я любил и боготворил Python, было предвкушение каждой новой версии, как у нетерпеливого ребёнка, ждущего Рождество. Мелкие, едва заметные улучшения приводили меня в восторг. Даже возможность указывать начало индекса для функции enumerate вызывали у меня чувство благодарности за новый релиз Python. И всё это с учётом обратной совместимости.\n",
      "\n",
      "Импорт из __future__ — это то, что мы порой так ненавидим и именно то, что делало апгрейды лёгкими и безболезненными. Когда-то я пользовался PHP и совершенно не радовался новым релизам. В PHP не было пространств имён, зато всегда появлялись новые встроенные функции и с каждым релизом я очень надеялся избежать коллизий в названиях (знаю, что мог бы их избежать, если бы использовал префиксы, но это было задолго до того, как я научился основам разработки ПО).\n",
      "\n",
      "Что же изменилось?\n",
      "Как же получилось, что мне стало не до новых релизов Python? Могу говорить лишь за себя, но я заметил, что и у других изменилось отношение к новым релизам.\n",
      "\n",
      "Я никогда не задавался вопросами о том, чем занимались разработчики ядра очередного Python 2.x.\n",
      "Конечно, что-то было не так уж и хорошо продуманно, например реализация абстрактных классов или особенности их семантики. Но в основном всё сводилось к критике высокоуровневого функционала.\n",
      "\n",
      "С появлением Python 3 появились и внешние факторы, из-за которых мне неожиданно пришлось изменить общий подход к работе с языком. Раньше я долго не использовал новые возможности языка, хоть и был им рад, т.к. в основном писал библиотеки. Было бы ошибкой использовать самое новое и самое лучшее. Код Werkzeug'a до сих пор забит хаками позволяющими ему работать на Python 2.3, хотя сейчас минимальные требования поднялись до версии 2.5. Я оставлял в коде багфиксы для стандартной библиотеки, ведь некоторые производители (печально известная Apple) никогда не обновляют интерпретатор до тех пор, пока в нём не будет найдена критическая уязвимость.\n",
      "\n",
      "Всё это невозможно с Python 3. С ним всё превращается в разработку для 2.х либо 3.х. И никакого срединного решения не предвидится.\n",
      "\n",
      "После анонса Python 3, Гвидо всегда восхищенно говорил о 2to3 и том, как она облегчит портирование. А вышло так, что 2to3 это худшее, что могло случиться с Python.\n",
      "\n",
      "Я испытал огромные трудности при портировании Jinja2 с помощью 2to3, о чём впоследствии сильно сожалел. Более того, в отпочковавшемся проекте JSON Jinja, я убрал все хаки написанные для корректной работы 2to3 и никогда больше не буду его использовать. Как и многие другие, сейчас я вовсю стараюсь поддерживать код работающий как на версиях 2.х так и 3.х. Вы спросите почему? Потому, что 2to3 очень нетороплива, из рук вон плохо интегрируется в процесс тестирования, зависит от версии используемого Python 3 и ко всему прочему настраивается разве что с применением чёрной магии. Это болезненный процесс, сводящий на нет всё удовольствие получаемое от написания библиотек. Я любил обтёсывать Jinja2, но перестал это делать в тот момент, когда порт на Python 3 был готов, т.к. боялся что-либо в нём сломать.\n",
      "\n",
      "Сейчас же, идея о разделяемой кодовой базе упирается в то, что я должен поддерживать Python вплоть до версии 2.5.\n",
      "\n",
      "Перемены вызванные Python 3 привели весь наш код в негодность, что никак не служит оправданием для его незамедлительного переписывания и апгрейда. По моему глубоко субъективному мнению, Python 3.3/3.4 должен больше походить на Python 2 а Python 2.8 должен быть ближе к Python 3. Так сложилось, что Python 3 — это XHTML в мире языков программирования. Он не совместим с тем, что пытается заменить, а взамен практически ничего не предлагает кроме того, что он более «правильный».\n",
      "\n",
      "Немного о Юникоде\n",
      "Очевидно, что самым большим изменением в Python 3 стала обработка Юникода. Может показаться, что насаживание Юникода всем и каждому — это благо. А ещё, это взгляд на мир сквозь розовые очки, ведь в настоящем мире мы сталкиваемся не только с байтами и Юникодом, но и со строками с известной кодировкой. Хуже всего то, что во многом Python 3 стал этаким Fisher Price* в мире языков программирования. Некоторые возможности были удалены, т.к. разработчики ядра посчитали, что о них можно будет «легко порезаться». И всё это далось ценой изъятия широко используемого функционала.\n",
      "\n",
      "Вот конкретный пример: операции с кодеками в 3.х на данный момент ограничены преобразованиями Unicode <-> bytes. Никаких bytes <-> bytes или Unicode <-> Unicode. Выглядит разумно, но приглядевшись вы увидите, что сей удалённый функционал как раз то, что жизненно необходимо.\n",
      "\n",
      "Одним из самых замечательных свойств системы кодеков в Python 2 было то, что она создавалась с прицелом на разнообразную работу с огромным количеством кодировок и алгоритмов. Можно было использовать кодек чтобы кодировать и раскодировать строки, а также у кодека можно было попросить объект, предоставляющий операции на потоках и других неполных данных. A ещё, система кодеков одинаково хорошо работала с контент- и transfer- кодировками. Стоило написать новый кодек и зарегистрировать его, как каждая часть системы узнавала о нём автоматически.\n",
      "\n",
      "Любой, кто брался за написание HTTP библиотеки на Python, с удовольствием открывал для себя, что кодеки можно использовать не только для декодирования UTF-8 (актуальная кодировка символов), но например и для gzip (алгоритм сжатия). Это относится не только к строкам, но и к генераторам или файловым объектам, если конечно знать, как ими пользоваться.\n",
      "\n",
      "На настоящий момент, в Python 3, всё это попросту не работает. Они не только удалили эти функии из объекта string, но убрали и кодирование byte -> byte, взамен ничего не оставив. Если я не ошибаюсь, понадобилось 3 года для признания проблемы и начала обсуждения о возвращение вышеперечисленного функционала в 3.3.\n",
      "\n",
      "Далее, Юникод протолкнули туда, где ему совсем не место. К таким местам относятся слой файловой системы и модуль URL. A ещё, куча Юникод-функционала была написана с точки зрения программиста живущего в 70-х.\n",
      "\n",
      "Файловые системы UNIX основаны на байтах. Так уж оно устроено и с этим ничего не поделаешь. Естественно, было бы здорово это изменить, что вообще-то невозможно, не сломав существующего кода. A всё потому, что сменa кодировки — лишь малая часть того, что необходимо для Юникод-ориентированной файловой системы. Вдобавок, вопросы форм нормализации и хранения информации о регистре при уже проведённой нормализации остаются открытыми. Останься тип bytestring в Python 3, этих проблем можно было бы избежать. Однако его нет и его замена, тип byte, не ведёт себя так, как себя ведут строки. Он ведёт себя как тип данных, написанный в наказание людям пользующимися байтовыми данными, которые одновременно существуют в виде строки. Не похоже, чтобы он разрабатывался как инструмент, с помощью которого программисты могли бы решить эти проблемы. Проблемы, которые более чем реальны.\n",
      "\n",
      "Так вот, если вы работаете с файловой системой из Python 3, то странное чувство не будет покидать вас несмотря на наличие новой кодировки с суррогатными парами и экранированием. Это болезненный процесс, болезненный потому, что не существует инструментa для разгребания этого бедлама. Python 3 как-бы обращается к вам, «Приятель, с этого момента у твоей файловой системы Юникод», но при этом не объясняет, с какого конца надо разгребать этот беспорядок. Он даже не проясняет, на самом ли деле файловая система поддерживает Юникод, или же это Python подделывает эту поддержку. Oн не раскрывает подробности о нормализации или о том, как нужно сравнивать имена файлов.\n",
      "\n",
      "Он работает в лабораторных условиях, но ломается в условиях полевых. Так сложилось, что у моего мака американская раскладка клавиатуры, американская локаль, да практически всё американское, разве что даты и числа форматируются по-другому. В результате всего этого (и как я предполaгаю того, что я проапгрейдил свой мак со времён Tiger'a), у меня возникла следующая ситуация: зайдя на свой удалённый сервер, я получил локаль выставленную в строковое значение «POSIX». Вы спрашиваете, что за «POSIX»? А хрен его знает. Вот и Python будучи в таком же неведении как и я, решил работать с «ANSI_X3.4_1968». В этот памятный день я узнал, что у ASCII есть много имён. Оказалось, что это всего лишь одно из названий ASCII. И вот тебе на, мой удалённый интерпретатор Python криво отобразил содержимое директории с интернациализированными именами файлов. Как они там оказались? Я накидал туда статьи из Википедии с их изначальными названиями. Делал же я это с помощью Python 3.1, который замалчивал происходящее с файлами, вместо того, чтобы выдавать исключения или задействовать какие-либо хаки.\n",
      "\n",
      "Но неисправности с файловыми системами — это всего лишь цветочки. Python также использует переменные окружения (где как вы знаете, полно мусора) для установки файловой кодировки по-умолчанию. Во время конференции, я попросил парочку посетителей угадать кодировку, использующуюся по-умолчанию для текстовых файлов в Python 3. Более 90% этой маленькой выборки были уверены, что это UTF-8. А вот и нет! Она устанавливается в зависимости от локали платформы. Как я вам и говорил, привет из 70-х.\n",
      "\n",
      "Смеха ради, я залогинился на оба контролируемых мною сервера и обнаружил, что у одного из них при входе через консоль стоит кодировка latin1, которая переключается на latin15 когда вход осуществляется через ssh под рутом, и UTF-8, если я заходил через своего пользователя. Чертовски занимательно, но винить остаётся лишь самого себя. Не сомневаюсь, что я не единственный, чей сервер волшебным образом переключает кодировки учитывая, что по-умолчанию SSH пересылает настройки локали при логине.\n",
      "\n",
      "Почему я пишу об этом здесь? Да потому, что снова и снова приходится доказывать, что поддержка Юникода в Python 3 доставляет мне куда больше неприятностей, чем в Python 2.\n",
      "\n",
      "Кодирование и декодирование Юникода не встаёт на пути у того, кто следует Дзену Python 2 в том, что «явное лучше неявного». «Байты входят, Юникод выходит» — именно так работают куски приложений, которые общаются с другими сервисами. Это можно объяснить. Вы можете объяснить это хорошенько задокументировав. Вы подчеркнёте, что для внутренней обработки текста в виде Юникода есть свои причины. Вы расскажите пользователю о том, что мир вокруг нас суров и основан на байтах, поэтому вам приходится кодировать и декодировать для общения с этим миром. Эта концепция может быть в новинку для пользователя. Но стоит лишь подобрать нужные слова и все хорошенько расписать, как одной головной болью станет меньше.\n",
      "\n",
      "Почему я говорю об этом с такой уверенностью? Потому, что с 2006 года, все мои программы насаживают пользователям Юникод, a количество запросов касательно Юникода не идет ни в какое сравнение с прорвой запросов о работе с пакетами или системой импортирования. Даже с distutils2, в царстве Python, пакеты остаются гораздо большей проблемой, чем Юникод.\n",
      "\n",
      "Куда уж не естественное развитие событий: запрятать Юникод подальше от пользователя Python 3. Но обернулось это тем, что людям стало ещё сложнее представлять, как всё это работает. Нужно ли нам априори неявное поведение? Я в этом не уверен.\n",
      "\n",
      "Несомненно, уже сейчас Python 3 на правильном пути. Я обнаружил, что всё больше разговоров идёт о возвращении некоторых API для работы с байтами. Моей наивной задумкой была идея о третьем типе строки в Python 3, под названием estr, или чего-нибудь в этом роде. Он работал бы точно так, как str в Python 2, хранил бы байты и имел такой же набор строковых API. Однако в нём бы также содержалась информация о кодировке, которая бы использовалась для прозрачного декодирования в Юникод-строку или приведения к байтовому объекту. Этот тип был бы священным граалем могущим облегчить портирование.\n",
      "\n",
      "Но его нет, а интерпретатор Python не разрабатывался с заделом на ещё один тип строки.\n",
      "\n",
      "Мы разрушили их Мир\n",
      "Ник рассказывал о том, как разработчики ядра Python разрушили мир веб-программистов. Пока разрушения простилаются до тех пор, где заканчивается обратная НЕсовместимость Python. Но наш мир был разрушен не более, чем мир других разработчиков. Ведь у нас единый мир. Сеть основана на байтах с кодировками, но в основном это касается протоколов низкого уровня. Общение с большей частью того, что лежит на нижнем уровне, происходит на языке байтов с кодировками.\n",
      "\n",
      "Однако, главные изменения коснулись образа мышления, который нужен при работе на этих уровнях. В Python 2 очень часто использовались объекты Unicode для общения с нижними уровнями. При необходимости, объекты кодировались в байты и наоборот. Приятным для нас побочным эффектом, была например возможность ускорить некоторые операции, кодируя и декодируя данные на ранних стадиях и передавая их в канал понимающий Юникод. Во многом это позволяет функционировать модулю сериализации в ядре. К примеру, Pickle общается с потоками поддерживающими как байты, так и Юникод. В какой-то степени то же самое можно сказать и о simplejson. И вот, появляется Python 3, в котором внезапно нужно разделять Unicode и байтовые потоки. Многие API не выживут на пути к Python 3, без кардинальных изменений в их интерфейсах.\n",
      "\n",
      "Да, это более правильный подход, но на деле у него больше нет никаких достоинств, кроме того что он более правильный.\n",
      "\n",
      "Работая с функционалом ввода/вывода в Python 3, я убедился, что он великолепен. Но в реальности, он не идет ни в какое сравнение с тем, как работал Python 2. Может показаться, что у меня много предубеждений, ведь я так много работал с Python 2 и так мало с Python 3, однако, написание большего количества кода для достижения одинакового фунционала, считается дурным тоном. А с Python 3, мне приходиться всё это делать, учитывая все его аспекты.\n",
      "\n",
      "Но ведь портирование работает!\n",
      "Конечно же, портирование на Python 3 работает. Это было доказано, и не один раз. Но только потому что что-то возможно и проходит все тесты не значит, что всё хорошо сделано. Я — человек с недостатками и совершаю кучу ошибок. В то же время, я горжусь стремлением довести до блеска свои любимые API. Иногда я ловлю себя на том, что снова и снова переписываю кусок кода, чтобы он стал более удобным для пользователя. При работе с Flask я столько времени потратил на оттачивание основного функционала, что самое время начать говорить об одержимости.\n",
      "\n",
      "Я хочу, чтобы он работал идеально. Когда я использую API для обычной задачи, мне хочется, чтобы они имели такой же уровень совершенства, каков присущ дизайну Porshe. Да, это — лишь внешний слой для разработчика, но продукт должен быть хорошо разработан от начала и до конца.\n",
      "\n",
      "Я могу заставить свой код «работать» на Python 3 и всё равно я буду его ненавидеть. Я хочу, чтобы он работал. Но при этом, используя свои или чужие библиотеки, мне хочется получать такое же удовольствие с Python 3, какое я получаю от Python 2.\n",
      "\n",
      "Jinja2, например, некорректно использует слой ввода/вывода в Python 3, так как невозможно использовать один и тот же код на 2.x и 3.x без переключения между реализацией во время исполнения. Теперь, шаблоны открываются в бинарном режиме как в 2.х, так и в 3.х, т.к. это — единственный надёжный подход, а после, Jinja2 сама декодирует данные из этого бинарного потока. Вообще-то это работает, спасибо нормализации разделителей новых строк. Но я более чем уверен, что все, кто работает в Windows и самостоятельно не нормализует разделители строк, рано или поздно попадут в ситуацию с месивом из различных разделителей, совершенно об этом не подозревая. \n",
      "\n",
      "Принимая Python 3\n",
      "Python 3 многое изменил, это факт. Без сомнения, за ним будущее в которое мы направляемся. Многое в Python 3 подаёт большие надежды: значительно улучшенная система импортирования, появление __qualname__, новый способ распространения пакетов Python, унифицированное представление строк в памяти.\n",
      "\n",
      "Но в настоящее время, портирование библиотеки на Python 3 выглядит как разработка библиотеки на Python 2 и создание её (прошу прощения за мой французский) хитрожопой версии для Python 3 лишь бы доказать, что она работает. Про Jinja2 на Python 3 можно во всех отношениях сказать, что она чертовски уродлива. Это ужасно и мне должно быть стыдно за это. Например, в версии для Python 3, Jinja2 загружала два одномегабайтовых регулярных выражения в память, и я совершенно не заботился о её освобождении. Мне просто хотелось, чтобы она хоть как-нибудь работала.\n",
      "\n",
      "Так почему же мне пришлось использовать мегабайтовые регулярные выражения в Jinja2? Да потому, что движок регулярных выражений в Python не поддерживает Unicode-категории. А с такими ограничениями пришлось выбирать меньшее зло из двух: либо забить на новые Unicode-идентификаторы в Python 3 и ограничиться идентификаторами ASCII, либо создать огромное регулярное выражение вручную, вписав в него все необходимые определения.\n",
      "\n",
      "Сказанное выше — лучший пример объясняющий, почему я пока не готов к Python 3. Он не предоставляет инструменты для работы с его же нововведениями. Python 3 жизненно необходимы Unicode-ориентированные регулярные выражения, ему нужны API для работы с локалями, учитывающими взятый курс на Unicode. Ему нужен более продвинутый модуль path, раскрывающий поведение нижележащей файловой системы. Он должен сильнее насаждать единую стандартную кодировку для текстовых файлов, не зависящую от среды исполнения. Он должен предоставлять больше инструментов для работы с закодированными строками. Ему нужна поддержка IRI, а не только URL. Ему нужно что-то большее чем «yield from». В нём должны быть вспомогательные механизмы для транскодирования, которые необходимы для отображения URL в файловую систему.\n",
      "\n",
      "Ко всему вышеперечисленному можно добавить выпуск Python 2.8, который бы был чуть ближе к Python 3. По мне, существует лишь один реалистичный способ перехода на Python 3: библиотеки и программы должны быть полностью осведомлены о Юникоде и интегрированы в новую экосистему Python 3. \n",
      "\n",
      "Не пускайте дилетантов прокладывать ваш Путь\n",
      "Самой большой ошибкой Python 3 является его бинарная несовместимость с Python 2. Тут я подразумеваю отсутствие возможности совместной работы интерпретаторов Python 2 и Python 3 в пространстве общего процесса. В результате, вы не можете запустить Gimp одновременно со скриптовыми интерфейсами как Python 2 так и Python 3. То же самое относится к vim и Blender. Мы просто-напросто не можем. Не сложно понаписать кучу хаков с отдельными процессами и вычурным IPC, но это никому не нужно.\n",
      "\n",
      "Таким образом, программист, которому придётся раньше других осваивать Python 3, будет делать это из-под палки. И не факт, что этот программист вообще хорошо знаком с Python. А причина, положа руку на сердце, в том, что деньги крутятся вокруг Python 2. Даже если по ночам тратить все наши силы на Python 3, то днём мы всё-равно будем возвращаться к Python 2. Так будет до поры, до времени. Однако, если кучка графических дизайнеров начнёт писать скрипты на Blender под Python 3, то вот вам и нужная адоптация.\n",
      "\n",
      "Мне совсем не хочется видеть kak CheeseShop* будет мучаться от обилия кривых портов библиотек на Python 3. Мне совсем не хочется видеть ещё одну Jinja2 и особенно уродливую кучу кода призванного работать и на 2.х, и на 3.х. Туда же, хаки вроде sys.exc_info()[1], для обхода синтактических различий, хаки конвертирования литералов во время исполнения для совместимости с 2.х и 3.х и многое многое другое. Всё это плохо отражается не только на производительности во время исполнения, но и на основных постулатax Python: красивый и разборчивый код без хаков. \n",
      "\n",
      "Признать неудачи, Учиться и Приспосабливаться\n",
      "Сейчас самое время для нас собраться и обсудить всё то, что люди делают для работы их кода на 2.x и 3.х. Технологии эволюционируют быстрыми темпами и мне будет очень обидно наблюдать, как Python разваливается только потому, что кто-то упустил из виду тёмные тучи на горизонте.\n",
      "\n",
      "Python не «слишком велик, чтобы о нём забыли». Он может очень быстро потерять свою популярность. Pascal и Delphi попали в узкую нишу, несмотря на то, что они оставались восхитительными языками даже после появления на свет C# и .NET framework. Больше всего на их падении сказался неправильный менеджмент. Люди до сих пор разрабатывают на Pascal, но много ли тех, кто начинает писать на нём новые проекты? Deplhi не работает на iPhone и Android. Он не очень-то хорошо интегрирован в рынок UNIX. И если быть честными, в некоторых областях Python уже сдаёт позиции. Python был достаточно популярен в области компьютерных игр, но этот поезд уже давно ушёл. В web-сообществе, новые конкуренты появляются как грибы после дождя, и нравится это нам или нет, JavaScript всё чаще и чаще наступает на позиции Python как скриптового языка программирования.\n",
      "\n",
      "Delphi не смог вовремя приспособиться и народ просто перешёл на другие технологии. Если 2to3 это наш путь перехода на Python 3, то py2js — это путь перехода на JavaScript.\n",
      "\n",
      "И вот что я предлагаю: могли бы мы составить список всего, что усложняет переход на Python 3 и список решений, позволяющих эти проблемы решить? Могли бы мы заново обсудить разработку Python 2.8, если он сможет помочь с портированием? Могли бы мы признать PyPy действительной имплементацией Python, достаточно весомой, чтобы повлиять на то, как мы пишем код?\n",
      "\n",
      "Armin Ronacher,\n",
      "7 Декабря 2011.\n",
      "\n",
      "От переводчика: Прочитав эту статью, первым же желанием было поделиться с другими, возникло острое ощущение, что «мир должен знать». Пересказать статью помогали моя коллега Ирина Пирогова и моя жена Айла Мехтиева, за что им огромное спасибо!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Boost.Python во всех отношениях замечательная библиотека, выполняющая своё предназначение на 5+, хотите ли вы сделать модуль на С++ для Python либо хотите построить скриптовую обвязку на Python для нативного приложения написанного на С++.\r\n",
      "Самое сложное в Boost.Python — это обилие тонкостей, поскольку и C++ и Python — два языка изобилующие возможностями, и потому на стыке их приходится учитывать все нюансы: передать объект по ссылке или по значению, отдать в Python копию объекта или существующий класс, преобразовать во внутренний тип Python или в обёртку написанного на C++, как передать конструктор объекта, перегрузить операторы, навесить несуществующие в C++, но нужные в Python методы.\r\n",
      "Не обещаю, что в своих примерах опишу все тонкости взаимодействия этих фундаментальных языков, но постараюсь сразу охватить как можно больше частоиспользуемых примеров, чтобы вы не лазили за каждой мелочью в документацию, а увидели все необходимые основы здесь, или хотя бы получили о них базовое представление.\n",
      "\n",
      "Оглавление\n",
      "\n",
      "Объединяя C++ и Python. Тонкости Boost.Python. Часть первая\n",
      "Объединяя C++ и Python. Тонкости Boost.Python. Часть вторая\n",
      "Конвертация типов в Boost.Python. Делаем преобразование между привычными типами C++ и Python\n",
      "Путешествие исключений между C++ и Python или «Туда и обратно»\n",
      "\n",
      "Введение\r\n",
      "Исходим из того, что у вас уже установлен удобный инструментарий для сборки динамически-линкуемой библиотеки на C++, а также установлен интерпретатор Python.\r\n",
      "Также понадобится скачать библиотеку Boost, после чего собрать её, следуя инструкции для своей ОС Windows или Linux.\r\n",
      "В двух словах в Windows все действия сводятся к двум строкам в командной строке. Распакуйте скачанный архив Boost в любое место на диске, перейдите туда в командной строке и наберите последовательно две команды:\n",
      "bootstrap\n",
      "b2 --build-type=complete stage\n",
      "\r\n",
      "Для сборки x64 нужно добавить аргумент address-model=64\r\n",
      "Если у вас уже есть библиотека Boost, но вы не устанавливали Python, либо вы скачали и установили свежий интерпретатор Python и хотите собрать только Boost.Python, это делается дополнительным ключом --with-python\r\n",
      "То есть вся строка для сборки только Boost.Python с 64-разрядной адресацией выглядит так:\n",
      "b2 --build-type=complete address-model=64 --with-python stage\n",
      "\r\n",
      "Стоит заметить, что x64 сборку следует заказывать, если у вас установлен Python x64. Также и модули для него нужно будет собирать с 64-разрядной адресацией.\r\n",
      "Ключ --with-python серьёзно сэкономит вам время, если вам из библиотеки Boost кроме функционала Boost.Python ничего не нужно.\r\n",
      "Если у вас установлено несколько интерпретаторов, крайне рекомендую прочитать подробную документацию по сборке Boost.Python\r\n",
      "После сборки у вас появятся в папке Boost\\stage\\lib собранные библиотеки Boost.Python, они нам очень скоро понадобятся.\n",
      "\n",
      "Настраиваем проект на C++\r\n",
      "Создаём проект для создания динамически-линкуемой библиотеки на C++, предлагаю назвать его example.\r\n",
      "После создания проекта, требуется указать дополнительные INCLUDE каталоги Python\\include и корень Boost, а также каталоги для поиска библиотек Python\\libs и Boost\\stage\\lib\r\n",
      "Под Windows также следует в настройках Post-build events задать переименование $(TargetPath) в модуль с расширением example.pyd в корне проекта. \r\n",
      "Также возможно стоит скопировать собранные библиотеки Boost.Python в каталог с собираемым модулем.\r\n",
      "Подключение модуля после запуска интерпретатора в том же каталоге сведётся к одной команде:\n",
      "from example import *\n",
      "\r\n",
      "Не забываем также про сборку под x64 если вы собираете для 64-разрядного Python.\n",
      "\n",
      "Обычный класс с простыми полями\r\n",
      "Итак, давайте заведём нашем новом проекте сразу три файла:\r\n",
      "some.h\r\n",
      "some.cpp\r\n",
      "wrap.cpp\n",
      "\r\n",
      "В файлах some.h и some.cpp опишем некий замечательный класс Some, который обернём для Python в модуле example в файле wrap.cpp — для этого в файле wrap.cpp следует подключить <boost/python.hpp> и использовать макрос BOOST_PYTHON_MODULE( example ) {… }, также для лаконичности будет совсем не лишним использовать using namespace boost::python. В целом наш будущий модуль будет выглядеть вот так:\n",
      "#include <boost/python.hpp>\n",
      "...\n",
      "\n",
      "using namespace boost::python;\n",
      "...\n",
      "\n",
      "BOOST_PYTHON_MODULE( example )\n",
      "{\n",
      "    ...\n",
      "}\n",
      "...\n",
      "\n",
      "\r\n",
      "В файле some.h нам следует наваять объявление нашего чудо-класса. Для объяснения большинства базовых механизмов нам достаточно всего два поля:\n",
      "private:\n",
      "    int mID;\n",
      "    string mName;\n",
      "\r\n",
      "Допустим класс содержит описание чего-то, что имеет имя и целочисленный идентификатор. Как ни странно этот несложный класс вызовет кучу сложностей, благодаря в основном стандартному классу string, перегрузкам методов, константной ссылке и статическому свойству NOT_AN_IDENTIFIER, которое мы конечно же тоже введём:\n",
      "public:\n",
      "    static const int NOT_AN_IDENTIFIER = -1;\n",
      "\r\n",
      "Разумеется эта константа нужна как идентификатор для объекта созданного конструктором по умолчанию, опишем также и другой конструктор, задающий оба поля:\n",
      "    Some();\n",
      "    Some( int some_id, string const& name );\n",
      "\r\n",
      "В файле some.cpp опишем реализацию данных конструкторов, в дальнейшем реализацию описывать я не буду, но давайте конструкторы напишем вместе:\n",
      "Some::Some()\n",
      "    : mID(NOT_AN_IDENTIFIER)\n",
      "{\n",
      "}\n",
      "\n",
      "Some::Some( int some_id, string const& name )\n",
      "    : mID(some_id), mName(name)\n",
      "{\n",
      "}\n",
      "\r\n",
      "Одновременно с появлением класса Some будет появляться обёртка класса для Python в файле wrap.cpp:\n",
      "BOOST_PYTHON_MODULE( example )\n",
      "{\n",
      "    class_<Some>( \"Some\" )\n",
      "        .def( init<int,string>( args( \"some_id\", \"name\" ) ) )\n",
      "    ;\n",
      "}\n",
      "\r\n",
      "Здесь используется бессовестный обман зрения и шаблон boost::python::class_, который создаёт описание класса для Python в указанном модуле с помощью Python C-API, жутко сложного и непонятно при описании методов, а потому полностью скрытого за объявлением простого метода def() на каждой строчке.\r\n",
      "Конструктор по умолчанию и конструктор копирования создаются для объекта по умолчанию, если не указано обратное, но мы этого ещё коснёмся чуть ниже.\r\n",
      "Уже сейчас можно собрать модуль, импортировать его из интерпретатора Python и даже создать экземпляр класса, но ни прочитать его свойства, ни вызывать методы мы у него пока не можем, пока они физически отсутствуют.\r\n",
      "Давайте это исправим, создадим «богатейшее» API нашего чудо класса. Вот полный код нашего заголовочного файла some.h:\n",
      "#pragma once\n",
      "\n",
      "#include <string>\n",
      "\n",
      "using std::string;\n",
      "\n",
      "class Some\n",
      "{\n",
      "public:\n",
      "    static const int NOT_AN_IDENTIFIER = -1;\n",
      "\n",
      "    Some();\n",
      "    Some( int some_id, string const& name );\n",
      "\n",
      "    int ID() const;\n",
      "    string const& Name() const;\n",
      "\n",
      "    void ResetID();\n",
      "    void ResetID( int some_id );\n",
      "\n",
      "    void ChangeName( string const& name );\n",
      "\n",
      "    void SomeChanges( int some_id, string const& name );\n",
      "\n",
      "private:\n",
      "    int mID;\n",
      "    string mName;\n",
      "};\n",
      "\n",
      "\r\n",
      "Раз реализация методов получилась также довольно короткой, давайте приведу и код some.cpp:\n",
      "#include \"some.h\"\n",
      "\n",
      "Some::Some()\n",
      "    : mID(NOT_AN_IDENTIFIER)\n",
      "{\n",
      "}\n",
      "\n",
      "Some::Some( int some_id, string const& name )\n",
      "    : mID(some_id), mName(name)\n",
      "{\n",
      "}\n",
      "\n",
      "int Some::ID() const\n",
      "{ \n",
      "    return mID;\n",
      "}\n",
      "\n",
      "string const& Some::Name() const\n",
      "{\n",
      "    return mName;\n",
      "}\n",
      "\n",
      "void Some::ResetID()\n",
      "{\n",
      "    mID = NOT_AN_IDENTIFIER;\n",
      "}\n",
      "\n",
      "void Some::ResetID( int some_id )\n",
      "{\n",
      "    mID = some_id;\n",
      "}\n",
      "\n",
      "void Some::ChangeName( string const& name )\n",
      "{\n",
      "    mName = name;\n",
      "}\n",
      "\n",
      "void Some::SomeChanges( int some_id, string const& name )\n",
      "{\n",
      "    mID = some_id;\n",
      "    mName = name;\n",
      "}\n",
      "\r\n",
      "Что ж, самое время описать обёртку в файле wrap.cpp:\r\n",
      "Первый метод Some::ID() оборачивается без каких-либо проблем:\n",
      "        .def( \"ID\", &Some::ID )\n",
      "\r\n",
      "Зато второй с результатом в виде константной ссылки на строку уже показывает, что всё не так просто:\n",
      "        .def( \"Name\", &Some::Name, return_value_policy<copy_const_reference>() )\n",
      "\r\n",
      "Как видим, можно указать как Python должен интерпретировать возвращаемое значение, если метод в C++ возвращает указатель или ссылку. Дело в том, что зверский Garbage Collector (GC) очень любит удалять всё бесхозное, поэтому просто так объявить метод возвращающий указатель или ссылку, вам никто не даст, всё печально закончится на этапе компиляции, поскольку GC должен знать что ему делать с возвращаемым значением, для разработчика будет весьма печально, если он начнёт удалять содержимое объекта в C++. Всего есть несколько вариантов return_value_policy для разных случаев, самые важные из них следующие:\n",
      "\n",
      "copy_non_const_reference — создаёт новый объект в Python, который содержит неконстантную ссылку на объект в C++, не требует обёртки для класса из C++ (пример: string не имеет обёртки, только конвертер в питоновский str)\n",
      "copy_const_reference — создаёт новый объект в Python, который содержит константную ссылку на объект в C++, не требует обёртки для класса из C++ (пример: тот же string)\n",
      "manage_new_object — создаёт новый объект в Python, используя обёртку класса из C++, по завершении содержимое удаляется\n",
      "reference_existing_object — создаёт новый объект в Python, используя обёртку класса из C++, по завершении содержимое остаётся\n",
      "\r\n",
      "Понимание того, как работает тот или иной return_value_policy в деталях приходит со временем, эксперементируйте, пробуйте, читайте документацию и набивайте руку. Для стандартного string ссылка в зависимости от константности при возвращении почти всегда copy_const_reference либо copy_non_const_reference, просто запомните, т.к. string по значению преобразуется на уровне Python в объект встроенного класса str, а по ссылке нужно явно указывать return_value_policy.\n",
      "\r\n",
      "Метод Some::ResetID я намерено перегрузил, чтобы усложнить задачу с передачей указателя на метод в .def():\n",
      "        .def( \"ResetID\", static_cast< void (Some::*)() >( &Some::ResetID ) )\n",
      "        .def( \"ResetID\", static_cast< void (Some::*)(int) >( &Some::ResetID ), args( \"some_id\" ) )\n",
      "\n",
      "\r\n",
      "Как видите, можно указать, с каким именем в Python будет создан аргумент метода. Как известно имя аргумента в Python куда важнее чем в C++. Рекомендую указывать имена аргументов для каждой обёртки метода, принимающего параметры:\n",
      "        .def( \"ChangeName\", &Some::ChangeName, args( \"name\" ) )\n",
      "        .def( \"SomeChanges\", &Some::SomeChanges, args( \"some_id\", \"name\" ) )\n",
      "\n",
      "\r\n",
      "Осталось описать статическим свойством константу NOT_AN_IDENTIFIER:\n",
      "        .add_static_property( \"NOT_AN_IDENTIFIER\", make_getter( &Some::NOT_AN_IDENTIFIER ) )\n",
      "\r\n",
      "Здесь используется специальная функция boost::python::make_getter, которая по свойству класса генерирует get-функцию.\r\n",
      "Вот так примерно выглядит наша обёртка:\n",
      "#include <boost/python.hpp>\n",
      "#include \"some.h\"\n",
      "\n",
      "using namespace boost::python;\n",
      "\n",
      "BOOST_PYTHON_MODULE( example )\n",
      "{\n",
      "    class_<Some>( \"Some\" )\n",
      "        .def( init<int,string>( args( \"some_id\", \"name\" ) ) )\n",
      "        .def( \"ID\", &Some::ID )\n",
      "        .def( \"Name\", &Some::Name, return_value_policy<copy_const_reference>() )\n",
      "        .def( \"ResetID\", static_cast< void (Some::*)() >( &Some::ResetID ) )\n",
      "        .def( \"ResetID\", static_cast< void (Some::*)(int) >( &Some::ResetID ), args( \"some_id\" ) )\n",
      "        .def( \"ChangeName\", &Some::ChangeName, args( \"name\" ) )\n",
      "        .def( \"SomeChanges\", &Some::SomeChanges, args( \"some_id\", \"name\" ) )\n",
      "        .add_static_property( \"NOT_AN_IDENTIFIER\", make_getter( &Some::NOT_AN_IDENTIFIER ) )\n",
      "    ;\n",
      "}\n",
      "\r\n",
      "Если написать несложный тестовый скрипт вроде этого (Python 3.x):\n",
      "from example import *\n",
      "s = Some() \n",
      "print( \"s = Some(); ID: {ID}, Name: {Name}\".format(ID=s.ID(),Name=s.Name()) )\n",
      "s = Some(123,'asd')\n",
      "print( \"s = Some(123,'asd'); ID: {ID}, Name: {Name}\".format(ID=s.ID(),Name=s.Name()) )\n",
      "s.ResetID(234); print(\"s.ResetID(234); ID:\",s.ID())\n",
      "s.ResetID(); print(\"s.ResetID(); ID:\",s.ID())\n",
      "s.ChangeName('qwe'); print(\"s.ChangeName('qwe'); Name:'%s'\" % s.Name())\n",
      "s.SomeChanges(345,'zxc')\n",
      "print( \"s.SomeChanges(345,'zxc'); ID: {ID}, Name: {Name}\".format(ID=s.ID(),Name=s.Name()) )\n",
      "\r\n",
      "Мы увидим вывод:\n",
      "s = Some(); ID: -1, Name: ''\n",
      "s = Some(123,'asd'); ID: 123, Name: 'asd'\n",
      "s.ResetID(234); ID: 234\n",
      "s.ResetID(); ID: -1\n",
      "s.ChangeName('qwe'); Name:'qwe'\n",
      "s.SomeChanges(345,'zxc'); ID: 345, Name: 'zxc'\n",
      "\n",
      "\n",
      "Питонизируем обёртку класса\r\n",
      "Итак, класс со всеми методами обёрнут, но счастья не наступило. При попытке из командной строки Python выполнив Some(123,'asd') мы не увидим описания полей и вообще объекта, поскольку мы не обзавелись методом __repr__, так же как и преобразование к строке, тот же print( Some(123,'asd') ) будет ужасно неинформативен, так как мы не обзавелись методом __str__. Очевидно также, что работать со свойствами через методы на C++ на Python не имеет смысла, это в C++ мы не имеем возможности заводить property, в Python их можно и нужно завести. Однако как же мы навесим методы на готовый класс C++ предназначенные для Python?\r\n",
      "Очень просто: вспоминаем, что в Python методы не отличаются от функций, принимающих первым параметром ссылку на self — экземпляр класса. Заводим в C++ такие функции прямо во wrap.cpp и описываем их как методы в обёртке:\n",
      "...\n",
      "string Some_Str( Some const& );\n",
      "string Some_Repr( Some const& );\n",
      "...\n",
      "BOOST_PYTHON_MODULE( example )\n",
      "{\n",
      "    class_<Some>( \"Some\" )\n",
      "        ...\n",
      "        .def( \"__str__\", Some_Str )\n",
      "        .def( \"__repr__\", Some_Repr )\n",
      "...\n",
      "\r\n",
      "Сами функции можно описать например вот так:\n",
      "string Some_Str( Some const& some )\n",
      "{\n",
      "    stringstream output;\n",
      "    output << \"{ ID: \" << some.ID() << \", Name: '\" << some.Name() << \"' }\";\n",
      "    return output.str();\n",
      "}\n",
      "\n",
      "string Some_Repr( Some const& some )\n",
      "{\n",
      "    return \"Some: \" + Some_Str( some );\n",
      "}\n",
      "\n",
      "\r\n",
      "Со свойствами идентификатора и имени ещё проще, так как методы set и get для них уже описаны в классе:\n",
      "        .add_property( \"some_id\", &Some::ID, static_cast< void (Some::*)(int) >( &Some::ResetID ) )\n",
      "        .add_property( \"name\", make_function( static_cast< string const& (Some::*)() const >( &Some::Name ), return_value_policy<copy_const_reference>() ), &Some::ChangeName )\n",
      "\n",
      "\r\n",
      "При описании свойств однако было два тонких момента:\r\n",
      "1. Для set-метода свойства some_id было явное приведение к типу метода, принимающего int, т.к. есть ещё одна перегрузка метода.\r\n",
      "2. Для get-метода свойства name была использована конструкция boost::python::make_function, которая позволила повесить return_value_policy на результат метода возвращающего константную ссылку на string.\n",
      "\r\n",
      "Выполняем print( Some(123,'asd') ) и просто Some(123,'asd') из командной строки после from example import * и видим что подозрительно похожее на встроенный питоновский dict: { ID: 123, Name: 'asd' }\r\n",
      "Почему бы не завести свойство инициализирующее экземпляр Some от стандартного dict и обратно?\r\n",
      "Заведём ещё пару питонистических функций и заведём свойство as_dict:\n",
      "...\n",
      "dict Some_ToDict( Some const& );\n",
      "void Some_FromDict( Some&, dict const& );\n",
      "...\n",
      "BOOST_PYTHON_MODULE( example )\n",
      "{\n",
      "    class_<Some>( \"Some\" )\n",
      "        ...\n",
      "        .add_property( \"as_dict\", Some_ToDict, Some_FromDict )\n",
      "    ;\n",
      "    ...\n",
      "}\n",
      "...\n",
      "dict Some_ToDict( Some const& some )\n",
      "{\n",
      "    dict res;\n",
      "    res[\"ID\"] = some.ID();\n",
      "    res[\"Name\"] = some.Name();\n",
      "    return res;\n",
      "}\n",
      "\n",
      "void Some_FromDict( Some& some, dict const& src )\n",
      "{\n",
      "    src.has_key( \"ID\" ) ? some.ResetID( extract<int>( src[\"ID\"] ) ) : some.ResetID();\n",
      "    some.ChangeName( src.has_key( \"Name\" ) ? extract<string>( src[\"Name\"] ) : string() );\n",
      "}\n",
      "\r\n",
      "Здесь использован класс boost::python::dict, для доступа на уровне C++ к стандартному dict Python.\r\n",
      "Также есть классы для доступа к str, list, tuple, называются они соответственно. Ведут себя классы в C++ так же как и в Python в плане операторов, вот только возвращают по большей части boost::python::object, из которого требуется ещё извлечь значение через функцию boost::python::extract.\n",
      "\n",
      "В заключение первой части\r\n",
      "В первой части был рассмотрен вполне стандартный класс с конструктором по умолчанию и дефолтным конструктором копирования. Несмотря на некоторые тонкости с работой со строками, и перегрузкой методов, класс вполне стандартный.\r\n",
      "Работать с Boost.Python довольно просто, обёртка любой функции сводится обычно к одной строке, которая выглядит как аналогичное объявление метода в Python.\r\n",
      "В следующей части мы научимся оборачивать классы, которые создаются не так тревиально, создадим класс на основе структуры, обернём enum, познакомимся на практике с другим важным return_value_policy<reference_existing_object>.\r\n",
      "В третьей части рассмотрим конвертеры типов в стандартные типы Python напрямую без обёртки на примере массива байт. Научимся пробрасывать исключения определённого типа из C++ в Python и обратно.\r\n",
      "Тема довольно обширная.\n",
      "\n",
      "Ссылка на проект\r\n",
      "Проект первой части для Windows выложен здесь.\r\n",
      "Проект MSVS v11 настроен на сборку с Python 3.3 x64. Собранные .dll Boost.Python соответствующей версии прилагаются.\r\n",
      "Но ничего не мешает собрать файлы some.h, some.cpp, wrap.cpp любым другим сборочным аппаратом с привязкой к любой другой версии Python.\n",
      "\n",
      "Полезные ссылки\n",
      "Документация Boost.Python\n",
      "Политики возвращаемых по ссылке значений в Boost.Python\n",
      "Начало работы с Boost для Windows\n",
      "Начало работы с Boost для *nix\n",
      "Тонкости сборки Boost.Python    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Некоторое время назад, в силу определенных причин, мне пришла в голову мысль о том, чтобы начать изучать какой-нибудь новый язык программирования. В качестве альтернатив для этого начинания я определил два языка: Java и Python. После продолжительного метания между ними и сопутствующих нытья и долбежки головой о стену (у меня с новыми языками всегда так — сомнения, раздумья, проблема выбора и т.д.), я все-таки остановился на Python. Окей, выбор сделан. Что дальше? А дальше я стал искать материал для изучения…\n",
      "\r\n",
      "Взялся я за это довольно основательно. Ограничиваться документацией и выпущенными печатными изданиями, на мой взгляд, — не кофильфо. Всегда интересно «потрогать руками». Стоит сказать, что для меня, например, большая проблема придумать для себя задачу, причем ту, которую интересно выполнить. Но опять же, перерывать весь Интернет в поиске чего-то, что поможет «загореться» — излишняя трата времени. Поэтому я нашел компромиссное решение — я перерыл Хабр, точнее его хаб «Python». \n",
      "\r\n",
      "Перечень найденного материала ниже. Там не всё — что-то мне было неинтересно, в каких-то статьях я не нашел полезной информации, но большинство статей сгруппированы по категориям, под спойлерами. Категории довольно условные, прошу за это не пинать. Надеюсь, что еще кому-нибудь, помимо меня, этот пост будет полезен. И если вы думаете, стоит ли начать изучение Python или нет — считайте это знаком, что стоит :)\n",
      "\n",
      "ОсновыОсновы Python в кратком изложении: 1 | 2 | 3 | 4 | 5 | 6\n",
      "Основы языка программирования Python за 10 минут\n",
      "Пишем красивый идиоматический Python\n",
      "Be Pythonic\r\n",
      "Code Like a Pythonista: Idiomatic Python: 1 | 2 | 3\n",
      "Python-way. Работа над ошибками\n",
      "Вещи, о которых следует помнить, программируя на Python\n",
      " Python качественно\n",
      "Pythonic\n",
      "Путеводитель по Python. Пишем великолепный код\n",
      "Доклад по Python\n",
      "Уроки Python от компании Google\r\n",
      "Советы Google по кодированию на языке Python: 1 | 2\n",
      "Прочие варианты использования оператора else\n",
      "Онлайн-репетитор по Python\r\n",
      "Регулярные выражения, пособие для новичков: 1 | 2\n",
      "Юникод для чайников\n",
      "Продвинутый уровеньPython изнутри: 1 | 2 | 3 | 4\n",
      "Материалы продвинутого уровня по Питону\r\n",
      "Заметки об объектной системе языка: 1 | 2 | 3\n",
      "Абстрактные классы и интерфейсы\n",
      "Некоторые возможности Python о которых вы возможно не знали\n",
      "Сопрограммы\n",
      "Как устроен namedtuple или динамическое создание типов\n",
      "Итерируем все и вся\n",
      "Why itertools rocks\n",
      "Порядок разрешения методов в Python\n",
      "Руководство по магическим методам в Питоне\n",
      "Ultimate benchmark пяти с половиной способов проверить наличие атрибута объекта\n",
      "Интервально-ассоциативный массив\n",
      "Неочевидное поведение некоторых конструкций\n",
      "Консервация объектов в Python\n",
      "Устранение утечек памяти в приложении на Питоне\n",
      "Memoization в Python\n",
      "Сортировки: key vs cmp\n",
      "Всё, что Вы хотели знать о слайсах\n",
      "Неочевидная оптимизация по скорости при решении конкретной задачи на Python\n",
      "Ускорение кода на Python средствами самого языка\n",
      "Производительность в Python. Легкий путь\n",
      "Пользовательские атрибуты в Python\n",
      "Руководство к дескрипторам\n",
      "Еще немного о дескрипторах в Python\n",
      "Как работает yield\n",
      "Пайпы, the pythonic way\n",
      "Сравнение эффективности способов запуска веб-приложений на языке Python\n",
      "Использование памяти в Python\n",
      "О порядке поиска пакетов и модулей для импорта в Python\n",
      "Организация текучих (fluent) интерфейсов в Python\n",
      "Антипаттерн settings.py\n",
      "Tips & TricksPython-неизвестный\n",
      "6 способов слияния списка списков\r\n",
      "Python: советы, уловки, хаки: 1 | 2 | 3 | 4\n",
      "Консольные хитрости Питон: история команд + автодополнение\n",
      "Python — оптимизация хвостовой рекурсии\n",
      "Устранение Хвостовой рекурсии\n",
      "Python: надежная защита от потери запятой в вертикальном списке строк\n",
      "Сортировка миллиона 32-битных int'ов в 2 мегабайтах памяти на Питоне\n",
      "Размышления о PythonPython, философия дизайна — Guido van Rossum: 1 | 2\n",
      "Мысли о Python 3\n",
      "Перестаньте писать классы\n",
      "Всё о декораторахПонимаем декораторы в Python'e, шаг за шагом. Шаг 1 и Шаг 2\n",
      "Добавляем чуть больше рефлексии: декораторы\n",
      "Сила и красота декораторов\n",
      "Python: декорируем декораторы. Снова\n",
      "Декоратор cached_property\n",
      "Тестирование и отладкаТестирование. Начало\n",
      "Полное покрытие кода\n",
      "Генерация юнит-тестов\n",
      "Непрерывное тестирование питонопроекта\r\n",
      "TextTest — кроссплатформенный фреймворк на python для тестирования GUI и не только: 1 | 2\n",
      "Модуль Mock: макеты-пустышки в тестировании\n",
      "pdb – Интерактивный отладчик\r\n",
      "Профилирование и отладка Python: 1 | 2 | 3 | 4\n",
      "Параллельный Python и многопоточныйПараллельный Питон, начало\n",
      "Параллельное программирование в Python при помощи multiprocessing и shared array\n",
      "Основы работы с потоками в языке Python\n",
      "Еще раз о многопоточности и Python\n",
      "Python threading или GIL нам почти не помеха\n",
      "Python и Twisted — Заметки о параллельной обработке данных\n",
      "Учимся писать многопоточные и многопроцессные приложения на Python\n",
      "Как устроен GIL в Python\n",
      "Stackless Python и Concurrence\n",
      "Конкурентность в асинхронном приложении на примере twisted\n",
      "ИнтерфейсыPyQt:\r\n",
      "Цикл статей о PyQt4 #1: 1 | 2 | 3 | 4 | 5\r\n",
      "Цикл статей о PyQt4 #2: Посиделка первая | Посиделка вторая\n",
      "PyQt: простая работа с потоками\n",
      "Реализация паттерна MVC для PyQt\n",
      "Написание приложений, основаных на Qt, на языке Python\n",
      "\n",
      "WxPython:\n",
      "Сказ о wx.Python\n",
      "Знакомство с wxPython\r\n",
      "Пример использования WxPython для создания нодового интерфейса: 1 | 2 | 3 | 4 | 5\n",
      "Потоки в wxPython\n",
      "\n",
      "PyGTK:\n",
      "Компоновка — начало начал\n",
      "Ещё немного о компоновке и виджетах\n",
      "Прогресбар и нити в PyGTK\n",
      "PyGTK: потоки и магия обёрток\n",
      "Введение в pygtk/gtkbuilder: пишем калькулятор\n",
      "\n",
      "Tkinter:\n",
      "Введение в Tkinter\n",
      "Рисование графиков. Python. Tkinter\n",
      "Функциональный PythonФункциональное программирование для землян — функции\n",
      "Функциональное программирование для землян — списки\n",
      "Откуда идут «функциональные» корни Python\n",
      "Ненормальное функциональное программирование на Python\n",
      "Не ещё одна статья о функциональном программировании\n",
      "Для тех, кто хочет странного: монады в Python\n",
      "Монады в Python поподробнее\n",
      "Абстрагирование потока управления\n",
      "Правильное абстрагирование потока управления\n",
      "Паттерны проектирования без ООП\n",
      "МетапрограммированиеМетаклассы в Python\n",
      "Использование метаклассов в Python\n",
      "«Наследование» не от классов\n",
      "Заметки о метапрограммировании в Python\n",
      "Python и редакторыVim: \r\n",
      "Vim и Python: Vim+Python. Для начинающих или Настраиваем Vim под Python\r\n",
      "+ Django:Настройка Vim для работы с Python и Django или VIM, Django… Django, VIM\n",
      "\n",
      "Emacs:\r\n",
      "Цикл Emacs и Python: 1 | 2\n",
      "\n",
      "Sublime Text 2:\n",
      "Пишем простой плагин для Sublime Text 2\n",
      "\n",
      "Geany:\n",
      "Настройка и использование Geany с Python\n",
      "\n",
      "Notepad++\n",
      "Скрипт для Notepad++ на Python\n",
      "Различный практикумAPI:\r\n",
      "В основном API различных социальных сетей и сервисов.\n",
      "Пишем себе немного OpenID-авторизации\n",
      "Экспорт друзей VK в Google Contacts\r\n",
      "Скачивание музыки из VK: 1 | 2\r\n",
      "Twitter + VK: 1 | 2\n",
      "Клиент для SOAP API Почты России\n",
      "Jabber-to-Evernote Gateway изнутри\n",
      "Скрипт проверки наличия свободных дат в посольстве\n",
      "Flightstats API: Пишем свое табло прилетов с Боингами и Аэробусами\n",
      "Получение любимых аудиозаписей с pandora.com\n",
      "Bing + Python, поиск изображений\n",
      "\n",
      "Чат:\n",
      "Программа-мечта начинающего питоновода\n",
      "WebSocket-чат на Tornado для вашего Django-проекта\n",
      "Yet another python Chat client\n",
      "\n",
      "Боты:\n",
      "ICQ бот\n",
      "Простой Twitter-бот\n",
      "Не совсем обычный XMPP-бот: туннелирование\n",
      "Пишем бота для игры «Найди отличие»\n",
      "Skype-бот для голосовых конференций\n",
      "Пишем бота для Twitter на основе GitHub API \n",
      "\n",
      "Сокеты, серверы:\n",
      "Сокеты в Python для начинающих\n",
      "Асинхронный удар\n",
      "Асинхронный http-клиент, или почему многопоточность — лишнее\n",
      "Как применять Linux epoll в Python\n",
      "Сервер на python для websockets\n",
      "Inbox.py: самый простой SMTP-сервер\n",
      "Реализация небольшого асинхронного сервера\n",
      "FTP сервер с авторизацией через базу данных\n",
      "\n",
      "Система:\n",
      "Мониторинг за изменениями файловой системы\n",
      "Python на примере демона уведомления о новых коммитах Git\n",
      "Git. Автоматическая проверка сообщения коммита на стороне сервера с помощью Python\n",
      "Скрипт для рекурсивного сравнения директорий\n",
      "Python для системных администраторов\n",
      "Gnome Applets. Введение\n",
      "Пишем апплет для GNOME\n",
      "Пишем апплет-переводчик для Gnome\n",
      "Totem Gnome Applet\n",
      "Переименование процессов в Python\n",
      "Как написать дополнение для GIMP на языке Python\n",
      "KDE4 + Python. Плазма-змей\n",
      "KDE4 Plasma Desktop. Создание плазмоида\n",
      "Whois: практическое руководство пользователя\n",
      "Симуляция нажатий Home, End, PgUp, PgDown\n",
      "Демон для удаленного управления компьютером через e-mail\n",
      "\n",
      "Работа с консолью:\n",
      "Простой консольный прогрессбар на питоне\n",
      "Пишем консольный переводчик для *nix на Python\n",
      "\n",
      "Парсинг, обработка текста:\n",
      "Первые шаги в программировании на Python\n",
      "Пишем свой шаблонизатор на Python\n",
      "Как я учился работать с XML\n",
      "Кузявые ли бутявки, т.е. пишем морфологический анализатор\n",
      "Обработка Excel файлов с использованием Python\n",
      "Социальный граф хабрасообщества\n",
      "Regexp и Python: извлечение токенов из текста\n",
      "Автоматизируем проверку трекинга почты России\n",
      "Доставка свежей прессы с помощью Python прямо в почтовый ящик\n",
      "Разбор кода и построение синтаксических деревьев с PLY. Основы\n",
      "Частотный анализатор английских слов\n",
      "Grab — python библиотека для парсинга сайтов\n",
      "Документация по Grab — библиотеке для парсинга сайтов\n",
      "Парсинг на Pуthon. Как собрать архив Голубятен\n",
      "Генерим PDF бочками\n",
      "\n",
      "Работа с изображениями:\n",
      "Играемся с изображениями в Python\n",
      "Получаем тип и размеры изображения без скачивания его целиком\n",
      "Решение проблемы оперативного ресайза изображений\n",
      "\n",
      "Геймдев:\r\n",
      "Пишем платформер на Python, используя pygame: 1 | 2.1 | 2.2\n",
      "Игра Жизнь на Python\n",
      "Blender 2.49b + Python 2.6 – используем клавиатуру в своей игре\n",
      "Blender 2.6 + Python 3.2 – задействуем устройства ввода в собственной игре\n",
      "\n",
      "Python + Django:\n",
      "Заметки для построения эффективных Django-ORM запросов в нагруженных проектах\n",
      "Django ORM, gevent и грабли в зелени\n",
      "Хостинг картинок за полчаса\n",
      "Фотогалерея на Django с использованием Google Picasa в качестве хостинга\n",
      "Сумбурные заметки про python и django\n",
      "Пишем backend для мобильного приложения за несколько минут\n",
      "Капча с помощью PIL или практический велосипед\n",
      "\n",
      "Flask:\r\n",
      "Мега-Учебник Flask: 1 | 2 | 3 | 4\n",
      "Создание RESTful API в Google App Engine на основе Flask\n",
      "\n",
      "Алгоритмы и структуры данных, ИИ, анализ данных:\n",
      "Декодирование капчи на Python\n",
      "Распознавание некоторых современных CAPTCHA\n",
      "Задачка о восьми ферзях\n",
      "Реализация графов и деревьев на Python\n",
      "Поиск в строке. Реализация в CPython\n",
      "Анализ рынка ноутбуков с помощью Python\n",
      "Введение в анализ текстовой информации с помощью Python и методов машинного обучения\n",
      "Введение в анализ данных с помощью Pandas\n",
      "Шифр Виженера. Разбор алгоритма на Python\n",
      "Латентно-семантический анализ и поиск на python\n",
      "Необыкновенный способ генерации лабиринтов\n",
      "PyBrain работаем с нейронными сетями на Python\n",
      "Простой классификатор на PyBrain и PyQt4\n",
      "Обучаем компьютер чувствам (sentiment analysis по-русски)\n",
      "Определение части речи слов в русском тексте (POS-tagging)\n",
      "Частотный анализатор английских слов\n",
      "\n",
      "Вне категории:\n",
      "Реализация кеша с ограничением по числу элементов на Python — решения: простое и посложнее\n",
      "Случайные числа из звуковой карты\n",
      "Пишем модуль расширения для Питона на C\n",
      "Создаем симулятор солнечной системы\n",
      "Взламываем шифры с Python\n",
      "История одной оптимизации\n",
      "Кодим безумный пассивный сниффер в виде модуля для Python\n",
      "Рисуем волну .wav-файла\n",
      "Tilt-Shift фотографии своими руками\n",
      "Программный захват с вебкамеры\n",
      "Распознаём изображение с токена при помощи камеры\n",
      "Простой интерпретатор с нуля на Python\n",
      "Руководство: пишем интерпретатор с JIT на PyPy\n",
      "Играемся с гироскопом ноутбука thinkpad в linux\n",
      "\r\n",
      "P.S. Если есть какие-то предложения о добавлении сюда ссылки на материал — прошу в личку. По поводу опечаток и чего-нибудь этакое — туда же.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Недавно мы писали о забавных, хитрых и странных примерах на JavaScript. Теперь пришла очередь Python. У Python, высокоуровневого и интерпретируемого языка, много удобных свойств. Но иногда результат работы некоторых кусков кода на первый взгляд выглядит неочевидным.\n",
      "Ниже — забавный проект, в котором собраны примеры неожиданного поведения в Python с обсуждением того, что происходит под капотом. Часть примеров не относятся к категории настоящих WTF?!, но зато они демонстрируют интересные особенности языка, которых вы можете захотеть избегать. Я думаю, это хороший способ изучить внутреннюю работу Python, и надеюсь, вам будет интересно.\n",
      "Если вы уже опытный программист на Python, то многие примеры могут быть вам знакомы и даже вызовут ностальгию по тем случаям, когда вы ломали над ними голову :)\n",
      "Содержание\n",
      "\n",
      "Структура примеров\n",
      "Использование\n",
      "Примеры\n",
      "\n",
      "Пропуск строк? \n",
      "Ну, как-то сомнительно... \n",
      "Время для хеш-пирожных!\n",
      "Несоответствие времени обработки\n",
      "Преобразование словаря во время его итерирования\n",
      "Удаление элемента списка во время его итерирования\n",
      "Обратные слеши в конце строки\n",
      "Давайте сделаем гигантскую строку!\n",
      "Оптимизации интерпретатора конкатенации строк\n",
      "Да, оно существует! \n",
      "is не то, что оно есть\n",
      "is not ... отличается от is (not ...) \n",
      "Функция внутри цикла выдаёт один и тот же результат\n",
      "Утечка переменных цикла из локальной области видимости\n",
      "Крестики-нолики, где Х побеждает с первой попытки\n",
      "Опасайтесь изменяемых аргументов по умолчанию\n",
      "Те же операнды, но другая история\n",
      "Изменение неизменяемого\n",
      "Использование переменной, не определённой в области видимости\n",
      "Исчезновение переменной из внешней области видимости\n",
      "Return возвращает везде\n",
      "Когда True на самом деле False \n",
      "Будьте осторожны с цепочками операций\n",
      "Разрешение имён (name resolution) игнорирует область видимости класса\n",
      "От заполненности до None в одной инструкции \n",
      "Явное приведение типов в строковых значениях\n",
      "Атрибуты классов и экземпляров\n",
      "Ловля исключений\n",
      "Полночь не существует?\n",
      "Что не так с булевыми значениями?\n",
      "Игла в стоге сена\n",
      "For что? \n",
      "Узел not \n",
      "А вы могли такое предположить?\n",
      "Мелкие примеры\n",
      "\n",
      "Внести свой вклад \n",
      "Полезные ссылки\n",
      "\n",
      "Структура примеров\n",
      "Примечание: Все приведённые примеры протестированы на интерактивном интерпретаторе Python 3.5.2 и должны работать во всех версиях языка, если иное явно не указано в описании.\n",
      "Структура примеров:\n",
      "Какой-нибудь дурацкий заголовок\n",
      "# Код.\n",
      "# Подготовка к магии...\n",
      "Результат (версия Python):\n",
      ">>> инициирующее_выражение\n",
      "Вероятно, неожиданный результат\n",
      "(Опционально): Однострочное описание неожиданного результата.\n",
      "Объяснение:\n",
      "\n",
      "Краткое объяснение того, что произошло и почему. \n",
      "Поясняющие примеры (если необходимо)\n",
      "Результат: \n",
      ">>> инициирование # какого-то примера, срывающего покровы с магии\n",
      "# объясняющий результат\n",
      "Использование\n",
      "Мне кажется, что лучший способ извлечь максимальную пользу из этих примеров — это читать их в хронологическом порядке:\n",
      "\n",
      "Внимательно изучать начальный код. Если вы опытный Python-программист, то чаще всего будете успешно предсказывать, что произойдёт. \n",
      "\n",
      "Изучать результаты и:\n",
      "\n",
      "Проверять, совпали ли они с вашими ожиданиями. \n",
      "Убеждаться, что вы понимаете, почему получен именно такой результат. \n",
      "Если не понимаете, то читайте объяснение (если всё равно не понимаете, то заорите и напишите здесь). \n",
      "Если понимаете, то погладьте себя по головке и переходите к следующему примеру. \n",
      "\n",
      "\n",
      "\n",
      "P. S. Также можете читать эти примеры в командной строке. Только сначала установите npm-пакет wtfpython,\n",
      "$ npm install -g wtfpython\n",
      "Теперь запустите wtfpython в командной строке, и в результате эта коллекция откроется в вашем $PAGER.\n",
      "#TODO: Добавьте пакет pypi для чтения в командной строке.\n",
      "Примеры\n",
      "Пропуск строк?\n",
      "Результат:\n",
      ">>> value = 11\n",
      ">>> valuе = 32\n",
      ">>> value\n",
      "11\n",
      "Wat?\n",
      "Примечание: проще всего воспроизвести этот пример с помощью копирования и вставки в ваш файл/оболочку.\n",
      "Объяснение\n",
      "Некоторые Unicode-символы выглядят так же, как и ASCII, но различаются интерпретатором.\n",
      ">>> value = 42 #ascii e\n",
      ">>> valuе = 23 #cyrillic e, Python 2.x interpreter would raise a `SyntaxError` here\n",
      ">>> value\n",
      "42\n",
      "Ну, как-то сомнительно...\n",
      "def square(x):\n",
      "    \"\"\"\n",
      "    Простая функция для вычисления квадрата числа путём сложения.\n",
      "    \"\"\"\n",
      "    sum_so_far = 0\n",
      "    for counter in range(x):\n",
      "        sum_so_far = sum_so_far + x\n",
      "  return sum_so_far\n",
      "Результат (Python 2.x):\n",
      ">>> square(10)\n",
      "10\n",
      "Разве должно было получиться не 100?\n",
      "Примечание: если не можете воспроизвести результат, попробуйте запустить в оболочке файл mixed_tabs_and_spaces.py.\n",
      "Объяснение\n",
      "\n",
      "Не смешивайте табуляцию и пробелы! Символ, предшествующий return, это табуляция, он распознаётся как четыре пробела.\n",
      "Вот как Python обрабатывает табуляции:\n",
      "Сначала они заменяются (слева направо) пробелами, от одного до восьми, так что общее количество заменяемых символов может быть в восемь раз больше...\n",
      "Поэтому табуляция в последней строке функции square заменяется восемью пробелами и попадает в цикл.\n",
      "Python 3 в таких случаях умеет автоматически кидать ошибку. \n",
      "\n",
      "Результат (Python 3.x):\n",
      "TabError: inconsistent use of tabs and spaces in indentation\n",
      "Время для хеш-пирожных!\n",
      "1.\n",
      "some_dict = {}\n",
      "some_dict[5.5] = \"Ruby\"\n",
      "some_dict[5.0] = \"JavaScript\"\n",
      "some_dict[5] = \"Python\"\n",
      "Результат:\n",
      ">>> some_dict[5.5]\n",
      "\"Ruby\"\n",
      ">>> some_dict[5.0]\n",
      "\"Python\"\n",
      ">>> some_dict[5]\n",
      "\"Python\"\n",
      "Python уничтожил существование JavaScript?\n",
      "Объяснение\n",
      "\n",
      "Словари в Python проверяют эквивалентность и сравнивают значение хешей, чтобы определить, одинаковы ли два ключа. \n",
      "\n",
      "Неизменяемые объекты с одинаковыми значениями в Python всегда получают одинаковые хеши.\n",
      ">>> 5 == 5.0\n",
      "True\n",
      ">>> hash(5) == hash(5.0)\n",
      "True\n",
      "Примечание: объекты с разными значениями тоже могут получить одинаковые хеши (такая ситуация называется хеш-коллизией). \n",
      "\n",
      "При выполнении выражения some_dict[5] = \"Python\" существующее выражение «JavaScript» переписывается на «Python», потому что Python распознаёт 5 и 5.0 как одинаковые ключи словаря some_dict. \n",
      "\n",
      "На StackOverflow прекрасно объясняется причина такого поведения. \n",
      "\n",
      "\n",
      "Несоответствие времени обработки\n",
      "array = [1, 8, 15]\n",
      "g = (x for x in array if array.count(x) > 0)\n",
      "array = [2, 8, 22]\n",
      "Результат:\n",
      ">>> print(list(g))\n",
      "[8]\n",
      "Объяснение\n",
      "\n",
      "В выражении генератора клауза in обрабатывается во время объявления, а условная клауза — во время run time. \n",
      "Поэтому перед run time выполняется переприсваивание array к списку [2, 8, 22], а поскольку из 1, 8 и 15 только значение счётчика 8 больше 0, то генератор выдаёт только 8. \n",
      "\n",
      "Преобразование словаря во время его итерирования\n",
      "x = {0: None}\n",
      "\n",
      "for i in x:\n",
      "    del x[i]\n",
      "    x[i+1] = None\n",
      "    print(i)\n",
      "Результат:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Да, выполняется ровно восемь раз и останавливается.\n",
      "Объяснение:\n",
      "\n",
      "В языке не поддерживается возможность итерирования словаря, который вы редактируете. \n",
      "Выполняется восемь раз потому, что в этом месте словарь увеличивается, чтобы вмещать больше ключей (у нас восемь записей удаления, так что нужно менять размер словаря). Это особенности реализации. \n",
      "Аналогичный пример разбирается на StackOverflow. \n",
      "\n",
      "Удаление элемента списка во время его итерирования\n",
      "list_1 = [1, 2, 3, 4]\n",
      "list_2 = [1, 2, 3, 4]\n",
      "list_3 = [1, 2, 3, 4]\n",
      "list_4 = [1, 2, 3, 4]\n",
      "\n",
      "for idx, item in enumerate(list_1):\n",
      "    del item\n",
      "\n",
      "for idx, item in enumerate(list_2):\n",
      "    list_2.remove(item)\n",
      "\n",
      "for idx, item in enumerate(list_3[:]):\n",
      "    list_3.remove(item)\n",
      "\n",
      "for idx, item in enumerate(list_4):\n",
      "    list_4.pop(idx)\n",
      "Результат:\n",
      ">>> list_1\n",
      "[1, 2, 3, 4]\n",
      ">>> list_2\n",
      "[2, 4]\n",
      ">>> list_3\n",
      "[]\n",
      ">>> list_4\n",
      "[2, 4]\n",
      "Знаете, почему получился результат [2, 4]?\n",
      "Объяснение:\n",
      "\n",
      "Менять объект во время его итерирования — всегда плохая идея. Лучше тогда итерировать копию объекта, что и делает list_3[:].\n",
      ">>> some_list = [1, 2, 3, 4]\n",
      ">>> id(some_list)\n",
      "139798789457608\n",
      ">>> id(some_list[:]) # Notice that python creates new object for sliced list.\n",
      "139798779601192\n",
      "Разница между del, remove и pop:\n",
      "\n",
      "del var_name просто убирает привязку var_name локального или глобального пространства имён (поэтому list_1 остаётся незатронутым). \n",
      "\n",
      "remove убирает первое совпадающее значение, а не конкретный индекс, вызывая ValueError при отсутствии значения. \n",
      "\n",
      "pop убирает элемент с конкретным индексом и возвращает его, вызывая IndexError, если задан неверный индекс. \n",
      "\n",
      "\n",
      "Почему получилось [2, 4]?\n",
      "\n",
      "Список итерируется индекс за индексом, и когда мы убираем 1 из list_2 или list_4, то содержимым списков становится [2, 3, 4]. Оставшиеся сдвигаются вниз, то есть 2 оказывается на индексе 0, 3 — на индексе 1. Поскольку следующая итерация будет выполняться применительно к индексу 1 (где у нас 3), 2 окажется пропущена. То же самое произойдёт с каждым вторым элементом в списке. Похожий пример, связанный со словарями в Python, прекрасно объяснён на StackOverflow. \n",
      "\n",
      "Обратные слеши в конце строки\n",
      "Результат:\n",
      ">>> print(\"\\\\ some string \\\\\")\n",
      ">>> print(r\"\\ some string\")\n",
      ">>> print(r\"\\ some string \\\")\n",
      "\n",
      "    File \"<stdin>\", line 1\n",
      "      print(r\"\\ some string \\\")\n",
      "                             ^\n",
      "SyntaxError: EOL while scanning string literal\n",
      "Объяснение\n",
      "\n",
      "В необработанном строковом литерале (raw string literal), на что указывает префикс r, обратный слеш не имеет особого значения. \n",
      "Но интерпретатор меняет поведение обратных слешей, в результате они и последующий символ просто пропускаются. Поэтому обратные слеши в конце необработанных строк не действуют.\n",
      "\n",
      "Давайте сделаем гигантскую строку!\n",
      "Это вовсе не WTF, а лишь некоторые прикольные вещи, и их нужно опасаться :)\n",
      "def add_string_with_plus(iters):\n",
      "    s = \"\"\n",
      "    for i in range(iters):\n",
      "        s += \"xyz\"\n",
      "    assert len(s) == 3*iters\n",
      "\n",
      "def add_string_with_format(iters):\n",
      "    fs = \"{}\"*iters\n",
      "    s = fs.format(*([\"xyz\"]*iters))\n",
      "    assert len(s) == 3*iters\n",
      "\n",
      "def add_string_with_join(iters):\n",
      "    l = []\n",
      "    for i in range(iters):\n",
      "        l.append(\"xyz\")\n",
      "    s = \"\".join(l)\n",
      "    assert len(s) == 3*iters\n",
      "\n",
      "def convert_list_to_string(l, iters):\n",
      "    s = \"\".join(l)\n",
      "    assert len(s) == 3*iters\n",
      "Результат:\n",
      ">>> timeit(add_string_with_plus(10000))\n",
      "100 loops, best of 3: 9.73 ms per loop\n",
      ">>> timeit(add_string_with_format(10000))\n",
      "100 loops, best of 3: 5.47 ms per loop\n",
      ">>> timeit(add_string_with_join(10000))\n",
      "100 loops, best of 3: 10.1 ms per loop\n",
      ">>> l = [\"xyz\"]*10000\n",
      ">>> timeit(convert_list_to_string(l, 10000))\n",
      "10000 loops, best of 3: 75.3 µs per loop\n",
      "Объяснение\n",
      "\n",
      "Можете подробнее почитать про timeit. Обычно с её помощью измеряют, как долго выполняются фрагменты кода. \n",
      "Не используйте + для генерирования длинных строк: в Python str — неизменяемая, поэтому для каждой пары конкатенаций левая и правая строки должны быть скопированы в новую строку. Если вы конкатенируете четыре строки длиной по 10 символов, то копируйте (10 + 10) + ((10 + 10) + 10) + (((10 + 10) +10) +10) = 90 символов вместо 40. По мере увеличения количества и размера строк ситуация вчетверо ухудшается. \n",
      "Поэтому рекомендуется использовать синтаксис .format. или % (но на коротких строках это работает чуть медленнее, чем +). \n",
      "А если ваш контент уже доступен в виде итерируемого объекта, то лучше выбирать гораздо более быстрое ''.join(iterable_object). \n",
      "\n",
      "Оптимизации интерпретатора конкатенации строк\n",
      ">>> a = \"some_string\"\n",
      ">>> id(a)\n",
      "140420665652016\n",
      ">>> id(\"some\" + \"_\" + \"string\") # Notice that both the ids are same.\n",
      "140420665652016\n",
      "# using \"+\", three strings:\n",
      ">>> timeit.timeit(\"s1 = s1 + s2 + s3\", setup=\"s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000\", number=100)\n",
      "0.25748300552368164\n",
      "# using \"+=\", three strings:\n",
      ">>> timeit.timeit(\"s1 += s2 + s3\", setup=\"s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000\", number=100)\n",
      "0.012188911437988281\n",
      "Объяснение:\n",
      "\n",
      "+= быстрее + более чем двух строк, потому что первая строка (например, s1 для s1 += s2 + s3) не уничтожается, пока строка не будет обработана целиком. \n",
      "Обе строки ссылаются на один объект, потому что оптимизация CPython в некоторых случаях старается использовать существующие неизменяемые объекты (особенность реализации), а не создавать каждый раз новые. Почитать подробнее.\n",
      "\n",
      "Да, оно существует!\n",
      "Клауза else для циклов. Типичный пример:\n",
      "  def does_exists_num(l, to_find):\n",
      "      for num in l:\n",
      "          if num == to_find:\n",
      "              print(\"Exists!\")\n",
      "              break\n",
      "      else:\n",
      "          print(\"Does not exist\")\n",
      "Результат:\n",
      ">>> some_list = [1, 2, 3, 4, 5]\n",
      ">>> does_exists_num(some_list, 4)\n",
      "Существует!\n",
      ">>> does_exists_num(some_list, -1)\n",
      "Не существует.\n",
      "Клауза else в обработке исключений. Пример:\n",
      "try:\n",
      "    pass\n",
      "except:\n",
      "    print(\"Exception occurred!!!\")\n",
      "else:\n",
      "    print(\"Try block executed successfully...\")\n",
      "Результат:\n",
      "Try block executed successfully...\n",
      "Объяснение:\n",
      "\n",
      "Клауза else исполняется после цикла только тогда, когда после всех итераций нет явного break. \n",
      "Клауза else после блока try также называется клаузой завершения (completion clause), поскольку доступность else в выражении try означает, что блок try успешно завершён. \n",
      "\n",
      "is не то, что оно есть\n",
      "Этот пример очень широко известен.\n",
      ">>> a = 256\n",
      ">>> b = 256\n",
      ">>> a is b\n",
      "True\n",
      "\n",
      ">>> a = 257\n",
      ">>> b = 257\n",
      ">>> a is b\n",
      "False\n",
      "\n",
      ">>> a = 257; b = 257\n",
      ">>> a is b\n",
      "True\n",
      "Объяснение:\n",
      "Разница между is и ==\n",
      "\n",
      "Оператор is проверяет, чтобы оба операнда ссылались на один объект (т. е. проверяет, идентичны ли они друг другу). \n",
      "Оператор == сравнивает значения операндов и проверяет на идентичность. \n",
      "Так что is используется для эквивалентности ссылок, а == — для эквивалентности значений. Поясняющий пример:\n",
      "\n",
      ">>> [] == []\n",
      "True\n",
      ">>> [] is [] # These are two empty lists at two different memory locations.\n",
      "False\n",
      "256 — существующий объект, а 257 — нет\r\n",
      "При запуске Python в памяти размещаются числа от -5 до 256. Они используются часто, так что целесообразно держать их наготове.\n",
      "Цитата из https://docs.python.org/3/c-api/long.html\n",
      "В текущей реализации поддерживается массив целочисленных объектов для всех чисел с –5 по 256, так что когда вы создаёте int из этого диапазона, то получаете ссылку на существующий объект. Поэтому должна быть возможность изменить значение на 1. Но подозреваю, что в этом случае поведение Python будет непредсказуемым. :-)\n",
      ">>> id(256)\n",
      "10922528\n",
      ">>> a = 256\n",
      ">>> b = 256\n",
      ">>> id(a)\n",
      "10922528\n",
      ">>> id(b)\n",
      "10922528\n",
      ">>> id(257)\n",
      "140084850247312\n",
      ">>> x = 257\n",
      ">>> y = 257\n",
      ">>> id(x)\n",
      "140084850247440\n",
      ">>> id(y)\n",
      "140084850247344\n",
      "Интерпретатор оказался не так умён, и во время исполнения y = 257 не понял, что мы уже создали целое число со значением 257, поэтому создаёт в памяти другой объект.\n",
      "a и b ссылаются на один объект при инициализации с одинаковым значением в одной строке.\n",
      ">>> a, b = 257, 257\n",
      ">>> id(a)\n",
      "140640774013296\n",
      ">>> id(b)\n",
      "140640774013296\n",
      ">>> a = 257\n",
      ">>> b = 257\n",
      ">>> id(a)\n",
      "140640774013392\n",
      ">>> id(b)\n",
      "140640774013488\n",
      "\n",
      "Когда в одной строке a и b присваивается значение 257, интерпретатор Python создаёт новый объект, в то же время делая на него ссылку из второй переменной. Если же присвоить значения в разных строках, то интерпретатор не будет «знать», что у нас уже есть 257 в виде объекта. \n",
      "Это оптимизация компилятора, специфически применяемая к интерактивному окружению. Когда вы вводите в работающий интерпретатор две строки, они компилируются, а значит, и оптимизируются раздельно. Если попробуете прогнать этот пример в файле .py, то не увидите такого поведения, потому что файл компилируется за раз. \n",
      "\n",
      "is not ... отличается от is (not ...)\n",
      ">>> 'something' is not None\n",
      "True\n",
      ">>> 'something' is (not None)\n",
      "False\n",
      "Объяснение\n",
      "\n",
      "is not — это одиночный бинарный оператор, поведение которого отличается от ситуации, когда по отдельности используются is и not. \n",
      "is not выдаёт False, если переменные с обеих сторон оператора указывают на один объект. В противном случае выдаётся True. \n",
      "\n",
      "Функция внутри цикла выдает один и тот же результат\n",
      "funcs = []\n",
      "results = []\n",
      "for x in range(7):\n",
      "    def some_func():\n",
      "        return x\n",
      "    funcs.append(some_func)\n",
      "    results.append(some_func())\n",
      "\n",
      "funcs_results = [func() for func in funcs]\n",
      "Результат:\n",
      ">>> results\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      ">>> funcs_results\n",
      "[6, 6, 6, 6, 6, 6, 6]\n",
      "Если до добавления some_func в funcs значения x в каждой итерации были разными, все функции возвращали 6.\n",
      "//OR\n",
      ">>> powers_of_x = [lambda x: x**i for i in range(10)]\n",
      ">>> [f(2) for f in powers_of_x]\n",
      "[512, 512, 512, 512, 512, 512, 512, 512, 512, 512]\n",
      "Объяснение\n",
      "\n",
      "При определении функции в цикле, в теле которого используется переменная цикла, замыкание функции цикла привязано к переменной, а не к её значению. Так что все функции для вычисления используют последнее значение, присвоенное переменной.\n",
      "Чтобы получить желаемое поведение, вы можете передавать в функцию переменную цикла в качестве именованной переменной. Почему это работает? Потому что таким образом переменная снова будет определена в области видимости функции.\n",
      "\n",
      "funcs = []\n",
      "for x in range(7):\n",
      "    def some_func(x=x):\n",
      "        return x\n",
      "    funcs.append(some_func)\n",
      "Результат:\n",
      ">>> funcs_results = [func() for func in funcs]\n",
      ">>> funcs_results\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "Утечка переменных цикла из локальной области видимости\n",
      "1.\n",
      "for x in range(7):\n",
      "    if x == 6:\n",
      "        print(x, ': for x inside loop')\n",
      "print(x, ': x in global')\n",
      "Результат:\n",
      "6 : for x inside loop\n",
      "6 : x in global\n",
      "Но x не был определён для цикла вне области видимости.\n",
      "2.\n",
      "# This time let's initialize x first\n",
      "x = -1\n",
      "for x in range(7):\n",
      "    if x == 6:\n",
      "        print(x, ': for x inside loop')\n",
      "print(x, ': x in global')\n",
      "Результат:\n",
      "6 : for x inside loop\n",
      "6 : x in global\n",
      "3.\n",
      "x = 1\n",
      "print([x for x in range(5)])\n",
      "print(x, ': x in global')\n",
      "Результат (on Python 2.x):\n",
      "[0, 1, 2, 3, 4]\n",
      "(4, ': x in global')\n",
      "Результат (on Python 3.x):\n",
      "[0, 1, 2, 3, 4]\n",
      "1 : x in global\n",
      "Объяснение\n",
      "\n",
      "В Python циклы for используют то пространство видимости, в котором они существуют, не заботясь о своих определённых переменных цикла. Это относится и к ситуации, если мы до этого явно определили переменную цикла for в глобальном пространстве имён. Тогда она будет перепривязана к существующей переменной.\n",
      "Разница в результатах работы интерпретаторов Python 2.x и Python 3.x применительно к примеру с генерированием списков (list comprehension) может быть объяснена с помощью изменения, описанного в документации What’s New In Python 3.0:\n",
      "«Для генерирования списков больше не поддерживается синтаксическая форма [... for var in item1, item2, ...]. Используйте вместо неё [... for var in (item1, item2, ...)]. Также обратите внимание, что генерирования списков имеют разные семантики: они ближе к синтаксическому сахару применительно к генерирующему выражению внутри конструктора list(), и, в частности, переменные управления циклом больше не утекают в окружающую область видимости».\n",
      "\n",
      "\n",
      "Крестики-нолики, где Х побеждает с первой попытки\n",
      "# Let's initialize a row\n",
      "row = [\"\"]*3 #row i['', '', '']\n",
      "# Let's make a board\n",
      "board = [row]*3\n",
      "Результат:\n",
      ">>> board\n",
      "[['', '', ''], ['', '', ''], ['', '', '']]\n",
      ">>> board[0]\n",
      "['', '', '']\n",
      ">>> board[0][0]\n",
      "''\n",
      ">>> board[0][0] = \"X\"\n",
      ">>> board\n",
      "[['X', '', ''], ['X', '', ''], ['X', '', '']]\n",
      "Но мы же не присваивали три X, верно?\n",
      "Объяснение\n",
      "Эта визуализация объясняет, что происходит в памяти при инициализации переменной row:\n",
      "\n",
      "А когда посредством умножения row инициализируется board, то в памяти происходит вот что (каждый из элементов board[0], board[1] и board[2] является ссылкой на один и тот же список, указанный в row):\n",
      "\n",
      "Опасайтесь изменяемых аргументов по умолчанию\n",
      "def some_func(default_arg=[]):\n",
      "    default_arg.append(\"some_string\")\n",
      "    return default_arg\n",
      "Результат:\n",
      ">>> some_func()\n",
      "['some_string']\n",
      ">>> some_func()\n",
      "['some_string', 'some_string']\n",
      ">>> some_func([])\n",
      "['some_string']\n",
      ">>> some_func()\n",
      "['some_string', 'some_string', 'some_string']\n",
      "Объяснение\n",
      "\n",
      "В Python изменяемые аргументы по умолчанию на самом деле не инициализируются при каждом вызове функции. Вместо этого в качестве значения по умолчанию берётся недавно присвоенное значение. Когда мы явным образом передали [] в качестве аргумента в some_func, то для переменной default_arg не было использовано значение по умолчанию, поэтому функция вернула то, что ожидалось.\n",
      "\n",
      "def some_func(default_arg=[]):\n",
      "    default_arg.append(\"some_string\")\n",
      "    return default_arg\n",
      "Результат:\n",
      ">>> some_func.__defaults__ #This will show the default argument values for the function\n",
      "([],)\n",
      ">>> some_func()\n",
      ">>> some_func.__defaults__\n",
      "(['some_string'],)\n",
      ">>> some_func()\n",
      ">>> some_func.__defaults__\n",
      "(['some_string', 'some_string'],)\n",
      ">>> some_func([])\n",
      ">>> some_func.__defaults__\n",
      "(['some_string', 'some_string'],)\n",
      "\n",
      "Стандартный способ избежать багов из-за изменяемых аргументов — присваивание None в качестве значения по умолчанию с последующей проверкой, передано ли какое-то значение в функцию, соответствующую этому аргументу. Пример:\n",
      "def some_func(default_arg=None):\n",
      "if not default_arg:\n",
      "    default_arg = []\n",
      "default_arg.append(\"some_string\")\n",
      "return default_arg\n",
      "Те же операнды, но другая история\n",
      "1.\n",
      "a = [1, 2, 3, 4]\n",
      "b = a\n",
      "a = a + [5, 6, 7, 8]\n",
      "Результат:\n",
      "\n",
      ">>> a\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      ">>> b\n",
      "[1, 2, 3, 4]\n",
      "2.\n",
      "a = [1, 2, 3, 4]\n",
      "b = a\n",
      "a += [5, 6, 7, 8]\n",
      "Результат:\n",
      ">>> a\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      ">>> b\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Объяснение\n",
      "\n",
      "Выражение a += b ведёт себя не так же, как a = a + b\n",
      "Выражение a = a + [5,6,7,8] генерирует новый объект и присваивает a ссылку на него, оставляя b без изменений.\n",
      "Выражение a + =[5,6,7,8] фактически преобразуется (mapped to) в функцию extend, которая работает с объектом таким образом, что a и b всё ещё указывают на один и тот же объект, который был изменён на месте.\n",
      "\n",
      "Изменение неизменяемого\n",
      "some_tuple = (\"A\", \"tuple\", \"with\", \"values\")\n",
      "another_tuple = ([1, 2], [3, 4], [5, 6])\n",
      "Результат:\n",
      ">>> some_tuple[2] = \"change this\"\n",
      "TypeError: 'tuple' object does not support item assignment\n",
      ">>> another_tuple[2].append(1000) #This throws no error\n",
      ">>> another_tuple\n",
      "([1, 2], [3, 4], [5, 6, 1000])\n",
      ">>> another_tuple[2] += [99, 999]\n",
      "TypeError: 'tuple' object does not support item assignment\n",
      ">>> another_tuple\n",
      "([1, 2], [3, 4], [5, 6, 1000, 99, 999])\n",
      "Но ведь кортежи неизменяемы, разве нет...\n",
      "Объяснение\n",
      "\n",
      "Цитата из https://docs.python.org/2/reference/datamodel.html\n",
      "Объект неизменяемого типа последовательности (immutable sequence type) не может измениться после своего создания. Если объект содержит ссылки на другие объекты, то эти объекты могут быть изменяемыми и могут быть изменены. Однако коллекцию объектов, на которую прямо ссылается неизменяемый объект, изменить нельзя.\n",
      "\n",
      "Оператор += изменяет список на месте. Присвоение элемента (item assignment) не работает, но, когда возникает исключение, элемент уже был изменён на месте.\n",
      "\n",
      "Использование переменной, не определeнной в области видимости\n",
      "a = 1\n",
      "def some_func():\n",
      "    return a\n",
      "\n",
      "def another_func():\n",
      "    a += 1\n",
      "    return a\n",
      "Результат:\n",
      ">>> some_func()\n",
      "1\n",
      ">>> another_func()\n",
      "UnboundLocalError: local variable 'a' referenced before assignment\n",
      "Объяснение\n",
      "\n",
      "Когда вы присваиваете переменную в области видимости, она становится локальной для этой области. То есть a становится локальной переменной области видимости another_func, но она не была ранее инициализирована в той же области, которая кидает ошибку.\n",
      "Почитайте это короткое замечательное руководство, чтобы больше узнать о том, как в Python работают пространства имён и разрешение области видимости (scope resolution).\n",
      "Используйте ключевое слово global для модифицирования переменной внешней области видимости a в another_func.\n",
      "\n",
      "def another_func()\n",
      "    global a\n",
      "    a += 1\n",
      "    return a\n",
      "Результат:\n",
      ">>> another_func()\n",
      "2\n",
      "Исчезновение переменной из внешней области видимости\n",
      "e = 7\n",
      "try:\n",
      "    raise Exception()\n",
      "except Exception as e:\n",
      "    pass\n",
      "Результат (Python 2.x):\n",
      ">>> print(e)\n",
      "# prints nothing\n",
      "Результат (Python 3.x):\n",
      ">>> print(e)\n",
      "NameError: name 'e' is not defined\n",
      "Объяснение\n",
      "\n",
      "Источник.\r\n",
      "Если назначает исключение с целевым as, оно очищается в конце клаузы except. Как если бы\n",
      "except E as N:\n",
      "foo\n",
      "было преобразовано в\n",
      "except E as N:\n",
      "try:\n",
      "    foo\n",
      "finally:\n",
      "    del N\n",
      "Это означает, что исключение нужно назначать на другое имя, чтобы можно было ссылаться на него после клаузы except. Исключения очищаются потому, что к ним прикрепляется обратная трассировка (traceback), в результате во фрейме стека формируется ссылочный цикл (reference cycle), поддерживающий все локалы в этом фрейме живыми, пока не пройдёт следующая итерация сборки мусора.\n",
      "\n",
      "В Python клаузы не входят в область видимости. В этом примере всё представлено в одной области видимости, и переменная e убирается из-за исполнения клаузы except. Но это не относится к функциям, имеющим отдельные внутренние области видимости. Иллюстрация:\n",
      "def f(x):\n",
      "del(x)\n",
      "print(x)\n",
      "\n",
      "\n",
      "x = 5\r\n",
      "y = [5, 4, 3]\n",
      "**Результат:**\n",
      "\n",
      "f(x)\r\n",
      "UnboundLocalError: local variable 'x' referenced before assignment\r\n",
      "f(y)\r\n",
      "UnboundLocalError: local variable 'x' referenced before assignment\r\n",
      "x\r\n",
      "5\r\n",
      "y\r\n",
      "[5, 4, 3]\n",
      "- В Python 2.x имя переменной e присвоено экземпляру `Exception()`, так что при попытке вывода на экран вы ничего не увидите.\n",
      "\n",
      "\n",
      "Результат (Python 2.x):\n",
      ">>> e\n",
      "Exception()\n",
      ">>> print e\n",
      "# Nothing is printed!\n",
      "Return возвращает везде\n",
      "def some_func():\n",
      "    try:\n",
      "        return 'from_try'\n",
      "    finally:\n",
      "        return 'from_finally'\n",
      "Результат:\n",
      ">>> some_func()\n",
      "'from_finally'\n",
      "Объяснение\n",
      "\n",
      "Когда в блоке try выражения try…finally выполняется return, break или continue, то на выходе также исполняется клауза finally. \n",
      "Возвращаемое функцией значение определяется последним выполненным выражением return. Поскольку клауза finally исполняется всегда, выражение return, исполненное в клаузе finally, всегда будет последним исполненным. \n",
      "\n",
      "Когда True на самом деле False\n",
      "True = False\n",
      "if True == False:\n",
      "    print(\"I've lost faith in truth!\")\n",
      "Результат:\n",
      "I've lost faith in truth!\n",
      "Объяснение\n",
      "\n",
      "Изначально в Python не было типа bool (программисты использовали 0 для false и ненулевое значение вроде 1 для true). Затем в язык добавили True, False и тип bool, но из-за обратной совместимости нельзя было сделать True и False константами — они представляли собой просто встроенные переменные. \n",
      "Python 3 стал обратно несовместимым, поэтому в нём наконец исправили ситуацию с булевыми значениями, так что этот пример не работает в Python 3.x! \n",
      "\n",
      "Будьте осторожны с цепочками операций\n",
      ">>> True is False == False\n",
      "False\n",
      ">>> False is False is False\n",
      "True\n",
      ">>> 1 > 0 < 1\n",
      "True\n",
      ">>> (1 > 0) < 1\n",
      "False\n",
      ">>> 1 > (0 < 1)\n",
      "False\n",
      "Объяснение\n",
      "Как сказано в https://docs.python.org/2/reference/expressions.html#not-in\n",
      "Формально, если a, b, c, ..., y, z — выражения, а op1, op2, ..., opN —операторы сравнения, тогда a op1 b op2 c… y opN z эквивалентно op1 b и b op2 c и… y opN z, за исключением того, что каждое выражение вычисляется однократно.Хотя такое поведение могло показаться вам глупостью, оно очень удобно в ситуациях вроде a == b == c и 0 <= x <= 100.\n",
      "\n",
      "False is False is False эквивалентно (False is False) and (False is False)\n",
      "True is False == False эквивалентно True is False and False == False, и поскольку первая часть выражения (True is False) вычисляется как False, то и всё выражение вычисляется как False. \n",
      "1 > 0 < 1 эквивалентно 1 > 0 and 0 < 1, что вычисляется как True. \n",
      "Выражение (1 > 0) < 1 эквивалентно True < 1 и \n",
      "\n",
      ">>> int(True)\n",
      "1\n",
      ">>> True + 1 #not relevant for this example, but just for fun\n",
      "2\n",
      "Так что 1 < 1 вычисляется как False\n",
      "Разрешение имен игнорирует область видимости класса\n",
      "1.\n",
      "x = 5\n",
      "class SomeClass:\n",
      "    x = 17\n",
      "    y = (x for i in range(10))\n",
      "Результат:\n",
      ">>> list(SomeClass.y)[0]\n",
      "5\n",
      "2.\n",
      "x = 5\n",
      "class SomeClass:\n",
      "    x = 17\n",
      "    y = [x for i in range(10)]\n",
      "Результат (Python 2.x):\n",
      ">>> SomeClass.y[0]\n",
      "17\n",
      "Результат (Python 3.x):\n",
      ">>> SomeClass.y[0]\n",
      "5\n",
      "Объяснение\n",
      "\n",
      "Области видимости, вложенные внутрь определения класса, игнорируют имена, привязанные к уровню класса. \n",
      "Генерирующее выражение имеет собственную область видимости. \n",
      "Начиная с Python 3.X генераторы списков (list comprehensions) тоже имеют свои области видимости. \n",
      "\n",
      "От заполненности до None в одной инструкции\n",
      "some_list = [1, 2, 3]\n",
      "some_dict = {\n",
      "  \"key_1\": 1,\n",
      "  \"key_2\": 2,\n",
      "  \"key_3\": 3\n",
      "}\n",
      "\n",
      "some_list = some_list.append(4)\n",
      "some_dict = some_dict.update({\"key_4\": 4})\n",
      "Результат:\n",
      ">>> print(some_list)\n",
      "None\n",
      ">>> print(some_dict)\n",
      "None\n",
      "Объяснение\n",
      "Большинство методов, изменяющих элементы объектов последовательности/преобразования (sequence/mapping objects) вроде list.append, dict.update, list.sort и т. д., изменяют объекты на месте и возвращают None. Причина — в улучшении производительности благодаря избеганию созданий копии объекта, если операцию можно выполнить на месте (взято отсюда)\n",
      "Явное приведение типов в строковых значениях\n",
      "Это вовсе не WTF, но у меня ушла куча времени на осознание того, что в Python существуют такие вещи. Делюсь с начинающими.\n",
      "a = float('inf')\n",
      "b = float('nan')\n",
      "c = float('-iNf')  #These strings are case-insensitive\n",
      "d = float('nan')\n",
      "Результат:\n",
      ">>> a\n",
      "inf\n",
      ">>> b\n",
      "nan\n",
      ">>> c\n",
      "-inf\n",
      ">>> float('some_other_string')\n",
      "ValueError: could not convert string to float: some_other_string\n",
      ">>> a == -c #inf==inf\n",
      "True\n",
      ">>> None == None # None==None\n",
      "True\n",
      ">>> b == d #but nan!=nan\n",
      "False\n",
      ">>> 50/a\n",
      "0.0\n",
      ">>> a/a\n",
      "nan\n",
      ">>> 23 + b\n",
      "nan\n",
      "Объяснение\n",
      "'inf' и 'nan' — специальные строковые значения (чувствительные к регистру). Если явно привести их к типу float, то можно использовать их для представления, соответственно, математических «бесконечности» и «не числа».\n",
      "Атрибуты классов и экземпляров\n",
      "1.\n",
      "class A:\n",
      "    x = 1\n",
      "\n",
      "class B(A):\n",
      "    pass\n",
      "\n",
      "class C(A):\n",
      "    pass\n",
      "Результат:\n",
      ">>> A.x, B.x, C.x\n",
      "(1, 1, 1)\n",
      ">>> B.x = 2\n",
      ">>> A.x, B.x, C.x\n",
      "(1, 2, 1)\n",
      ">>> A.x = 3\n",
      ">>> A.x, B.x, C.x\n",
      "(3, 2, 3)\n",
      ">>> a = A()\n",
      ">>> a.x, A.x\n",
      "(3, 3)\n",
      ">>> a.x += 1\n",
      ">>> a.x, A.x\n",
      "(4, 3)\n",
      "2.\n",
      "class SomeClass:\n",
      "    some_var = 15\n",
      "    some_list = [5]\n",
      "    another_list = [5]\n",
      "    def __init__(self, x):\n",
      "        self.some_var = x + 1\n",
      "        self.some_list = self.some_list + [x]\n",
      "        self.another_list += [x]\n",
      "Результат:\n",
      ">>> some_obj = SomeClass(420)\n",
      ">>> some_obj.some_list\n",
      "[5, 420]\n",
      ">>> some_obj.another_list\n",
      "[5, 420]\n",
      ">>> another_obj = SomeClass(111)\n",
      ">>> another_obj.some_list\n",
      "[5, 111]\n",
      ">>> another_obj.another_list\n",
      "[5, 420, 111]\n",
      ">>> another_obj.another_list is SomeClass.another_list\n",
      "True\n",
      ">>> another_obj.another_list is some_obj.another_list\n",
      "True\n",
      "Объяснение\n",
      "\n",
      "Переменные классов и переменные в экземплярах классов внутренне обрабатываются как словари объектов классов. Если имя переменной не найдено в словаре текущего класса, то поиск выполняется в родительских классах.\n",
      "Оператор += модифицирует изменяемый объект на месте без создания нового объекта. Так что изменение атрибута одного экземпляра влияет также на другие экземпляры и атрибут класса.\n",
      "\n",
      "Ловля исключений\n",
      "some_list = [1, 2, 3]\n",
      "try:\n",
      "    # This should raise an ``IndexError``\n",
      "    print(some_list[4])\n",
      "except IndexError, ValueError:\n",
      "    print(\"Caught!\")\n",
      "\n",
      "try:\n",
      "    # This should raise a ``ValueError``\n",
      "    some_list.remove(4)\n",
      "except IndexError, ValueError:\n",
      "    print(\"Caught again!\")\n",
      "Результат (Python 2.x):\n",
      "Caught!\n",
      "\n",
      "ValueError: list.remove(x): x not in list\n",
      "Результат (Python 3.x):\n",
      "  File \"<input>\", line 3\n",
      "    except IndexError, ValueError:\n",
      "                     ^\n",
      "SyntaxError: invalid syntax\n",
      "Объяснение\n",
      "\n",
      "Для добавления нескольких исключений в клаузу except вам нужно передавать их в первый аргумент в виде взятого в круглые скобки кортежа. Второй аргумент — опциональное имя, которое потом привязывается к экземпляру после кидаемого исключения. Пример:\n",
      "\n",
      "some_list = [1, 2, 3]\n",
      "try:\n",
      "   # This should raise a ``ValueError``\n",
      "   some_list.remove(4)\n",
      "except (IndexError, ValueError), e:\n",
      "   print(\"Caught again!\")\n",
      "   print(e)\n",
      "Результат (Python 2.x):\n",
      "Caught again!\n",
      "list.remove(x): x not in list\n",
      "Результат (Python 3.x):\n",
      "  File \"<input>\", line 4\n",
      "    except (IndexError, ValueError), e:\n",
      "                                     ^\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      "\n",
      "Не рекомендуется отделять исключение от переменной с помощью запятой, это не работает в Python 3; нужно использовать as. Пример:\n",
      "\n",
      "some_list = [1, 2, 3]\n",
      "try:\n",
      "    some_list.remove(4)\n",
      "\n",
      "except (IndexError, ValueError) as e:\n",
      "    print(\"Caught again!\")\n",
      "    print(e)\n",
      "Результат:\n",
      "Caught again!\n",
      "list.remove(x): x not in list\n",
      "Полночь не существует?\n",
      "from datetime import datetime\n",
      "\n",
      "midnight = datetime(2018, 1, 1, 0, 0)\n",
      "midnight_time = midnight.time()\n",
      "\n",
      "noon = datetime(2018, 1, 1, 12, 0)\n",
      "noon_time = noon.time()\n",
      "\n",
      "if midnight_time:\n",
      "    print(\"Time at midnight is\", midnight_time)\n",
      "\n",
      "if noon_time:\n",
      "    print(\"Time at noon is\", noon_time)\n",
      "Результат:\n",
      "('Time at noon is', datetime.time(12, 0))\n",
      "The midnight time is not printed.\n",
      "Объяснение\n",
      "До Python 3.5 булевым значением для объекта datetime.time было False, если требовалось представить полночь в формате UTC. Из-за этого могут возникать ошибки при использовании синтаксиса if obj: при проверке, имеет ли obj значение null или другой эквивалент «пустоты».\n",
      "Что не так с булевыми значениями?\n",
      "1.\n",
      "# A simple example to count the number of boolean and\n",
      "# integers in an iterable of mixed data types.\n",
      "mixed_list = [False, 1.0, \"some_string\", 3, True, [], False]\n",
      "integers_found_so_far = 0\n",
      "booleans_found_so_far = 0\n",
      "\n",
      "for item in mixed_list:\n",
      "    if isinstance(item, int):\n",
      "        integers_found_so_far += 1\n",
      "    elif isinstance(item, bool):\n",
      "        booleans_found_so_far += 1\n",
      "Результат:\n",
      ">>> booleans_found_so_far\n",
      "0\n",
      ">>> integers_found_so_far\n",
      "4\n",
      "2.\n",
      "another_dict = {}\n",
      "another_dict[True] = \"JavaScript\"\n",
      "another_dict[1] = \"Ruby\"\n",
      "another_dict[1.0] = \"Python\"\n",
      "Результат:\n",
      ">>> another_dict[True]\n",
      "\"Python\"\n",
      "Объяснение\n",
      "\n",
      "Булевые значения — подкласс int\n",
      ">>> isinstance(True, int)\n",
      "True\n",
      ">>> isinstance(False, int)\n",
      "True\n",
      "Целое число True равно 1, а False — 0.\n",
      ">>> True == 1 == 1.0 and False == 0 == 0.0\n",
      "True\n",
      "Причины описаны на StackOverflow.\n",
      "\n",
      "Игла в стоге сена\n",
      "Почти каждый Python-программист сталкивался с этой ситуацией.\n",
      "t = ('one', 'two')\n",
      "for i in t:\n",
      "    print(i)\n",
      "\n",
      "t = ('one')\n",
      "for i in t:\n",
      "    print(i)\n",
      "\n",
      "t = ()\n",
      "print(t)\n",
      "Результат:\n",
      "one\n",
      "two\n",
      "o\n",
      "n\n",
      "e\n",
      "tuple()\n",
      "Объяснение\n",
      "\n",
      "Корректным выражением для ожидаемого поведения будет t = ('one',) или t = 'one', (отсутствует запятая), иначе интерпретатор решит, что t является str и итерирует её символ за символом. \n",
      "() — специальный токен, обозначающий пустой tuple. \n",
      "\n",
      "For что?\n",
      "Предложено @MittalAshok.\n",
      "some_string = \"wtf\"\n",
      "some_dict = {}\n",
      "for i, some_dict[i] in enumerate(some_string):\n",
      "    pass\n",
      "Результат:\n",
      ">>> some_dict # An indexed dict is created.\n",
      "{0: 'w', 1: 't', 2: 'f'}\n",
      "Объяснение\n",
      "\n",
      "Выражение for определено в учебнике Python как:\n",
      "\n",
      "for_stmt: 'for' exprlist 'in' testlist ':' suite ['else' ':' suite]\n",
      "Здесь exprlist — цель присвоения. Это означает, что эквивалент {exprlist} = {next_value} исполняется для каждого элемента из итерируемых. Интересный пример предложен @tukkek:\n",
      "for i in range(4):\n",
      "    print(i)\n",
      "    i = 10\n",
      "Результат:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Вы думали, что цикл будет прогнан только один раз?\n",
      "Объяснение\n",
      "\n",
      "Выражение присваивания i = 10 никогда не влияет на итерации цикла из-за особенностей работы цикла в Python. Перед началом каждой итерации следующий элемент, предоставленный итератором (range(4) в данном случае), распаковывается и присваивается к переменной из целевого списка (i в данном случае). \n",
      "Функция enumerate(some_string) в каждой итерации извлекает новое значение i (счётчик A увеличивается) и символ из some_string. Затем она задаёт (просто присваивает) этому символу ключ i из словаря some_dict. Развёртку цикла можно упростить до:\n",
      "\n",
      ">>> i, some_dict[i] = (0, 'w')\n",
      ">>> i, some_dict[i] = (1, 't')\n",
      ">>> i, some_dict[i] = (2, 'f')\n",
      ">>> some_dict\n",
      "Узел not\n",
      "Предложено @MostAwesomeDude.\n",
      "x = True\n",
      "y = False\n",
      "Результат:\n",
      ">>> not x == y\n",
      "True\n",
      ">>> x == not y\n",
      "  File \"<input>\", line 1\n",
      "    x == not y\n",
      "           ^\n",
      "SyntaxError: invalid syntax\n",
      "Объяснение\n",
      "\n",
      "Старшинство оператора влияет на вычисление выражения, и в Python оператор == выше по старшинству, чем оператор not. \n",
      "Поэтому not x == y эквивалентно not (x == y), что эквивалентно выражению not (True == False), которое вычисляется как True. \n",
      "Но x == not y выдаёт SyntaxError, потому что его можно представить как эквивалент (x == not) y, а не x == (not y), как вы могли подумать в первый момент. \n",
      "Парсер ожидает, что токен not — часть оператора not in (потому что операторы == и not in имеют одинаковое старшинство), но когда следом за токеном not он не находит токен in, то выдаёт SyntaxError. \n",
      "\n",
      "А вы могли такое предположить?\n",
      "Предложено PiaFraus.\n",
      "a, b = a[b] = {}, 5\n",
      "Результат:\n",
      ">>> a\n",
      "{5: ({...}, 5)}\n",
      "Объяснение\n",
      "\n",
      "Согласно языковой справке, оператор присваивания имеет форму:\n",
      "\n",
      "(target_list \"=\")+ (expression_list | yield_expression)\n",
      "а также:\n",
      "Оператор присваивания обрабатывает список выражений (это может быть одиночное выражение или список выражений, разделённых запятыми, во втором случае получается кортеж) и присваивает единственный результирующий объект каждому элементу из списка, слева направо.\n",
      "\n",
      "Выражение + в (target_list \"=\")+ означает, что может быть один или несколько целевых списков. В этом случае целевыми списками являются a, b и a[b] (обратите внимание, что список выражений только один, в нашем случае {}, 5).\n",
      "После обработки списка выражений его значение распаковывается в целевой список слева направо. В нашем случае сначала кортеж {}, 5 распаковывается в a, b, и теперь у нас a = {} и b = 5.\n",
      "a теперь присваивается {}, который является изменяемым объектом.\n",
      "Второй целевой список — a[b] (вы можете подумать, что будет выдана ошибка, потому что a и b не были перед этим определены. Но помните, мы просто присвоили a к {} и b к 5).\n",
      "Теперь задаём ключ 5 из словаря кортежу ({}, 5), создавая тем самым циклическую ссылку ({...} в выходных данных ссылается на тот же объект, на который теперь ссылается a). Более простой пример циклической ссылки:\n",
      "\n",
      ">>> some_list = some_list[0] = [0]\n",
      ">>> some_list\n",
      "[[...]]\n",
      ">>> some_list[0]\n",
      "[[...]]\n",
      ">>> some_list is some_list[0]\n",
      "[[...]]\n",
      "Аналогично вышеприведённому примеру (a[b][0] это тот же объект, что и a)\n",
      "\n",
      "То есть вы можете разбить пример на:\n",
      "\n",
      "a, b = {}, 5\n",
      "a[b] = a, b\n",
      "И циклическая ссылка может быть оправдана тем, что a[b][0] является тем же объектом, что и \n",
      "a\n",
      ">>> a[b][0] is a\n",
      "True\n",
      "Мелкие примеры\n",
      "\n",
      "Join() — это строковая операция (string operation), а не операция списка (list operation). В первое время это выглядит неочевидным.\n",
      "Объяснение: если join() — это метод для строки, тогда он может оперировать любыми итерируемыми (списками, кортежами, итераторами). Если бы это был метод для списка, то он реализовывался бы каждым типом отдельно. Кроме того, нет смысла помещать предназначенный для строковых значений метод в API обычного объекта list.\n",
      "Несколько странно выглядящих, но семантически корректных выражений:\n",
      "\n",
      "[] = () (распаковывает пустой tuple в пустой list) \n",
      "'a'[0][0][0][0][0] также семантически корректно, потому что строки в Python итерируемые. \n",
      "3 --0-- 5 == 8 и --5 == 5 семантически верны и вычисляются как True. \n",
      "\n",
      "Python использует два байта для хранения локальной переменной в функции. В теории это означает, что в функции можно определить только 65 536 переменных. Но в Python есть удобное встроенное решение, которое позволяет хранить более 2^16 имён переменных. В этом коде показано, что произойдёт в стеке, когда определено более 65 536 локальных переменных (внимание: код выводит около 2^18 строк текста!):\n",
      "\n",
      "import dis\n",
      "exec(\"\"\"\n",
      "def f():*     \"\"\" + \"\"\"\n",
      "    \"\"\".join([\"X\"+str(x)+\"=\" + str(x) for x in range(65539)]))\n",
      "\n",
      "f()\n",
      "\n",
      "print(dis.dis(f))\n",
      "\n",
      "Несколько потоков выполнения Python не будут работать параллельно (да, именно так!). Если вы создадите несколько потоков и попытаетесь запустить их параллельно, то из-за Global Interpreter Lock в Python все ваши потоки будут выполняться на одном ядре шаг за шагом. Для реального распараллеливания воспользуйтесь модулем многопроцессорной обработки.\n",
      "Создание срезов списка (list slicing) с выходящими за границы индексами не приведёт к ошибкам:\n",
      "\n",
      ">>> some_list = [1, 2, 3, 4, 5]\n",
      ">>> some_list[111:]\n",
      "[]\n",
      "Внести свой вклад\n",
      "Все патчи приветствуются! Только перед самой публикацией прошу сначала создавать тему:) Подробности описаны в CONTRIBUTING.md.\n",
      "Полезные ссылки\n",
      "• https://www.youtube.com/watch?v=sH4XF6pKKmk\r\n",
      "• https://www.reddit.com/r/Python/comments/3cu6ej/what_are_some_wtf_things_about_python\r\n",
      "• https://sopython.com/wiki/Common_Gotchas_In_Python\r\n",
      "• https://stackoverflow.com/questions/530530/python-2-x-gotchas-and-landmines\r\n",
      "• https://stackoverflow.com/questions/1011431/common-pitfalls-in-python (В этой теме на StackOverflow приведены полезные рекомендации, что можно и что нельзя делать в Python.)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Наверное, многие из тех, кто занимается анализом данных, когда-нибудь думали о том, возможно ли использовать в работе одновременно R и Python. И если да, то зачем это может быть нужно? В каких случаях будет полезным и эффективным для проектов? Да и как вообще выбрать лучший способ совмещения языков, если гугл выдает примерно 100500 вариантов? \n",
      "\r\n",
      "Давайте попробуем разобраться в этих вопросах. \n",
      " \n",
      "Зачем \n",
      "\n",
      "В первую очередь, это возможность использовать преимущества двух наиболее популярных языков программирования для анализа данных на сегодняшний день. Совмещая наиболее мощные и стабильные библиотеки R и Python в некоторых случаях можно повысить эффективность расчетов или избежать изобретения велосипедов для реализации каких-либо статистических моделей.\n",
      "Во вторую очередь, это повышение скорости и удобства выполнения проектов, в случае если разные люди в вашей команде (или вы сами) обладают хорошими знаниями разных языков. Здесь может помочь разумная комбинация имеющихся навыков программирования на R и Python.\n",
      "\r\n",
      "Попробуем поговорить подробнее о первом пункте. Summary, которое последует ниже, безусловно, субъективно, и его можно дополнять. Оно создано на основе систематизации ключевых статей о преимуществах языков и личного опыта. Но мир, как мы знаем, очень быстро меняется. \n",
      "\r\n",
      "Python создавался умными программистами и является языком общего назначения, уже впоследствии — с развитием data science — адаптированным под специфические задачи анализа данных. Отсюда и следуют главные плюсы этого языка. При анализе данных его использование оптимально для: \n",
      "\n",
      "\n",
      "Web scraping и crawling (beautifulsoup, Scrapy, и т.д.)\n",
      "Эффективной работы с базами данных и приложениями (sqlachemy, и т.д.)\n",
      "Реализации классических ML алгоритмов (scikit-learn, pandas, numpy, scipy, и т.д.)\n",
      "Задач Computer Vision\n",
      "\r\n",
      "Главное в R — это обширная коллекция библиотек. Этот язык, особенно на начальном этапе, развивался по большей части благодаря усилиям статистиков, а не программистов. Статистики очень старались и их достижения сложно оспорить. \n",
      "\r\n",
      "Если вдруг вы подумываете о том, чтобы попробовать новую вкусную статмодель, о которой недавно услышали на конференции, прежде чем садиться писать ее с нуля, загуглите сначала R package <INSERT NAME: new great stats model>. Ваши шансы на успех очень велики! Так, несомненным плюсом R являются возможности продвинутого статистического анализа. В особенности, для ряда специфических областей науки и практики (эконометрика, биоинформатика и др.). На мой взгляд, в R на текущий момент все еще существенно более развит анализ временных рядов. \n",
      "\r\n",
      "Другим ключевым и пока неоспоримым преимуществом R над Python является интерактивная графика. Возможности для создания и настройки дашбордов и простых приложений для людей без знаний JS поистине огромны. Не верите — потратьте немного времени на изучение возможностей пары библиотек из списка: htmlwidgets, flexdashboard, shiny, slidify. Например, изначально, материалы для этой статьи были собраны в виде интерактивной презентации на slidify.\n",
      "\r\n",
      "Но как бы статистики ни старались, сильны они не во всем. Такой высокой эффективности управления памятью, как в Python, им достичь не удалось. Вернее, в R хороший и быстроработающий на больших объемах данных код вполне возможен. Но при гораздо больших усилиях и самоконтроле, чем в Python.\n",
      "\r\n",
      "Постепенно все различия стираются, и оба языка становятся все более взаимозаменяемы. В Python развиваются возможности визуализации (большим шагом вперед стал seaborn) и добавляются не всегда работающие эконометрические библиотеки (puFlux, pymaclab, и др.), в R — растет эффективность управления памятью и улучшаются возможности обработки данных (data.table). Вот тут, например, можно посмотреть примеры базовых операций с данными на R и Python. Так что есть ли преимущество в совмещении языков для вашего проекта, решать только вам.\n",
      "\r\n",
      "Что касается второго пункта о повышении скорости и удобства выполнения проектов, то здесь речь в основном об организации проекта. К примеру, есть у вас на проект два человека, один из которых больше и сильнее может на R, другой — на Python. При условии, что вы можете обеспечить code review и другой контроль для обоих языков, можно попробовать распределить задачи так, чтобы каждый участник использовал свои лучшие навыки. Конечно, также имеет значение ваш опыт решения конкретных задач на различных языках.\n",
      "\r\n",
      "Хотя тут следует уточнить, что речь идет об исследовательских проектах работы с данными. Для продакшен решений важны другие критерии. Совмещение, скорее всего, не будет полезным для устойчивости и масштабируемости расчетов. Так мы плавно и переходим к вопросу о том, когда удобнее совмещать языки.\n",
      "\n",
      "Когда \r\n",
      "С учетом особенностей обоих языков выиграть от совмещения R и Python можно при: \n",
      "\n",
      "\n",
      "Исследовательском анализе данных\n",
      "Прототипировании\n",
      "Реализации проекта/набора задач с широким охватом по различным научно-практическим областям\n",
      "\r\n",
      "Приведу примеры для возможного совмещения языков: \n",
      "\n",
      "\n",
      "Исследование региональных рынков труда. Подключение к официальному API HH.ru с помощью Python, исследование трендов и зависимостей (xgboost, xgboostExplainer) + визуализация (Markdown-отчеты) с помощью R\n",
      "Приложение на данных Bloomberg. Подключение к официальному API и обработка данных на Python (numpy, pandas) + вывод результата в dashboard или shiny приложение на R (flexdashboard, htmlwidgets)\n",
      "Анализ данных социальных медиа. Парсинг и ML стек с помощью Python + эконометрика, network analysis, визуализация и сайт — с помощью R\n",
      "Модель для прогнозирования спроса на продукцию в отдельных точках. Блок прогноза спроса на товар в единичных локациях c помощью Python (ML алгоритмы) + макроэкономический прогноз с помощью R (модели общего равновесия и структурные var модели)\n",
      "Анализ новостного потока. Парсинг на R (rvest) + NLP на Python + параметризованный отчет на R (RMarkdown Parameterized Reports)\n",
      "\r\n",
      "Все приведенные примеры — реальные проекты. \n",
      "\r\n",
      "Повторюсь, что даже при том, что с момента моего первого выступления о возможностях совмещения R и Python прошло уже 2 года, я все еще не решусь рекомендовать совмещать языки в продакшене. Разве что если это почти 2 отдельных сущности/модели, не критично завязанные друг на друга. \n",
      "\r\n",
      "Если есть счастливчики, запилившие R+Python продакшен что-то — поделитесь, пожалуйста, в комментах! \n",
      "\n",
      "Как \r\n",
      "Теперь непосредственно о стульях. Среди подходов к совмещению R и Python можно выделить три основных категории: \n",
      "\n",
      "\n",
      "Command line tools. Исполнение скриптов с помощью командной строки + промежуточное хранение файлов на диске (filling air gap)\n",
      "Interfacing R and Python. Одновременный запуск процессов R и Python и передача данных между ними в оперативной памяти (in-memory)\n",
      "Другие подходы\n",
      "\r\n",
      "Рассмотрим подробнее каждый из подходов. \n",
      "\n",
      "Command line tools \r\n",
      "Суть в разделении проекта на отдельные относительно самостоятельные части, выполняемые на R или Python и передаче данных через диск в каком-либо удобном для обоих языков формате. \r\n",
      "По синтаксису все предельно просто. Исполнение скриптов с помощью командной строки осуществляется по схеме: \n",
      "\n",
      "<cmd_to_run> <path_to_script> <any_additional_args>\n",
      "\n",
      "<cmd_to_run> — команда для выполнения скрипта R или Python с помощью командной строки \n",
      "<path_to_script> — директория расположения скрипта \n",
      "<any_additional_args> — список аргументов на вход скрипту \n",
      "\r\n",
      "В таблице ниже более детально показаны схемы выполнения скриптов из командной строки и считывания переданных аргументов. В комментариях указан тип объекта, в который записывается список аргументов. \n",
      "\n",
      "\n",
      "\n",
      "Command\n",
      "Python\n",
      "R\n",
      "\n",
      "\n",
      "Cmd\n",
      "python path/to/myscript.py arg1 arg2 arg3\n",
      "Rscript path/to/myscript.R arg1 arg2 arg3\n",
      "\n",
      "\n",
      "Fetch arguments\n",
      "# list, 1st el. - file executed \r\n",
      "import sys \r\n",
      "my_args = sys.argv\n",
      "\n",
      "# character vector of args\r\n",
      "myArgs <- commandArgs(trailingOnly = TRUE)\n",
      "\n",
      "\n",
      " \r\n",
      "Для желающих есть совсем подробные примеры ниже. \n",
      "\n",
      "R script из Python \n",
      "\r\n",
      "Для начала сформируем простой R script для определения максимального числа из списка и назовем его max.R. \n",
      "# max.R\n",
      "randomvals <- rnorm(75, 5, 0.5)\n",
      "par(mfrow = c(1, 2))\n",
      "hist(randomvals, xlab = 'Some random numbers')\n",
      "plot(randomvals, xlab = 'Some random numbers', ylab = 'value', pch = 3)\n",
      "\n",
      "\r\n",
      "А теперь выполним его на Python, задействовав cmd и передав список чисел для поиска максимального значения. \n",
      "\n",
      "# calling R from Python\n",
      "import subprocess\n",
      "\n",
      "# Define command and arguments\n",
      "command = 'Rscript'\n",
      "path2script = 'path/to your script/max.R'\n",
      "\n",
      "# Variable number of args in a list\n",
      "args = ['11', '3', '9', '42']\n",
      "\n",
      "# Build subprocess command\n",
      "cmd = [command, path2script] + args\n",
      "\n",
      "# check_output will run the command and store to result\n",
      "x = subprocess.check_output(cmd, universal_newlines=True)\n",
      "\n",
      "print('The maximum of the numbers is:', x)\n",
      "\n",
      " \n",
      "Python script из R \r\n",
      "Вначале создадим простой скрипт на Python по разделению текстовой строки на части и назовем его `splitstr.py`. \n",
      "\n",
      "# splitstr.py\n",
      "import sys\n",
      "\n",
      "# Get the arguments passed in\n",
      "string = sys.argv[1]\n",
      "pattern = sys.argv[2]\n",
      "\n",
      "# Perform the splitting\n",
      "ans = string.split(pattern)\n",
      "\n",
      "# Join the resulting list of elements into a single newline\n",
      "# delimited string and print\n",
      "print('\\n'.join(ans))\n",
      "\r\n",
      "А теперь выполним его на R, задействовав cmd и передав текстовую строку для удаления нужного паттерна. \n",
      "\n",
      "# calling Python from R\n",
      "command = \"python\"\n",
      "\n",
      "# Note the single + double quotes in the string (needed if paths have spaces)\n",
      "path2script ='\"path/to your script/splitstr.py\"'\n",
      "\n",
      "# Build up args in a vector\n",
      "string = \"3523462---12413415---4577678---7967956---5456439\"\n",
      "pattern = \"---\"\n",
      "args = c(string, pattern)\n",
      "\n",
      "# Add path to script as first arg\n",
      "allArgs = c(path2script, args)\n",
      "\n",
      "output = system2(command, args=allArgs, stdout=TRUE)\n",
      "\n",
      "print(paste(\"The Substrings are:\\n\", output))\n",
      "\n",
      " \r\n",
      "Для промежуточного хранения файлов при передаче информации от одного скрипта к другому можно использовать множество форматов — в зависимости от целей и предпочтений. Для работы с каждым из форматов в обоих языках есть библиотеки (и не по одной). \n",
      "\n",
      "Форматы файлов для R и Python\n",
      "\n",
      "Medium Storage\n",
      "Python\n",
      "R\n",
      "\n",
      "\n",
      "Flat files\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "csv\n",
      "csv, pandas\n",
      "readr, data.table\n",
      "\n",
      "\n",
      "json\n",
      "json\n",
      "jsonlite\n",
      "\n",
      "\n",
      "yaml\n",
      "PyYAML\n",
      "yaml\n",
      "\n",
      "\n",
      "Databases\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "SQL\n",
      "sqlalchemy, pandasql, pyodbc\n",
      "sqlite, RODBS, RMySQL, sqldf, dplyr\n",
      "\n",
      "\n",
      "NoSQL\n",
      "PyMongo\n",
      "RMongo\n",
      "\n",
      "\n",
      "Feather\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "for data frames\n",
      "feather\n",
      "feather\n",
      "\n",
      "\n",
      "Numpy\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "for numpy objects\n",
      "numpy\n",
      "RcppCNPy\n",
      "\n",
      " \n",
      "\r\n",
      "Классический формат — это, конечно, flat файлы. Часто csv — это наиболее просто, удобно и надежно. Если хочется структуризации или хранение информации планируется на относительно длительный срок, то, вероятно, оптимальным выбором будет хранение в базах данных (SQL/NoSQL). \n",
      "\r\n",
      "Для быстрой передачи numpy объектов в R и обратно есть быстрая и устойчивая библиотека RCppCNPy. Пример её использования можно посмотреть тут. \n",
      "\r\n",
      "При этом существует также формат feather, разрабатываемый специально для передачи дата фреймов между R и Python. Изначальная фишка формата — заточенность на R и Python, легкость обработки на обоих языках и очень быстрая запись и чтение. Идея отличная, но с реализацией, как это иногда бывает, есть нюансы. Сами разработчики формата не раз отмечали, что он пока не подходит для долгосрочных решений. При апдейте библиотек для работы с форматом весь процесс может сломаться и потребовать значительных изменений кода. \n",
      "\r\n",
      "Но при этом запись и чтение feather в R и Python действительно быстрые. Тест-сравнение по скорости чтения-записи файла на 10 млн строк представлены на рисунке ниже. В обоих случаях feather значимо опережает по скорости ключевые библиотеки для работы с классическим форматом csv. \n",
      "\n",
      "Сравнение скорости работы с форматами feather и CSV для R и Python \n",
      "\n",
      "Файл: data frame, 10 млн. строк. По 10 попыток на каждую библиотеку. \n",
      "\n",
      "R: CSV чтение/запись через пакеты data.table и dplyr. Работа с feather оказалась наиболее быстрой, но обход по скорости data.table не высокий. При этом есть определенные сложности с настройкой и работой с feather для R. И поддержка вызывает сомнения. Пакет feather последний раз обновлялся год назад. \n",
      "\n",
      "Python: CSV чтение/запись с помощью pandas. Выигрыш feather по скорости оказался значительным, проблем с работой с форматом на Python 3.5 не обнаружилось. \n",
      "\r\n",
      "Тут важно отметить, что сравнивать скорость можно только отдельно для R и отдельно для Python. Между языками будет некорректно, потому что весь тест осуществлялся из R — для удобства формирования итогового рисунка. \n",
      " \n",
      "Подведем итоги \n",
      "Преимущества \n",
      "\n",
      "\n",
      "Простой метод и поэтому часто наиболее быстрый\n",
      "Просто увидеть промежуточный результат\n",
      "Возможность чтения/записи большинства форматов реализована в обоих языках\n",
      "\n",
      "Недостатки \n",
      "\n",
      "\n",
      "Конструкция быстро становится громоздкой и сложноуправляемой по мере роста числа переходов между языками\n",
      "Существенная потеря в скорости записи/чтения файлов при росте объема данных\n",
      "Необходимость заранее согласовывать схему взаимодействия между языками и формат промежуточных файлов\n",
      "\n",
      "Interfacing R and Python \r\n",
      "Этот подход заключается в непосредственном запуске одного языка из другого и предусматривает внутреннюю (in-memory) передачу информации. \n",
      "\r\n",
      "За время, в которое народ задумывался о совмещении R и Python вместо их противопоставления, было создано достаточно много библиотек. Из них удачных и устойчивых к разным параметрам, включая использование операционной системы Windows, всего две. Но уже их наличие открывает новые горизонты, значительно облегчая процесс совмещения языков. \n",
      "\r\n",
      "Для вызова каждого языка через другой ниже представлено по три библиотеки — по убыванию качества. \n",
      "\n",
      "R from Python \r\n",
      "Наиболее популярная, стабильная и устойчивая библиотека — это rpy2. Работает быстро, имеет хорошее описание как часть официальной документации pandas, так и на отдельном сайте. Главная фишка — интеграция с pandas. Ключевым объектом для передачи информации между языками является data frame. Также декларируется прямая поддержка наиболее популярного пакета для визуализации R ggplot2. Т.е. пишете код в python, график видите непосредственно в IDE Python. Тема с ggplot2, правда, барахлит для Windows. \n",
      "\r\n",
      "Недостаток у rpy2, пожалуй, один — необходимость потратить некоторое время на изучение туториалов. Для корректной работы с двумя языками это необходимо, поскольку есть неочевидные нюансы в синтаксисе и сопоставлении типов объектов при передаче. К примеру, при передаче числа из Python на вход в R вы получаете не число, а вектор из одного элемента. \n",
      "\r\n",
      "Ключевое преимущество библиотеки pipe, которая находится на втором месте в таблице ниже, — это скорость. Реализация через пайпы, действительно, в среднем ускоряет работу (на эту тему даже есть статья в JSS), а наличие поддержки работы с pandas объектами в R-Python внушает надежду. Но имеющиеся минусы надежно сдвигают библиотеку на второе место. Ключевой недостаток — это плохая поддержка установки библиотек в R через Python. Если вы хотите воспользоваться какой-либо нестандартной библиотекой в R (а R часто нужен именно для этого), то чтобы она установилась и заработала, нужно последовательно (!!!) загрузить все её dependencies. А для некоторых библиотек их может быть примерно 100500 и маленькая тележка. Второй важный недостаток — неудобная работа с графикой. Построенный график можно посмотреть, только записав его в файл на диске. Третий недостаток — слабая документация. Если случается косяк чуть за границами стандартного набора, решение часто не найдешь ни в документации, ни на stackoverflow. \n",
      "\r\n",
      "Библиотека pyrserve проста в использовании, но и значительно ограничена по функционалу. Например, не поддерживает передачу таблиц. Обновление и поддержка библиотеки разработчиками также оставляет желать лучшего. Версия 0.9.1 является последней из доступных уже более 2 лет. \n",
      "\n",
      "\n",
      "\n",
      "Библиотеки\n",
      "Комментарии\n",
      "\n",
      "\n",
      "rpy2\n",
      " — C-level interface\r\n",
      " — прямая поддержка pandas\r\n",
      " — поддержка графики (+ggplot2)\r\n",
      " — слабая поддержка Windows\n",
      "\n",
      "\n",
      "pyper\n",
      " — Python code\r\n",
      " — use of pipes (в среднем быстрее)\r\n",
      " — косвенная поддержка pandas\r\n",
      " — ограниченная поддержка графики\r\n",
      " — плохая документация\n",
      "\n",
      "\n",
      "\n",
      "pyrserve\n",
      " — Python code\r\n",
      " — use of pipes (в среднем быстрее)\r\n",
      " — косвенная поддержка pandas\r\n",
      " — ограниченная поддержка графики\r\n",
      " — плохая документация \r\n",
      " — низкий уровень поддержки проекта\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Python from R \r\n",
      "Лучшая библиотека на текущий момент — это официальная разработка RStudio reticulate. Минусы в ней пока не просматриваются. А вот преимуществ достаточно: активная поддержка, понятная и удобная документация, вывод результатов выполнения скриптов и ошибок сразу в консоль, легкая передача объектов (как и в rpy2, основной объект — дата фрейм). По поводу активной поддержки приведу пример из личного опыта: средняя скорость ответа на вопрос в stackoverflow и github/issues составляет примерно 1 минуту. Для работы можно как выучить синтаксис по туториалу и затем подключать отдельные python-модули и писать функции. Или же выполнять отдельные куски кода в python через функции py_run. Они позволяют легко выполнить python-скрипт из R, передав необходимые аргументы и получить объект со всем выходом скрипта. \n",
      "\r\n",
      "Второе место по качеству у библиотеки rPython. Ключевое преимущество — простота синтаксиса. Всего 4 команды и вы мастер использования R и Python. Документация также не отстает: все изложено понятно, просто и доступно. Главный недостаток — кривая реализация передачи data frame. И в R и в Python требуется дополнительный шаг, чтобы переданный объект стал таблицей. Второй важный недостаток — отсутствие вывода результата выполнения функции в консоль. При запуске какой-то python-команды из R непосредственно из R вы не сможете быстро понять, успешно она выполнилась или нет. Даже в документации был хороший пассаж от авторов, что для того, чтобы увидеть результат выполнения python кода нужно зайти в сам python и посмотреть. Работа с пакетом из Windows возможна только через боль, но возможна. Полезные ссылки есть в таблице. \n",
      "\r\n",
      "Третье место у библиотеки Rcpp. На самом деле, это очень хороший вариант. Как правило, то, что реализовано в R с помощью C++ работает устойчиво, качественно и быстро. Но требует времени, чтобы разобраться. Поэтому здесь и указано в конце списка. \n",
      "\r\n",
      "Еще вне таблицы можно упомянуть RSPython. Идея была хорошая — единая платформа в обе стороны с единой логикой и синтаксисом — но реализация подкачала. Пакет не поддерживается с 2005 г. Хотя в целом старую версию завести и потыкать палочкой можно. \n",
      "\n",
      "\n",
      "\n",
      "Библиотеки\n",
      "Комментарии\n",
      "\n",
      "\n",
      "reticulate\n",
      " — хорошая документация\r\n",
      " — активное развитие (с авг 2016 г.)\r\n",
      " — волшебная функция py_run_file(\"script.py\")\n",
      "\n",
      "\n",
      "rPython\n",
      " — передача данных через json\r\n",
      " — непрямая передача таблиц\r\n",
      " — хорошая документация\r\n",
      " — слабая поддержка Windows\r\n",
      " — часто падает с Anaconda\n",
      "\n",
      "\n",
      "\n",
      "Rcpp\n",
      " — через C++ (Boost.Python и Rcpp)\r\n",
      " — нужны специфические навыки\r\n",
      " — хороший пример\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Для двух наиболее популярных библиотек ниже приведены подробные примеры использования. \n",
      "\n",
      "Interfacing R from Python: rpy2Для простоты будем использовать скрипт на R, указанный еще в первых примерах — max.R. \n",
      "\n",
      "from rpy2.robjects import pandas2ri # loading rpy2\n",
      "from rpy2.robjects import r\n",
      "pandas2ri.activate() # activating pandas module\n",
      "df_iris_py = pandas2ri.ri2py(r['iris']) # from r data frame to pandas\n",
      "df_iris_r = pandas2ri.py2ri(df_iris_py) # from pandas to r data frame \n",
      "plotFunc = r(\"\"\"\n",
      "   library(ggplot2)\n",
      "   function(df){\n",
      "   p <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) \n",
      "          + geom_point(aes(color = Species))\n",
      "   print(p)\n",
      "   ggsave('iris_plot.pdf', plot = p, width = 6.5, height = 5.5)\n",
      " }\n",
      "\"\"\") # ggplot2 example\n",
      "gr = importr('grDevices') # necessary to shut the graph off\n",
      "plotFunc(df_iris_r)\n",
      "gr.dev_off()\n",
      "\n",
      "\n",
      "Interfacing Python from R: reticulateДля простоты будем использовать скрипт на Python, указанный еще в первых примерах — splitstr.py. \n",
      "\n",
      "library(reticulate)\n",
      "\n",
      "# aliasing the main module\n",
      "py <- import_main()\n",
      "\n",
      "# set parameters for Python directly from R session\n",
      "py$pattern <- \"---\"\n",
      "py$string = \"3523462---12413415---4577678---7967956---5456439\"\n",
      "\n",
      "# run splitstr.py from the slide 11\n",
      "result <- py_run_file('splitstr.py')\n",
      "\n",
      "# read Python script result in R\n",
      "result$ans\n",
      "# [1] \"3523462\"  \"12413415\" \"4577678\"  \"7967956\"  \"5456439\"\n",
      "\n",
      "\n",
      "Подведем итоги \n",
      "Преимущества \n",
      "\n",
      "\n",
      "Гибкий и интерактивный способ\n",
      "Быстрая взаимная передача объектов в рамках оперативной памяти\n",
      "\n",
      "Недостатки \n",
      "\n",
      "\n",
      "Необходимость читать туториалы!\n",
      "Тонкости в сопоставимости объектов и способах их передачи между языками\n",
      "Передача больших объемов данных также может быть затруднительной\n",
      "Слабая поддержка Windows\n",
      "Нестабильность между версиями библиотек\n",
      "\n",
      "Другие подходы\r\n",
      "В качестве отдельной категории хотелось бы привести еще несколько способов совмещения языков: с помощью специального функционала ноутбуков и отдельных платформ. Обращу внимание, что большинство приведенных ниже ссылок — это практические примеры. \n",
      "\r\n",
      "При выборе ноутбуков можно использовать: \n",
      "\n",
      "\n",
      "Классический jupyter и синтаксис R majic вместе с обсуждавшейся выше rpy2. Несложно, в целом, но нужно опять же потратить некоторое время на изучение синтаксиса. Другой вариант: установка IRKernel. Но в этом случае получится только запускать кернелы отдельно, передавая файлы через запись на диске.\n",
      "R Notebook. Достаточно просто установить уже упомянутую библиотеку reticulate и далее указывать для каждой ячейки ноутбука язык, на котором вы будете творить. Удобная штука!\n",
      "Apache Zeppelin. Говорят, достойно и также в использовании удобно.\n",
      "\r\n",
      "Был еще замечательный проект Beaker Notebooks, позволявший очень удобно перемещаться между языками в интерфейсе, похожем и на Jupyter и на Zeppelin. Для передачи объектов in-memory авторами была написана отдельная библиотека beaker. Плюс можно было сразу удобно публиковать результат (в общий доступ). Но почему-то всего этого больше нет, даже старые публикации удалены — разработчики сконцентрировались на проекте BeakerX. \n",
      "\r\n",
      "Среди специального софта, дающего возможность совмещать R и Python следует выделить: \n",
      "\n",
      "\n",
      "Отличную open-source платформу H2O. Продукт предполагает отдельные библиотеки-коннекторы как для Python, так и для R.\n",
      "Возможности SAS\n",
      "Cloudera\n",
      "\n",
      "Расширенный пример\r\n",
      "В завершение статьи хотелось бы также разобрать детальный пример решения небольшой исследовательской задачи, в которой удобно использовать R и Python. \n",
      "\r\n",
      "Предположим, поставлена задача: сравнить уровни инфляции и темпов экономического роста в странах, реализующих инфляционное таргетирование (или близкий аналог) как режим денежно-кредитной политики с 2013 г. — примерного старта внедрения этого режима в России. \n",
      "\r\n",
      "Решить задачу нужно быстро, а как быструю загрузку помним только с Python, обработку и визуализацию — с R. \n",
      "\r\n",
      "Поэтому трясущимися руками заводим R Notebook, пишем в верхней ячейке ```python``` и загружаем данные с сайта World Bank. Данные будем передавать через CSV. \n",
      "\n",
      "Python: загрузка данных с сайта World Bankimport pandas as pd\n",
      "import wbdata as wd\n",
      "\n",
      "# define a period of time\n",
      "start_year = 2013\n",
      "end_year = 2017\n",
      "\n",
      "# list of countries under inflation targeting monetary policy regime\n",
      "countries = ['AM', 'AU', 'AT', 'BE', 'BG', 'BR', 'CA', 'CH', 'CL', 'CO', 'CY', 'CZ', 'DE', 'DK', 'XC', 'ES', 'EE', 'FI', 'FR', 'GB', 'GR', 'HU', 'IN', 'IE', 'IS', 'IL', 'IT', 'JM', 'JP', 'KR', 'LK', 'LT', 'LU', 'LV', 'MA', 'MD', 'MX', 'MT', 'MY', 'NL', 'NO', 'NZ', 'PK', 'PE', 'PH', 'PL', 'PT', 'RO', 'RU', 'SG', 'SK', 'SI', 'SE', 'TH', 'TR', 'US', 'ZA']\n",
      "\n",
      "# set dictionary for wbdata\n",
      "inflation = {'FP.CPI.TOTL.ZG': 'CPI_annual', 'NY.GDP.MKTP.KD.ZG': 'GDP_annual'}\n",
      "\n",
      "# download wb data\n",
      "df = wd.get_dataframe(inflation, country = countries, data_date = (pd.datetime(start_year, 1, 1), pd.datetime(end_year, 1, 1)))\n",
      "\n",
      "print(df.head())\n",
      "df.to_csv('WB_data.csv', index = True)\n",
      "\n",
      "\r\n",
      "Далее осуществляется предобработка данных на R: разброс значений инфляции в разных странах (min/max/mean) и средний темп экономического роста (по темпам роста реального ВВП). Изменяем также названия некоторых стран на более короткие — чтобы потом было удобнее сделать визуализацию. \n",
      "\n",
      "R: предобработка данныхlibrary(tidyverse)\n",
      "library(data.table)\n",
      "library(DT)\n",
      "\n",
      "# get df with python results\n",
      "cpi <- fread('WB_data.csv')\n",
      "\n",
      "cpi <- cpi %>% group_by(country) %>% summarize(cpi_av = mean(CPI_annual), cpi_max = max(CPI_annual),\n",
      "                                               cpi_min = min(CPI_annual), gdp_av = mean(GDP_annual)) %>% ungroup\n",
      "cpi <- cpi %>% mutate(country = replace(country, country %in% c('Czech Republic', 'Korea, Rep.', 'Philippines',\n",
      "                                                                'Russian Federation', 'Singapore', 'Switzerland',\n",
      "                                                                'Thailand', 'United Kingdom', 'United States'),\n",
      "                                        c('Czech', 'Korea', 'Phil', 'Russia', 'Singap', 'Switz', 'Thai', 'UK', 'US')),\n",
      "                      gdp_sign = ifelse(gdp_av > 0, 'Positive', 'Negative'),\n",
      "                      gdp_sign = factor(gdp_sign, levels = c('Positive', 'Negative')),\n",
      "                      country = fct_reorder(country, gdp_av),\n",
      "                      gdp_av = abs(gdp_av),\n",
      "                      coord = rep(ceiling(max(cpi_max)) + 2, dim(cpi)[1])\n",
      "                     )\n",
      "\n",
      "print(head(data.frame(cpi)))\n",
      "\n",
      "\r\n",
      "Затем можно с помощью небольшого кода и R создать читабельный и приятный график, позволяющий ответить на исходный вопрос о сравнении уровней инфляции и ВВП в странах, применяющих режим инфляционного таргетирования.\n",
      "\n",
      "R: визуализацияlibrary(viridis)\n",
      "library(scales)\n",
      "\n",
      "ggplot(cpi, aes(country, y = cpi_av)) +\n",
      "  geom_linerange(aes(x = country, y = cpi_av, ymin = cpi_min, ymax = cpi_max, colour = cpi_av), size = 1.8, alpha = 0.9) + \n",
      "  geom_point(aes(x = country, y = coord, size = gdp_av, shape = gdp_sign), alpha = 0.5) +\n",
      "  scale_size_area(max_size = 8) +\n",
      "  scale_colour_viridis() +\n",
      "  guides(size = guide_legend(title = 'Average annual\\nGDP growth, %', title.theme = element_text(size = 7, angle = 0)),\n",
      "         shape = guide_legend(title = 'Sign of\\nGDP growth, %', title.theme = element_text(size = 7, angle = 0)),\n",
      "         colour = guide_legend(title = 'Average\\nannual CPI, %', title.theme = element_text(size = 7, angle = 0))) +\n",
      "  ylim(floor(min(cpi$cpi_min)) - 2, ceiling(max(cpi$cpi_max)) + 2) +\n",
      "  labs(title = 'Average Inflation and GDP Rates in Inflation Targeting Countries',\n",
      "       subtitle = paste0('For the period 2013-2017'),\n",
      "       x = NULL, y = NULL) +\n",
      "  coord_polar() +\n",
      "  theme_bw() +\n",
      "  theme(legend.position = 'right',\n",
      "        panel.border = element_blank(),\n",
      "        axis.text.x = element_text(colour = '#442D25', size = 6, angle = 21, vjust = 1))\n",
      "\n",
      "ggsave('IT_countries_2013_2017.png', width = 11, height = 5.7)\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "На полученном графике в полярной системе координат показан разброс значений инфляции в период с 2013 г. по 2017 г., а также средние значения ВВП (над значениями инфляции). Круг означает положительный темп роста ВВП, треугольник — отрицательный.\n",
      "\r\n",
      "Рисунок в целом позволяет сделать некоторые предварительные выводы об успешности режима инфляционного таргетирования в России относительно других стран. Но это уже за рамками данной статьи. А если интересно — могу дать ссылки на различные материалы по теме.\n",
      "\n",
      "Выводы\n",
      "\n",
      "Можно и нужно совмещать R и Python!\r\n",
      "В особенности в целях исследовательского анализа данных и прототипирования\n",
      "Внимательно относитесь к выбору способов совмещения, основываясь на целях проекта:\n",
      "\n",
      "\n",
      "Command line tools: просто, понятно и обязательно будет работать.\n",
      "Отличный способ для начала!\n",
      "Внимание к формату feather!\n",
      "\n",
      "Interfacing R and Python: быстро, гибко и временами интерактивно.\n",
      "\n",
      "\n",
      "Специфический синтаксис — необходимо читать туториалы, более сложная настройка.\n",
      "Используйте проверенные и стабильные библиотеки: rpy2 и reticulate.\n",
      "\n",
      "Другие подходы. Внимание к ноутбукам!\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем привет!\n",
      "\r\n",
      "Запустили юбилейный пятый поток курса «Разработчик Python». Перед стартом каждого потока часто спрашивают какую версию мы используем в обучении (вообще обе и не только, как бы это странно не звучало, учитывая как называется курс, Python) и разные нюансы миграции с одной версии на другую. Сегодня мы хотим поделиться статьёй об опыте миграцию с 2 на 3 в Facebook, которую рассказывали на PyConf.\n",
      "\r\n",
      "Поехали.\n",
      "\r\n",
      "Переход на Python 3 стал значительно популярней за последние годы, но процесс еще далек от завершения. В инфраструктуре многих крупных компаний, использующих Python, остаются большие блоки кода на Python 2.7, и Facebook — не исключение. Джейсон Фрайд (Jason Fried) посетил PyCon 2018, чтобы рассказать об изменениях, произошедших в компании за последние 4 года — в самом начале Python 3 практически отсутствовал, но в итоге стал основной версией Python в компании. Джейсон помог достичь этой цели, и его доклад [видео на YouTube] — отличный источник идей для других организаций, которые хотят провести миграцию.\n",
      "\r\n",
      "Фрайд начал работать в Facebook в 2011 году и быстро понял, что нужно учить Python, если он хочет получать ревью кода быстрее. Чуть позже он понял, что стал основной движущей силой за переход на Python 3 в Facebook. Это никогда не входило в его планы, и случилось само собой по мере его работы с Python.\n",
      "\n",
      "\n",
      "\r\n",
      "Фрайд начал с того, что стал принимать активное участие во внутренней группе Python, и часто оказывался первым, готовым ответить на возникающие вопросы. В итоге, он прославился (“скорее приобрел дурную славу”) среди питонистов в Facebook, тем что фиксил код самостоятельно, не спрашивая разрешения, если видел, что язык используется неправильно. Это возможно в Facebook, в связи с отсутствием вертикальной иерархии контроля; у всех есть столько же прав, чтобы откатить ваши изменения, сколько у вас, чтобы изначально эти изменения совершить. Со временем эти правки помогли укрепить авторитет Фрайда в сообществе Python Facebook, который пригодится ему в процессе миграции.\n",
      "\r\n",
      "По словам Фрайда, менять что-то вроде версии языка Python в масштабах Facebook требовало некоторого времени и огромного количества дипломатии. Он хотел рассказать “историю о том, как я и пара других инженеров в свободное время и без каких-либо полномочий сделали Python 3 основной версией в Facebook”.\n",
      "\r\n",
      "В 2013 году в Facebook присутствовала зачаточная поддержка Python 3.3. Она появилась как часть задачи по добавлению поддержки Python 3 в систему сборки. Но эта задача была заблокирована поддержкой Python 3 в библиотеках Facebook, которая в свою очередь была заблокирована отсутствием поддержки Python 3 в системе сборки. Ситуация как из «Уловки-22»: Python 3 был “доступен”, но ничто в среде Facebook его не поддерживало. \n",
      "\r\n",
      "Кроме того, в 2013 году Python 3 в Facebook был окружен негативными настроениями. Почти все думали, что компания просто останется на Python 2.7 навсегда. Также велись разговоры о переходе на совершенно другой язык. Даже сам Фрайд говорил (во внутренней группе), что переход на Python 3 никогда не произойдет в Facebook. Только один человек бросил вызов этому заявлению и предложил что-то с этим сделать; в тот момент Фрайд проигнорировал призыв к действию но продолжил думать об этой идее. \n",
      "\n",
      "Луч надежды\n",
      "\r\n",
      "Но, по словам Фрайда, надежда теплилась. В январе 2013 года, используемому “линтеру” потребовались четыре импорта из __future__ (print_function, division, absolute_imports, andunicode_literals). Изначально они появились с целью продлить жизнь кодовой базы Python 2, и были добавлены везде, чтобы линтер перестал ругаться. Это, в итоге, упростило конвертирование модулей на Python 3.\n",
      "\r\n",
      "В Facebook “повсеместно используется” фреймворк для сериализация и удаленных вызовов процедур Apache Thrift. И его исключительная совместимость с Python 2 стала основным блокером. Но в опросе на тему потенциально интересных нововведений, проведенном группой Thrift в Facebook, популярным вариантом оказалось добавление поддержки Python 3. Фрайд проголосовал именно за него, но еще не потому что хотел заниматься продвижением Python 3; он просто посчитал, что интерфейс Python 2 похож на что-то из Java, и поэтому требует рефакторинга. \n",
      "\r\n",
      "Его мыслительный процесс стал меняться после лекции Гвидо ван Россум (Guido van Rossum) в Yelp, Сан-Франциско на тему “Tulip”, который оказался модулем asyncio. Фрайд всегда был фанатом асинхронного программирования на Python, но считал его фрагментированным из-за различий между фреймворками (например, Twisted, gevent), которые его предоставляли. Tulip выглядел так, будто может сделать асинхронный ввод-вывод интероперабельным. Лекция еще не успела закончиться, а Фрайд уже общался с командой Thrift Facebook, предлагая внедрить поддержку Tulip для Python 3 вместого того, чтобы ждать порта Twisted, gevent и других. Через несколько дней Trift опубликовали роудмап, показывающий предстоящую поддержку Python 3 и Tulip.\n",
      "\r\n",
      "Это произошло в начале 2014 года, но ничего не менялось еще в течение шести месяцев; пользователи не появились и не планировали это делать, на самом деле, они и не знали ни о каких изменениях.\n",
      "\n",
      "Новый проект\n",
      "\r\n",
      "В августе 2014 года, Фрайд начал проект по переписыванию унаследованного сервиса. Изначально для этих целей он планировал использовать Python 2 и gevent, но затем понял, что к моменту завершения работы оно уже устареет. Кто-то должен быть первым, чтобы произошли изменения; для Facebook и Python 3 это был именно Фрайд. “Именно вы должны стать этим человеком для Python 3 в вашей организации”.\n",
      "\r\n",
      "Итак, он начала свой проект, используя Python 3 и “все было сломано”; неудивительно, что никто не пользовался Python 3. Система сборки даже не могла собрать его код, а все сторонние пакеты wheel были доступны только для Python 2. А когда он все-таки починил достаточно вещей, чтобы собрать свой сервис, тот мгновенно падал на запуске — из-за чего-то глубоко в коде, что настраивало точки входа в системе Facebook.\n",
      "\r\n",
      "Поэтому, чтобы заставить код работать, Фрайд был вынужден починить все остальное; Он пересобрал сотни сторонних wheel’ов, чтобы они работали с обеими версиями Python, обновил все внутренние библиотеки, чтобы сделать их совместимыми с 2/3. Однако, каждый день кто-нибудь коммитил в его зависимости изменения только для Python 2. Неудивительно, Фрайд устал фиксить регрессии. Единственное решение — принудительно ввести соответствие требованиям Python 3 внутри организации, но в Facebook подобное невозможно. Однако, если начать себя вести так, будто у тебя есть на это полномочия, люди начнут верить, что они у тебя действительно есть.\n",
      "\r\n",
      "Фрайд, используя свое социальное влияние, добавил линтер Pyflakes в процесс сборки. С учетом наличия PEP 8, он обосновал добавление нового тем, что тот будет решать другую категорию проблем кода; плюс, в Pyflakes было меньше ложных срабатываний, поэтому он меньше раздражал разработчиков. Фрайд настроил все так, чтобы Pyflakes запускался на всем коде, отправленном на ревью, сначала для Python 2, а затем Python 3. Это помогло распределить работу по соблюдению совместимости с Python 3 между всеми разработчиками, что позволило достичь прогресса в проекте Фрайда.\n",
      "\r\n",
      "Сначала ему приходилось объяснять людям, что линтер не поломан, и есть смысл доработать код до совместимости с Python 3. Если бы разработчики поверили, что переход к Python 3 слишком сложен, они бы вернулись к мысли “останемся с Python 2 навсегда”. Силами Фрайда, сделать код совместимым с Python 3 стало гораздо проще. “Заткнуть линтер”, а значит и самого Фрайда, стало проще, чем жаловаться на необходимость править код, поэтому все так и поступили.\n",
      "\n",
      "Обучение\n",
      "\r\n",
      "Все это помогло остановить “кровотечение”, но не помогло достичь заметного прогресса по внедрению Python 3 в Facebook. Фрайд присоединился к команде, которая преподавала Python новым сотрудникам. Линтеры уже выдавали ошибки, когда код не был совместим с 2 или 3, но Фрайд хотел достичь точки, где код, совместимый с 2/3, писался только для легаси проектов, а новый код писался только на Python 3. В который раз Фрайд взял ситуацию в свои руки: чтобы сделать это заявление, в 2015 году он поменял слайды в классе Python для новых сотрудников. Идея заключалась в том, что в какой-то неизвестный момент в будущем, Facebook захочет переключиться на Python 3, поэтому нет смысла писать код для Python 2 — когда-нибудь его придется переписать. Он объяснил новичкам, что все это должно работать внутри инфраструктуры Facebook и систем сборки, а если это было невозможно, нужно было отправить баг, или починить все самостоятельно. “Как ни странно, это сработало”.\n",
      "\r\n",
      "В январе 2015 года Фрайд “наконец выпустил” свой проект. Он потратил остаток года, рассказывая людям о его успешности и о необходимости переключиться на Python 3. В течение года появились новые союзники в переходе на Python 3 в Facebook.\n",
      "\n",
      "\n",
      "\r\n",
      "Одним из них был Лука Ланга (Łukasz Langa), который “как-то убедил Instagram перейти на Python 3”. В 2016 году Ланга сформировал в Facebook новую команду, ответственную за контролем Python, которую назвали “Министерство Глупых Походок”. Так как они были “командой Python”, “фиктивный авторитет”, упомянутый выше, сработал; люди считали, что они могут принимать решения насчет Python в Facebook.\n",
      "\r\n",
      "В 2016 году наблюдался медленный, но стабильный рост использования Python 3 в компании. О нем говорили на собраниях, использовали в новых проектах. Мнение менялось, хоть Python 3 все еще не считался версией по умолчанию, и проекты добровольно выбирали его использование. В мае 2016 года Фрайд сообщил о своем намерении переключить систему сборки на Python 3 по умолчанию. Идея была всецело поддержана, поэтому через несколько дней переключение было проведено — без негативных последствий.\n",
      "\r\n",
      "В конце 2016 года команда проекта выпустила отчет с результатами переключения на Python 3. Разработчики просто запустили 2to3 в коде и исправили все моменты, на которые он ругался. Когда запустили полученный код, обнаружилось, что он на 40% быстрее и использует только половину памяти. Это указало на миф, с которым Фрайд часто сталкивался: Python 3 медленнее, чем Python 2. Это могло быть правдой для ранних версий Python 3, но сейчас это не актуально.\n",
      "\n",
      "Хорошие вещи\n",
      "\r\n",
      "В начале 2017 года Instagram завершил миграцию на Python 3 и Facebook пожинал плоды “славного будущего, где можно радоваться хорошим вещам”. Обновление версии Python оказалось не таким страшным процессом и открыло возможность использовать новые функции. Разработчики Facebook теперь могут сконцентрироваться на таких проблемах, как новые функции статической типизации или миграция сервисов для использования asyncio. “Python в Facebook снова стал веселым”.\n",
      "\r\n",
      "Новая проблема — люди спрашивают, когда можно отказаться от поддержки Python 2. Когда появляются регрессии в поддержке Python 2 библиотек или модулей, разработчики часто спрашивают, можно ли им просто перейти на Python 3. Проблема противоположна той, что была несколько лет назад. “О, как прекрасен мир, в котором я живу”.\n",
      "\r\n",
      "Во время лекции Фрайд показал график точек входа в Python сервиса Facebook, начиная с Q3 2015 года — на тот момент в общей сложности было всего 4 точки Python 3. В момент перехода к Python 3, в середине 2016 года, 4% всех точек уже были Python 3. В марте 2018 года их количество перевалило за 50%; в середине мая, на момент лекции, их было 55% “из десятков тысяч точек входа Facebook”. По словам Фрайда, в компании теперь стыдно писать код, который работает только на Python 2.\n",
      "\r\n",
      "Затем Фрайд проанализировал процесс. Он отметил, что необходимо сделать больше, чем просто создать что-то новое; нужно вести к этому разработчиков, “будучи тем изменением, которое вы хотите увидеть”. Нужно привлекать людей, даже если они не будут понимать, что помогают. В этом содействуют линтеры и юнит-тесты. Очень важно учить новых сотрудников тому, к чему вы стремитесь. Когда вы достигните результата, отпразднуйте, наслаждаясь “хорошими вещами”: напишите “классный код на Python 3”. Видя, как новые функции могут быть использованы, остальные тоже захотят перейти. \n",
      "\r\n",
      "Фрайд ответил на несколько вопросов из аудитории. Один вопрос был посвящен тому, как провести такое изменение в более традиционной, иерархической организации. Фрайд предположил, что это может быть даже проще, так как не нужно убеждать тысячи разработчиков, а лишь цепочку менеджеров, начиная с того, кто видит преимущества перехода. Но с другой стороны, это может быть сложнее, если культура компании консервативна. Здесь поможет фокусирование на улучшении качества кода. Другой вопрос касался неделимого кода, который не разбит на множество входных точек. Для такой ситуации Фрайд предложил посмотреть презентацию Instagram (видео на YouTube) с PyCon 2017.\n",
      "\r\n",
      "Другие организации могут многое почерпнуть из этой лекции, но ясно, что важно иметь упорного сторонника, готового контролировать и координировать весь процесс. Компаниям, планирующим подобный переход, понадобится такой человек как Фрайд.\n",
      "\r\n",
      "THE END\n",
      "\r\n",
      "Ждём вопрос и комментарии, которые можно оставить тут или задать напрямую Стасу на его открытом уроке.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      "Если вы когда-нибудь работали с такими низкоуровневыми языками, как С или С++, то наверняка слышали про указатели. Они позволяют сильно повышать эффективность разных кусков кода. Но также они могут запутывать новичков — и даже опытных разработчиков — и приводить к багам управления памятью. А есть ли указатели в Python, можно их как-то эмулировать?\n",
      "\r\n",
      "Указатели широко применяются в С и С++. По сути, это переменные, которые содержат адреса памяти, по которым находятся другие переменные. Чтобы освежить знания об указателях, почитайте этот обзор.\n",
      "\r\n",
      "Благодаря этой статье вы лучше поймёте модель объектов в Python и узнаете, почему в этом языке на самом деле не существуют указатели. На случай, если вам понадобится сымитировать поведение указателей, вы научитесь эмулировать их без сопутствующего кошмара управления памятью.\n",
      "\r\n",
      "С помощью этой статьи вы:\n",
      "\n",
      "\n",
      "Узнаете, почему в Python нет указателей.\n",
      "Узнаете разницу между переменными C и именами в Python.\n",
      "Научитесь эмулировать указатели в Python.\n",
      "С помощью ctypes поэкспериментируете с настоящими указателями. \n",
      "\n",
      "Примечание: Здесь термин «Python» применяется к реализации Python на C, которая известна под названием CPython. Все обсуждения устройства языка справедливы для CPython 3.7, но могут не соответствовать последующим итерациям.\n",
      "\n",
      "Почему в Python нет указателей?\r\n",
      "Не знаю. Могут ли указатели существовать в Python нативно? Вероятно, но судя по всему, указатели противоречат понятию Zen of Python, потому что провоцируют неявные изменения вместо явных. Нередко указатели довольно сложны, особенно для новичков. Более того, они подталкивают вас к неудачным решениям или к тому, чтобы сделать что-нибудь действительно опасное, вроде чтения из области памяти, откуда вам не следовало считывать.\n",
      "\r\n",
      "Python старается абстрагировать от пользователя подробности реализации, например адреса памяти. Часто в этом языке упор делается на удобство использования, а не на скорость. Поэтому указатели в Python не имеют особого смысла. Но не переживайте, по умолчанию язык предоставляет вам некоторые преимущества использования указателей.\n",
      "\r\n",
      "Чтобы разобраться с указателями в Python, давайте кратко пройдёмся по особенностями реализации языка. В частности, вам нужно понять:\n",
      "\n",
      "\n",
      "Что такое изменяемые и неизменяемые объекты.\n",
      "Как устроены переменные/имена в Python.\n",
      "\r\n",
      "Держитесь за свои адреса памяти, поехали!\n",
      "\n",
      "Объекты в Python\r\n",
      "Всё в Python является объектами. Например, откройте REPL и посмотрите, как используется isinstance():\n",
      "\n",
      ">>> isinstance(1, object)\n",
      "True\n",
      ">>> isinstance(list(), object)\n",
      "True\n",
      ">>> isinstance(True, object)\n",
      "True\n",
      ">>> def foo():\n",
      "...    pass\n",
      "...\n",
      ">>> isinstance(foo, object)\n",
      "True\r\n",
      "Этот код демонстрирует, что всё в Python — на самом деле объекты. Каждый объект содержит как минимум три вида данных:\n",
      "\n",
      "\n",
      "Счётчик ссылок.\n",
      "\n",
      "Тип.\n",
      "\n",
      "Значение.\n",
      "\n",
      "\n",
      "Счётчик ссылок используется для управления памятью. Подробно об этом управлении написано в Memory Management in Python. Тип используется на уровне CPython для обеспечения типобезопасности в ходе исполнения (runtime). А значение — это фактическое значение, ассоциированное с объектом.\n",
      "\r\n",
      "Но не все объекты одинаковы. Есть одно важное отличие: объекты бывают изменяемые и неизменяемые. Понимание этого различия между типами объектов поможет вам лучше осознать первый слой луковицы, которая называется «указатели в Python».\n",
      "\n",
      "Изменяемые и неизменяемые объекты\r\n",
      "В Python есть два типа объектов:\n",
      "\n",
      "\n",
      "Неизменяемые объекты (не могут быть изменены);\n",
      "\n",
      "Изменяемые объекты (могут быть изменены).\n",
      "\n",
      "\r\n",
      "Осознание этой разницы — первый ключ к путешествию по миру указателей в Python. Вот характеристика неизменяемости некоторых популярных типов:\n",
      "\n",
      "\n",
      "\n",
      "Тип\n",
      "\n",
      "Неизменяемый?\n",
      "\n",
      "\n",
      "\n",
      "int\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "float\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "bool\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "complex\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "tuple\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "frozenset\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "str\n",
      "\n",
      "Да\n",
      "\n",
      "\n",
      "\n",
      "list\n",
      "\n",
      "Нет\n",
      "\n",
      "\n",
      "\n",
      "set\n",
      "\n",
      "Нет\n",
      "\n",
      "\n",
      "\n",
      "dict\n",
      "\n",
      "Нет\n",
      "\n",
      "\n",
      "\r\n",
      "Как видите, многие из часто используемых примитивных типов являются неизменяемыми. Проверить это можно, написав кое-какой код на Python. Вам понадобится два инструмента из стандартной библиотеки:\n",
      "\n",
      "\n",
      "id() возвращает адрес памяти объекта;\n",
      "\n",
      "is возвращает True, если и только если два объекта имеют одинаковый адрес памяти.\n",
      "\n",
      "\r\n",
      "Можете прогнать этот код в REPL-окружении:\n",
      "\n",
      ">>> x = 5\n",
      ">>> id(x)\n",
      "94529957049376\r\n",
      "Здесь мы присвоили переменной x значение 5. Если вы попробуете изменить значение с помощью сложения, то получите новый объект:\n",
      "\n",
      ">>> x += 1\n",
      ">>> x\n",
      "6\n",
      ">>> id(x)\n",
      "94529957049408\r\n",
      "Хотя может показаться, что этот код просто меняет значение x, но на самом деле вы получаете в качестве ответа новый объект.\n",
      "\r\n",
      "Тип str тоже неизменяем:\n",
      "\n",
      ">>> s = \"real_python\"\n",
      ">>> id(s)\n",
      "140637819584048\n",
      ">>> s += \"_rocks\"\n",
      ">>> s\n",
      "'real_python_rocks'\n",
      ">>> id(s)\n",
      "140637819609424\r\n",
      "И в этом случае s после операции += получает другой адрес памяти.\n",
      "\n",
      "Бонус: Оператор += преобразовывается в различные вызовы методов.\n",
      "\r\n",
      "Для некоторых объектов, таких как список, += преобразует в __iadd__() (локальное добавление). Оно изменит себя и вернёт тот же ID. Однако у str и int нет этих методов, и в результате будет вызываться __add__() вместо __iadd__().\n",
      "\r\n",
      "Подробнее об этом рассказывается в документации по моделям данных Python.\n",
      "\r\n",
      "При попытке напрямую изменить строковое значение s мы получим ошибку:\n",
      "\n",
      ">>> s[0] = \"R\"\r\n",
      "Обратная трассировка (последними отображаются самые свежие вызовы):\n",
      "\n",
      "  File \"<stdin>\", line 1, in <mоdule>\n",
      "TypeError: 'str' object does not support item assignment\r\n",
      "Приведённый выше код сбоит и Python сообщает, что str не поддерживает это изменение, что соответствует определению неизменяемости типа str.\n",
      "\r\n",
      "Сравните с изменяемым объектом, например, со списком:\n",
      "\n",
      ">>> my_list = [1, 2, 3]\n",
      ">>> id(my_list)\n",
      "140637819575368\n",
      ">>> my_list.append(4)\n",
      ">>> my_list\n",
      "[1, 2, 3, 4]\n",
      ">>> id(my_list)\n",
      "140637819575368\r\n",
      "Этот код демонстрирует основное различие между двумя типами объектов. Изначально у my_list есть ID. Даже после добавления к списку 4, my_list всё ещё имеет тот же ID. Причина в том, что тип list является изменяемым.\n",
      "\r\n",
      "Вот ещё одна демонстрация изменяемости списка с помощью присваивания:\n",
      "\n",
      ">>> my_list[0] = 0\n",
      ">>> my_list\n",
      "[0, 2, 3, 4]\n",
      ">>> id(my_list)\n",
      "140637819575368\r\n",
      "В этом коде мы изменили my_list и задали ему в качестве первого элемента 0. Однако список сохранил тот же ID после этой операции. Следующим шагом на нашем пути к познанию Python будет исследование его экосистемы.\n",
      "\n",
      "Разбираемся с переменными\r\n",
      "Переменные в Python в корне отличаются от переменных в C и C++. По сути, их просто нет в Python. Вместо переменных здесь имена.\n",
      "\r\n",
      "Это может звучать педантично, и по большей части так оно и есть. Чаще всего можно воспринимать имена в Python в качестве переменных, но необходимо понимать разницу. Это особенно важно, когда изучаешь такую непростую тему, как указатели.\n",
      "\r\n",
      "Чтобы вам было проще разобраться, давайте посмотрим, как работают переменные в С, что они представляют, а затем сравним с работой имён в Python.\n",
      "\n",
      "Переменные в C\r\n",
      "Возьмём код, который определяет переменную x:\n",
      "\n",
      "int x = 2337;\r\n",
      "Исполнение это короткой строки проходит через несколько различных этапов:\n",
      "\n",
      "\n",
      "Выделение достаточного количества памяти для числа.\n",
      "\n",
      "Присвоение этому месту в памяти значения 2337.\n",
      "\n",
      "Отображение, что x указывает на это значение.\n",
      "\n",
      "\r\n",
      "Упрощённо память может выглядеть так:\n",
      "\n",
      "\n",
      "\r\n",
      "Здесь переменная x имеет фальшивый адрес 0x7f1 и значение 2337. Если позднее вам захочется изменить значение x, можете сделать так:\n",
      "\n",
      "x = 2338;\r\n",
      "Этот код присваивает переменной x новое значение 2338, тем самым перезаписывая предыдущее значение. Это означает, что переменная x изменяема. Обновлённая схема памяти для нового значения:\n",
      "\n",
      "\n",
      "\r\n",
      "Обратите внимание, что расположение x не поменялось, только само значение. Это важно. Нам это говорит о том, что x — это место в памяти, а не просто имя.\n",
      "\r\n",
      "Можно также рассматривать этот вопрос в рамках концепции владения. С одной стороны, x владеет местом в памяти. Во-первых, x — это пустая коробка, которая может содержать лишь одно число (integer), в котором могут храниться целочисленные значения.\n",
      "\r\n",
      "Когда вы присваиваете x какое-то значение, вы тем самым помещаете значение в коробку, принадлежащую x. Если вы хотите представить новую переменную y, то можете добавить такую строку:\n",
      "\n",
      "int y = x;\r\n",
      "Этот код создаёт новую коробку под названием y и копирует в неё значение из x. Теперь схема памяти выглядит так:\n",
      "\n",
      "\n",
      "\r\n",
      "Обратите внимание на новое местоположение y — 0x7f5. Хотя в y и было скопировано значение x, однако переменная y владеет новым адресом в памяти. Следовательно, вы можете перезаписывать значение y, не влияя на x:\n",
      "\n",
      "y = 2339;\r\n",
      "Теперь схема памяти выглядит так:\n",
      "\n",
      "\n",
      "\r\n",
      "Повторюсь: вы изменили значение y, но не местоположение. Кроме того, вы никак не повлияли на исходную переменную x. \n",
      "\r\n",
      "С именами в Python совершенно иная ситуация.\n",
      "\n",
      "Имена в Python\r\n",
      "В Python нет переменных, вместо них имена. Вы можете на своё усмотрение использовать термин «переменные», однако важно знать разницу между переменными и именами.\n",
      "\r\n",
      "Давайте возьмём эквивалентный код из вышеприведённого примера на С и напишем его на Python:\n",
      "\n",
      ">>> x = 2337\r\n",
      "Как и в C, в ходе исполнения этого код проходит несколько отдельных этапов:\n",
      "\n",
      "\n",
      "Создаётся PyObject.\n",
      "\n",
      "Числу для PyObject’а присваивается typecode.\n",
      "\n",
      "2337 присваивается значение для PyObject’а.\n",
      "\n",
      "Создаётся имя x.\n",
      "x указывает на новый PyObject.\n",
      "Счётчик ссылок PyObject’а увеличивается на 1.\n",
      "\n",
      "\n",
      "Примечание: PyObject — не то же самое, что объект в Python, эта сущность характерна для CPython и представляет базовую структуру всех объектов Python.\n",
      "\r\n",
      "PyObject определяется как C-структура, так что если вы удивляетесь, почему нельзя напрямую вызвать typecode или счётчик ссылок, то причина в том, что у вас нет прямого доступа к структурам. Вызовы методов вроде sys.getrefcount() могут помочь получить какие-то внутренние вещи.\n",
      "\r\n",
      "Если говорить о памяти, то это может выглядеть таким образом:\n",
      "\n",
      "\n",
      "\r\n",
      "Здесь схема памяти сильно отличается от схемы в С, показанной выше. Вместо того, чтобы x владел блоком памяти, в котором хранится значение 2337, свежесозданный объект Python владеет памятью, в которой живёт 2337. Python-имя x не владеет напрямую каким-либо адресом в памяти, как С-переменная владеет статической ячейкой.\n",
      "\r\n",
      "Если хотите присвоить x новое значение, попробуйте такой код:\n",
      "\n",
      ">>> x = 2338\r\n",
      "Поведение системы будет отличаться от того, что происходит в С, но будет не слишком сильно отличаться от исходной привязки (bind) в Python.\n",
      "\r\n",
      "В этом коде:\n",
      "\n",
      "\n",
      "Создаётся новый PyObject.\n",
      "\n",
      "Числу для PyObject’а присваивается typecode.\n",
      "\n",
      "2 присваивается значение для PyObject’а.\n",
      "\n",
      "x указывает на новый PyObject.\n",
      "\n",
      "Счётчик ссылок нового PyObject увеличивается на 1.\n",
      "\n",
      "Счётчик ссылок старого PyObject уменьшается на 1.\n",
      "\n",
      "\r\n",
      "Теперь схема памяти выглядит так:\n",
      "\n",
      "\n",
      "\r\n",
      "Эта иллюстрация демонстрирует, что x указывает на ссылку на объект и не владеет областью памяти, как раньше. Также вы видите, что команда x = 2338 является не присваиванием, а, скорее, привязкой (binding) имени x к ссылке.\n",
      "\r\n",
      "Кроме того, предыдущий объект (содержавший значение 2337) теперь находится в памяти со счётчиком ссылок, равным 0, и будет убран сборщиком мусора.\n",
      "\r\n",
      "Вы можете ввести новое имя y, как в примере на С:\n",
      "\n",
      ">>> y = x\r\n",
      "В памяти появится новое имя, но не обязательно новый объект:\n",
      "\n",
      "\n",
      "\r\n",
      "Теперь вы видите, что новый Python-объект не создан, создано только новое имя, которое указывает на тот же объект. Кроме того, счётчик ссылок объекта увеличился на 1. Можете проверить эквивалентность идентичности объектов, чтобы подтвердить их одинаковость:\n",
      "\n",
      ">>> y is x\n",
      "True\r\n",
      "Этот код показывает, что x и y являются одним объектом. Но не ошибитесь: y всё ещё является неизменяемым. Например, вы можете выполнить с y операцию сложения:\n",
      "\n",
      ">>> y += 1\n",
      ">>> y is x\n",
      "False\r\n",
      "После вызова сложения, вам вернётся новый Python-объект. Теперь память выглядит так:\n",
      "\n",
      "\n",
      "\r\n",
      "Был создан новый объект, и y теперь указывает на него. Любопытно, что точно такое же конечное состояние мы получили бы, если напрямую привязали y к 2339:\n",
      "\n",
      ">>> y = 2339\r\n",
      "После этого выражения мы получим такое конечное состояние памяти, как и при операции сложения. Напомню, что в Python вы не присваиваете переменные, а привязываете имена к ссылкам.\n",
      "\n",
      "Об интернированных (intern) объектах в Python\r\n",
      "Теперь вы понимаете, как создаются новые объекты в Python и как к ним привязываются имена. Пришло время поговорить об интернированных (interned) объектах.\n",
      "\r\n",
      "У нас есть такой Python-код:\n",
      "\n",
      ">>> x = 1000\n",
      ">>> y = 1000\n",
      ">>> x is y\n",
      "True\r\n",
      "Как и раньше, x и y являются именами, указывающими на один и тот же Python-объект. Но это объект, содержащий значение 1000, не может всегда иметь одинаковый адрес памяти. Например, если вы сложили два числа и получили 1000, то получите другой адрес:\n",
      "\n",
      ">>> x = 1000\n",
      ">>> y = 499 + 501\n",
      ">>> x is y\n",
      "False\r\n",
      "На этот раз строка x is y возвращает False. Если вас это смутило, не беспокойтесь. Вот что происходит при исполнении этого кода:\n",
      "\n",
      "\n",
      "Создаётся Python-объект (1000).\n",
      "\n",
      "Ему присваивается имя x.\n",
      "\n",
      "Создаётся Python-объект (499).\n",
      "\n",
      "Создаётся Python-объект (501).\n",
      "\n",
      "Эти два объекта складываются.\n",
      "\n",
      "Создаётся новый Python-объект (1000).\n",
      "\n",
      "Ему присваивается имя y.\n",
      "\n",
      "\n",
      "Технические пояснения: описанные шаги имеют место только в том случае, когда этот код исполняется внутри REPL. Если вы возьмёте приведённый пример, вставите в файл и запустите его, то строка x is y вернёт True.\n",
      "\r\n",
      "Причина в сообразительности компилятора CPython, который старается выполнить peephole-оптимизации, помогающие по мере возможности экономить шаги исполнения кода. Подробности вы можете найти в исходном коде peephole-оптимизатора CPython.\n",
      "\r\n",
      "Но разве это не расточительно? Ну да, но эту цену вы платите за все замечательные преимущества Python. Вам не нужно думать об удалении подобных промежуточных объектов, и даже не нужно знать об их существовании! Прикол в том, что эти операции выполняются относительно быстро, и вы бы о них не узнали до этого момента.\n",
      "\r\n",
      "Создатели Python мудро подметили эти накладные расходы и решили сделать несколько оптимизаций. Их результатом является поведение, которое может удивить новичков:\n",
      "\n",
      ">>> x = 20\n",
      ">>> y = 19 + 1\n",
      ">>> x is y\n",
      "True\r\n",
      "В этом примере почти такой же код, как и выше, за исключением того, что мы получаем True. Всё дело в интернированных (interned) объектах. Python предварительно создаёт в памяти определённое подмножество объектов и хранит их в глобальном пространстве имён для повседневного использования.\n",
      "\r\n",
      "Какие объекты зависят от реализации Python? В CPython 3.7 интернированными являются:\n",
      "\n",
      "\n",
      "Целые числа в диапазоне от -5 до 256.\n",
      "\n",
      "Строки, содержащие только ASCII-буквы, цифры или знаки подчёркивания.\n",
      "\n",
      "\r\n",
      "Так сделано потому, что эти переменные очень часто используются во многих программах. Интернируя, Python предотвращает выделение памяти для постоянно используемых объектов.\n",
      "\r\n",
      "Строки размером меньше 20 символов и содержащие ASCII-буквы, цифры или знаки подчёркивания будут интернированы, поскольку предполагается, что они будут применяться в качестве идентификаторов:\n",
      "\n",
      ">>> s1 = \"realpython\"\n",
      ">>> id(s1)\n",
      "140696485006960\n",
      ">>> s2 = \"realpython\"\n",
      ">>> id(s2)\n",
      "140696485006960\n",
      ">>> s1 is s2\n",
      "True\r\n",
      "Здесь s1 и s2 указывают на один и тот же адрес в памяти. Если бы мы вставили не ASCII-букву, цифру или знак подчёркивания, то получили бы другой результат:\n",
      "\n",
      ">>> s1 = \"Real Python!\"\n",
      ">>> s2 = \"Real Python!\"\n",
      ">>> s1 is s2\n",
      "False\r\n",
      "В этом примере использован восклицательный знак, поэтому строки не интернированы и являются разными объектами в памяти.\n",
      "\n",
      "Бонус: Если хотите, чтобы эти объекты ссылались на один и тот же интернированный объект, то можете воспользоваться sys.intern(). Один из способов применения этой функции описан в документации:\n",
      "\n",
      "Интернирование строк полезно для небольшого повышения производительности при поиске по словарю: если ключи в словаре и искомый ключ интернированы, то сравнение ключей (после хэширования) может выполняться с помощью сравнения указателей, а не строк. (Источник)\r\n",
      "Интернированные объекты часто путают программистов. Просто запомните, что если начнёте сомневаться, то всегда можете воспользоваться id() и is для определения эквивалентности объектов.\n",
      "\n",
      "Эмулирование указателей в Python\r\n",
      "Тот факт, что указатели в Python отсутствуют нативно, не означает, что вы не можете воспользоваться преимуществами применения указателей. На самом деле есть несколько способов эмулирования указателей в Python. Здесь мы рассмотрим два из них:\n",
      "\n",
      "\n",
      "Применение в качестве указателей изменяемых типов.\n",
      "\n",
      "Применение специально подготовленных Python-объектов.\n",
      "\n",
      "\n",
      "Применение в качестве указателей изменяемых типов\r\n",
      "Вы уже знаете, что такое изменяемые типы. Именно благодаря их изменяемости мы можем эмулировать поведение указателей. Допустим, нужно реплицировать этот код:\n",
      "\n",
      "void add_one(int *x) {\n",
      "    *x += 1;\n",
      "}\r\n",
      "Этот код берёт указатель на число (*x) и инкрементирует значение на 1. Вот основная функция для исполнения кода:\n",
      "\n",
      "#include <stdiо.h>\n",
      "\n",
      "int main(void) {\n",
      "    int y = 2337;\n",
      "    printf(\"y = %d\\n\", y);\n",
      "    add_one(&y);\n",
      "    printf(\"y = %d\\n\", y);\n",
      "    return 0;\n",
      "}\r\n",
      "В приведённом фрагменте мы присвоили y значение 2337, вывели на экран текущее значение, увеличили его на 1, а затем вывели новое значение. На экране появляется:\n",
      "\n",
      "y = 2337\n",
      "y = 2338\r\n",
      "Один из способов репликации этого поведения в Python — использовать изменяемый тип. Например, применить список и изменить первый элемент:\n",
      "\n",
      ">>> def add_one(x):\n",
      "...    x[0] += 1\n",
      "...\n",
      ">>> y = [2337]\n",
      ">>> add_one(y)\n",
      ">>> y[0]\n",
      "2338\r\n",
      "Здесь add_one(x) обращается к первому элементу и увеличивает его значение на 1. Применение списка означает, что в результате мы получим изменённое значение. Так значит в Python существуют указатели? Нет. Описанное поведение стало возможным потому, что список — это изменяемый тип. Если вы попытаетесь использовать кортеж, то получите ошибку:\n",
      "\n",
      ">>> z = (2337,)\n",
      ">>> add_one(z)\r\n",
      "Обратная трассировка (последними идут самые свежие вызовы):\n",
      "\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 2, in add_one\n",
      "TypeError: 'tuple' object does not support item assignment\r\n",
      "Этот код демонстрирует неизменяемость кортежа, поэтому он не поддерживает присваивание элементов. \n",
      "\n",
      "list не единственный изменяемый тип, указатели части эмулируются и с помощью dict.\n",
      "\r\n",
      "Допустим, у вас есть приложение, которое должно отслеживать возникновение интересных событий. Это можно сделать с помощью создания словаря и использования одного из его элементов в качестве счётчика:\n",
      "\n",
      ">>> counters = {\"func_calls\": 0}\n",
      ">>> def bar():\n",
      "...    counters[\"func_calls\"] += 1\n",
      "...\n",
      ">>> def foo():\n",
      "...    counters[\"func_calls\"] += 1\n",
      "...    bar()\n",
      "...\n",
      ">>> foo()\n",
      ">>> counters[\"func_calls\"]\n",
      "2\r\n",
      "В этом примере словарь использует счётчики для отслеживания количества вызовов функции. После вызова foo() счётчик увеличился на 2, как и ожидалось. И всё благодаря изменяемости dict.\n",
      "\r\n",
      "Не забывайте, это лишь эмуляция поведения указателя, оно никак не связано с настоящими указателями в C и C++. Можно сказать, эти операции обходятся дороже, чем если бы они выполнялись в C или C++.\n",
      "\n",
      "Использование объектов Python\n",
      "dict — прекрасный способ эмулирования указателей в Python, но иногда бывает утомительно помнить, какое имя ключа вы использовали. Особенно, если вы применяете словарь в разных частях приложения. Здесь может помочь настраиваемый класс Python.\n",
      "\r\n",
      "Допустим, вам нужно отслеживать метрики в приложении. Отличный способ абстрагироваться от раздражающих подробностей — это создать класс:\n",
      "\n",
      "class Metrics(object):\n",
      "    def __init__(self):\n",
      "        self._metrics = {\n",
      "            \"func_calls\": 0,\n",
      "            \"cat_pictures_served\": 0,\n",
      "        }\r\n",
      "В этом коде определён класс Metrics. Он всё ещё использует словарь для хранения актуальных данных, которые лежат в переменной члена _metrics. Это даст вам требуемую изменяемость. Теперь нужно лишь получить доступ к этим значениям. Можно сделать это с помощью свойств:\n",
      "\n",
      "class Metrics(object):\n",
      "    # ...\n",
      "\n",
      "    @property\n",
      "    def func_calls(self):\n",
      "        return self._metrics[\"func_calls\"]\n",
      "\n",
      "    @property\n",
      "    def cat_pictures_served(self):\n",
      "        return self._metrics[\"cat_pictures_served\"]\r\n",
      "Здесь мы используем @property. Если вы не знакомы с декораторами, то почитайте статью Primer on Python Decorators. В данном случае декоратор @property позволяет обратиться к func_calls и cat_pictures_served, как если бы они были атрибутами:\n",
      "\n",
      ">>> metrics = Metrics()\n",
      ">>> metrics.func_calls\n",
      "0\n",
      ">>> metrics.cat_pictures_served\n",
      "0\r\n",
      "То, что вы можете обратиться к этим именам как к атрибутам, означает, что вы абстрагированы от факта, что эти значения хранятся в словаре. К тому же вы делаете имена атрибутов более явными. Конечно, у вас должна быть возможность увеличивать значения:\n",
      "\n",
      "class Metrics(object):\n",
      "    # ...\n",
      "\n",
      "    def inc_func_calls(self):\n",
      "        self._metrics[\"func_calls\"] += 1\n",
      "\n",
      "    def inc_cat_pics(self):\n",
      "        self._metrics[\"cat_pictures_served\"] += 1\r\n",
      "Мы ввели два новых метода:\n",
      "\n",
      "\n",
      "inc_func_calls()\n",
      "inc_cat_pics()\n",
      "\r\n",
      "Они меняют значения в словаре metrics. Теперь у вас есть класс, который можно изменить так же, как и указатель:\n",
      "\n",
      ">>> metrics = Metrics()\n",
      ">>> metrics.inc_func_calls()\n",
      ">>> metrics.inc_func_calls()\n",
      ">>> metrics.func_calls\n",
      "2\r\n",
      "Вы можете обращаться к func_calls и вызывать inc_func_calls() в разных частях приложений и эмулировать указатели в Python. Это полезно в ситуациях, когда у вас есть что-то вроде metrics, что нужно часто использовать и обновлять в разных частях приложений.\n",
      "\n",
      "Примечание: В данном случае, явное создание inc_func_calls() и inc_cat_pics() вместо использования @property.setter не даёт пользователям задавать эти значения произвольному int, или неправильное значение вроде словаря.\n",
      "\r\n",
      "Вот полный исходный код класса Metrics:\n",
      "\n",
      "class Metrics(object):\n",
      "    def __init__(self):\n",
      "        self._metrics = {\n",
      "            \"func_calls\": 0,\n",
      "            \"cat_pictures_served\": 0,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def func_calls(self):\n",
      "        return self._metrics[\"func_calls\"]\n",
      "\n",
      "    @property\n",
      "    def cat_pictures_served(self):\n",
      "        return self._metrics[\"cat_pictures_served\"]\n",
      "\n",
      "    def inc_func_calls(self):\n",
      "        self._metrics[\"func_calls\"] += 1\n",
      "\n",
      "    def inc_cat_pics(self):\n",
      "        self._metrics[\"cat_pictures_served\"] += 1\n",
      "Реальные указатели с помощью ctypes\r\n",
      "Может быть, всё-таки есть указатели в Python, особенно в CPython? С помощью встроенного модуля ctypes можно создать настоящие указатели, как в C. Если вы не знакомы с ctypes, можете почитать статью Extending Python With C Libraries and the «ctypes» Module.\n",
      "\r\n",
      "Вам это может понадобиться в тех случаях, когда нужно вызвать библиотеку С, которой необходимы указатели. Вернёмся к упомянутой выше С-функции add_one():\n",
      "\n",
      "void add_one(int *x) {\n",
      "    *x += 1;\n",
      "}\r\n",
      "Напомню, что этот код увеличивает значение x на 1. Чтобы им воспользоваться, сначала скомпилируем код в общий (shared) объект. Будем считать, что наш файл хранится в add.c, сделать это можно с помощью gcc:\n",
      "\n",
      "$ gcc -c -Wall -Werror -fpic add.c\n",
      "$ gcc -shared -o libadd1.so add.o\r\n",
      "Первая команда компилирует исходный файл C в объект add.o. Вторая команда берёт этот несвязанный объект и создаёт общий объект libadd1.so.\n",
      "\n",
      "libadd1.so должен лежать в вашей текущей директории. Можете с помощью ctypes загрузить его в Python:\n",
      "\n",
      ">>> import ctypes\n",
      ">>> add_lib = ctypes.CDLL(\"./libadd1.so\")\n",
      ">>> add_lib.add_one\n",
      "<_FuncPtr object at 0x7f9f3b8852a0>\r\n",
      "Код ctypes.CDLL возвращает объект, который представляет общий объект libadd1. Поскольку в нём вы определили add_one(), вы можете обращаться к этой функции, как если бы это был любой другой Python-объект. Но прежде чем вызывать функцию, нужно определить её сигнатуру. Так Python будет знать, что вы передаёте функции правильный тип.\n",
      "\r\n",
      "В нашем случае сигнатурой функции является указатель на число, ctypes позволит задать это с помощью такого кода:\n",
      "\n",
      ">>> add_one = add_lib.add_one\n",
      ">>> add_one.argtypes = [ctypes.POINTER(ctypes.c_int)]\r\n",
      "Здесь мы задаём сигнатуру функции, чтобы удовлетворить ожиданиям C. Теперь, если попробуем вызвать этот код с неправильным типом, то вместо непредсказуемого поведения получим красивое предупреждение:\n",
      "\n",
      ">>> add_one(1)\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "ctypes.ArgumentError: argument 1: <class 'TypeError'>: \\\n",
      "expected LP_c_int instance instead of int\r\n",
      "Python бросает ошибку и объясняет, что add_one() хочет получить указатель, а не просто целое число. К счастью, в ctypes есть способ передавать указатели таким функциям. Сначала объявим целое число в стиле С:\n",
      "\n",
      ">>> x = ctypes.c_int()\n",
      ">>> x\n",
      "c_int(0)\r\n",
      "Здесь мы создали целое число x со значением 0. ctypes предоставляет удобную функцию byref(), которая позволяет передавать переменную по ссылке.\n",
      "\n",
      "Примечание: Словосочетание по ссылке является антонимом передаче переменной по значению.\n",
      "\r\n",
      "При передаче по ссылке вы передаёте ссылку на исходную переменную, поэтому изменения будут отражены и на ней. При передаче по значению вы получаете копию исходной переменной, и изменения эту исходную переменную уже не затрагивают.\n",
      "\r\n",
      "Для вызова add_one() можете использовать этот код:\n",
      "\n",
      ">>> add_one(ctypes.byref(x))\n",
      "998793640\n",
      ">>> x\n",
      "c_int(1)\r\n",
      "Отлично! Ваше число увеличилось на 1. Поздравляю, вы успешно использовали в Python настоящие указатели.\n",
      "\n",
      "Заключение\r\n",
      "Теперь вы лучше понимаете взаимосвязь между объектами Python и указателями. Хотя некоторые уточнения касательно имён и переменных выглядят проявлениями педантизма, однако понимание сути эти ключевых терминов улучшает ваше понимание механизма обработки переменных в Python.\n",
      "\r\n",
      "Также мы узнали некоторые способы эмулирования указателей в Python:\n",
      "\n",
      "\n",
      "Использование изменяемых объектов в качестве указателей с низкими накладными расходами.\n",
      "\n",
      "Создание настраиваемых Python-объектов для простоты использования.\n",
      "\n",
      "Разлочивание настоящих указателей с помощью модуля ctypes.\n",
      "\n",
      "\r\n",
      "Эти методы позволяют эмулировать указатели в Python без необходимости жертвовать предоставляемой языком безопасностью памяти.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Эта статья посвящена Python Gateway — комьюнити-проекту с открытым исходным кодом для платформы данных InterSystems IRIS. Этот проект позволяет оркестрировать любые алгоритмы машинного обучения, созданные на языке Python (основная среда для многих Data Scientists), использовать многочисленные готовые библиотеки для быстрого создания адаптивных, роботизированных аналитических AI/ML-решений на платформе InterSystems IRIS. В этой статье я покажу как InterSystems IRIS может оркестровать процессы на языке Python, эффективно осуществлять двустороннюю передачу данных и создавать интеллектуальные бизнес-процессы.\n",
      "План\n",
      "\n",
      "Введение.\n",
      "Инструментарий.\n",
      "Установка.\n",
      "API.\n",
      "Интероперабельность.\n",
      "Jupyter Notebook.\n",
      "Выводы.\n",
      "Ссылки.\n",
      "MLToolkit.\n",
      "\n",
      "Введение\n",
      "Python — высокоуровневый язык программирования общего назначения, ориентированный на повышение производительности разработчика и читаемости кода. В данной серии статей я расскажу о возможностях применения языка Python на платформе InterSystems IRIS, при этом основным фокусом данной статьи является применение Python как языка создания и применения моделей машинного обучения.\n",
      "Машинное обучение (ML) — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, а обучение в процессе решения множества сходных задач. \n",
      "Алгоритмы и модели машинного обучения становятся все более распространенными. Причин тому множество, но все сводится к доступности, простоте и достижению практических результатов. Является ли кластеризация или даже нейросетевое моделирование новой технологией? \n",
      "Конечно нет, но в настоящее время нет необходимости писать сотни тысяч строк кода, чтобы запустить одну модель, а затраты на создание и применение моделей становятся всё меньше и меньше.\n",
      "Инструменты развиваются — в то время как у нас нет полностью GUI-ориентированных AI/ML-инструментов, тот прогресс, который мы наблюдали со многими другими классами информационных систем, например, BI (от написания кода до использования фреймворков и GUI-ориентированных конфигурируемых решений), наблюдается и в инструментах для создания AI/ML. Мы уже прошли этап написания кода и сегодня используем фреймворки для построения и обучения моделей.\n",
      "Другие улучшения, например, возможность распространения предобученной модели, когда конечный пользователь должен просто закончить обучение модели на его специфических данных также упрощают начало применения машинного обучения. Эти достижения значительно облегчают изучение машинного обучения как непосредственно для специалистов, так и для компаний в целом.\n",
      "С другой стороны, мы собираем всё больше данных. Благодаря унифицированной платформе данных, такой как InterSystems IRIS, вся эта информация может быть немедленно подготовлена и использована в качестве исходных данных для моделей машинного обучения.\n",
      "С переходом на облако запуск AI/ML-проектов становится легче, чем когда-либо. Мы можем потреблять только те ресурсы, которые нам необходимы. Более того, благодаря параллелизации, предлагаемой облачными платформами, мы можем сэкономить затрачиваемое время.\n",
      "Но как насчет результатов? Здесь все становится сложнее. Существует множество инструментов для построения моделей, о которых я расскажу далее. Построить хорошую модель непросто, но что дальше? Получение прибыли от использования модели бизнесом также является нетривиальной задачей. Корень проблемы в разделении аналитических и транзакционных нагрузок, и моделей данных. Когда мы обучаем модель, мы обычно делаем это на исторических данных. Но место для построенной модели — в транзакционной обработке данных. Что хорошего в лучшей модели обнаружения мошеннических транзакций, если мы запускаем ее раз в день? Мошенники уже давно ушли с деньгами. Нам нужно тренировать модель на исторических данных, но мы также должны применять её в реальном времени на новых поступающих данных, чтобы наши бизнес-процессы могли действовать в соответствии с прогнозами, сделанными моделью.\n",
      "ML Toolkit — набор инструментов, целью которого является именно это: объединение моделей и транзакционной среды, чтобы построенные модели можно было легко использовать прямо в ваших бизнес-процессах. Python Gateway является частью ML Toolkit и обеспечивает интеграцию с языком Python (аналогично как R Gateway, являясь частью ML Toolkit обеспечивает интеграцию с языком R).\n",
      "Инструментарий\n",
      "Прежде чем мы продолжим, я хотел бы описать несколько инструментов и библиотек для Python, которые мы будем использовать позже.\n",
      "Технологии\n",
      "\n",
      "Python — интерпретируемый, высокоуровневый язык программирования общего назначения. Основным преимуществом языка является большая библиотека математических, ML- и AI-библиотек. Как и ObjectScript, это объектно-ориентированный язык, но всё определяется динамически, а не статично. Также все является объектом. Более поздние статьи предполагают мимолетное знакомство с языком. Если вы хотите начать обучение, я рекомендую начать с документации.\n",
      "Для наших последующих упражнений установите Python 3.6.7 64 bit.\n",
      "IDE: Я использую PyCharm, но вообще их много. Если вы используете Atelier, то существует Eclipse-плагин для разработчиков на Python. Если вы используете VS Code, то существует расширение для Python. \n",
      "Notebook: вместо IDE вы можете писать и делиться своими скриптами в онлайн-ноутбуках. Самый популярный из них — Jupyter.\n",
      "\n",
      "Библиотеки\n",
      "Вот (неполный) список библиотек для машинного обучения:\n",
      "\n",
      "Numpy — фундаментальный пакет для точных вычислений.\n",
      "Pandas — высокопроизводительные структуры данных и инструменты анализа данных.\n",
      "Matplotlib — создание графиков.\n",
      "Seaborn — визуализация данных, основанная на matplotlib.\n",
      "Sklearn — методы машинного обучения.\n",
      "XGBoost — алгоритмы машинного обучения в рамках методологии градиентного бустинга (Gradient Boosting).\n",
      "Gensim — NLP.\n",
      "Keras — нейронные сети.\n",
      "Tensorflow — платформа для создания моделей машинного обучения.\n",
      "PyTorch — платформа для создания моделей машинного обучения, ориентированная на Python.\n",
      "Nyoka — PMML из различных моделей.\n",
      "\n",
      "Технологии AI/ML позволяют сделать бизнес более эффективным и адаптируемым. Более того, сегодня эти технологии становятся проще в разработке и внедрении. Начните изучать AI/ML-технологии и то, как они могут помочь вашей организации расти.\n",
      "Установка\n",
      "Существует несколько способов установки и использования Python Gateway:\n",
      "\n",
      "ОС\n",
      "\n",
      "Windows\n",
      "Linux\n",
      "Mac\n",
      "\n",
      "Docker\n",
      "\n",
      "Используйте образ из DockerHub\n",
      "Создайте свой собственный образ\n",
      "\n",
      "\n",
      "Независимо от способа установки, вам понадобится исходный код. Единственное место для скачивания кода — страница релизов. Она содержит протестированные стабильные релизы, просто берите последний. На данный момент это 0.8, но со временем будут и новые. Не клонируйте/загружайте репозиторий, скачайте последний релиз.\n",
      "ОС\n",
      "Если вы устанавливаете Python Gateway в операционную систему, то сначала (вне зависимости от операционной системы) вам необходимо установить Python. Для этого:\n",
      "\n",
      "Установить Python 3.6.7 64 bit. Рекомендуется установить Python в директорию по умолчанию.\n",
      "Установите модуль dill: pip install dill.\n",
      "Загрузите код ObjectScript (т.е. do $system.OBJ.ImportDir(\"C:\\InterSystems\\Repos\\Python\\isc\\py\\\", \"*.cls\", \"c\",,1)) в любую область с продукциями. В случае, если вы хотите, чтобы существующая область поддерживала продукции, выполните: write ##class(%EnsembleMgr).EnableNamespace($Namespace, 1).\n",
      "Поместите callout DLL/SO/DYLIB в папку bin вашего инстанса InterSystems IRIS. Файл библиотеки должен быть доступен по пути, возвращаемом write ##class(isc.py.Callout).GetLib(). \n",
      "\n",
      "Windows\n",
      "\n",
      "Убедитесь, что переменная окружения PYTHONHOME указывает на Python 3.6.7.\n",
      "Убедитесь, что системная переменная окружения PATH содержит переменную PYTHONHOME (или директорию, на которую она указывает).\n",
      "\n",
      "Linux (Debian/Ubuntu)\n",
      "\n",
      "Проверьте, что переменная окружения PATH содержит /usr/lib и /usr/lib/x86_64-linux-gnu. Используйте файл /etc/environment для установки переменных окружения.\n",
      "В случае ошибок undefined symbol: _Py_TrueStruct установите настройку PythonLib. Также в Readme есть раздел Troubleshooting.\n",
      "\n",
      "Mac\n",
      "\n",
      "В настоящее время поддерживается только питон 3.6.7 из Python.org. Проверьте переменную PATH.\n",
      "\n",
      "Если вы изменяли переменные окружения, перезапустите ваш продукт InterSystems.\n",
      "Docker\n",
      "Использование контейнеров имеет ряд преимуществ: \n",
      "\n",
      "Портативность\n",
      "Эффективность\n",
      "Изоляция\n",
      "Легковесность\n",
      "Иммутабельность\n",
      "\n",
      "Ознакомьтесь с этой серией статей для получения более подробной информации об использовании Docker с продуктами InterSystems. \n",
      "Все сборки Python Gateway на данный момент основаны на контейнерах 2019.4.\n",
      "Готовый образ\n",
      "Выполните: docker run -d -p 52773:52773 --name irispy intersystemscommunity/irispy-community:latest, чтобы загрузить и запустить Python Gateway с InterSystems IRIS Community Edition. Вот и все.\n",
      "Создайте свой собственный образ\n",
      "Для сборки докер-образа выполните в корне репозитория: docker build --force-rm --tag intersystemscommunity/irispy:latest ..\r\n",
      "По умолчанию образ собирается на основе образа store/intersystems/iris-community:2019.4.0.383.0, однако вы можете изменить это, установив переменную IMAGE.\r\n",
      "Для сборки из InterSystems IRIS выполните: `docker build --build-arg IMAGE=store/intersystems/iris:2019.4.0.383.0 --force-rm --tag intersystemscommunity/irispy:latest ``.\n",
      "После этого вы можете запустить докер-образ:\n",
      "docker run -d \\\n",
      "  -p 52773:52773 \\\n",
      "  -v /<HOST-DIR-WITH-iris.key>/:/mount \\\n",
      "  --name irispy \\\n",
      "  intersystemscommunity/irispy:latest \\\n",
      "  --key /mount/iris.key\n",
      "Если вы используете образ, основанный на InterSystems IRIS Community Edition, вы можете не указывать ключ.\n",
      "Комментарии\n",
      "\n",
      "Тестовый процесс isc.py.test.Process сохраняет ряд изображений во временный каталог. Возможно, вы захотите изменить этот путь на смонтированный каталог. Для этого отредактируйте настройку WorkingDir указав смонтированную директорию.\n",
      "Для доступа к терминалу выполните: docker exec -it irispy sh.\n",
      "Доступ к Порталу Управления Системой по логину SuperUser/SYS.\n",
      "Чтобы остановить контейнер, выполните: docker stop irispy && docker rm --force irispy.\n",
      "\n",
      "Проверка установки\n",
      "После того, как вы установили Python Gateway, стоит проверить, что он работает. Выполните этот код в терминале InterSystems IRIS:\n",
      "set sc = ##class(isc.py.Callout).Setup() \n",
      "set sc = ##class(isc.py.Main).SimpleString(\"x='HELLO'\", \"x\", , .var).\n",
      "write var\n",
      "В результате должно быть выведено HELLO — значение Python-переменной x. Если возвращаемый статус sc является ошибкой или var пусто, проверьте Readme — Troubleshooting section.\n",
      "API\n",
      "Python Gateway установлен, и вы убедились, что он работает. Пора начинать его использовать!\r\n",
      "Главный интерфейс к Python это isc.py.Main. Он предлагает следующие группы методов (все возвращают %Status):\n",
      "\n",
      "Исполнение кода\n",
      "Передача данных\n",
      "Вспомогательные\n",
      "\n",
      "Исполнение кода\n",
      "Эти методы позволяют исполнять произвольный Python-код.\n",
      "SimpleString\n",
      "SimpleString — это основной метод. Он принимает 4 опциональных аргумента:\n",
      "\n",
      "code — строка кода для выполнения. Символ перевода строки: $c(10).\n",
      "returnVariable — имя переменной для возврата.\n",
      "serialization — как сериализовать returnVariable. 0 — string (по умолчанию), 1 — repr.\n",
      "result — ByRef ссылка на переменную, в которую записывается значение returnVariable.\n",
      "\n",
      "Выше мы выполнили:\n",
      "set sc = ##class(isc.py.Main).SimpleString(\"x='HELLO'\", \"x\", , .var).\n",
      "В данном примере мы присваиваем переменной Python x значение Hello и хотим вернуть значение Python переменной x в ObjectScript переменную var.\n",
      "ExecuteCode\n",
      "ExecuteCode является более безопасной и менее ограниченной альтернативой SimpleString.\r\n",
      "Строки в платформе InterSystems IRIS ограничены 3 641 144 символами, и если вы хотите выполнить более длинный кусок кода, вам необходимо использовать потоки.\r\n",
      "Принимаются два аргумента:\n",
      "\n",
      "code — строка или поток Python кода для исполнения.\n",
      "variable — (опционально) присваивает результат выполнения code этой Python переменной.\n",
      "\n",
      "Приме использовать:\n",
      "set sc = ##class(isc.py.Main).ExecuteCode(\"2*3\", \"y\").\n",
      "В этом примере мы умножаем 2 на 3 и записываем результат в Python переменную y.\n",
      "Передача данных\n",
      "Передавайте данные в Python и обратно.\n",
      "Python -> InterSystems IRIS\n",
      "Есть 4 способа получить значение Python переменной в InterSystems IRIS, в зависимости от сериализации, которая вам нужна:\n",
      "\n",
      "String для простых типов данных и отладки.\n",
      "Repr для хранения простых объектов и отладки.\n",
      "JSON для легкого манипулирования данными на стороне InterSystems IRIS.\n",
      "Pickle для сохранения объектов.\n",
      "\n",
      "Эти методы позволяют получать переменные с Python в виде строки или потоков.\n",
      "\n",
      "GetVariable(variable, serialization, .stream, useString) — получить serialization переменной variable в stream. Если useString равно 1 и сериализация помещается в строку, то возвращается строка а не поток.\n",
      "GetVariableJson(variable, .stream, useString) — получить JSON сериализацию переменной.\n",
      "GetVariablePickle(variable, .stream, useString, useDill) -получить Pickle (или Dill) сериализацию переменной.\n",
      "\n",
      "Попробуем получить нашу переменную y.\n",
      "set sc = ##class(isc.py.Main).GetVariable(\"y\", , .val, 1)\n",
      "write val\n",
      ">6\n",
      "InterSystems IRIS -> Python\n",
      "Загружаем данные из InterSystems IRIS в Python.\n",
      "\n",
      "ExecuteQuery(query, variable, type, namespace) — создает набор данных (pandas dataframe или list) из sql запроса и устанавливает его в Python переменную variable. Пакет isc.py должен быть доступен в области namespace — там будет исполняться запрос.\n",
      "ExecuteGlobal(global, variable, type, start, end, mask, labels, namespace) — загружает данные глобала global от сабскрипта start до end в Python как переменную типа type: list, либо pandas dataframe. Описание опциональных аргументов mask and labels доступно в документации класса и репозитории Data Transfer docs.\n",
      "ExecuteClass(class, variable, type, start, end, properties, namespace) — загружает данные класса class от id start до end в Python как переменную типа type: list, либо pandas dataframe. properties — список (разделитель — запятая) свойств класса, которые нужно загрузить в набор данных. Поддерживаются маски * и ?. По умолчанию — * (все свойства). Свойство %%CLASSNAME игнорируется.\n",
      "ExecuteTable(table, variable, type, start, end, properties, namespace) — загружает данные таблицы table от id start до end в Python.\n",
      "\n",
      "ExecuteQuery — универсален (любой корректный SQL-запрос будет передан в Python). Однако, ExecuteGlobal и его обёртки ExecuteClass и ExecuteTable работают с рядом ограничений. Они намного быстрее (в 3-5 раз быстрее ODBC-драйвера и в 20 раз быстрее ExecuteQuery). Дополнительная информация в Data Transfer docs.\r\n",
      "Все эти методы поддерживают передачу данных из любой области. Пакет isc.py должен быть доступен в целевой области.\n",
      "ExecuteQuery\n",
      "ExecuteQuery(request, variable, type, namespace) — передача результатов любого корректного SQL-запроса на Python. Это самый медленный метод передачи данных. Используйте его, если ExecuteGlobal и его обёртки недоступны.\n",
      "Аргументы:\n",
      "\n",
      "query — sql запрос.\n",
      "variable — название Python переменной в которую записываются данные.\n",
      "type — list или Pandas dataframe.\n",
      "namespace — область в которой будет исполняться запрос.\n",
      "\n",
      "ExecuteGlobal\n",
      "ExecuteGlobal(global, variable, type, start, end, mask, labelels, namespace) — передача глобала в Python.\n",
      "Аргументы:\n",
      "\n",
      "global — имя глобала без ^\n",
      "variable — название Python переменной в которую записываются данные.\n",
      "type — list или Pandas dataframe.\n",
      "start — первый сабскрипт глобала. Обязательно %Integer.\n",
      "end — последний сабскрипт глобала. Обязательно %Integer.\n",
      "mask — маска значений глобала. Маска может быть короче, чем количество полей в глобале (в этом случае поля в конце будут пропущены). Как форматировать маску:\n",
      "\n",
      "+ передать значение как есть.\n",
      "- пропустить значение.\n",
      "b — Логический тип (0 — False, всё остальное — True).\n",
      "d — Дата (из $horolog, на Windows от 1970, на Linux от 1900).\n",
      "t — Время ($horolog, секунды после полуночи).\n",
      "m — Метка времени (строка формата YEAR-MONTH-DAY HOUR:MINUTE:SECOND).\n",
      "\n",
      "labels — %List названий колонок. Первый элемент — название сабскрипта.\n",
      "namespace — область в которой будет исполняться запрос.\n",
      "\n",
      "ExecuteClass\n",
      "Обертка над ExecuteGlobal. На основе определения класса подготавливает вызов ExecuteGlobal и вызывает его.\n",
      "ExecuteClass(class, variable, type, start, end, properties, namespace) — передача данных класса в Python.\n",
      "Аргументы:\n",
      "\n",
      "class — имя класса\n",
      "variable — название Python переменной в которую записываются данные.\n",
      "type — list или Pandas dataframe.\n",
      "start — стартовый Id.\n",
      "end — конечный Id\n",
      "properties — список (разделитель — запятая) свойств класса, которые нужно загрузить в набор данных. Поддерживаются маски * и ?. По умолчанию — * (все свойства). Свойство %%CLASSNAME игнорируется.\n",
      "namespace — область в которой будет исполняться запрос.\n",
      "\n",
      "Все свойства передаются как есть кроме свойств типов %Date, %Time, %Boolean и %TimeStamp — они конвертируются в соответствующие классы Python.\n",
      "ExecuteTable\n",
      "Обертка над ExecuteClass. Транслирует имя таблицы в имя класса и вызывает ExecuteClass. Сигнатура:\n",
      "ExecuteTable(table, variable, type, start, end, properties, namespace) — передача данных таблицы в Python.\n",
      "Аргументы:\n",
      "\n",
      "table — имя таблицы.\r\n",
      "Все остальные аргументы передаются как есть в ExecuteClass.\n",
      "\n",
      "Заметки\n",
      "\n",
      "ExecuteGlobal, ExecuteClass и ExecuteTable работают одинаково быстро.\n",
      "ExecuteGlobal в 20 раз быстрее чем ExecuteQuery на больших наборах данных (время передачи более >0.01 секунды).\n",
      "ExecuteGlobal, ExecuteClass и ExecuteTable работают на глобалах с данной структурой: ^global(key) = $lb(prop1, prop2, ..., propN) где key — целое число.\n",
      "Для ExecuteGlobal, ExecuteClass и ExecuteTable поддерживаемый диапазон значений %Date соответствует диапазону mktime и зависит от ОС (windows: 1970-01-01, linux 1900-01-01, mac). Используйте %TimeStampдля передачи данных вне этого диапазона или используйте pandas dataframe т.к. это ограничение только для списка.\n",
      "Для ExecuteGlobal, ExecuteClass и ExecuteTable все аргументы кроме источника данных (глобала, класса или таблицы) и переменной опциональны.\n",
      "\n",
      "Примеры\n",
      "Тестовый класс isc.py.test.Person содержит метод, демонстрирующий все варианты передачи данных:\n",
      "set global = \"isc.py.test.PersonD\"\n",
      "set class = \"isc.py.test.Person\"\n",
      "set table = \"isc_py_test.Person\"\n",
      "set query = \"SELECT * FROM isc_py_test.Person\"\n",
      "\n",
      "// Общие аргументы\n",
      "set variable = \"df\"\n",
      "set type = \"dataframe\"\n",
      "set start = 1\n",
      "set end = $g(^isc.py.test.PersonD, start)\n",
      "\n",
      "// Способ 0: ExecuteGlobal без аргументов\n",
      "set sc = ##class(isc.py.Main).ExecuteGlobal(global, variable _ 0, type)\n",
      "\n",
      "// Способ 1: ExecuteGlobal с аргументами    \n",
      "// При передаче глобала названия полей задаются вручную\n",
      "// globalKey - название сабсткрипта \n",
      "set labels = $lb(\"globalKey\", \"Name\", \"DOB\", \"TS\", \"RandomTime\", \"AgeYears\", \"AgeDecimal\", \"AgeDouble\", \"Bool\")\n",
      "\n",
      "// mask содержит на 1 элемент меньше чем labels потому что \"globalKey\" - название сабскипта\n",
      "// Пропускаем %%CLASSNAME\n",
      "set mask = \"-+dmt+++b\"\n",
      "\n",
      "set sc = ##class(isc.py.Main).ExecuteGlobal(global, variable _ 1, type, start, end, mask, labels)\n",
      "\n",
      "// Способ 2: ExecuteClass\n",
      "set sc = ##class(isc.py.Main).ExecuteClass(class, variable _ 2, type, start, end)\n",
      "\n",
      "// Способ 3: ExecuteTable\n",
      "set sc = ##class(isc.py.Main).ExecuteTable(table, variable _ 3, type, start, end)\n",
      "\n",
      "// Способ 4: ExecuteTable\n",
      "set sc = ##class(isc.py.Main).ExecuteQuery(query, variable _ 4, type)\n",
      "Вызовите метод do ##class(isc.py.test.Person).Test() чтобы посмотреть как работают все методы передачи данных.\n",
      "Вспомогательные методы\n",
      "\n",
      "GetVariableInfo(variable, serialization, .defined, .type, .length) — получить информацию о переменной: определена ли она, класс и длинну сериализации.\n",
      "GetVariableDefined(variable, .defined) — определена ли переменная.\n",
      "GetVariableType(variable, .type) — получить класс переменной.\n",
      "GetStatus() — получить и удалить последнее исключение на стороне Python.\n",
      "GetModuleInfo(module, .imported, .alias) — получить переменную модуля и статус импорта.\n",
      "GetFunctionInfo(function, .defined, .type, .docs, .signature, .arguments) — получить информацию о функции.\n",
      "\n",
      "Интероперабельность\n",
      "Вы научились вызывать Python Gateway из терминала, теперь начнем использовать его в продукции. Основа взаимодействия с Python в таком режиме — isc.py.ens.Operation. Он позволяет нам:\n",
      "\n",
      "Выполнять код на Python\n",
      "Сохранять/Восстанавливать Python контекст\n",
      "Загружать и получать данные из Python\n",
      "\n",
      "В принципе, Pyhton операция это обертка над isc.py.Main. Операция isc.py.ens.Operation дает возможность взаимодействия с процессом Python из продукций InterSystems IRIS. Поддерживается пять запросов:\n",
      "\n",
      "isc.py.msg.ExecutionRequest для исполнения Python кода. Возвращает isc.py.msg.ExecutionResponse с результатом исполнения и значениями запрошенных переменных.\n",
      "isc.py.msg.StreamExecutionRequest для исполнения Python кода. Возвращает isc.py.msg.StreamExecutionResponse результатом исполнения и значениями запрошенных переменных. Аналог isc.py.msg.ExecutionRequest, но принимает и возвращает потоки вместо строк.\n",
      "isc.py.msg.QueryRequest для передачи результата исполнения SQL запроса. Возвращает Ens.Response.\n",
      "isc.py.msg.GlobalRequest/isc.py.msg.ClassRequest/isc.py.msg.TableRequest для передачи данных глобала/класса/таблицы. Возвращает Ens.Response.\n",
      "isc.py.msg.SaveRequest для сохранения Python контекста. Возвращает Ens.StringResponse с идентификатором контекста.\n",
      "isc.py.msg.RestoreRequest для восстановления Python контекста.\n",
      "Кроме того, isc.py.ens.Operation имеет две настройки:\n",
      "\n",
      "Initializer — выбор класса, реализующего интерфейс isc.py.init.Abstract. Он может быть использован для загрузки функций, модулей, классов и т.п. Он выполняется один раз при запуске процесса.\n",
      "PythonLib — (только для Linux) если вы видите ошибки при загрузке, установите его значение равным libpython3.6m.so или даже в полному пути к библиотеке Python. \n",
      "\n",
      "\n",
      "\n",
      "Создание бизнес-процессов\n",
      "Доступно два класса, которые облегчают разработку бизнес-процессов:\n",
      "\n",
      "isc.py.ens.ProcessUtils позволяет извлекать аннотации из активностей с подстановкой переменных.\n",
      "isc.py.util.BPEmulator позволяет легко тестировать бизнес-процессы с Python. Он может выполнять бизнес-процесс (части на языке Python) в текущем процессе.\n",
      "\n",
      "Подстановка переменных\n",
      "Все бизнес-процессы, унаследованные от isc.py.ens.ProcessUtils, могут использовать метод GetAnnotation(name) для получения значения аннотации активности по её названию. Аннотация активности может содержать переменные, которые будут вычислены на стороне InterSystems IRIS перед передачей в Python. Вот синтаксис подстановки переменных:\n",
      "\n",
      "${class:method:arg1:...:argN} — вызов метода\n",
      "#{expr} — исполнить код на языке ObjectScript.\n",
      "\n",
      "Пример доступен в тестовом бизнес-процессе isc.py.test.Process, например, в активности Correlation Matrix: Graph: f.savefig(r'#{process.WorkDirectory}SHOWCASE${%PopulateUtils:Integer:1:100}.png'). В этом примере:\n",
      "\n",
      "#{process.WorkDirectory} возвращает свойство WorkDirectory объекта process, являющегося экземпляром класса isc.py.test.Process т.е. текущего бизнес-процесса.\n",
      "${%PopulateUtils:Integer:1:100} вызывает метод Integer класса %PopulateUtils, передавая аргументы 1 и 100, возвращая случайное целое число в диапазоне 1...100.\n",
      "\n",
      "Тестовый бизнес-процесс\n",
      "Тестовая продукция и тестовый бизнес-процесс доступны по умолчанию как часть шлюза Python Gateway. Для их использования:\n",
      "\n",
      "В терминале ОС выполните: pip install pandas matplotlib seaborn. \n",
      "В терминале InterSystems IRIS выполните: do ##class(isc.py.test.CannibalizationData).Import() для заполнения тестовых данных.\n",
      "Запустите продукцию isc.py.test.Production.\n",
      "Отправить запрос типа Ens.Request в isc.py.test.Process.\n",
      "\n",
      "Посмотрим, как все это работает вместе. Откройте isc.py.test.Process в редакторе BPL:\n",
      "\n",
      "Исполнение кода\n",
      "Самый важный вызов — исполнение Python кода:\n",
      "\n",
      "Используется запрос isc.py.msg.ExecutionRequest, вот его свойства:\n",
      "\n",
      "Code — Python код.\n",
      "SeparateLines — разделять ли код на строки для выполнения. $c(10) (\\n) используется для разделения строк. Обратите внимание, что НЕ рекомендуется обрабатывать сообщение целиком сразу, эта функция предназначена только для обработки def и подобных многострочных выражений. По умолчанию 0.\n",
      "Variables — разделенный запятыми список переменных, которые будут добавлены в ответ.\n",
      "Serialization — Как сериализовать переменные, которые мы хотим вернуть. Варианты: Str, Repr, JSON, Pickle и Dill, по умолчанию Str.\n",
      "\n",
      "В нашем случае, мы только устанавливаем свойство Code, так что все остальные свойства используют значения по умолчанию. Мы устанавливаем его вызовом process.GetAnnotation(\"Import pandas\"), который во время выполнения возвращает аннотацию после выполнения подстановки переменных. В конце концов, код import pandas as pd будет передан в Python. GetAnnotation может быть полезна для получения многострочных Python скриптов, но никаких ограничений на данный способ получения кода нет. Вы можете установить свойство Code любым удобным для вас способом.\n",
      "Получение переменных\n",
      "Еще один интересный вызов с использованием isc.py.msg.ExecutionRequest — Correlation Matrix: Tabular:\n",
      "\n",
      "Он вычисляет Матрицу Корреляции на стороне Python и извлекает переменную corrmat обратно в InterSystems IRIS в формате JSON, путем установки свойств запроса:\n",
      "\n",
      "Variables: \"corrmat\"\n",
      "Serialization: \"JSON\"\n",
      "\n",
      "Мы можем видеть результаты в Visual Trace:\n",
      "\n",
      "И если нам это значение понадобится в БП, его можно получить так: callresponse.Variables.GetAt(\"corrmat\").\n",
      "Передача данных\n",
      "Далее поговорим о передаче данных из InterSystems IRIS в Python, все запросы на передачу данных реализуют интерфейс isc.py.msg.DataRequest, который предоставляет следующие свойства:\n",
      "\n",
      "Variable — переменная Python в которую записываются данные.\n",
      "Type — тип переменной: dataframe (pandas dataframe) или list.\n",
      "Namespace — область из которой получаем данные. Пакет isc.py должен быть доступен в этой области. Это может быть область без поддержки продукций.\n",
      "\n",
      "На основе этого интерфейса реализованы 4 класса запросов:\n",
      "\n",
      "isc.py.msg.QueryRequest — установите свойство Query для передачи SQL запроса.\n",
      "isc.py.msg.ClassRequest — установите свойство Class для передачи данных класса.\n",
      "isc.py.msg.TableRequest — установить свойство Table для передачи данных таблицы.\n",
      "isc.py.msg.GlobalRequest — установить свойство Global для передачи данных глобала.\n",
      "\n",
      "В тестовом процессе посмотрите на активность RAW, где isc.py.msg.QueryRequest показан в действии.\n",
      "\n",
      "Сохранение/восстановление Python контекста\n",
      "Наконец, мы можем сохранить Python контекст в InterSystems IRIS, чтобы сделать это, отправим isc.py.msg.SaveRequest с аргументами:\n",
      "\n",
      "Mask — Сохраняются только переменные, удовлетворяющие маске. Поддерживаются * и ?. Пример: \"Data*, Figure?\". По умолчанию *.\n",
      "MaxLength — Максимальная длина сохраняемой переменной. Если сериализация переменной длиннее, то она будет проигнорирована. Установите 0, чтобы получить переменные любой длины. По умолчанию $$$MaxStringLength.\n",
      "Name — Имя контекста (опционально).\n",
      "Description — Описание контекста (опционально).\n",
      "\n",
      "Возвращает Ens.StringResponse с Id сохраненного контекста. В тестовом процессе посмотрите на активность Save Context.\n",
      "Соответствующий запрос isc.py.msg.RestoreRequest загружает контекст из InterSystems IRIS в Python:\n",
      "\n",
      "ContextId — идентификатор контекста.\n",
      "Clear — очистить контекст перед восстановлением.\n",
      "\n",
      "Jupyter Notebook\n",
      "Jupyter Notebook — это веб-приложение с открытым исходным кодом, позволяющее создавать ноутбуки, содержащие код, визуализации и текст и публиковать их. Python Gateway позволяет просматривать и редактировать BPL-процессы в виде Jupyter Notebook. Обратите внимание, что в настоящее время используется обычный executor Python 3.\n",
      "Это расширение предполагает, что аннотации содержат код Python и используют названия активностей в качестве предшествующих заголовков. Теперь возможно разрабатывать бизнес-процессы PythonGateway в Jupyter Notebook. Вот что возможно: \n",
      "\n",
      "Создавать новые бизнес-процессы\n",
      "Удалять бизнес-процессы\n",
      "Создавать новые активности\n",
      "Изменять активности\n",
      "Удалять активности\n",
      "\n",
      "Вот демо-видео. И несколько скриншотов:\n",
      "Проводник процессов\n",
      "\n",
      "Редактор процесса\n",
      "\n",
      "Установка\n",
      "\n",
      "Вам понадобится InterSystems IRIS 2019.2+.\n",
      "Установите PythonGateway v0.8+ (требуется только isc.py.util.Jupyter, isc.py.util.JupyterCheckpoints и isc.py.ens.ProcessUtils).\n",
      "Обновите код ObjectScript из репозитория.\n",
      "Выполните do ##class(isc.py.util.Jupyter).Install() и следуйте подсказкам.\n",
      "\n",
      "Документация.\n",
      "Выводы\n",
      "MLToolkit — набор инструментов, целью которого является объединение моделей и транзакционной среды, чтобы построенные модели можно было легко использовать прямо в ваших бизнес-процессах. Python Gateway является частью MLToolkit и обеспечивает интеграцию с языком Python позволяя оркестрировать любые алгоритмы машинного обучения, созданные на языке Python (основная среда для многих Data Scientists), использовать многочисленные готовые библиотеки для быстрого создания адаптивных, роботизированных аналитических AI/ML-решений на платформе InterSystems IRIS.\n",
      "Ссылки\n",
      "\n",
      "Предыдущая статья\n",
      "Python Gateway\n",
      "Python Gateway Samples\n",
      "Convergent-Analytics\n",
      "Python 3.6.7 64 bit\n",
      "Документация и курсы по Python\n",
      "\n",
      "MLToolkit\n",
      "Группа пользователей MLToolkit — это частный GitHub-репозиторий, созданный как часть корпоративной GitHub-организации InterSystems. Она адресована внешним пользователям, которые устанавливают, изучают или уже используют компоненты MLToolkit, включая Python Gateway. В группе доступен ряд реализованных кейсов (с исходным кодом и тестовыми данными) в сферах маркетинга, производства, медицины и многих других отраслях. Чтобы присоединиться к группе пользователей ML Toolkit, пожалуйста, отправьте короткое сообщение по электронной почте по следующему адресу: MLToolkit@intersystems.com и укажите в своем письме следующие данные:\n",
      "\n",
      "Имя пользователя GitHub\n",
      "Организация (вы работаете или учитесь)\n",
      "Должность (ваша фактическая должность в вашей организации, или \"Студент\", или \"Независимый\").\n",
      "Страна\n",
      "\n",
      "Тем, кто прочитал статью и заинтересовался возможностями InterSystems IRIS как платформы для разработки или размещения механизмов искусственного интеллекта и машинного обучения, мы предлагаем обсудить возможные сценарии, представляющие интерес для вашего предприятия. Мы с готовностью проанализируем потребности вашего предприятия и совместно определим план действий; контактный адрес электронной почты нашей экспертной группы AI/ML – MLToolkit@intersystems.com.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Есть много статей, объясняющих, для чего нужен Python GIL (The Global Interpreter Lock) (я подразумеваю CPython). Если вкратце, то GIL не даёт многопоточному чистому коду на Python использовать несколько ядер процессора.\n",
      "\r\n",
      "Однако мы в Vaex исполняем большинство задач с интенсивными вычислениями на С++ с отключением GIL. Это нормальная практика для высокопроизводительных Python—библиотек, в которых Python всего лишь выступает в роли высокоуровневого связующего звена.\n",
      "\r\n",
      "GIL нужно отключать явно, и это ответственность программиста, о которой он может забыть, что приведёт к неэффективному использованию мощностей. Недавно я сам побывал в роли забывшего, и нашёл подобную проблему в Apache Arrow (это зависимость Vaex, так что когда GIL не отключается в Arrow, мы (и все остальные) сталкиваемся с падением производительности).\n",
      "\r\n",
      "Кроме того, при исполнении на 64 ядрах производительность Vaex иногда далека от идеала. Возможно, он использует 4000 % процессора вместо 6400 %, что меня не устраивает. Вместо того, чтобы наугад вставлять выключатели для изучения этого эффекта, я хочу разобраться в происходящем, и если проблема в GIL, то хочу понять, почему и как он тормозит Vaex.\n",
      "\n",
      "Почему я это пишу\r\n",
      "Я планирую написать серию статей, рассказывающих о некоторых инструментах и методиках профилирования и трассировки Python с нативными расширениями, а также о том, как эти инструменты можно объединить для анализа и визуализации работы Python при включённом и отключённом GIL.\n",
      "\r\n",
      "Надеюсь, это поможет улучшить трассировку, профилирование и другие измерения производительности в экосистеме языка, а также производительности всей экосистемы Python.\n",
      "\n",
      "Требования\n",
      "Linux\r\n",
      "Вам нужен доступ к Linux-машине с root-привилегиями (sudo достаточно). Или попросите сисадмина выполнить для вас нижеописанные команды. Для остальной статьи достаточно пользовательских привилегий.\n",
      "\n",
      "Perf\r\n",
      "Убедитесь, что у вас установлен perf, к примеру, на Ubuntu это можно сделать так:\n",
      "\n",
      "$ sudo yum install perf\n",
      "\n",
      "Конфигурация ядра\r\n",
      "Запуск в роли пользователя:\n",
      "\n",
      "# Enable users to run perf (use at own risk)\n",
      "$ sudo sysctl kernel.perf_event_paranoid=-1\n",
      "\n",
      "# Enable users to see schedule trace events:\n",
      "$ sudo mount -o remount,mode=755 /sys/kernel/debug\n",
      "$ sudo mount -o remount,mode=755 /sys/kernel/debug/tracing\n",
      "\n",
      "Пакеты Python\r\n",
      "Мы будем использовать VizTracer и per4m\n",
      "\n",
      "$ pip install \"viztracer>=0.11.2\" \"per4m>=0.1,<0.2\"\n",
      "\n",
      "Отслеживание состояний потоков и процессов с помощью perf\r\n",
      "В Python нельзя выяснить состояние GIL (кроме использования поллинга), потому что для этого нет API. Мы можем отслеживать состояние из ядра, и для этого нам нужен инструмент perf.\n",
      "\r\n",
      "С его помощью (он также известен как perf_events) мы можем прослушивать изменения состояний процессов и потоков (нас интересует только засыпание и исполнение) и журналировать их. Perf может выглядеть пугающе, но это мощный инструмент. Если хотите узнать о нём больше, рекомендую почитать статью Джулии Эванс или сайт Брендана Грегга.\n",
      "\r\n",
      "Чтобы настроиться, для начала применим perf к простенькой программе:\n",
      "\n",
      "import time\n",
      "from threading import Thread\n",
      "\n",
      "def sleep_a_bit():\n",
      "    time.sleep(1)\n",
      "\n",
      "def main():\n",
      "    t = Thread(target=sleep_a_bit)\n",
      "    t.start()\n",
      "    t.join()\n",
      "\n",
      "main()\n",
      "\r\n",
      "Мы прослушиваем всего несколько событий, чтобы уменьшить зашумлённость (обратите внимание на использование символов подстановки (wildcards)):\n",
      "\n",
      "$ perf record -e sched:sched_switch  -e sched:sched_process_fork \\\n",
      "        -e 'sched:sched_wak*' -- python -m per4m.example0\n",
      "[ perf record: Woken up 2 times to write data ]\n",
      "[ perf record: Captured and wrote 0,032 MB perf.data (33 samples) ]\n",
      "\r\n",
      "И применим скриптовую команду perf для вывода читабельного результата, пригодного для парсинга.\n",
      "\n",
      "\n",
      "Скрытый текст\n",
      "$ perf script\n",
      "        :3040108 3040108 [032] 5563910.979408:                sched:sched_waking: comm=perf pid=3040114 prio=120 target_cpu=031\n",
      "        :3040108 3040108 [032] 5563910.979431:                sched:sched_wakeup: comm=perf pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563910.995616:                sched:sched_waking: comm=kworker/31:1 pid=2502104 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563910.995618:                sched:sched_wakeup: comm=kworker/31:1 pid=2502104 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563910.995621:                sched:sched_waking: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563910.995622:                sched:sched_wakeup: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563910.995624:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=R+ ==> next_comm=kworker/31:1 next_pid=2502104 next_prio=120\n",
      "          python 3040114 [031] 5563911.003612:                sched:sched_waking: comm=kworker/32:1 pid=2467833 prio=120 target_cpu=032\n",
      "          python 3040114 [031] 5563911.003614:                sched:sched_wakeup: comm=kworker/32:1 pid=2467833 prio=120 target_cpu=032\n",
      "          python 3040114 [031] 5563911.083609:                sched:sched_waking: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563911.083612:                sched:sched_wakeup: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563911.083613:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=R ==> next_comm=ksoftirqd/31 next_pid=198 next_prio=120\n",
      "          python 3040114 [031] 5563911.108984:                sched:sched_waking: comm=node pid=2446812 prio=120 target_cpu=045\n",
      "          python 3040114 [031] 5563911.109059:                sched:sched_waking: comm=node pid=2446812 prio=120 target_cpu=045\n",
      "          python 3040114 [031] 5563911.112250:          sched:sched_process_fork: comm=python pid=3040114 child_comm=python child_pid=3040116\n",
      "          python 3040114 [031] 5563911.112260:            sched:sched_wakeup_new: comm=python pid=3040116 prio=120 target_cpu=037\n",
      "          python 3040114 [031] 5563911.112262:            sched:sched_wakeup_new: comm=python pid=3040116 prio=120 target_cpu=037\n",
      "          python 3040114 [031] 5563911.112273:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==> next_comm=swapper/31 next_pid=0 next_prio=120\n",
      "          python 3040116 [037] 5563911.112418:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040116 [037] 5563911.112450:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040116 [037] 5563911.112473: sched:sched_wake_idle_without_ipi: cpu=31\n",
      "         swapper     0 [031] 5563911.112476:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563911.112485:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==> next_comm=swapper/31 next_pid=0 next_prio=120\n",
      "          python 3040116 [037] 5563911.112485:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040116 [037] 5563911.112489:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040116 [037] 5563911.112496:                sched:sched_switch: prev_comm=python prev_pid=3040116 prev_prio=120 prev_state=S ==> next_comm=swapper/37 next_pid=0 next_prio=120\n",
      "         swapper     0 [031] 5563911.112497:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040114 [031] 5563911.112513:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==> next_comm=swapper/31 next_pid=0 next_prio=120\n",
      "         swapper     0 [037] 5563912.113490:                sched:sched_waking: comm=python pid=3040116 prio=120 target_cpu=037\n",
      "         swapper     0 [037] 5563912.113529:                sched:sched_wakeup: comm=python pid=3040116 prio=120 target_cpu=037\n",
      "          python 3040116 [037] 5563912.113595:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "          python 3040116 [037] 5563912.113620:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "         swapper     0 [031] 5563912.113697:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "\n",
      "\n",
      "\r\n",
      "Взгляните на четвёртую колонку (время в секундах): программа заснула (прошла одна секунда). Здесь мы видим вход в состояние сна:\n",
      "\n",
      "python 3040114 [031] 5563911.112513: sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==> next_comm=swapper/31 next_pid=0 next_prio=120\n",
      "\r\n",
      "Это означает, что ядро изменило состояние потока Python на S (=sleeping).\n",
      "\r\n",
      "Секунду спустя программа пробудилась:\n",
      "\n",
      "swapper 0 [031] 5563912.113697: sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031\n",
      "\r\n",
      "Конечно, чтобы разобраться в происходящем придётся собрать инструментарий. Да, результат можно также легко отпарсить с помощью per4m, но прежде чем мы продолжим, хочу с помощью VizTracer визуализировать поток чуть более сложной программы.\n",
      "\n",
      "VizTracer\r\n",
      "Это трассировщик Python, способный визуализировать работу, которую программа выполняет в браузере. Давайте применим его к более сложной программе:\n",
      "\n",
      "import threading\n",
      "import time\n",
      "\n",
      "def some_computation():\n",
      "    total = 0   \n",
      "    for i in range(1_000_000):\n",
      "        total += i\n",
      "    return total\n",
      "\n",
      "def main():\n",
      "    thread1 = threading.Thread(target=some_computation)\n",
      "    thread2 = threading.Thread(target=some_computation)\n",
      "    thread1.start()\n",
      "    thread2.start()\n",
      "    time.sleep(0.2)\n",
      "    for thread in [thread1, thread2]:\n",
      "        thread.join()\n",
      "\n",
      "main()\n",
      "\r\n",
      "Результат работы трассировщика:\n",
      "\n",
      "$ viztracer -o example1.html --ignore_frozen -m per4m.example1\n",
      "Loading finish                                        \n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...\n",
      "Dumping trace data to json, total entries: 94, estimated json file size: 11.0KiB\n",
      "Generating HTML report\n",
      "Report saved.\n",
      "\r\n",
      "Так выглядит получившийся HTML: \n",
      "\n",
      "\r\n",
      "Похоже, some_computation исполнялась параллельно (дважды), хотя мы знаем, что GIL это предотвращает. Что тут происходит?\n",
      "\n",
      "Объединение результатов работы VizTracer и perf\r\n",
      "Давайте применим к программе perf, как в случае с example0.py. Только теперь добавим аргумент -k CLOCK_MONOTONIC, чтобы использовать те же часы, что и VizTracer, и попросим его сгенерировать JSON вместо HTML:\n",
      "\n",
      "$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*' \\\n",
      "   -k CLOCK_MONOTONIC  -- viztracer -o viztracer1.json --ignore_frozen -m per4m.example1\n",
      "\r\n",
      "Затем с помощью per4m преобразуем результаты скрипта perf в JSON, который может прочитать VizTracer:\n",
      "\n",
      "$ perf script | per4m perf2trace sched -o perf1.json\n",
      "Wrote to perf1.json\n",
      "\r\n",
      "Теперь с помощью VizTracer объединим два JSON-файла:\n",
      "\n",
      "$ viztracer --combine perf1.json viztracer1.json -o example1_state.html\n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...\n",
      "Dumping trace data to json, total entries: 131, estimated json file size: 15.4KiB\n",
      "Generating HTML report\n",
      "Report saved.\n",
      "\r\n",
      "Вот что получили: \n",
      "\n",
      "\r\n",
      "Очевидно, что потоки регулярно засыпают из-за GIL и не исполняются параллельно.\n",
      "\n",
      "Примечание: длина фазы сна около 5 мс, что соответствует значению по умолчанию sys.getswitchinterval\n",
      "\n",
      "Определение GIL\r\n",
      "Наш процесс засыпает, но мы не видим разницы между состояниями сна, которое инициируется вызовом time.sleep или GIL. Есть несколько способов определить разницу, рассмотрим два из них.\n",
      "\n",
      "Через трассировку стека\r\n",
      "С помощью perf record -g (а лучше с помощью perf record --call-graph dwarf, что подразумевает -g), мы получим трассировки стека для каждого события perf.\n",
      "\n",
      "$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*'\\\n",
      "   -k CLOCK_MONOTONIC  --call-graph dwarf -- viztracer -o viztracer1-gil.json --ignore_frozen -m per4m.example1\n",
      "Loading finish                                        \n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/viztracer1-gil.json ...\n",
      "Dumping trace data to json, total entries: 94, estimated json file size: 11.0KiB\n",
      "Report saved.\n",
      "[ perf record: Woken up 3 times to write data ]\n",
      "[ perf record: Captured and wrote 0,991 MB perf.data (164 samples) ]\n",
      "\r\n",
      "Взглянув на результат скрипта perf (в который мы ради производительности добавили --no-inline), мы получим много информации. Посмотрите на событие изменения состояния, оказывается, вызывался take_gil!\n",
      "\n",
      "\n",
      "Скрытый текст\n",
      "$ perf script --no-inline | less\n",
      "...\n",
      "viztracer 3306851 [059] 5614683.022539:                sched:sched_switch: prev_comm=viztracer prev_pid=3306851 prev_prio=120 prev_state=S ==> next_comm=swapper/59 next_pid=0 next_prio=120\n",
      "        ffffffff96ed4785 __sched_text_start+0x375 ([kernel.kallsyms])\n",
      "        ffffffff96ed4785 __sched_text_start+0x375 ([kernel.kallsyms])\n",
      "        ffffffff96ed4b92 schedule+0x42 ([kernel.kallsyms])\n",
      "        ffffffff9654a51b futex_wait_queue_me+0xbb ([kernel.kallsyms])\n",
      "        ffffffff9654ac85 futex_wait+0x105 ([kernel.kallsyms])\n",
      "        ffffffff9654daff do_futex+0x10f ([kernel.kallsyms])\n",
      "        ffffffff9654dfef __x64_sys_futex+0x13f ([kernel.kallsyms])\n",
      "        ffffffff964044c7 do_syscall_64+0x57 ([kernel.kallsyms])\n",
      "        ffffffff9700008c entry_SYSCALL_64_after_hwframe+0x44 ([kernel.kallsyms])\n",
      "        7f4884b977b1<a href=\"https://www.maartenbreddels.com/cdn-cgi/l/email-protection\"> [email protected]@GLIBC_2.3.2+0x271 (/usr/lib/x86_64-linux-gnu/libpthread-2.31.so)\n",
      "            55595c07fe6d take_gil+0x1ad (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfaa0b3 PyEval_RestoreThread+0x23 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c000872 lock_PyThread_acquire_lock+0x1d2 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfe71f3 _PyMethodDef_RawFastCallKeywords+0x263 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfe7313 _PyCFunction_FastCallKeywords+0x23 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d657 call_function+0x3b7 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6b00 _PyFunction_FastCallKeywords+0x520 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6b00 _PyFunction_FastCallKeywords+0x520 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060ae4 _PyEval_EvalFrameDefault+0x3f4 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c074e5d builtin_exec+0x33d (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfe7078 _PyMethodDef_RawFastCallKeywords+0xe8 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfe7313 _PyCFunction_FastCallKeywords+0x23 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c066c39 _PyEval_EvalFrameDefault+0x6549 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfb77e0 _PyEval_EvalCodeWithName+0xc80 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6b62 _PyFunction_FastCallKeywords+0x582 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c060ae4 _PyEval_EvalFrameDefault+0x3f4 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595bfb81e2 PyEval_EvalCode+0x22 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c0c51d1 run_mod+0x31 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c0cf31d PyRun_FileExFlags+0x9d (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c0cf50a PyRun_SimpleFileExFlags+0x1ba (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c0d05f0 pymain_main+0x3e0 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            55595c0d067b _Py_UnixMain+0x3b (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "            7f48849bc0b2 __libc_start_main+0xf2 (/usr/lib/x86_64-linux-gnu/libc-2.31.so)\n",
      "            55595c075100 _start+0x28 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "…\n",
      "\n",
      "\n",
      "\n",
      "Примечание: также вызывался pthread_cond_timedwait, он используется https://github.com/sumerc/gilstats.py для eBPF, если вас интересуют другие мьютексы.\n",
      "\n",
      "Ещё примечание: обратите внимание, что здесь нет трассировки стека Python, а вместо неё мы получили _PyEval_EvalFrameDefault и прочее. В будущем я планирую написать, как вставлять трассировку стека.\n",
      "\r\n",
      "Инструмент для конвертирования per4m perf2trace понимает это и генерирует другой результат, когда в трассировке присутствует take_gil:\n",
      "\n",
      "$ perf script --no-inline | per4m perf2trace sched -o perf1-gil.json\n",
      "Wrote to perf1-gil.json\n",
      "$ viztracer --combine perf1-gil.json viztracer1-gil.json -o example1-gil.html\n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...\n",
      "Dumping trace data to json, total entries: 131, estimated json file size: 15.4KiB\n",
      "Generating HTML report\n",
      "Report saved.\n",
      "\r\n",
      "Получаем: \n",
      "\n",
      "\r\n",
      "Теперь мы точно видим, где GIL играет роль!\n",
      "\n",
      "Через зондирование (kprobes/uprobes)\r\n",
      "Мы знаем, когда процессы засыпают (из-за GIL или иных причин), но если хочется подробнее узнать о том, когда включается или выключается GIL, то нужно знать, когда вызываются и возвращаются результаты take_gil и drop_gil. Такую трассировку можно получить благодаря зондированию с помощью perf. В пользовательской среде зондами являются uprobes, это аналог kprobes, которые, как вы могли догадаться, работают в среде ядра. Опять же, прекрасный источник дополнительной информации — Джулия Эванс.\n",
      "\r\n",
      "Установим 4 зонда:\n",
      "\n",
      "sudo perf probe -f -x `which python` python:take_gil=take_gil\n",
      "sudo perf probe -f -x `which python` python:take_gil=take_gil%return\n",
      "sudo perf probe -f -x `which python` python:drop_gil=drop_gil\n",
      "sudo perf probe -f -x `which python` python:drop_gil=drop_gil%return\n",
      "\n",
      "Added new events:\n",
      "\n",
      "  python:take_gil      (on take_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "  python:take_gil_1    (on take_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "\n",
      "You can now use it in all perf tools, such as:\n",
      "\n",
      "        perf record -e python:take_gil_1 -aR sleep 1\n",
      "\n",
      "Failed to find \"take_gil%return\",\n",
      " because take_gil is an inlined function and has no return point.\n",
      "Added new event:\n",
      "  python:take_gil__return (on take_gil%return in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "\n",
      "You can now use it in all perf tools, such as:\n",
      "\n",
      "        perf record -e python:take_gil__return -aR sleep 1\n",
      "\n",
      "Added new events:\n",
      "  python:drop_gil      (on drop_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "  python:drop_gil_1    (on drop_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "\n",
      "You can now use it in all perf tools, such as:\n",
      "\n",
      "        perf record -e python:drop_gil_1 -aR sleep 1\n",
      "\n",
      "Failed to find \"drop_gil%return\",\n",
      " because drop_gil is an inlined function and has no return point.\n",
      "Added new event:\n",
      "  python:drop_gil__return (on drop_gil%return in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)\n",
      "\n",
      "You can now use it in all perf tools, such as:\n",
      "\n",
      "        perf record -e python:drop_gil__return -aR sleep 1\n",
      "\r\n",
      "Есть какие-то жалобы, а из-за инлайненных drop_gil и take_gil добавилось несколько зондов/событий (то есть функция представлена в бинарном файле несколько раз), но всё работает.\n",
      "\n",
      "Примечание: возможно, мне повезло, что мой бинарник Python (от conda-forge) скомпилирован так, что соответствующие take_gil/drop_gil (и их результаты), успешно отработавшие, подошли для решения этой проблемы.\n",
      "\r\n",
      "Обратите внимание, что зонды не влияют на производительность, и только когда они «активны» (например, когда мы их мониторим из perf), то запускают код по другой ветке. При мониторинге копируются затронутые страницы для отслеживаемого процесса, а в нужные места вставляются контрольные точки (INT3 для процессоров x86). Контрольная точка запускает событие для perf с маленькими накладными расходами. Если вы хотите убрать зонды, то выполните команду:\n",
      "\n",
      "$ sudo perf probe --del 'python*'\n",
      "\r\n",
      "Теперь perf знает о новых событиях, которые он может прослушивать, так что давайте снова его запустим с дополнительным аргументом -e 'python:*gil*':\n",
      "\n",
      "$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*' -k CLOCK_MONOTONIC  \\\n",
      "  -e 'python:*gil*' -- viztracer  -o viztracer1-uprobes.json --ignore_frozen -m per4m.example1\n",
      "\n",
      "Примечание: мы убрали --call-graph dwarf, иначе perf не успеет и мы потеряем события.\n",
      "\r\n",
      "Затем с помощью per4m perf2trace преобразуем в JSON, который понятен для VizTracer, и заодно получим новую статистику.\n",
      "\n",
      "$ perf script --no-inline | per4m perf2trace gil -o perf1-uprobes.json\n",
      "...\n",
      "Summary of threads:\n",
      "\n",
      "PID         total(us)    no gil%    has gil%    gil wait%\n",
      "--------  -----------  -----------  ------------  -------------\n",
      "3353567*     164490.0         65.9          27.3            6.7\n",
      "3353569       66560.0          0.3          48.2           51.5\n",
      "3353570       60900.0          0.0          56.4           43.6\n",
      "\n",
      "High 'no gil' is good, we like low 'has gil',\n",
      " and we don't want 'gil wait'. (* indicates main thread)\n",
      "... \n",
      "Wrote to perf1-uprobes.json\n",
      "\r\n",
      "Подкоманда per4m perf2trace gil также выдаёт в качестве результата gil_load. Из него мы узнаём, что оба потока ждут GIL примерно половину времени, как и ожидалось.\n",
      "\r\n",
      "С помощью того же файла perf.data, записанного perf, мы также можем сгенерировать информацию о состоянии потока или процесса. Но поскольку мы выполняли без трассировок стека, мы не знаем, заснул ли процесс из-за GIL или нет.\n",
      "\n",
      "$ perf script --no-inline | per4m perf2trace sched -o perf1-state.json\n",
      "Wrote to perf1-state.json\n",
      "\r\n",
      "Наконец, соберём вместе все три результата:\n",
      "\n",
      "$ viztracer --combine perf1-state.json perf1-uprobes.json viztracer1-uprobes.json -o example1-uprobes.html \n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1-uprobes.html ...\n",
      "Dumping trace data to json, total entries: 10484, estimated json file size: 1.2MiB\n",
      "Generating HTML report\n",
      "Report saved.\n",
      "\r\n",
      "VizTracer даёт хорошее представление о том, у кого был GIL и кто его ждал:\n",
      "\n",
      "\r\n",
      "Над каждым потоком написано, когда поток или процесс ждёт GIL и когда тот включается (помечено как LOCK). Обратите внимание, что эти периоды пересекаются с периодами, когда поток или процесс не спит (выполняется!). Также обратите внимание, что в состоянии выполнения мы видим только один поток или процесс, как и должно быть из-за GIL. \n",
      "\r\n",
      "Время между вызовами take_gil, то есть между блокировками (а следовательно, между засыпанием или пробуждением), точно такое же, как в вышеприведённой таблице в колонке gil wait%. Длительность включения GIL для каждого потока, помеченная как LOCK, соответствует времени в колонке gil%.\n",
      "\n",
      "Выпускаем Кракена… гхм, GIL\r\n",
      "Мы видели, как при многопоточном исполнении программы на чистом Python GIL ограничивает производительность, разрешая одновременно исполнять только один поток или процесс (для одного процесса Python, разумеется, и, возможно, в будущем для одного (суб)интерпретатора). Теперь посмотрим, что будет, если отключить GIL, как это происходит при исполнении функций NumPy.\n",
      "\r\n",
      "Во втором примере исполняется some_numpy_computation, которая вызывает функцию NumPy M=4 раз параллельно в двух потоках, в то время как основной поток исполняет чистый код на Python.\n",
      "\n",
      "import threading\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "N = 1024*1024*32\n",
      "M = 4\n",
      "x = np.arange(N, dtype='f8')\n",
      "\n",
      "def some_numpy_computation():\n",
      "    total = 0\n",
      "    for i in range(M):\n",
      "        total += x.sum()\n",
      "    return total\n",
      "\n",
      "def main(args=None):\n",
      "    thread1 = threading.Thread(target=some_numpy_computation)\n",
      "    thread2 = threading.Thread(target=some_numpy_computation)\n",
      "    thread1.start()\n",
      "    thread2.start()\n",
      "    total = 0\n",
      "    for i in range(2_000_000):\n",
      "        total += i\n",
      "    for thread in [thread1, thread2]:\n",
      "        thread.join()\n",
      "main()\n",
      "\r\n",
      "Вместо исполнения этого скрипта с помощью perf и VizTracer мы применим утилиту per4m giltracer, которая автоматизирует все вышеописанные этапы. Она сделает это немного умнее. По сути, мы дважды запустим perf, один раз без трассировки стека, а второй раз с ней, и импортируем модуль/скрипт до исполнения его основной функции, чтобы избавиться от неинтересной трассировки вроде того же импортирования. Это произойдёт достаточно быстро, чтобы мы не потеряли события.\n",
      "\n",
      "$ giltracer --state-detect -o example2-uprobes.html -m per4m.example2\n",
      "...\n",
      "\r\n",
      "Итоги по потокам:\n",
      "\n",
      "PID         total(us)    no gil%    has gil%    gil wait%\n",
      "--------  -----------  -----------  ------------  -------------\n",
      "3373601*    1359990.0         95.8           4.2            0.1\n",
      "3373683       60276.4         84.6           2.2           13.2\n",
      "3373684       57324.0         89.2           1.9            8.9\n",
      "\n",
      "High 'no gil' is good, we like low 'has gil',\n",
      " and we don't want 'gil wait'. (* indicates main thread)\n",
      "...\n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example2-uprobes.html ...\n",
      "...\n",
      "\n",
      "\r\n",
      "Хотя в основном потоке исполняется код на Python (для него включён GIL, что обозначается словом LOCK), параллельно исполняются и другие потоки. Обратите внимание, что в примере с чистым Python у нас одновременно исполняется один поток или процесс. А здесь действительно параллельно исполняется три потока. Это стало возможно потому, что подпрограммы NumPy, входящие в C/C++/Fortran, отключили GIL.\n",
      "\r\n",
      "Однако GIL всё же влияет на потоки, потому что когда функция NumPy возвращается в Python, ей снова нужно получить GIL, как видно из длительных блоков take_gil. На это тратится 10 % времени каждого потока.\n",
      "\n",
      "Интеграция Jupyter\r\n",
      "Поскольку мой рабочий процесс часто подразумевает работу на MacBook (который не исполняет perf, но поддерживает dtrace), удалённо подключённом к Linux-машине, я использую Jupyter notebook для удалённого исполнения кода. И раз я Jupyter-разработчик, мне пришлось сделать обёртку с помощью cell magic.\n",
      "\n",
      "# this registers the giltracer cell magic\n",
      "%load_ext per4m\n",
      "\n",
      "%%giltracer\n",
      "# this call the main define above, but this can also be a multiline code cell\n",
      "main()\n",
      "\n",
      "Saving report to /tmp/tmpvi8zw9ut/viztracer.json ...\n",
      "Dumping trace data to json, total entries: 117, estimated json file size: 13.7KiB\n",
      "Report saved.\n",
      "\n",
      "[ perf record: Woken up 1 times to write data ]\n",
      "[ perf record: Captured and wrote 0,094 MB /tmp/tmpvi8zw9ut/perf.data (244 samples) ]\n",
      "\n",
      "Wait for perf to finish...\n",
      "perf script -i /tmp/tmpvi8zw9ut/perf.data --no-inline --ns | per4m perf2trace gil -o /tmp/tmpvi8zw9ut/giltracer.json -q -v \n",
      "Saving report to /home/maartenbreddels/github/maartenbreddels/fastblog/_notebooks/giltracer.html ...\n",
      "Dumping trace data to json, total entries: 849, estimated json file size: 99.5KiB\n",
      "Generating HTML report\n",
      "Report saved.\n",
      "\n",
      "Скачать giltracer.html\n",
      "\n",
      "Открыть giltracer.html в новой вкладке (может не работать в связи с обеспечением безопасности)\n",
      "\n",
      "Заключение\r\n",
      "С помощью perf мы можем определять состояния процессов или потоков, что помогает понять, у кого из них включался GIL в Python. А с помощью трассировок стека можно определить, что причиной сна был GIL, а не time.sleep, например.\n",
      "\r\n",
      "Сочетание uprobes с perf позволяет трассировать вызов и возвращение результатов функций take_gil и drop_gil, получая ещё больше информации о влиянии GIL на вашу Python-программу.\n",
      "\r\n",
      "Работу нам облегчает экспериментальный пакет per4m, преобразующий скрипт perf в JSON-формат VizTracer, а также некоторые инструменты оркестрации.\n",
      "\n",
      "Много букаф, не осилил\r\n",
      "Если вам просто хочется посмотреть на влияние GIL, то однократно запустите это:\n",
      "\n",
      "sudo yum install perf\n",
      "sudo sysctl kernel.perf_event_paranoid=-1\n",
      "sudo mount -o remount,mode=755 /sys/kernel/debug\n",
      "sudo mount -o remount,mode=755 /sys/kernel/debug/tracing\n",
      "sudo perf probe -f -x `which python` python:take_gil=take_gil\n",
      "sudo perf probe -f -x `which python` python:take_gil=take_gil%return\n",
      "sudo perf probe -f -x `which python` python:drop_gil=drop_gil\n",
      "sudo perf probe -f -x `which python` python:drop_gil=drop_gil%return\n",
      "pip install \"viztracer>=0.11.2\" \"per4m>=0.1,<0.2\"\n",
      "\r\n",
      "Пример использования:\n",
      "\n",
      "# module\n",
      "$ giltracer per4m/example2.py\n",
      "# script\n",
      "$ giltracer -m per4m.example2\n",
      "# add thread/process state detection\n",
      "$ giltracer --state-detect -m per4m.example2\n",
      "# without uprobes (in case that fails)\n",
      "$ giltracer --state-detect --no-gil-detect -m per4m.example2\n",
      "\n",
      "Планы на будущее\r\n",
      "Хотелось бы мне, чтобы не пришлось разрабатывать эти инструменты. Надеюсь, мне удалось вдохновить кого-то на создание более совершенных средств, чем мои. Я хочу сосредоточиться на написании высокопроизводительного кода. Однако у меня есть такие планы на будущее:\n",
      "\n",
      "\n",
      "Раскопать измеритель производительности в VizTracer, чтобы посмотреть на промахи кеша или простои процесса.\n",
      "Внедрить трассировку стека Python в трассировки perf, чтобы объединить их с инструментами вроде http://www.brendangregg.com/offcpuanalysis.html\n",
      "Повторить то же упражнение с помощью dtrace, чтобы использовать в MacOS.\n",
      "Автоматически определять, какие C-функции не отключают GIL (https://github.com/vaexio/vaex/pull/1114, https://github.com/apache/arrow/pull/7756 )\n",
      "Применить всё это для решения других задач, например https://github.com/h5py/h5py/issues/1516\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " На конференции Russian Python Week 2020 мы пообщались с двумя людьми, от которых непосредственно зависит будущее Python. На наши вопросы ответили Core-developer community on Google Cloud Дастин Инграм и Director at Python Software Foundation Кэрол Виллинг. Заглянув под кат, вы узнаете, чего не хватает языку, и как он будет развиваться дальше.Многие Python разработчики не очень много знают про PSF, чем он (фонд) занимается? Дастин Инграм: Python Software Foundation (PSF) — американская некоммерческая организация, работающая по всему миру. Ее цель — продвижение Python. PSF принадлежат связанные с Python права интеллектуальной собственности, включая права на логотипы. Кроме того, организация привлекает средства, обеспечивающие дальнейшую разработку Python, а также финансирует сопутствующие проекты. В PSF всего пара оплачиваемых сотрудников: исполнительный директор, директор по инфраструктуре и несколько бухгалтеров. Организацию возглавляет совет из 11 директоров, работающих на добровольных началах. Python — язык с открытым исходным кодом. Как финансируется организация разработки и деятельность постоянных сотрудников, поддерживающих работу Python-сообщества?Кэрол Виллинг: Python, как язык программирования, не приносит прибыли. Но я была директором PSF и занималась привлечением средств через проведение конференции PyCon. Основная часть финансирования приходится на пожертвования и поступления от организаций в США. Именно они позволяют нам оплачивать множество вещей, включая работу персонала и гранты, которые мы реализуем по всему мируДастин Инграм: Да, сам Python не приносит денег, а PSF получает большую часть средств (а это несколько миллионов долларов США в год), главным образом, благодаря конференциям PyCon. PyCon US приносит около 85% прибыли, за счет которой и работает PSF. Оставшаяся часть финансирования обеспечивается пожертвованиями и поддержкой спонсоров. Также среди соучредителей PSF есть крупные компании, такие как Google, Microsoft и Amazon. Они финансируют проекты в большей или меньшей степени и помогают PSF держаться на плаву.Как стать директором PSF? Кэрол Виллинг: Раньше совет директоров переизбирался ежегодно в полном составе. Со временем мы поняли, что переизбирать отдельных директоров через определенные промежутки времени намного эффективнее. Дастин Инграм: Номинироваться на должность директора может любой желающий. Это легко — достаточно написать заявление на участие в выборах. Гораздо сложнее в них победить. Как правило, сообщество выбирает на должности директоров людей, которые постоянно и на добровольных началах участвуют в деятельности сообщества Python и уделяют много времени развитию организации.С другой стороны, хотя директора и руководят деятельностью PSF, они, как и штатные сотрудники, небольшая часть того, что необходимо для деятельности организации. Большая часть работы выполняется людьми на добровольных началах. Эти люди не избираются и не получают денег за свою работу по развитию экосистемы. Но они участвуют в разработке ядра Python и всего, что с этим связано. Именно их вклад обеспечивает дальнейшее развитие и продвижение языка.Как конкретный человек может участвовать в деятельности PSF? Кэрол Виллинг: Во-первых, вы можете быть представителем Python в сообществе вашей страны. Со временем вы можете участвовать в жизни сообщества все больше, брать на себя ответственность за организацию или выбрать то, что интересует вас.Дастин Инграм: По-моему, многие считают, что единственный способ участвовать — стать основным разработчиком или работать непосредственно с ядром Python. Но существует множество других способов участвовать в жизни сообщества. Например, заниматься сторонними проектами в репозитории PyPI или экосистемами. Можно участвовать в рабочих группах, которые не занимаются ядром Python. Такая деятельность не менее важна, чем работа над языком. Кэрол Виллинг: Вам не обязательно участвовать в разработке ядра. Многие участники экосистемы Python организуют работу сообщества, проводят конференции и выступают на них. Они занимаются обучением, а также пишут документацию. Внести ценный вклад можно по-разному.Дастин Инграм: Особую роль играет обучение. Python становится чрезвычайно популярным. В следующие десять лет появятся миллионы новых разработчиков. Поэтому чем больше мы делимся своими знаниями, делая сообщество более привлекательным, тем более успешным будет Python в будущем.Расскажите про статус, цели и проекты организации PSF на GitHub.Дастин Инграм: Наша организация на GitHub поддерживает множество проектов PSF, которые не относятся к ядру Python, включая такие масштабные проекты, как PyPI. Одновременно мы  поддерживаем проекты сообщества. Например, недавно нам был передан популярный проект Requests. Хотя такие проекты не относятся напрямую к языку Python, они очень важны для экосистемы.Также мы предоставляем репозитории для различных рабочих групп, которые не участвуют в разработке ядра Python. Например, мы недавно организовали рабочую группу по финансированию критически важных проектов языка. Эта рабочая группа успешно привлекает внешнее финансирование от организаций и компаний для реализации конкретных проектов Python. Так, за последние пару лет большие суммы пожертвовала компания Mozilla.Что изменилось после выхода Гвидо Ван Россума на пенсию?Дастин Инграм: Выход Гвидо на пенсию связан и с PEP, и с моделью управления разработкой Python в целом. В предыдущей модели управления Гвидо был пожизненным диктатором, имеющим решающее слово по все вопросам, связанным с Python. Так не могло продолжаться вечно. В какой-то момент ситуация должна была измениться, хотя многие не ожидали, что перемены наступят так быстро. Гвидо работал над Python большую часть жизни и просто устал, в том числе от всех дополнительных забот.Поэтому когда он вышел на пенсию, то заставил оставил сообщество задуматься, что делать и как создать новую модель управления, потому что после ухода диктатора всегда остается вакуум.Кэрол Виллинг: Отставка Гвидо была полной неожиданностью. Мы надеялись, что с у нас впереди есть еще несколько лет. Однако жизнь продолжается, и мы создаем новую модель управления разработкой Python. В течение пары месяцев мы изучали другие проекты с открытым кодом, в которых мы участвовали или о которых нам было известно. После чего сформулировали PEP с описанием различных моделей. Основные разработчики Python собрались на мозговой штурм в Сиэтле. На круглом столе, в котором принимал участие и Гвидо, мы обсудили, что нам нравится в Python, и как нам необходимо развиваться в будущем. Большинство проголосовало за модель под управлением Наблюдательного комитета, чтобы обеспечить максимальную автономность разработчиков.Процесс обсуждения открыл нам глаза на то, как много лежало на плечах Гвидо — организация совместной работы разработчиков, решения о выходе из технических тупиков, пересмотр предложений по расширению и т. д. Сейчас эта работа распределена между пятью членами руководящего комитета. Они еженедельно обсуждают возникающие вопросы, в том числе касающиеся культуры и сообщества основных разработчиков. Мы планируем выбрать новый руководящий комитет вскоре после выпуска следующей версии Python.В настоящее время опубликовано много новых PEP-предложений по расширению. Какие из них вам запомнились больше всего, а какие вы считаете спорными?Дастин Инграм: Я обычно шучу, что мое любимое предложение то, которое я подготовил сам. Из недавних мне больше всего нравится PEP 615, которое добавляет базу данных часовых поясов в стандартную библиотеку Python. Мне кажется, этого очень не хватало.Мне также очень нравится новое предложение PEP 632, которое удаляет из основного кода Python модуль distutils. Этого мы ждали уже очень давно. Модуль хоть остается в стандартной библиотеке, но не рекомендуется и фактически не поддерживается. Удаление этого модуля упростит жизнь пользователей, которые не очень хорошо разбираются в экосистеме репозитория.Кэрол Виллинг: Я читаю очень много поступающих предложений по расширению. И мое мнение основано на идеях о том, как бы я использовала конкретное предложение в Jupiter или в рамках научного программирования. Меня заинтересовало предложение по сопоставлению с образцом (pattern matching). Мы встречались с группой, которая его разрабатывает, чтобы обсудить существующие возражения. Фактически мы уменьшили количество новой информации в предложении и сделали его более понятным. Я хочу быть уверена, что мы можем научить миллионы пользователей языка Python использовать сопоставление с образцом и предложить им самые передовые методы. Другое интересное и заинтриговавшее меня предложение относится к удалению некоторых модулей из стандартной библиотеки или выносу за ее пределы. Одно из преимуществ Python — отличная экосистема сторонних библиотек. Если у модуля из стандартной библиотеки есть лучшая сторонняя альтернатива, нет смысла его там держать функциональность в стандартной библиотеке и создавать у пользователей впечатление, что мы ее активно поддерживаем.Дастин Инграм: Многие функции были добавлены в стандартную библиотеку еще до появления PyPI или до того, как он стал надежно работать. Сегодня PyPI предоставляет множество интересных проектов, поэтому спрос на такую же функциональность в стандартной библиотеке уменьшается.В наших PEP мне очень нравится раздел «Как этому научить». Он есть даже в шаблоне PEP и должен описывать способы донесения новых предложений до широкой аудитории. PEP предназначены для технических специалистов, и их авторы обычно не надеются, что каждый пользователь Python сможет понять суть предложения. Но нам необходим способ доступно излагать новые идеи для более широкой аудитории.Кэрол Виллинг: Мне очень нравится раздел PEP о том, как объяснять новые предложения. Если вы не можете пояснить идею, никто не будет ей пользоваться, кроме очень опытных пользователей, и только в определенных случаях. Но если вы можете предоставить хотя бы рекомендации о том, как и когда вашу идею можно применить, это упростит жизнь всем остальным.Сопоставление с образцом — достаточно спорная концепция в Python, и ее очень сложно объяснить. Даже некоторые специалисты с опытом функционального программирования не понимают, как и когда правильно ей пользоваться.Кэрол Виллинг: Это целый букет проблем. Сопоставление с образцом имеет как базовые, так и факультативные элементы. Оно использовалось в индустрии телекоммуникаций — в, в Erlang, Elixir, Scala и почти всех функциональных языках. Мне кажется, что принципы сопоставления с образцом отлично разъясняются в документации по Elixir. Иногда объяснение сложнее, чем необходимо, из-за терминологии. И все становится намного понятнее, если привести понятные примеры. Я думаю, что все противоречия больше связаны с PEP, чем со сложностью сопоставления с образцом.Что вы думаете о Global Interpreter Lock? Это проклятие или преимущество?Кэрол Виллинг: Это концепция из другого времени. Python уже 30 лет, и некоторые решения, которые сделали язык гибким, простым и успешным, относились к его объектной модели. Раньше Python отличался от таких языков, как C, C++ или Java намного более четким синтаксисом, даже визуально. Сейчас для ускорения некоторых операций Python можно использовать, например, Cython.Если оценивать производительность разных языков программирования, все зависит от того, что вы под ней понимаете. Если речь идет о скорости выполнения, выиграет C. А если о продуктивности разработчиков, Python выигрывает благодаря легко читаемому коду, богатой экосистеме библиотек, опытному и готовому делиться знаниями сообществуPython ищет способы увеличить скорость выполнения, в том числе благодаря финансированию, я надеюсь, PSF и основная команда смогут это предложить.Дастин Инграм: Будет здорово, если Python сможет обеспечить высокую производительность, удобство и безопасность. Но он также предлагает очень мощные инструменты для перехода на более производительный язык, в случае необходимости. Это одно из основных свойств Python, которое сделало этот язык популярным, а также повлияло на известность сторонних проектов, таких как NumPy. Кэрол Виллинг: Я параллельно работаю над проектом Jupiter Project. Jupiter Notebook, один из важнейших инструментов Python, стал намного более популярным, чем мы ожидали. Я думаю, что причина этого заключается в легкости использоваться библиотек. Особенно для пользователей, далеких от программирования, которые приходят из области естественных, общественных и других наук. Причина заключается, скорее, в описании и визуализации, чем в коде.Это также демонстрирует важность сообщества. Несколько лет назад в команде проекта Jupiter Project было меньше 20 человек, из которых только 10 работали постоянно. Рост его популярности, в основном, обеспечило именно сообщество. Также как оно обеспечивает развитие Python в целом.Вы собираетесь увеличивать скорость Python, например, как PHP? Планируете ли расширять типизацию, добавлять финализированные классы, интеграцию ABC? Будет ли что-то новое в обработке исключений?Кэрол Виллинг: Я думаю, мы достаточно подробно обсудили производительность и скорость. По поводу типизации у меня смешанные чувства. Я признаю ее преимущества в системах больших компаний, где необходим статический анализ до выполнения кода. Поскольку я проверяю код чаще, чем пишу, для меня применение статической типизации является непростым делом. Много лет назад Python меня заинтересовал именно отсутствием обязательной типизации. Она не обязательна и сейчас, и я считаю это правильным решением. Однако в некоторых случаях, например, в Pydantic, где вы используете API, типизация позволяет намного проще и быстрее создавать основные документы. Она важна также для производственного ПО, но для разработки и экспериментов с CircuitPython или MicroPython в ней нет необходимости.Я надеюсь, кто-нибудь когда-нибудь создаст редактор, который позволит включать и отключать типизацию по желанию. Дастин Инграм: Мне нравится статическая типизация. Это мощный инструмент в Python. Но мне также нравится, что она не обязательна, и ее можно добавлять постепенно. Я не думаю, что такое положение дел изменится. По-моему, участники сообщества еще недостаточно часто используют типизацию в своих рабочих процессах. Для обычного пользователя Python, который не работает в крупных компаниях, она не нужна.Возможно, через некоторое время, когда статическая типизация будет использоваться шире, мы будем добавлять новые инструменты, например, типизацию для сторонних пакетов. Когда нам стоит ждать Python 4.0?Кэрол Виллинг: Python 4.0 выйдет в отдаленном будущем. Сейчас мы будем выпускать Python 3.10, 3.11, 3.13 и т. д. В настоящее время нет масштабных проектов, которым нужен 4.0. И я уверена, что выпускать следующую версию в таких условиях не в интересах сообщества. Поэтому сейчас мы поэтапно развиваем язык, концентрируясь на его стабильности.К тому же в прошлом году было опубликовано предложение, предлагающее сократить цикл выпуска новых версий. Обновления каждые полтора-два года приводят к слишком ощутимым изменениям для людей, использующих их в исследовательских лабораториях или в компаниях. Так что Python 4.0 можно ждать, пожалуй, к 2100 году. Что нужно делать, чтобы войти в основную команду разработчиков Python?Кэрол Виллинг: Стать основным разработчиком Python очень круто, если вы помешаны на языке и стремитесь заниматься именно им. Энтони Шоу (Anthony Shaw) в своей новой книге отлично объяснил, как работает Python. Но если вы не хотите заниматься именно языком, у вас есть огромное количество возможностей участвовать в развитии экосистемы проектов, включая, например, PyPI и NumPy.Но не забывайте, что основной целью Python является стабильность, и только потом — широта возможностей. В других проектах цикл от идеи до внедрения намного короче, и вы можете более свободно принимать решения о том, что следует добавить в проект. Нет, я не отговариваю от участия в разработке ядра Python. Нам, конечно же, нужны разработчики, и их число постоянно увеличивается. Но я уверена, что опыт, полученный при работе над другими проектами тоже может быть очень богатым и ценным.Дастин Инграм: Чем больше времени и сил вы тратите на внешние проекты, тем больше пользы вы приносите. Например, я принял решение не участвовать в разработке ядра, потому что могу быть больше полезен для проектов, которым нужна поддержка, а с другой стороны — они могут оказать больше влияния на сообщество. Я считаю, что это, например, инструменты разработки пакетов и PyPI, и я уже вижу большие изменения. А если бы я был основным разработчиком, это сильно повлияло на экосистему Python.Кэрол Виллинг: Еще с 2014 года я говорю о том, что нам нужно улучшать процесс разработки ядра Python и делать его более современным. Поэтому мы перенесли код на GitHub. Это был только первый шаг, но он оказался правильным, потому что открыл для нас все инструменты непрерывной интеграции. Сегодня можно намного проще участвовать в разработке благодаря возможности тестирования и оценки результатов CI. Я надеюсь, когда мы перенесем список задач и ошибок на GitHub, желающим стать основным разработчиком, станет еще проще.Перенос система отслеживания ошибок — это большой проект. Вы планируете реализовать его на платной основе?Кэрол Виллинг: Вспомните 30-летнюю историю развития языка — историю исправления ошибок, версий, продвижения, перехода на новые платформы. Это не самый простой процесс. Кроме того, нам приходится поддерживать открытый формат, чтобы иметь возможность, если GitHub вдруг перестанет существовать, переместиться на другую площадку или создать собственную. И сделать это надо будет без прерывания процессов, к которым привыкли пользователи. Напротив, мы должны добавлять рабочие потоки. Кроме того, появилось много новых инструментов, в том числе благодаря более широкому использованию JavaScript, и я думаю, что дополнительное финансирование и правильное управление проектами позволит сделать переход более плавным. Иногда работа на добровольных началах означает, что вы не можете уделить проекту достаточное количество времени. Поэтому лучше отдать его под руководство менеджеру, который сможет обеспечить его непрерывную реализацию. Оплачиваемое управление проектом в таком случае — самое эффективное решение.Многие уверены, чтобы стать разработчиком ядра Python, нужно знать язык C. Дело обстоит именно так или можно сосредоточиться на чем-то еще?Кэрол Виллинг: C требуется, только если вы углубляетесь во взаимодействие с оборудованием или управление памятью. Как я уже говорила, блог и книга Энтони Шоу отлично демонстрируют случаи, когда необходим C. Но, например, большая часть стандартной библиотеки его знания не требует.Дастин Инграм: В настоящее время почти все проекты, встроенные в стандартную библиотеку, написаны на Python.Как должность разработчика Python и директора PSF изменила вашу жизнь и повлияла на вашу карьеру?Кэрол Виллинг: У меня стало намного меньше свободного времени, потому что я работаю на добровольных началах. Но для меня в этой работе важно не то, как она влияет на мою карьеру, а то, что я участвую в создании языка, который используется многими. Здорово, что я могу дать людям с отличными идеями инструменты для реализации. Да, меня теперь больше приглашают на конференции, в том числе в качестве докладчика. Но я не думаю, что это как-то влияет на мою карьеру.Дастин Инграм: Я недавно стал директором PSF. Иногда во время собеседований на новую работу я рассказывал, что очень активно участвую в сообщества Python на добровольных началах, работодателя волновало только, не занимаюсь ли я этим в рабочее время. Я думаю, что многие просто не понимают, как поощрять работу над открытым кодом.Однако свою текущую работу в качестве Developer Advocate for the Python community в Google я получил благодаря моему опыту работы с сообществом Python. От меня требовались знания о том, как работает сообщество, как оно построено и какие инструменты использует. Однако мои должностные инструкции не предусматривают, что началах.Кэрол Виллинг: Я хочу быть уверена, что тем, кто хочет участвовать в разработке Core Python, следует быть вежливыми, но настойчивыми. Мы работаем с огромным количеством людей, у которых очень мало времени. Поэтому время, которое они посвящают Python, нужно использовать эффективно. Чтение руководств для я должен быть директором и выполнять дополнительную работу на добровольных разработчиков и просмотр вебинаров на YouTube о разработке на Python могут помочь вам стать участником сообщества. Присоединяйтесь к нам! Видео доклада можно посмотреть здесь. Профессиональная конференция для Python-разработчиков пройдет 27 и 28 сентября в Москве. Но посмотреть расписание и выбрать самые интересные доклады можно уже сегодня.Не забудьте купить билеты. Встречаемся в офлайне в конце сентября!      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Привет, друзья!\n",
      "Я — JavaScript-разработчик. Код пишу в основном на React (иногда на Vue), немного на TypeScript, немного на Node, немного знаю SQL, но...\n",
      "\n",
      "со мной работает много людей, которые пишут код на Python\n",
      "вокруг много разговоров про Python\n",
      "повсюду преподают Python\n",
      "Яндекс любит Python (раньше Гугл тоже его любил, но теперь у него есть Go)\n",
      "\n",
      "Короче говоря, сложно было не заинтересоваться Python.\n",
      "Как известно, лучший способ научиться \"кодить\" — это писать код.\n",
      "Предлагаю вашему вниманию 50 популярных в сфере программирования задач, решенных с помощью Python и JavaScript.\n",
      "Цель — сравнить языки на предмет предоставляемых ими возможностей.\n",
      "Задачки не очень сложные, хотя имеется парочка из категории продвинутых.\n",
      "Предполагается, что вы владеете хотя бы одним из названных языков на среднем уровне.\n",
      "Я старался комментировать ключевые моменты. Однако, это не гайд по JavaScript или Python, поэтому за подробностями работы той или иной функции или метода вам придется обращаться к другим источникам (в конце концов, что это за разработчик, который не умеет гуглить).\n",
      "Если вы найдете более оптимальное или эффективное с точки зрения производительности решение, поделитесь им в комментариях или пришлите мне в личку, я обязательно добавлю его в список. Единственное условие — решение должно быть не просто другим, а лучшим по объективным причинам.\n",
      "Я старался выбирать задачи, которые, во-первых, можно без особого труда (т.е. без \"плясок с бубном\") решить на обоих языках, во-вторых, можно решить аналогичным или хотя бы похожим образом. Отчасти поэтому некоторые решения могут выглядеть нетипичными для того или иного языка. Это опять же обусловлено сравнением между функционалом и конструкциями JavaScript и Python.\n",
      "Приветствуются любые конструктивные замечания и предложения.\n",
      "Список задач:\n",
      "\n",
      "1. Вывести сообщение \"Hello world!\"\n",
      "2. Сложить два числа\n",
      "3. Извлечь квадратный корень из числа\n",
      "4. Вычислить площадь треугольника\n",
      "5. Произвести обмен значениями между переменными\n",
      "6. Функция для получения случайного целого числа в заданном диапазоне\n",
      "7. Функция для преобразования километров в мили\n",
      "8. Функция для преобразования градусов Цельсия в градусы Фаренгейта\n",
      "9. Функция для определения того, каким является число: положительным, отрицательным или нулем\n",
      "10. Функция для определения того, каким является число, четным или нечетным\n",
      "11. Функция для определения того, является ли год високосным\n",
      "12. Функция для определения наибольшего числа\n",
      "13. Функция для определения того, является ли число простым\n",
      "14. Функция для вывода всех простых чисел в заданном диапазоне\n",
      "15. Функция для вычисления факториала числа\n",
      "16. Функция для вывода таблицы умножения для указанного числа\n",
      "17. Функция для вычисления суммы чисел из последовательности Фибоначчи до указанного\n",
      "18. Функция для вычисления суммы натуральных чисел от 1 до указанного\n",
      "19. Функция для вывода чисел в степени 2 от 1 до указанного\n",
      "20. Функция для определения всех чисел, которые делятся на другое число без остатка\n",
      "21. Функция для вычисления наибольшего общего делителя\n",
      "22. Функция для вычисления наименьшего общего кратного\n",
      "23. Функция для определения всех чисел, на которые без остатка делится указанное\n",
      "24. Простой калькулятор\n",
      "25. Функция для преобразования числа в двоичное представление\n",
      "26. Функция для сложения матриц\n",
      "27. Функция для транспонирования матрицы\n",
      "28. Функция для определения того, является ли строка палиндромом\n",
      "29. Функция для удаления лишних символов из строки\n",
      "30. Функция для сортировки слов в алфавитном порядке\n",
      "30. Функция для определения количества гласных в строке\n",
      "31. Функция для рисования пирамиды\n",
      "32. Объединить два словаря (объекта) в один\n",
      "33. Преобразовать вложенный список (массив) в одноуровневый (плоский)\n",
      "34. Копировать список (массив) (сделать его срез)\n",
      "35. Перебрать ключи и значения словаря (объекта)\n",
      "36. Отсортировать словарь (объект) по ключам и значениям\n",
      "37. Определить, является ли список (массив) пустым\n",
      "38. Объединить два списка (массива) в один\n",
      "39. Извлечь подстроку из строки\n",
      "40. Функция для получения случайного элемента массива\n",
      "41. Определить количество вхождений элемента в массиве\n",
      "42. Объединить два списка (массива) в словарь (объект)\n",
      "43. Удалить лишние пробелы из строки с помощью регулярного выражения\n",
      "44. Создать перечисления (enum)\n",
      "45. Функция для определения анаграмм\n",
      "46. \"Капитализировать\" строку\n",
      "47. Функция для определения всех вариантов строки\n",
      "48. Создание счетчика (таймера)\n",
      "49. Создание вложенной директории\n",
      "50. Получение названий файлов с расширением \".txt\"\n",
      "\n",
      "1. Вывести сообщение \"Hello world!\"\n",
      "Python\n",
      "print('Hello world!')\n",
      "JavaScript\n",
      "console.log('Hello world!')\n",
      "// или\n",
      "alert('Hello world!')\n",
      "Фраза \"Hello world\" переводится не как \"Привет, мир\", а как \"Привет всем\" или, если быть более аутентичным, \"Привет, народ\". Вы знали об этом?\n",
      "2. Сложить два числа\n",
      "Python\n",
      "# переменные для чисел\n",
      "num1 = 2\n",
      "num2 = 4\n",
      "# num1, num2 = 2, 4 - так делать можно, но не рекомендуется\n",
      "\n",
      "# переменная для суммы\n",
      "sum = num1 + num2\n",
      "\n",
      "# f-строки позволяют интерполировать переменные\n",
      "print(f'{num1} + {num2} = {sum}') # 2 + 4 = 6\n",
      "JavaScript\n",
      "const num1 = 4\n",
      "const num2 = 2\n",
      "// const num1 = 4, num2 = 2 - так делать можно, но не рекомендуется\n",
      "\n",
      "const sum = num1 + num2\n",
      "\n",
      "// для интерполяции переменных в `JS` используются шаблонные или строковые литералы\n",
      "console.log(`${num1} + ${num2} = ${sum}`) // 2 + 4 = 6\n",
      "3. Извлечь квадратный корень из числа\n",
      "Python\n",
      "num = 4\n",
      "\n",
      "# получаем 2.0\n",
      "sqrt = num ** 0.5\n",
      "\n",
      "# встроенная функция `int()` преобразует число в целое\n",
      "print(f'Квадратным корнем {num} является {int(sqrt)}')\n",
      "# Квадратным корнем 4 являет\n",
      "\n",
      "# для выполнения математических операций в `Python`\n",
      "# имеется специальный модуль\n",
      "from math import sqrt\n",
      "print(sqrt(4)) # 2.0\n",
      "JavaScript\n",
      "const num = 4\n",
      "\n",
      "// получаем 2, поэтому необходимость в округлении числа до целого отсутствует\n",
      "const sqrt = num ** 0.5\n",
      "\n",
      "console.log(`Квадратным корнем ${num} является ${sqrt}`)\n",
      "\n",
      "// для выполнения математических операций в `JS`\n",
      "// имеется глобальный объект\n",
      "console.log(Math.sqrt(4)) // 2\n",
      "4. Вычислить площадь треугольника\n",
      "Здесь можно почитать про различные способы вычисления площади треугольника.\n",
      "Если a, b и c — три стороны треугольника, то согласно формуле Герона для того, чтобы вычислить площадь треугольника,\n",
      "\n",
      "сначала необходимо вычислить разность полупериметра и каждой его стороны\n",
      "затем найти произведение полученных чисел, умножить результат на полупериметр и найти корень из полученного числа\n",
      "\n",
      "p = (a + b + c) / 2\n",
      "s = √(p * (p - a) * (p - b) * (p - c))\n",
      "p — это полупериметр, а s — площадь.\n",
      "Python\n",
      "a = 5\n",
      "b = 6\n",
      "c = 7\n",
      "\n",
      "# вычисляем полупериметр\n",
      "p = (a + b + c) / 2\n",
      "# вычисляем площадь\n",
      "s = (p * (p - a) * (p - b) * (p - c)) ** 0.5\n",
      "\n",
      "# `round(num, count)` используется для округления числа в ближайшую сторону\n",
      "# `count` - количество цифр после запятой\n",
      "print(f'Площадь треугольника со сторонами {a}, {b} и {c} равняется {round(s, 2)}')\n",
      "JavaScript\n",
      "const a = 5\n",
      "const b = 6\n",
      "const c = 7\n",
      "\n",
      "const p = (a + b + c) / 2\n",
      "const s = (p * (p - a) * (p - b) * (p - c)) ** 0.5\n",
      "\n",
      "// в `JS` метод `Math.round()` округляет число до целого в ближайшуй сторону,\n",
      "// поэтому мы используем метод `toFixed(count)`\n",
      "console.log(`Площадь треугольника со сторонами ${a}, ${b} и ${c} равняется ${s.toFixed(2)}`)\n",
      "5. Произвести обмен значениями между переменными\n",
      "Python\n",
      "x = 5\n",
      "y = 10\n",
      "\n",
      "# с помощью дополнительной (временной) переменной\n",
      "t = x\n",
      "x = y\n",
      "y = t\n",
      "\n",
      "# такой трюк в `JS` провернуть нельзя,\n",
      "# но там есть другой\n",
      "x, y = y, x\n",
      "\n",
      "# с помощью сложения и вычитания\n",
      "x = x + y\n",
      "y = x - y\n",
      "x = x - y\n",
      "\n",
      "# c помощью умножения и деления\n",
      "x = x * y\n",
      "y = x / y\n",
      "x = x / y\n",
      "\n",
      "# с помощью исключающего ИЛИ (XOR)\n",
      "x = x ^ y\n",
      "y = x ^ y\n",
      "x = x ^ y\n",
      "JavaScript\n",
      "// в целом, все то же самое, за исключением следующего:\n",
      "// переменные должны объявляться с помощью ключевого слова `let`,\n",
      "// чтобы они были мутабельными (изменяемыми)\n",
      "// в `Python` по умолчанию все переменные являются мутабельными\n",
      "let x = 5\n",
      "let y = 10\n",
      "\n",
      "let t = x\n",
      "x = y\n",
      "y = t\n",
      "\n",
      "// трюк на `JS`\n",
      "// обратите внимание на ; перед [\n",
      ";[x, y] = [y, x]\n",
      "\n",
      "// с помощью сложения и вычитания\n",
      "// c помощью умножения и деления\n",
      "// с помощью исключающего ИЛИ (XOR)\n",
      "6. Функция для получения случайного целого числа в заданном диапазоне\n",
      "Python\n",
      "В Python для этого существует специальный модуль:\n",
      "# импортируем метод из модуля\n",
      "from random import randint\n",
      "\n",
      "# такая функция называется лямбдой\n",
      "get_random_int = lambda min, max: randint(min, max)\n",
      "\n",
      "print(f'Случайное целое число в диапазоне от 0 до 100: {get_random_int(0, 100)}')\n",
      "JavaScript\n",
      "В JS готовой функции для этого нет, поэтому придется реализовать ее самостоятельно:\n",
      "// такая функция называется стрелочной\n",
      "// ~~ - это сокращение для `Math.floor()` - округление числа до целого в меньшую сторону\n",
      "// `Math.random()` возвращает случайное число от 0 до 1\n",
      "const getRandomInt = (min, max) => ~~(min + Math.random() * (max - min + 1))\n",
      "\n",
      "console.log(`Случайное целое число в диапазоне от 0 до 100: ${getRandomInt(0, 100)}`)\n",
      "7. Функция для преобразования километров в мили\n",
      "Python\n",
      "# запрашиваем км у \"юзера\" с помощью `input()`\n",
      "# `float()` преобразует строку в число с запятой\n",
      "km = float(input('Введите значение в км: '))\n",
      "\n",
      "# фактор преобразования\n",
      "f = 0.621371\n",
      "\n",
      "# вычисляем мили\n",
      "m = km * f\n",
      "# km = m / f\n",
      "\n",
      "print(f'{km} километров - это {round(m, 2)} миль')\n",
      "JavaScript\n",
      "// запрашиваем км у юзера с помощью `prompt()`\n",
      "// `Number()` преобразует строку в число\n",
      "const km = Number(prompt('Введите значение в км: '))\n",
      "\n",
      "const f = 0.621371\n",
      "\n",
      "const m = km * f\n",
      "\n",
      "alert(`${km} километров - это ${m.toFixed(2)} миль`)\n",
      "8. Функция для преобразования градусов Цельсия в градусы Фаренгейта\n",
      "Python\n",
      "# запрашиваем градусы Цельсия у юзера\n",
      "c = float(input('Введите значение в градусах Цельсия: '))\n",
      "\n",
      "# преобразуем Цельсий в Фаренгейт\n",
      "f = (c * 1.8) + 32\n",
      "# c = (f - 32) / 1.8\n",
      "\n",
      "print(f'{c} градусов Цельсия - это {round(f, 2)} градусов Фаренгейта')\n",
      "JavaScript\n",
      "// запрашиваем градусы Цельсия у юзера\n",
      "const c = Number(prompt('Введите значение в градусах Цельсия: '))\n",
      "\n",
      "const f = c * 1.8 + 32\n",
      "\n",
      "alert(`${c} градусов Цельсия - это ${f.toFixed(2)} градусов Фаренгейта`)\n",
      "9. Функция для определения того, каким является число: положительным, отрицательным или нулем\n",
      "Python\n",
      "# сигнатура обычной функции\n",
      "# отступы имеют принципиальное значение,\n",
      "# обозначая блоки кода\n",
      "def is_pos_neg(n):\n",
      " # если\n",
      " if n > 0:\n",
      "   return 'Positive'\n",
      " # иначе если\n",
      " elif n == 0:\n",
      "   return 'Null'\n",
      " # иначе\n",
      " else:\n",
      "   return 'Negative'\n",
      "\n",
      "print(\n",
      " is_pos_neg(\n",
      "   # преобразуем строку в число c запятой\n",
      "   float(\n",
      "     input('Number: ')\n",
      "   )\n",
      " )\n",
      ")\n",
      "JavaScript\n",
      "// сигнатура обычной функции\n",
      "// фигурные скобки имеют принципиальное значение,\n",
      "// обозначая блоки кода\n",
      "function isPosNeg(n) {\n",
      " // если\n",
      " if (n > 0) {\n",
      "   return 'Positive'\n",
      " // иначе если\n",
      " } else if (n === 0) {\n",
      "   return 'Null'\n",
      " // иначе\n",
      " } else {\n",
      "   return 'Negative'\n",
      " }\n",
      "}\n",
      "\n",
      "alert(\n",
      " isPosNeg(\n",
      "   // `+` (унарный префиксный плюс) - это сокращение для `Number()`\n",
      "   +prompt('Number: ')\n",
      " )\n",
      ")\n",
      "10. Функция для определения того, каким является число, четным или нечетным\n",
      "Python\n",
      "# это называется коротким вычислением\n",
      "# истина if условие else ложь\n",
      "is_odd_even = lambda n: 'Even' if n % 2 == 0 else 'Odd'\n",
      "\n",
      "print(\n",
      " is_odd_even(\n",
      "   float(\n",
      "     input('Number: ')\n",
      "   )\n",
      " )\n",
      ")\n",
      "JavaScript\n",
      "// тернарный оператор\n",
      "// условие ? истина : ложь\n",
      "// обратите внимание, что в `JS` имеется 2 оператора равенства\n",
      "// старайтесь всегда использовать `===`\n",
      "const isOddEven = (n) => (n % 2 === 0 ? 'Even' : 'Odd')\n",
      "\n",
      "alert(\n",
      " isOddEven(\n",
      "   prompt('Number: ')\n",
      " )\n",
      ")\n",
      "Раз уж мы заговорили про наличие 2 операторов равенства в JS, здесь же обращу ваше внимание на следующее:\n",
      "\n",
      "логическими значениями в Python являются True и False (с большой буквы), а в JS — true и false\n",
      "индикатором отсутствия значения в Python является None, а в JS у нас целых 3 таких индикатора — undefined, null и NaN\n",
      "\n",
      "11. Функция для определения того, является ли год високосным\n",
      "Високосным является год, который делится на 4 без остатка, за исключением столетий (оканчивающихся на 00). В последнем случае год будет високосным, если делится без остатка на 400.\n",
      "Python\n",
      "def is_leap_year(year):\n",
      " if (year % 4) == 0:\n",
      "   if (year % 100) == 0:\n",
      "     if (year % 400) == 0:\n",
      "       return 'Leap'\n",
      "     else:\n",
      "       return 'Not leap'\n",
      "   else:\n",
      "     return 'Leap'\n",
      " else:\n",
      "   return 'Not leap'\n",
      "\n",
      "# функцию можно переписать с помощью логических операторов `and` (И), `or` (ИЛИ) и `not` (НЕ)\n",
      "def is_leap_year(year):\n",
      " # `\\` используется для объединения нескольких строк кода\n",
      " # в один блок\n",
      " if (year % 4) == 0 and (year % 100) == 0 and (year % 400) == 0 \\\n",
      " or (year % 4) == 0 and not (year % 100) == 0:\n",
      "   return 'Leap'\n",
      " else:\n",
      "   return 'Not leap'\n",
      "\n",
      "print(\n",
      " is_leap_year(\n",
      "   int(\n",
      "     input('Year: ')\n",
      "   )\n",
      " )\n",
      ")\n",
      "JavaScript\n",
      "function isLeapYear(year) {\n",
      " if (year % 4 === 0) {\n",
      "   if (year % 100 === 0) {\n",
      "     if (year % 400 === 0) {\n",
      "       return 'Leap'\n",
      "     } else {\n",
      "       return 'Not leap'\n",
      "     }\n",
      "   } else {\n",
      "     return 'Leap'\n",
      "   }\n",
      " } else {\n",
      "   return 'Not leap'\n",
      " }\n",
      "}\n",
      "\n",
      "// функцию можно переписать с помощью логических операторов `&&` (И), `||` (ИЛИ) и `!` (НЕ)\n",
      "// объединение нескольких строк кода в один блок\n",
      "// происходит автоматически\n",
      "function isLeapYear(year) {\n",
      " if (\n",
      "   (year % 4 === 0 && year % 100 === 0 && year % 400 === 0) ||\n",
      "   (year % 4 === 0 && !(year % 100 === 0))\n",
      " ) {\n",
      "   return 'Leap'\n",
      " } else {\n",
      "   return 'Not leap'\n",
      " }\n",
      "}\n",
      "\n",
      "alert(\n",
      " isLeapYear(\n",
      "   // еще одна функция для преобразования значения в целое число\n",
      "   // второй аргумент - система счисления (в данном случае десятичная)\n",
      "   parseInt(\n",
      "     prompt('Year: '),\n",
      "     10\n",
      "   )\n",
      " )\n",
      ")\n",
      "12. Функция для определения наибольшего числа\n",
      "Python\n",
      "def get_largest_num(n1, n2, n3):\n",
      " if (n1 >= n2) and (n1 >= n2):\n",
      "   return n1\n",
      " elif (n2 >= n1) and (n2 >= n3):\n",
      "   return n2\n",
      " else:\n",
      "   return n3\n",
      "\n",
      "print(get_largest_num(1, 3, 2)) # 3\n",
      "\n",
      "# существует встроенная функция\n",
      "print(max(3, 1, 2))\n",
      "JavaScript\n",
      "function getLargestNum(n1, n2, n3) {\n",
      " if (n1 >= n2 && n1 >= n3) {\n",
      "   return n1\n",
      " } else if (n2 >= n1 && n2 >= n3) {\n",
      "   return n2\n",
      " } else {\n",
      "   return n3\n",
      " }\n",
      "}\n",
      "\n",
      "console.log(getLargestNum(1, 3, 2)) // 3\n",
      "\n",
      "// существует встроенная функция\n",
      "console.log(Math.max(3, 1, 2))\n",
      "13. Функция для определения того, является ли число простым\n",
      "Простым является число, которое больше 1 и делится только на себя и 1. Простыми являются числа 2, 3, 5, 7 и т.д. А число 6, например, таковым не является, поскольку является составным: 2 * 3 = 6.\n",
      "Python\n",
      "def is_prime(n):\n",
      " if n > 1:\n",
      "   # `range()` осуществляет перебор (итерацию) в указанном количестве\n",
      "   # возможные сигнатуры:\n",
      "   # `range(end)`\n",
      "   # `range(start, end)`\n",
      "   # `range(start, end, step)`\n",
      "   for i in range(2, n):\n",
      "     # если имеется число, на которое `n` делится без остатка\n",
      "     if (n % i) == 0:\n",
      "       return 'Not prime'\n",
      "   # если такого числа нет\n",
      "   return 'Prime'\n",
      " # если число < 1\n",
      " return 'Invalid'\n",
      "\n",
      "print(\n",
      " is_prime(\n",
      "   int(\n",
      "     input('Number: ')\n",
      "   )\n",
      " )\n",
      ")\n",
      "JavaScript\n",
      "// в `JS` не существует аналога `range()`,\n",
      "// поэтому функцию придется реализовать по-другому\n",
      "function isPrime(n) {\n",
      " if (n > 1) {\n",
      "   let i = 2\n",
      "   // выполнять блок кода до тех пор, пока...\n",
      "   while (i < n) {\n",
      "     if (n % i === 0) {\n",
      "       return 'Not prime'\n",
      "     }\n",
      "     i++\n",
      "   }\n",
      "   return 'Prime'\n",
      " }\n",
      " return 'Invalid'\n",
      "}\n",
      "\n",
      "alert(\n",
      " isPrime(\n",
      "   parseInt(\n",
      "     prompt('Number: '),\n",
      "     10\n",
      "   )\n",
      " )\n",
      ")\n",
      "14. Функция для вывода всех простых чисел в заданном диапазоне\n",
      "Python\n",
      "def get_primes(min, max):\n",
      " primes = []\n",
      " for n in range(min, max + 1):\n",
      "   if n > 1:\n",
      "     for i in range(2, n):\n",
      "       if (n % i) == 0:\n",
      "         # оператор `break` используется для выхода из цикла\n",
      "         break\n",
      "     else:\n",
      "       # `append()` помещает элемент в конец списка\n",
      "       primes.append(n)\n",
      " return primes\n",
      "\n",
      "print(get_primes(1, 100))\n",
      "JavaScript\n",
      "function getPrimes(min, max) {\n",
      " const primes = []\n",
      " for (let i = min; i <= max; i++) {\n",
      "   if (i > 1) {\n",
      "     // индикатор того, что число является простым\n",
      "     let isPrime = true\n",
      "     for (let j = 2; j < i; j++) {\n",
      "       if (i % j === 0) {\n",
      "         isPrime = false\n",
      "         break\n",
      "       }\n",
      "     }\n",
      "     if (isPrime) {\n",
      "       // `push()` помещает элемент в конец массива\n",
      "       primes.push(i)\n",
      "     }\n",
      "   }\n",
      " }\n",
      " return primes\n",
      "}\n",
      "\n",
      "console.log(getPrimes(1, 100))\n",
      "15. Функция для вычисления факториала числа\n",
      "Факториал — это произведение всех целых чисел от 1 до указанного. Например, факториалом числа 6 является 1 * 2 * 3 * 4 * 5 * 6 = 720. \"Указанное число\" должно быть положительным. В качестве небольшой оптимизации следует принять во внимание, что факториалом чисел 1 и 2 являются, соответственно, 1 и 2.\n",
      "Python\n",
      "def get_factorial(n):\n",
      " if n < 0: return 'Invalid'\n",
      " if n < 2: return n\n",
      " # это называется рекурсией\n",
      " # чтобы понять рекурсию, необходимо сначала понять рекурсию ;)\n",
      " return n * get_factorial(n - 1)\n",
      "\n",
      "print(get_factorial(6)) # 720\n",
      "JavaScript\n",
      "function getFactorial(n) {\n",
      " if (n < 0) return 'Invalid'\n",
      " if (n < 2) return n\n",
      " return n * getFactorial(n - 1)\n",
      "}\n",
      "\n",
      "console.log(getFactorial(6)) // 720\n",
      "16. Функция для вывода таблицы умножения для указанного числа\n",
      "Python\n",
      "def get_mult_table(n):\n",
      " for i in range(1, 11):\n",
      "   print(f'{n} * {i} = {n * i}')\n",
      "\n",
      "get_mult_table(9)\n",
      "JavaScript\n",
      "function getMultTable(n) {\n",
      " for (let i = 1; i < 11; i++) {\n",
      "   console.log(`${n} * ${i} = ${n * i}`)\n",
      " }\n",
      "}\n",
      "\n",
      "getMultTable(9)\n",
      "17. Функция для вычисления суммы чисел из последовательности Фибоначчи до указанного\n",
      "Последовательность Фибоначчи — это последовательность целых чисел 0, 1, 1, 2, 3, 5, 8 и т.д. Два первых числа — это 0 и 1. Последующие числа — это результат сложения двух предыдущих.\n",
      "Python\n",
      "# рекурсивная реализация будет гораздо менее производительной\n",
      "def get_fibonacci_sum(n):\n",
      " if n < 0: return 'Invalid'\n",
      " if n <= 1: return n\n",
      "\n",
      " a = 1\n",
      " b = 1\n",
      "\n",
      " for i in range(3, n + 1):\n",
      "   c = a + b\n",
      "   a = b\n",
      "   b = c\n",
      "\n",
      " return b\n",
      "\n",
      "print(get_fibonacci_sum(10)) # 55\n",
      "JavaScript\n",
      "function getFibonacciSum(n) {\n",
      " if (n < 0) return 'Invalid'\n",
      " if (n <= 1) return n\n",
      "\n",
      " let a = 1\n",
      " let b = 1\n",
      "\n",
      " for (let i = 3; i <= n; i++) {\n",
      "   let c = a + b\n",
      "   a = b\n",
      "   b = c\n",
      " }\n",
      "\n",
      " return b\n",
      "}\n",
      "\n",
      "console.log(getFibonacciSum(10)) // 55\n",
      "18. Функция для вычисления суммы натуральных чисел от 1 до указанного\n",
      "Натуральные числа — числа, возникающие естественным образом при счете (1, 2, 3, 4, 5, 6, 7 и т.д.).\n",
      "Python\n",
      "def get_natural_sum(n):\n",
      " if n < 0: return 'Invalid'\n",
      "\n",
      " s = 0\n",
      "\n",
      " while n > 0:\n",
      "   s += n\n",
      "   n -= 1\n",
      "\n",
      " return s\n",
      "\n",
      "print(\n",
      " get_natural_sum(10)\n",
      ")\n",
      "\n",
      "# с помощью лямбда-функции\n",
      "get_nat_sum = lambda n: 'Invalid' if n < 0 else int(n * (n + 1) / 2)\n",
      "\n",
      "print(\n",
      " get_nat_sum(10)\n",
      ")\n",
      "JavaScript\n",
      "function getNaturalSum(n) {\n",
      " if (n < 0) return 'Invalid'\n",
      "\n",
      " let s = 0\n",
      "\n",
      " while (n > 0) {\n",
      "   s += n\n",
      "   n -= 1\n",
      " }\n",
      "\n",
      " return s\n",
      "}\n",
      "\n",
      "console.log(\n",
      " getNaturalSum(10)\n",
      ")\n",
      "\n",
      "// с помощью стрелочной функции\n",
      "const getNatSum = (n) => n < 0 ? 'Invalid' : n * (n + 1) / 2\n",
      "\n",
      "console.log(\n",
      " getNatSum(10)\n",
      ")\n",
      "19. Функция для вывода чисел в степени 2 от 1 до указанного\n",
      "Python\n",
      "def get_power_of_2(n):\n",
      " # `r` - список с результатами\n",
      " # `list(iterable)` создает список\n",
      " # `iterable` - итерируемая или перебираемая сущность\n",
      " # `map(function, iterable)` - вызывает `function` для каждого элемента в `iterable\n",
      " r = list(map(lambda x: 2 ** x, range(1, n + 1)))\n",
      "\n",
      " for i in range(n):\n",
      "   # `r[i]` извлекает элемент с указанным индексом\n",
      "   print(f'2 ** {i + 1} = {r[i]}')\n",
      "\n",
      "get_power_of_2(10)\n",
      "JavaScript\n",
      "function getPowerOf2(n) {\n",
      " // `Array.from(iterable, function)` - создает массив, в том числе с помощью функции,\n",
      " // вызываемой для каждого элемента массива\n",
      " // `length` - длина создаваемого массива\n",
      " // `function` принимает элемент и его индекс\n",
      " const r = Array.from({ length: n }, (_, i) => 2 ** (i + 1))\n",
      "\n",
      " for (const i in r) {\n",
      "   // `r[i]` извлекает элемент с указанным индексом\n",
      "   console.log(`2 ** ${+i + 1} = ${r[i]}`)\n",
      " }\n",
      "}\n",
      "\n",
      "getPowerOf2(10)\n",
      "20. Функция для определения всех чисел, которые делятся на другое число без остатка\n",
      "Python\n",
      "# создаем список, первым элементом которого является `10`, с шагом `10`, до `210`\n",
      "nums = list(map(lambda x: x, range(10, 210, 10)))\n",
      "\n",
      "# `filter(function, iterable)` выполняет фильтрацию списка\n",
      "# `function` возвращает логическое значение - критерий фильтрации\n",
      "div_nums = lambda n: list(filter(lambda x: (x % n) == 0, nums))\n",
      "\n",
      "print(div_nums(6))\n",
      "JavaScript\n",
      "const nums = Array.from({ length: 10 }, (_, i) => i > 0 ? (i + 1) * 10 : 10)\n",
      "\n",
      "// `filter(function)` возвращает отфильтрованный массив\n",
      "const div_nums = (n) => nums.filter(_n => _n % n === 0)\n",
      "\n",
      "console.log(div_nums(6))\n",
      "21. Функция для вычисления наибольшего общего делителя\n",
      "Наибольший общий делитель (он же наивысший общий фактор) — это положительное целое число, на которое без остатка делятся 2 (и более) других числа. Например, число 12 является НОД чисел 36 и 48.\n",
      "Python\n",
      "def get_gcd(x, y):\n",
      " # переменная для меньшего числа\n",
      " s = x\n",
      "\n",
      " if x > y: s = y\n",
      "\n",
      " for i in range(1, s + 1):\n",
      "   if ((x % i == 0) and (y % i == 0)):\n",
      "     # в данном случае мы можем \"переиспользовать\" переменную `s`\n",
      "     s = i\n",
      "\n",
      " return s\n",
      "\n",
      "print(get_gcd(48, 36)) # 12\n",
      "JavaScript\n",
      "function getGcd(x, y) {\n",
      " let s = x\n",
      "\n",
      " if (x > y) s = y\n",
      "\n",
      " // мы не можем переопределять переменную `s`,\n",
      " // потому что используем ее в цикле\n",
      " let r = 1\n",
      "\n",
      " for (let i = 1; i <= s; i++) {\n",
      "   if (x % i === 0 && y % i === 0) {\n",
      "     r = i\n",
      "   }\n",
      " }\n",
      "\n",
      " return r\n",
      "}\n",
      "\n",
      "console.log(getGcd(24, 54)) // 6\n",
      "22. Функция для вычисления наименьшего общего кратного\n",
      "Наименьшее общее кратное — это положительное целое число, которое делится без остатка на два (и более) других числа. Например, число 84 является НОК чисел 12 и 14.\n",
      "Python\n",
      "def get_lcm(x, y):\n",
      " # переменная для большего числа\n",
      " g = x\n",
      "\n",
      " if x < y: g = y\n",
      "\n",
      " # осторожно: потенциально бесконечный цикл\n",
      " while True:\n",
      "   if (g % x == 0) and (g % y == 0):\n",
      "     break\n",
      "   g += 1\n",
      "\n",
      " return g\n",
      "\n",
      "print(get_lcm(12, 14)) # 84\n",
      "JavaScript\n",
      "function getLCM(x, y) {\n",
      " let g = x\n",
      "\n",
      " if (x < y) g = y\n",
      "\n",
      " // осторожно: потенциально бесконечный цикл\n",
      " // в данном случае мы можем переиспользовать переменную `g`\n",
      " while (true) {\n",
      "   if (g % x === 0 && g % y === 0) {\n",
      "     break\n",
      "   }\n",
      "   g += 1\n",
      " }\n",
      "\n",
      " return g\n",
      "}\n",
      "\n",
      "console.log(getLCM(24, 54)) // 216\n",
      "23. Функция для определения всех чисел, на которые без остатка делится указанное\n",
      "Python\n",
      "def get_factors(n):\n",
      " f = []\n",
      "\n",
      " for i in range(1, n + 1):\n",
      "   if n % i == 0:\n",
      "     f.append(i)\n",
      "\n",
      " return f\n",
      "\n",
      "print(get_factors(123)) # [1, 3, 41, 123]\n",
      "JavaScript\n",
      "function getFactors(n) {\n",
      " const f = []\n",
      "\n",
      " for (let i = 1; i <= n; i++) {\n",
      "   if (n % i === 0) {\n",
      "     f.push(i)\n",
      "   }\n",
      " }\n",
      "\n",
      " return f\n",
      "}\n",
      "\n",
      "console.log(getFactors(321)) // [1, 3, 107, 321]\n",
      "24. Простой калькулятор\n",
      "Почему бы не реализовать его на классах?\n",
      "Python\n",
      "class Calc:\n",
      " # инициализация класса\n",
      " def __init__(self):\n",
      "   # результат\n",
      "   self.result = 0\n",
      "\n",
      " # метод для добавления числа\n",
      " def add(self, n):\n",
      "   self.result += n\n",
      "   # возвращаем экземпляр, чтобы иметь возможность выполнять операции в цепочке\n",
      "   return self\n",
      "\n",
      " # для вычитания\n",
      " def sub(self, n):\n",
      "   self.result -= n\n",
      "   return self\n",
      "\n",
      " # для умножения\n",
      " def mult(self, n):\n",
      "   self.result *= n\n",
      "   return self\n",
      "\n",
      " # для деления\n",
      " def div(self, n):\n",
      "   self.result /= n\n",
      "   return self\n",
      "\n",
      "# создаем экземпляр\n",
      "calc = Calc()\n",
      "\n",
      "# выполняем операции\n",
      "calc.add(5).sub(3).mult(4).div(2)\n",
      "\n",
      "# выводим результат\n",
      "print(int(calc.result)) # 4\n",
      "JavaScript\n",
      "class Calc {\n",
      " // инициализация класса\n",
      " constructor() {\n",
      "   this.result = 0\n",
      " }\n",
      "\n",
      " // в `JS` `this` (`self`) явно передавать не требуется\n",
      " add(n) {\n",
      "   this.result += n\n",
      "   // возвращаем экземпляр, чтобы иметь возможность выполнять операции в цепочке\n",
      "   return this\n",
      " }\n",
      "\n",
      " sub(n) {\n",
      "   this.result -= n\n",
      "   return this\n",
      " }\n",
      "\n",
      " mult(n) {\n",
      "   this.result *= n\n",
      "   return this\n",
      " }\n",
      "\n",
      " div(n) {\n",
      "   this.result /= n\n",
      "   return this\n",
      " }\n",
      "}\n",
      "\n",
      "// создаем экземпляр\n",
      "const calc = new Calc()\n",
      "\n",
      "// выполняем операции\n",
      "calc.add(5).sub(3).mult(4).div(2)\n",
      "\n",
      "// выводим результат\n",
      "console.log(calc.result) // 4\n",
      "Лично я предпочитаю функциональный стиль программирования, поэтому классов в примерах больше не будет.\n",
      "25. Функция для преобразования числа в двоичное представление\n",
      "Такое преобразование выполняется посредством деления числа на 2 с выводом остатка в обратном порядке. Проще показать:\n",
      "\n",
      "\n",
      "Python\n",
      "def convert_to_binary(n):\n",
      " if n < 0: return 'Invalid'\n",
      " if n > 1:\n",
      "   # `//` - оператор деления с округлением в меньшую сторону\n",
      "   # рекурсия\n",
      "   convert_to_binary(n // 2)\n",
      " # конкатенация вывода\n",
      " # в `JS` такой \"фичи\" нет\n",
      " print(n % 2, end = '')\n",
      "\n",
      "convert_to_binary(34)\n",
      "# вывод результата\n",
      "print() # 100010\n",
      "JavaScript\n",
      "function convertToBinary(n) {\n",
      " if (n < 0) return 'Invalid'\n",
      " // переменная для двоичного представления\n",
      " const binary = []\n",
      " while (n >= 1) {\n",
      "   // метод `unshift()` помещает элемент в начало массива\n",
      "   binary.unshift(n % 2)\n",
      "   n = ~~(n / 2)\n",
      " }\n",
      " // объединяем элементы массива в строку\n",
      " return binary.join('')\n",
      "}\n",
      "console.log(convertToBinary(34)) // 100010\n",
      "\n",
      "// в `JS` есть встроенная функция `toString(radix)`,\n",
      "// где `radix` - это система счисления\n",
      "console.log((34).toString(2)) // 100010\n",
      "26. Функция для сложения матриц\n",
      "Матрица может быть представлена в виде вложенного списка (массива), где каждый элемент — это строка матрицы. Пример матрицы 3x2, где 3 — это количество строк, а 2 — количество столбцов:\n",
      "[\n",
      " [2, 4],\n",
      " [6, 8],\n",
      " [1, 3]\n",
      "]\n",
      "Доступ к первой строке матрицы x можно получить через x[0], а доступ к первому элементу первой строки — через x[0][0].\n",
      "Обратите внимание: эта задача относится к категории продвинутых. Можете пропустить ее, если читаете статью в первый раз.\n",
      "Python\n",
      "def add_matrices(x, y):\n",
      " # результирующая матрица\n",
      " result = [\n",
      "   [0, 0, 0],\n",
      "   [0, 0, 0],\n",
      "   [0, 0, 0]\n",
      " ]\n",
      "\n",
      " # `len()` возвращает длину списка\n",
      " # выполняем итерацию по количеству строк\n",
      " for i in range(len(x)):\n",
      "   # выполняем итерацию по количеству столбцов\n",
      "   for j in range(len(x[0])):\n",
      "     result[i][j] = x[i][j] + y[i][j]\n",
      "\n",
      " return result\n",
      "\n",
      "# матрица раз\n",
      "x = [\n",
      " [1, 2, 3],\n",
      " [4, 5, 6],\n",
      " [7, 8, 9]\n",
      "]\n",
      "\n",
      "# матрица два\n",
      "y = [\n",
      " [9, 8, 7],\n",
      " [3, 2, 1],\n",
      " [6, 5, 4]\n",
      "]\n",
      "\n",
      "print(add_matrices(x, y))\n",
      "'''\n",
      "[\n",
      " [10, 10, 10],\n",
      " [7, 7, 7],\n",
      " [13, 13, 13]\n",
      "]\n",
      "'''\n",
      "\n",
      "# данная техника называется представлением списков (list comprehension)\n",
      "add_matrices_advanced = lambda x, y: [[x[i][j] + y[i][j] for j in range(len(x[0]))] for i in range(len(x))]\n",
      "\n",
      "print(add_matrices_advanced(x, y))\n",
      "JavaScript\n",
      "function addMatrices(x, y) {\n",
      " const result = [\n",
      "   [0, 0, 0],\n",
      "   [0, 0, 0],\n",
      "   [0, 0, 0]\n",
      " ]\n",
      "\n",
      " // выполняем итерацию по количеству строк\n",
      " for (const i in x) {\n",
      "   // выполняем итерацию по количеству столбцов\n",
      "   for (const j in x[0]) {\n",
      "     result[i][j] = x[i][j] + y[i][j]\n",
      "   }\n",
      " }\n",
      "\n",
      " return result\n",
      "}\n",
      "\n",
      "const x = [\n",
      " [1, 2, 3],\n",
      " [4, 5, 6],\n",
      " [7, 8, 9]\n",
      "]\n",
      "\n",
      "const y = [\n",
      " [9, 8, 7],\n",
      " [3, 2, 1],\n",
      " [6, 5, 4]\n",
      "]\n",
      "\n",
      "console.log(addMatrices(x, y))\n",
      "/*\n",
      "[\n",
      " [10, 10, 10],\n",
      " [7, 7, 7],\n",
      " [13, 13, 13]\n",
      "]\n",
      "*/\n",
      "\n",
      "// стрелочная функция\n",
      "// `map(function)` вызывает `function` для каждого элемента массива\n",
      "// она принимает 2 параметра: элемент и его индекс\n",
      "// и возвращает новый массив\n",
      "const addMatricesAdvanced = (x, y) =>\n",
      " x.map((_, i) => x[i].map((_, j) => x[i][j] + y[i][j]))\n",
      "\n",
      "console.log(addMatricesAdvanced(x, y))\n",
      "27. Функция для транспонирования матрицы\n",
      "Транспонированная матрица — это матрица, полученная из исходной посредством замены строк на столбцы.\n",
      "Обратите внимание: эта задача относится к категории продвинутых.\n",
      "Python\n",
      "def transpose_matrix(x):\n",
      " # мы хотим преобразовать матрицу 3x2 в матрицу 2x3\n",
      " # количество строк `x` количество столбцов\n",
      " result = [\n",
      "   [0, 0, 0],\n",
      "   [0, 0, 0]\n",
      " ]\n",
      "\n",
      " for i in range(len(x)):\n",
      "   for j in range(len(x[0])):\n",
      "     result[j][i] = x[i][j]\n",
      "\n",
      " return result\n",
      "\n",
      "x = [\n",
      " [1, 2],\n",
      " [4, 5],\n",
      " [7, 8]\n",
      "]\n",
      "\n",
      "print(transpose_matrix(x))\n",
      "'''\n",
      "[\n",
      " [1, 4, 7],\n",
      " [2, 5, 8]\n",
      "]\n",
      "'''\n",
      "\n",
      "# представление списков\n",
      "transpose_matrix_advanced = lambda x: [[x[j][i] for j in range(len(x))] for i in range(len(x[0]))]\n",
      "\n",
      "print(transpose_matrix_advanced(x))\n",
      "JavaScript\n",
      "function transposeMatrix(x) {\n",
      " const result = [\n",
      "   [0, 0, 0],\n",
      "   [0, 0, 0]\n",
      " ]\n",
      "\n",
      " for (const i in x) {\n",
      "   for (const j in x[0]) {\n",
      "     result[j][i] = x[i][j]\n",
      "   }\n",
      " }\n",
      "\n",
      " return result\n",
      "}\n",
      "\n",
      "const x = [\n",
      " [1, 2],\n",
      " [4, 5],\n",
      " [7, 8]\n",
      "]\n",
      "\n",
      "console.log(transposeMatrix(x))\n",
      "/*\n",
      "[\n",
      " [1, 4, 7],\n",
      " [2, 5, 8]\n",
      "]\n",
      "*/\n",
      "\n",
      "// стрелочная функция\n",
      "const transposeMatrixAdvanced = (x) =>\n",
      " x[0].map((_, i) => x.map((_, j) => x[j][i]))\n",
      "\n",
      "console.log(transposeMatrixAdvanced(x))\n",
      "28. Функция для определения того, является ли строка палиндромом\n",
      "Палиндром — это строка (слово, число и т.д.), которая читается одинаково в обоих направлениях. Примеры: \"borrow or rob\", \"а роза упала на лапу Азора\".\n",
      "Python\n",
      "def is_palindrome(str):\n",
      " # ''.join() - объединение в строку\n",
      " # str.split(' ') - преобразование строки в массив по разделителю (в данном случае - пробелу)\n",
      " # ''.join(str.split(' ')) - удаление пробелов из строки\n",
      " # casefold() - нечувствительность к регистру, приведение к нижнему регистру\n",
      " str = ''.join(str.split(' ')).casefold()\n",
      " # инверсия строки - изменение порядка следования букв на противоположный\n",
      " rev = reversed(str)\n",
      " # сравнение списков\n",
      " return list(str) == list(rev)\n",
      "\n",
      "print(is_palindrome('Borrow or rob')) # True\n",
      "JavaScript\n",
      "function isPalindrome(str) {\n",
      " // `replace()` - удаление пробелов с помощью регулярного выражения\n",
      " // `toLowerCase()` - приведение к нижнему регистру\n",
      " str = str.replace(/\\s/g, '').toLowerCase()\n",
      " // `split()` - преобразование строки в массив\n",
      " // `reverse()` - инверсия\n",
      " // `join()` - объединение в строку\n",
      " const rev = str.split('').reverse().join('')\n",
      " return str === rev\n",
      "}\n",
      "\n",
      "console.log(isPalindrome('А роза упала на лапу Азора')) // true\n",
      "29. Функция для удаления лишних символов из строки\n",
      "Python\n",
      "# лишние символы\n",
      "symbols = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
      "\n",
      "def clear(str):\n",
      " # переменная для строки, очищенной от лишних символов\n",
      " cleared = ''\n",
      "\n",
      " # перебираем строку по буквам\n",
      " for char in str:\n",
      "   # если буква не является одним из лишних символов\n",
      "   if char not in symbols:\n",
      "     # добавляем ее к очищенной строке\n",
      "     cleared += char\n",
      "\n",
      " return cleared\n",
      "\n",
      "print(\n",
      " clear('Привет!! - сказал он --- и вошел')\n",
      ") # Привет  сказал он  и вошел\n",
      "JavaScript\n",
      "const symbols = `!()-[]{};:'\"\\,<>./?@#$%^&*_~`\n",
      "\n",
      "function clear(str) {\n",
      " let cleared = ''\n",
      "\n",
      " for (const char of str) {\n",
      "   if (!symbols.includes(char)) cleared += char\n",
      " }\n",
      "\n",
      " return cleared\n",
      "}\n",
      "\n",
      "console.log(\n",
      " clear('Привет!! - сказал он --- и вошел')\n",
      ") // Привет  сказал он  и вошел\n",
      "30. Функция для сортировки слов в алфавитном порядке\n",
      "Python\n",
      "def sort_words(str):\n",
      " # разбиваем строку на массив слов и\n",
      " # приводим каждое слово к нижнему регистру\n",
      " words = [word.lower() for word in str.split()]\n",
      " # сортируем слова\n",
      " words.sort()\n",
      " return words\n",
      "\n",
      "print(sort_words('Остро нуждающаяся в сортировке строка'))\n",
      "'''\n",
      "[\n",
      " 'в',\n",
      " 'нуждающаяся',\n",
      " 'остро',\n",
      " 'сортировке',\n",
      " 'строка'\n",
      "]\n",
      "'''\n",
      "JavaScript\n",
      "function sortWords(str) {\n",
      " // разбиваем строку на массив слов по пробелу и\n",
      " // приводим каждое слово к нижнему регистру\n",
      " const words = str.split(' ').map((word) => word.toLowerCase())\n",
      " words.sort()\n",
      " return words\n",
      "}\n",
      "\n",
      "console.log(sortWords('Остро нуждающаяся в сортировке строк'))\n",
      "/*\n",
      "[\n",
      " 'в',\n",
      " 'нуждающаяся',\n",
      " 'остро',\n",
      " 'сортировке',\n",
      " 'строка'\n",
      "]\n",
      "*/\n",
      "30. Функция для определения количества гласных в строке\n",
      "Python\n",
      "# все гласные латиницы\n",
      "vowels = 'aeiou'\n",
      "\n",
      "def get_vowels_count(str):\n",
      " str = str.casefold()\n",
      "\n",
      " # `fromkeys(iterable, initialValue)`\n",
      " count = {}.fromkeys(vowels, 0)\n",
      "\n",
      " for char in str:\n",
      "   # если буква гласная\n",
      "   if char in count:\n",
      "     # увеличиваем значение соответствующего ключа словаря на 1\n",
      "     count[char] += 1\n",
      "\n",
      " return count\n",
      "\n",
      "str = 'Hello! How are you today?'\n",
      "print(\n",
      " get_vowels_count(str)\n",
      ") # { 'a': 2, 'e': 2, 'i': 0, 'o': 4, 'u': 1 }\n",
      "JavaScript\n",
      "const vowels = 'aeiou'\n",
      "\n",
      "function getVowelsCount(str) {\n",
      " str = str.toLowerCase()\n",
      "\n",
      " // `[...vowels]` - преобразуем строку в массив с помощью spread-оператора\n",
      " // `reduce(function, initialValue)` - аккумулирует результат на основе значений массива\n",
      " const count = [...vowels].reduce((a, c) => {\n",
      "   a[c] = 0\n",
      "   return a\n",
      " }, {})\n",
      "\n",
      " for (const char of str) {\n",
      "   if (vowels.includes(char)) {\n",
      "     count[char] += 1\n",
      "   }\n",
      " }\n",
      "\n",
      " return count\n",
      "}\n",
      "\n",
      "const str = 'Hello! How are you today?'\n",
      "console.log(\n",
      " getVowelsCount(str)\n",
      ") // { 'a': 2, 'e': 2, 'i': 0, 'o': 4, 'u': 1 }\n",
      "31. Функция для рисования пирамиды\n",
      "Обратите внимание: эта задача (точнее, 3 задачи) относятся к категории продвинутых. Это последняя относительно сложная задача, дальше будет намного проще.\n",
      "Половина пирамиды\n",
      "Python\n",
      "def draw_half_pyramid(rows):\n",
      " # для каждой строки\n",
      " for i in range(rows):\n",
      "   # рисуем `*` в количестве,\n",
      "   # соответствующем порядковому номеру строки\n",
      "   for j in range(i + 1):\n",
      "     # `stdout` остается открытым\n",
      "     print('* ', end = '')\n",
      "   # переход на новую строку\n",
      "   if (i + 1) < rows: print('\\n')\n",
      "\n",
      "draw_half_pyramid(5)\n",
      "'''\n",
      "*\n",
      "\n",
      "* *\n",
      "\n",
      "* * *\n",
      "\n",
      "* * * *\n",
      "\n",
      "* * * * *\n",
      "'''\n",
      "JavaScript\n",
      "function drawHalfPyramid(rows) {\n",
      " // строка со звездочками\n",
      " let pyramid = ''\n",
      " // для каждой строки\n",
      " for (let i = 1; i <= rows; i++) {\n",
      "   // рисуем `*` в количестве...\n",
      "   for (let j = 0; j < i; j++) {\n",
      "     pyramid += '* '\n",
      "   }\n",
      "   // переход на новую строку\n",
      "   pyramid += '\\n'\n",
      " }\n",
      "\n",
      " return pyramid\n",
      "}\n",
      "\n",
      "console.log(drawHalfPyramid(5))\n",
      "/*\n",
      "*\n",
      "\n",
      "* *\n",
      "\n",
      "* * *\n",
      "\n",
      "* * * *\n",
      "\n",
      "* * * * *\n",
      "*/\n",
      "Перевернутая половина пирамиды\n",
      "Python\n",
      "def draw_inverted_half_pyramid(rows):\n",
      " # движемся в обратную сторону\n",
      " for i in range(rows, 0, -1):\n",
      "   for j in range(0, i):\n",
      "     print('* ', end = '')\n",
      "   if i > 1: print('\\n')\n",
      "\n",
      "draw_inverted_half_pyramid(5)\n",
      "'''\n",
      "* * * * *\n",
      "\n",
      "* * * *\n",
      "\n",
      "* * *\n",
      "\n",
      "* *\n",
      "\n",
      "*\n",
      "'''\n",
      "JavaScript\n",
      "function drawInvertedHalfPyramid(rows) {\n",
      " let pyramid = ''\n",
      " for (let i = rows; i > 0; i--) {\n",
      "   for (let j = 0; j < i; j++) {\n",
      "     pyramid += '* '\n",
      "   }\n",
      "   pyramid += '\\n'\n",
      " }\n",
      " return pyramid\n",
      "}\n",
      "\n",
      "console.log(drawInvertedHalfPyramid(5))\n",
      "/*\n",
      "* * * * *\n",
      "\n",
      "* * * *\n",
      "\n",
      "* * *\n",
      "\n",
      "* *\n",
      "\n",
      "*\n",
      "*/\n",
      "Полная пирамида\n",
      "Python\n",
      "def draw_full_pyramid(rows):\n",
      " k = 0\n",
      " p = ''\n",
      " for i in range(1, rows + 1):\n",
      "   for j in range(1, (rows - i) + 1):\n",
      "     p += '  '\n",
      "   while k != (2 * i - 1):\n",
      "     p += '# '\n",
      "     k += 1\n",
      "   if i < rows: p += '\\n'\n",
      "   k = 0\n",
      " return p\n",
      "\n",
      "print(draw_full_pyramid(5))\n",
      "'''\n",
      "       #\n",
      "     # # #\n",
      "   # # # # #\n",
      " # # # # # # #\n",
      "# # # # # # # # #\n",
      "'''\n",
      "JavaScript\n",
      "На JS я решу эту задачу не таким лаконичным, но более понятным способом:\n",
      "function drawFullPyramid(rows) {\n",
      " let levels = ''\n",
      "\n",
      " const mid = ~~((2 * rows - 1) / 2)\n",
      "\n",
      " for (let row = 0; row < rows; row++) {\n",
      "   let level = ''\n",
      "\n",
      "   for (let col = 0; col < 2 * rows - 1; col++) {\n",
      "     // в данном случае мы сразу формируем тот или иной уровень\n",
      "     // в зависимости от положения колонки -\n",
      "     // до или после середины\n",
      "     level += mid - row <= col && col <= mid + row ? '#' : ' '\n",
      "   }\n",
      "\n",
      "   levels += level + '\\n'\n",
      " }\n",
      "\n",
      " return levels\n",
      "}\n",
      "\n",
      "console.log(drawFullPyramid(5))\n",
      "/*\n",
      "       #\n",
      "     # # #\n",
      "   # # # # #\n",
      " # # # # # # #\n",
      "# # # # # # # # #\n",
      "*/\n",
      "32. Объединить два словаря (объекта) в один\n",
      "Python\n",
      "dict_1 = { 'a': 1, 'b': 2 }\n",
      "dict_2 = { 'b': 3, 'c': 4 }\n",
      "\n",
      "# совпадающие ключи перезаписываются\n",
      "print(dict_1 | dict_2) # { 'a': 1, 'b': 3, 'c': 4 }\n",
      "\n",
      "# `**` - распаковка словарей\n",
      "print({ **dict_1, **dict_2 })\n",
      "JavaScript\n",
      "const obj1 = { a: 1, b: 2 }\n",
      "const obj2 = { b: 3, c: 4 }\n",
      "\n",
      "// совпадающие ключи перезаписываются\n",
      "console.log(\n",
      " Object.assign(obj1, obj2)\n",
      ") // { a: 1, b: 3, c: 4 }\n",
      "\n",
      "// `...` - распаковка объектов\n",
      "console.log({ ...obj1, ...obj2 })\n",
      "33. Преобразовать вложенный список (массив) в одноуровневый (плоский)\n",
      "Python\n",
      "my_list = [[1], [2, 3], [4, 5, 6]]\n",
      "\n",
      "# представление списков\n",
      "flat_list_1 = [n for sub in my_list for n in sub]\n",
      "print(flat_list_1) # [1, 2, 3, 4, 5, 6]\n",
      "\n",
      "# встроенная функция `sum()`\n",
      "flat_list_3 = sum(my_list, [])\n",
      "print(flat_list_3)\n",
      "\n",
      "# модуль `itertools`\n",
      "import itertools\n",
      "\n",
      "flat_list_2 = list(itertools.chain(*my_list))\n",
      "print(flat_list_2)\n",
      "JavaScript\n",
      "const myList = [[1], [2, 3], [4, 5, 6]]\n",
      "\n",
      "// встроенный метод `flat()`\n",
      "const flatList1 = myList.flat()\n",
      "console.log(flatList1) // [1, 2, 3, 4, 5, 6]\n",
      "\n",
      "// встроенный метод `reduce()`\n",
      "const flatList2 = myList.reduce((a, c) => {\n",
      " c.forEach((i) => a.push(i))\n",
      " return a\n",
      "}, [])\n",
      "\n",
      "console.log(flatList2)\n",
      "\n",
      "// двойная итерация\n",
      "// ключевое слово `const` делает иммутабельным\n",
      "// значение самой переменной, а не массива\n",
      "const flatList3 = []\n",
      "for (const i of myList) {\n",
      " for (const j of i) {\n",
      "   flatList3.push(j)\n",
      " }\n",
      "}\n",
      "console.log(flatList3)\n",
      "34. Копировать список (массив) (сделать его срез)\n",
      "Python\n",
      "# список (list)\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "\n",
      "# сигнатура\n",
      "# `[start:end:step]`\n",
      "\n",
      "# полная копия\n",
      "print(my_list[:]) # [1, 2, 3, 4, 5]\n",
      "\n",
      "# от второго до пятого элемента\n",
      "print(my_list[1:5]) # [2, 3, 4]\n",
      "\n",
      "# через 1 элемент, начиная с первого\n",
      "print(my_list[::2]) # [1, 3, 5]\n",
      "\n",
      "# извлекаем последний элемент\n",
      "# это не относится к копированию\n",
      "print(my_list[-1]) # 5\n",
      "JavaScript\n",
      "// массив (array)\n",
      "const myList = [1, 2, 3, 4, 5]\n",
      "\n",
      "// сигнатура\n",
      "// `slice(start, end)`\n",
      "// данный метод не такой мощный как питоновский аналог\n",
      "\n",
      "// полная копия\n",
      "console.log(myList.slice()) // [1, 2, 3, 4, 5]\n",
      "\n",
      "// от второго до пятого\n",
      "console.log(myList.slice(1, 5)) // [2, 3, 4]\n",
      "\n",
      "// последний элемент - хак\n",
      "console.log(...myList.slice(-1)) // 5\n",
      "\n",
      "// последний элемент - не хак\n",
      "console.log(myList[myList.length - 1])\n",
      "\n",
      "// последний элемент - скоро, в `Chrome` уже работает\n",
      "console.log(myList.at(-1)) // 5\n",
      "35. Перебрать ключи и значения словаря (объекта)\n",
      "Python\n",
      "# словарь (dictionary)\n",
      "dict = { 1: 'a', 2: 'b', 3: 'c' }\n",
      "\n",
      "# ключи и значения\n",
      "for key in dict:\n",
      " print(key, dict[key])\n",
      "'''\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "'''\n",
      "\n",
      "# еще ключи и значения\n",
      "for key, val in dict.items():\n",
      " print(key, val)\n",
      "# или\n",
      "for key, val in dict.iteritems():\n",
      " print(key, val)\n",
      "\n",
      "# ключи\n",
      "for key in dict.keys():\n",
      " print(key)\n",
      "\n",
      "# значения\n",
      "for val in dict.values():\n",
      " print(val)\n",
      "JavaScript\n",
      "// объект (object)\n",
      "const obj = { 1: 'a', 2: 'b', 3: 'c' }\n",
      "\n",
      "// ключи и значения\n",
      "for (const key in obj) {\n",
      " console.log(key, obj[key])\n",
      "}\n",
      "/*\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "*/\n",
      "\n",
      "// еще ключи и значение\n",
      "Object.entries(obj).forEach(([key, val]) => {\n",
      " console.log(key, val)\n",
      "})\n",
      "// или\n",
      "for (const [key, val] of Object.entries(obj)) {\n",
      " console.log(key, val)\n",
      "}\n",
      "\n",
      "// ключи\n",
      "Object.keys(obj).forEach((key) => {\n",
      " console.log(key)\n",
      "})\n",
      "\n",
      "// значения\n",
      "for (const val of Object.values(obj)) {\n",
      " console.log(val)\n",
      "}\n",
      "36. Отсортировать словарь (объект) по ключам и значениям\n",
      "Python\n",
      "dict = { 1: 'b', 3: 'c', 2: 'a'}\n",
      "\n",
      "# сортировка по значениям\n",
      "sorted_dict_1 = {\n",
      " key: val \\\n",
      " for key, val \\\n",
      " # key - критерий сортировки (в данном случае - значение)\n",
      " in sorted(dict.items(), key = lambda i: i[1])\n",
      "}\n",
      "print(sorted_dict_1) # { 2: 'a', 1: 'b', 3: 'c' }\n",
      "\n",
      "# сортировка по ключам\n",
      "sorted_dict_2 = { key: val for key, val in sorted(dict.items(), key = lambda i: i[0]) }\n",
      "print(sorted_dict_2) # { 1: 'b', 2: 'a', 3: 'c' }\n",
      "JavaScript\n",
      "// объект для примера пришлось изменить\n",
      "// поскольку встроенный метод `Object.fromEntries()`\n",
      "// выполняет автоматическую сортировку по ключам\n",
      "// при создании объекта из вложенного массива\n",
      "const obj = { a: '2', c: '1', b: '3' }\n",
      "\n",
      "// логику сортировки лучше вынести в отдельную функцию\n",
      "// по умолчанию функция выполняет сортировку по ключам\n",
      "// благодаря параметру `i` со значением `1` по умолчанию\n",
      "const sortObj = (obj, i = 0) =>\n",
      " Object.fromEntries(\n",
      "   // `sort()` принимает функцию для сортировки\n",
      "   // `localeCompare()` сравнивает строки с учетом локали\n",
      "   Object.entries(obj).sort((a, b) => a[i].localeCompare(b[i]))\n",
      " )\n",
      "\n",
      "// по ключам\n",
      "const sortedObj1 = sortObj(obj)\n",
      "console.log(sortedObj1) // { a: '2', b: '3', c: '1' }\n",
      "\n",
      "// по значениям\n",
      "const sortedObj2 = sortObj(obj, 1)\n",
      "console.log(sortedObj2) // { c: '1', a: '2', b: '3' }\n",
      "37. Определить, является ли список (массив) пустым\n",
      "Python\n",
      "my_list = []\n",
      "\n",
      "# `not`\n",
      "if not my_list:\n",
      " print('Empty')\n",
      "\n",
      "# + `len()`\n",
      "if not len(my_list):\n",
      " print('Empty')\n",
      "\n",
      "# в `JS` так не получится\n",
      "if my_list == []:\n",
      " print('Empty')\n",
      "JavaScript\n",
      "const myList = []\n",
      "\n",
      "// ! - [] -> 0 -> false + ! - false -> true\n",
      "if (!!myList) {\n",
      " console.log('Empty')\n",
      "}\n",
      "\n",
      "// ! - 0 -> false -> true\n",
      "if (!myList.length) {\n",
      " console.log('Empty')\n",
      "}\n",
      "\n",
      "// самый очевидный и надежный способ\n",
      "// или `myList.length === 0`\n",
      "if (myList.length < 1) {\n",
      " console.log('Empty')\n",
      "}\n",
      "38. Объединить два списка (массива) в один\n",
      "Python\n",
      "list_1 = ['a', 'b']\n",
      "list_2 = list(range(3, 5))\n",
      "list_2.append('a')\n",
      "\n",
      "# в `Python` списки можно конкатенировать путем сложения\n",
      "joined_1 = list_1 + list_2\n",
      "print(joined_1) # ['a', 'b', 3, 4, 'a']\n",
      "\n",
      "# распаковка\n",
      "joined_2 = [*list_1, *list_2]\n",
      "print(joined_2)\n",
      "\n",
      "# исключение дубликатов\n",
      "joined_uniq = list(set(joined_1))\n",
      "print(joined_uniq) # [3, 'b', 'a', 4]\n",
      "\n",
      "# модификация списка посредством его расширения\n",
      "list_2.extend(list_1)\n",
      "print(list_2) # [3, 4, 'a', 'a', 'b']\n",
      "JavaScript\n",
      "const list1 = ['a', 'b']\n",
      "// еще один встроенный метод для создания массивов\n",
      "const list2 = Array.of(3, 4)\n",
      "list2.push('a')\n",
      "\n",
      "// распаковка\n",
      "const joined1 = [...list1, ...list2]\n",
      "console.log(joined1) // ['a', 'b', 3, 4, 'a']\n",
      "\n",
      "// объединение\n",
      "const joined2 = list2.concat(list1)\n",
      "console.log(joined2) // [3, 4, 'a', 'a', 'b']\n",
      "\n",
      "// исключение дубликатов\n",
      "// набор (set) сразу преобразуется в обычный массив\n",
      "const joinedUniq = [...new Set(joined1)]\n",
      "console.log(joinedUniq) // ['a', 'b', 3, 4]\n",
      "\n",
      "// модификация массива путем его расширения\n",
      "list2.splice(-1, 0, ...list1)\n",
      "console.log(list2) // [3, 4, 'a', 'b', 'a']\n",
      "39. Извлечь подстроку из строки\n",
      "Python\n",
      "my_str = 'I like Python'\n",
      "\n",
      "# по аналогии со списком\n",
      "substr = my_str[2:6]\n",
      "print(substr) # like\n",
      "JavaScript\n",
      "const myStr = 'I like JavaScript'\n",
      "\n",
      "// по аналогии с массивом\n",
      "const subStr1 = myStr.slice(2, 6)\n",
      "console.log(subStr1) // like\n",
      "\n",
      "// специальные встроенные методы\n",
      "// deprecated\n",
      "const subStr2 = myStr.substr(2, 5)\n",
      "console.log(subStr2) // like\n",
      "\n",
      "const subStr3 = myStr.substring(2, 6)\n",
      "console.log(subStr3) // like\n",
      "// разница между методами `substr()` и `substring()`\n",
      "// состоит в том, включается ли последний элемент в подстроку\n",
      "40. Функция для получения случайного элемента массива\n",
      "Python\n",
      "import random\n",
      "# import secrets\n",
      "\n",
      "def get_random_item(list):\n",
      " if not list:\n",
      "   return 'Empty'\n",
      " else:\n",
      "   return random.choice(list) # secrets.choice(list)\n",
      "\n",
      "print(get_random_item(['a', 3, 'c', 1, 'b'])) # 3\n",
      "JavaScript\n",
      "// функция для получения случайного целого числа\n",
      "const getRandomInt = (min, max) => ~~(min + Math.random() * (max - min + 1))\n",
      "// функция для получения случайного элемента\n",
      "const getRandomItem = (arr) => arr[getRandomInt(0, arr.length - 1)]\n",
      "\n",
      "console.log(getRandomItem(['a', 3, 'c', 1, 'b'])) // 'b'\n",
      "41. Определить количество вхождений элемента в массиве\n",
      "Python\n",
      "my_list = ['a', 1, 1, 'a', 2, 'b', 'a']\n",
      "\n",
      "# `count()`\n",
      "a_count = my_list.count('a')\n",
      "print(a_count) # 3\n",
      "\n",
      "# это также работает для строк\n",
      "my_str = 'Hello world'\n",
      "l_count = my_str.count('l')\n",
      "print(l_count) # 3\n",
      "JavaScript\n",
      "const myList = ['a', 1, 1, 'a', 2, 'b', 'a']\n",
      "const myStr = 'Hello world'\n",
      "\n",
      "// фильтрация и длина массива\n",
      "const a_count = myList.filter((i) => i === 'a').length\n",
      "console.log(a_count) // 3\n",
      "\n",
      "// преобразуем строку в массив\n",
      "const l_count = [...myStr].filter((i) => i === 'l').length\n",
      "console.log(l_count) // 3\n",
      "42. Объединить два списка (массива) в словарь (объект)\n",
      "Python\n",
      "indexes = [0, 1, 2]\n",
      "languages = ['JavaScript', 'Python', 'PHP']\n",
      "\n",
      "# `zip()` & `dict()`\n",
      "my_dict_1 = dict(zip(indexes, languages))\n",
      "print(my_dict_1) # { 0: 'JavaScript', 1: 'Python', 2: 'PHP' }\n",
      "\n",
      "# `zip()` и представление списков\n",
      "my_dict_2 = { k: v for k, v in zip(indexes, languages) }\n",
      "print(my_dict_2)\n",
      "JavaScript\n",
      "const indexes = [0, 1, 2]\n",
      "const languages = ['JavaScript', 'Python', 'PHP']\n",
      "\n",
      "// цикл\n",
      "const obj1 = {}\n",
      "// нам нужен не только элемент, но и его индекс\n",
      "// потому что элементы первого массива не всегда будут совпадать с индексами\n",
      "for (const i in indexes) {\n",
      " obj1[indexes[i]] = languages[i]\n",
      "}\n",
      "console.log(obj1) // { 0: 'JavaScript', 1: 'Python', 2: 'PHP' }\n",
      "\n",
      "// `reduce()`\n",
      "const obj2 = indexes.reduce((a, c, i) => {\n",
      " // `c` - текущий элемент\n",
      " // `i` - его индекс\n",
      " a[c] = languages[i]\n",
      " return a\n",
      "}, {})\n",
      "console.log(obj2)\n",
      "43. Удалить лишние пробелы из строки с помощью регулярного выражения\n",
      "Python\n",
      "import re\n",
      "\n",
      "str_with_spaces = 'some    string  with  many  spaces   '\n",
      "\n",
      "# заменяем 2 и более пробела на один\n",
      "# `sub(regexp, replacement, string)`\n",
      "str_without_spaces = re.sub(r'\\s\\s+', ' ', str_with_spaces)\n",
      "\n",
      "print(str_without_spaces) # some string with many spaces\n",
      "JavaScript\n",
      "const strWithSpaces = 'some    string  with  many  spaces   '\n",
      "\n",
      "// заменяем 2 и более пробела на один\n",
      "// `replace(substring | regexp, replacement)`\n",
      "const strWithoutSpaces = strWithSpaces.replace(/\\s\\s+/g, ' ')\n",
      "\n",
      "console.log(strWithoutSpaces) // some string with many spaces\n",
      "44. Создать перечисления (enum)\n",
      "Python\n",
      "from enum import Enum\n",
      "\n",
      "class Color(Enum):\n",
      " RED = 1\n",
      " GREEN = 2\n",
      " BLUE = 1\n",
      "\n",
      "print(Color.RED) # Color.RED\n",
      "\n",
      "print(Color.GREEN.name) # GREEN\n",
      "\n",
      "print(Color.BLUE.value) # 3\n",
      "В JS не существует такого типа данных, как перечисление, но его можно сымитировать, \"заморозив\" объект:\n",
      "JavaScript\n",
      "const Color = Object.freeze({\n",
      " RED: 1,\n",
      " GREEN: 2,\n",
      " BLUE: 3\n",
      "})\n",
      "\n",
      "// value\n",
      "console.log(Color.RED) // 1\n",
      "45. Функция для определения анаграмм\n",
      "Два слова являются анаграммами, если состоят из одинаковых букв. Например, анаграммами являются слова \"binary\" и \"brainy\", или \"раздвоение\" и \"дозревание\".\n",
      "Python\n",
      "def is_anagrams(x, y):\n",
      " # приводим слова к нижнему регистру\n",
      " x = x.lower()\n",
      " y = y.lower()\n",
      " if (len(x) == len(y)):\n",
      "   # и сортируем их\n",
      "   sorted_1 = sorted(x)\n",
      "   sorted_2 = sorted(y)\n",
      "   if (sorted_1 == sorted_2):\n",
      "     return True\n",
      "   else:\n",
      "     return False\n",
      " else:\n",
      "   return False\n",
      "\n",
      "word_1 = 'Binary'\n",
      "word_2 = 'brainy'\n",
      "\n",
      "print(is_anagrams(word_1, word_2)) # True\n",
      "JavaScript\n",
      "function isAnagrams(x, y) {\n",
      " x = x.toLowerCase()\n",
      " y = y.toLowerCase()\n",
      " if (x.length === y.length) {\n",
      "   sorted_x = x.split('').sort().join('')\n",
      "   sorted_y = y.split('').sort().join('')\n",
      "   if (sorted_x === sorted_y) {\n",
      "     return true\n",
      "   }\n",
      "   return false\n",
      " }\n",
      " return false\n",
      "}\n",
      "\n",
      "const word1 = 'Раздвоение'\n",
      "const word2 = 'дозревание'\n",
      "console.log(isAnagrams(word1, word2)) // true\n",
      "Обратите внимание: при сравнении строк, состоящих из нескольких слов, необходимо также удалять из них все пробелы.\n",
      "46. \"Капитализировать\" строку\n",
      "Классика.\n",
      "Python\n",
      "my_str = 'hello'\n",
      "\n",
      "# `upper()` + slice\n",
      "cap_str_1 = my_str[0].upper() + my_str[1:]\n",
      "print(cap_str_1) # Hello\n",
      "\n",
      "# `capitilize()`\n",
      "cap_str_2 = my_str.capitalize()\n",
      "В JS отсутствует встроенный метод для капитализации строки.\n",
      "JavaScript\n",
      "const str = 'hello'\n",
      "\n",
      "const capStr1 = str[0].toUpperCase() + str.slice(1)\n",
      "console.log(capStr1) // Hello\n",
      "\n",
      "// показать всем, что ты крут ;)\n",
      "// деструктуризация и шаблонные литералы\n",
      "const capitilize = ([first, ...rest]) =>\n",
      " `${first.toUpperCase()}${rest.join('')}`\n",
      "\n",
      "const capStr2 = capitilize(str)\n",
      "console.log(capStr2)\n",
      "47. Функция для определения всех вариантов строки\n",
      "Python\n",
      "# рекурсивно\n",
      "def get_permutations(s, p = [], i = 0):\n",
      " if i == len(s):\n",
      "   p.append(''.join(s))\n",
      " for j in range(i, len(s)):\n",
      "   words = [c for c in s]\n",
      "   words[i], words[j] = words[j], words[i]\n",
      "   get_permutations(words, p, i + 1)\n",
      " return p\n",
      "\n",
      "print(get_permutations('qux'))\n",
      "# ['qux', 'qxu', 'uqx', 'uxq', 'xuq', 'xqu']\n",
      "\n",
      "# `itertools`\n",
      "from itertools import permutations\n",
      "\n",
      "per = [''.join(p) for p in permutations('baz')]\n",
      "\n",
      "print(per) # ['baz', 'bza', 'abz', 'azb', 'zba', 'zab']\n",
      "JavaScript\n",
      "function getPermutations(str) {\n",
      " if (str.length < 2) {\n",
      "   return str.length === 2 ? [str, str[1] + str[0]] : [str]\n",
      " }\n",
      "\n",
      " return str\n",
      "   .split('')\n",
      "   .reduce(\n",
      "     (a, c, i) =>\n",
      "       a.concat(\n",
      "         getPermutations(str.slice(0, i) + str.slice(i + 1)).map((v) => c + v)\n",
      "       ),\n",
      "     []\n",
      "   )\n",
      "}\n",
      "\n",
      "console.log(getPermutations('qux'))\n",
      "// ['qux', 'qxu', 'uqx', 'uxq', 'xqu', 'xuq']\n",
      "48. Создание счетчика (таймера)\n",
      "Python\n",
      "import time\n",
      "\n",
      "def timer(time_in_sec):\n",
      " while time_in_sec:\n",
      "   m, s = divmod(time_in_sec, 60)\n",
      "   formated = f'{m:02d}:{s:02d}'\n",
      "   print(formated, end = '\\r')\n",
      "   time.sleep(1)\n",
      "   time_in_sec -= 1\n",
      "\n",
      "timer(5)\n",
      "JavaScript\n",
      "function timer(timeInSec) {\n",
      " const timerId = setInterval(() => {\n",
      "   let m = ~~(timeInSec / 60)\n",
      "   let s = timeInSec - m * 60\n",
      "   if (m < 10) m = '0' + m\n",
      "   if (s < 10) s = '0' + s\n",
      "   console.log(`${m}:${s}`)\n",
      "   timeInSec -= 1\n",
      "   if (timeInSec < 0) {\n",
      "     clearInterval(timerId)\n",
      "   }\n",
      " }, 1000)\n",
      "}\n",
      "\n",
      "timer(5)\n",
      "49. Создание вложенной директории\n",
      "Пусть 2 последних задачки будут посвящены бэку.\n",
      "Python\n",
      "import os\n",
      "\n",
      "# абсолютный путь к текущей директории\n",
      "__dirname__ = os.path.abspath(os.getcwd())\n",
      "\n",
      "# небезопасно: при наличии создаваемой директории будет выброшено исключение\n",
      "try:\n",
      " os.makedirs(f'{__dirname__}/parent/child/nested')\n",
      "except FileExistsError:\n",
      " print('Exists')\n",
      "\n",
      "import distutils.dir_util\n",
      "\n",
      "# безопасно\n",
      "distutils.dir_util.mkpath(f'{__dirname__}/some/dir')\n",
      "JavaScript\n",
      "import { dirname } from 'path'\n",
      "import { fileURLToPath } from 'url'\n",
      "import { promises as fs } from 'fs'\n",
      "\n",
      "// абсолютный путь к текущей директории\n",
      "const __dirname = dirname(fileURLToPath(import.meta.url))\n",
      "// безопасно\n",
      "// для сравнения обработки исключений\n",
      "try {\n",
      " await fs.mkdir(`${__dirname}/parent/child/nested`, { recursive: true })\n",
      "} catch (e) {\n",
      " console.error(e.message || e)\n",
      "}\n",
      "Обратите внимание: приведенный код должен запускаться с помощью команды node filename. При этом файл должен быть модулем, т.е. иметь расширение .mjs или же в ближайшем package.json должно содержаться поле type со значением module.\n",
      "50. Получение названий файлов с расширением \".txt\"\n",
      "Предположим, что у нас имеется директория files с тремя файлами в формате TXT. Как нам получить названия этих файлов?\n",
      "Python\n",
      "import glob, os\n",
      "\n",
      "__dirname__ = os.path.abspath(os.getcwd()) + '/files'\n",
      "\n",
      "def get_filenames(ext):\n",
      " filenames = []\n",
      " # читаем содержимое директории `files`\n",
      " os.chdir(__dirname__)\n",
      "\n",
      " # если файл имеет указанное расширение\n",
      " for filename in glob.glob(f'*.{ext}'):\n",
      "   # добавляем его в список\n",
      "   filenames.append(filename)\n",
      "\n",
      " # возвращаем список\n",
      " return filenames\n",
      "\n",
      "print(get_filenames('txt'))\n",
      "# ['baz.txt', 'foo.txt', 'bar.txt']\n",
      "JavaScript\n",
      "import { dirname } from 'path'\n",
      "import { fileURLToPath } from 'url'\n",
      "import { promises as fs } from 'fs'\n",
      "\n",
      "const __dirname = dirname(fileURLToPath(import.meta.url))\n",
      "\n",
      "async function getFilenames(ext) {\n",
      " const result = []\n",
      " // читаем содержимое директории `files`\n",
      " const filenames = await fs.readdir(`${__dirname}/files`)\n",
      "\n",
      " for (const filename of filenames) {\n",
      "   // если файл имеет указанное расширение\n",
      "   if (filename.includes(`.${ext}`)) {\n",
      "     // добавляем его в массив\n",
      "     result.push(filename)\n",
      "   }\n",
      " }\n",
      "\n",
      " return result\n",
      "}\n",
      "\n",
      "// функция возвращает промис\n",
      "getFilenames('txt').then(console.log)\n",
      "// [ 'bar.txt', 'baz.txt', 'foo.txt' ]\n",
      "Фух… это было утомительно, но, вместе с тем, невероятно увлекательно, не правда ли?\n",
      "Как видите, синтаксические конструкции и основной функционал JavaScript и Python являются очень похожими, а во многом даже идентичными (без учета некоторых незначительных отличий), что при необходимости позволяет с относительной легкостью перейти с одного языка на другой.\n",
      "Вопрос в том, имеется ли необходимость в такой \"миграции\"?\n",
      "Обратите внимание: дальше я выскажу собственное мнение с позиции веб-разработчика.\n",
      "Существенное преимущество JavaScript заключается в том, что он не нуждается в интерпретаторе, т.е. код на JS выполняется браузером напрямую. Это очень сильно облегчает разработку веб-приложений.\n",
      "Если мы говорим о REST API, то реализовать его на Express, например, намного проще, чем на Django или FastAPI, хотя последний синтаксически схож с сочетанием Express и TypeORM или Prisma (во многом благодаря широкому использованию декораторов).\n",
      "С другой стороны, если мы говорим о реализации, скажем так, продвинутого сервера, то благодаря огромному количеству, в том числе, встроенных модулей, Python имеет некоторые преимущества перед Node.js.\n",
      "Есть мнение, что у Python имеются некоторые проблемы с асинхронностью — у меня не было возможности в этом убедиться.\n",
      "В свою очередь, в Node.js в последнее время царит некоторая неразбериха за счет одновременного существования двух разных подходов к разработке: одного, основанного на колбеках, который считается устаревшим, и другого, основанного на промисах и async/await, который требует дополнительных усилий по настройке окружения и не в полной мере заменяет первый подход.\n",
      "Благодарю за внимание и хорошего дня!\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем привет! Меня зовут Денис Аникин, я тимлид в команде Chat в Райффайзенбанке. А также представитель внутреннего Python-сообщества, так называемый «community lead» (об этом как-нибудь в другой раз). В этой статье я хотел поговорить про отношение к Python среди разработчиков и обсудить все основные претензии, которые очень давно следуют за языком по пятам.В начале я хочу коротко рассказать про причины этой статьи — она не претендует на статус серьезного исследования, статья, скорее, немного ироничная. И возникла она как ответ на бесконечные камни, летящие в сторону моего любимого языка.Я часто принимал участие в больших дискуссиях, действительно ли Python можно назвать серьезным языком программирования. Именно это, во многом, и стало причиной для написания этой статьи. Здесь я хочу попытаться донести идею, что как минимум для меня Python — действительно серьезный, без всяких скидок, язык программирования. Заодно я хочу воодушевить тех, кто уже пишет на Python, но начал немного стесняться этого, начитавшись интернета.Также я вспоминаю нашу внутреннюю шутку, которая сформулирована моим коллегой, Даниилом: «Денис начинает свой понедельник с очередного доклада на тему, почему Python — серьезный язык». Я долго думал над этой шуткой (извините, я не гений), а потом решил: почему бы и не начать с этого статью? :)Поэтому я хочу поговорить про проблемы Python и обсудить накопившиеся к нему основные претензии.Динамическая типизацияЗа динамическую типизацию Python ругали практически все — это одна из крупнейших претензий к нему в среде разработчиков. Но на самом ли деле динамическая типизация — такое уж зло? Как минимум, разработка на языках динамической типизации проще, быстрее и зачастую приятнее — конечно, это субъективный тезис, но он имеет право на существование.Какой-нибудь JSON-парсер на языке со статической типизацией будет гораздо более многословным и тяжелым в написании, чем на языке с динамической типизацией (ох уж эта элегантная связка orjson и pydantic). Где-то, где мы в Python отобьемся манипуляциями в рантайме, придется подтаскивать большие объёмы кодогенерации.Аннотации — благоПри этом у динамической типизации есть минусы. Например, нам приходится писать больше тестов, у нас может быть больше ошибок в рантайме при некоторых условиях. У нас динамическая типизация, поэтому до запуска понять, что с типами проблема, мы не сможем. И вроде бы статическая типизация здесь выигрывает. При этом Uncle Bob говорил, что тесты на бизнес-логику не отменяются статической типизацией, да и в Python теперь типы можно проверять статически с помощью аннотаций и MyPy. Конечно, мы пишем больше тестов и тщательно проверяем бизнес-логику, но, на мой взгляд, это идет на пользу сервисам. Всё очень неоднозначно.Да, у нас есть gradual типизация. MyPy, typing, аннотации типов — это позволяет нам значительно улучшить качество кода, уберечься от массы ошибок, получить экстрафункциональность языка. Например, в Python нет констант, но с помощью связки typing и MyPy можно получить их аналог. Аннотации в Python можно анализировать как статически, так и динамически: в typing есть интроспекция, появляются проекты вроде Beartype. На базе аннотаций типов построены современные фреймворки, такие как Pydantic (строгие схемы валидации в рантайме) или FastAPI. Я считаю вот так: Python в связке с аннотациями типов являет нам gradual типизацию и тем самым предоставляет здоровый компромисс между преимуществами динамической и статической типизаций. Он не порождает хтоническое чудовище, каким его часто объявляют в интернете. Напротив, нам дают способ писать и быстро и надежно, здесь и сейчас.Где-то тут к нам подкрадывается вопрос о типобезопасности. Полагаю, что аннотации типов проверку на типобезопасность не пройдут, хотя вопрос это довольно сложный, не для моего уровня понимания. На текущий момент ошибок типов с MyPy и аннотациями типов я не встречал, но уверен, что где-то при определенном стечении обстоятельств это возможно. Единственное, что видно у нас — увеличение надежности нашего кода (пока мы пишем наши бэкенды, аннотации часто помогают отлавливать ошибки), умение гибридно использовать аннотации и почти никаких type error в коде.Также стоит вспомнить, что люди, предпочитающие статическую типизацию, часто говорят, что для маленьких проектов динамическая типизация «не страшна», а на действительно больших мы начинаем чувствовать её недостатки.Честно говоря, пункт уже раздулся, поэтому я просто набросаю свои мысли списком:Статический анализ хорош везде;Как мы уже сказали выше, в Python есть статистический анализ, пускай и немного уступающий «настоящему» выводу типов — «настоящей» типобезопасности. Мы используем его в полный рост, и с ним можно писать действительно большие проектыМы живем в эпоху, когда микросервисная архитектура очень популярна, поэтому проблема проектов в сотни тысяч и миллионы строк кода уже не так сильна. И накал претензий к динамической типизации, соответственно, куда ниже. Даже если мы их принимаем.СкоростьЭтот аспект языка часто подвергается критике очень многими разработчиками. И если динамическая типизация — сложная и неоднозначная тема для обсуждения, то скорость — то, о чем говорят вообще все. Если посмотреть на синтетические бенчмарки, то Python часто можно обнаружить в самому низу турнирных таблиц. Многие разработчики считают это Ахиллесовой пятой Python, что не может не сказываться на восприятии языкаКогда обсуждают скорость Python, обычно вспоминают про C++, ведь на нем все бенчмарки выглядят в сто раз лучше, чем на Python. Но если мы начнём разбираться в вопросе, то станет ясно, что скорость Python на данном этапе — не такая уж проблема. Конечно, Python действительно медленный в CPU-bound задачах. И тут у вас в голове возникает мысль — если он такой медленный, то это точно проблема!На мой взгляд, дело обстоит так. В современной web-разработке полно i/o-bound нагрузки: часто мы принимаем запросы по сети, отправляем их в сеть, читаем из БД, пишем в БД, оперируем с файлами и так далее.Одни из действующих «лиц» CPU-bound и IO-bound задачДля этого типа задач у Python есть прекрасный ответ — асинхронный подсет языка, нативный async/await, event loop из коробки и uvloop, для тех, кто хочет ещё быстрее. С помощью этой части Python мы можем эффективно утилизировать ресурсы CPU. А для исключительно CPU-bound в мире Python тоже много «припарок»: multiprocessing, subprocess, Pypy, Cython, Numba и так далее. Поэтому асинхронный Python работает очень даже быстро.Подведу субъективный итог: многое, если не всё, можно смело писать на Python, а CPU-bound, при необходимости (если стандартная библиотека не тянет), переписывать на более быстрых языках и ставить эти микросервисы-воркеры за очередями сообщений. Разве это не идеальное сочетание?Язык для новичковДовольно часто Python считают языком для новичков. Ему учат на каждом шагу, в мире миллион курсов по Python. Топовые блоггеры обещают обучить ему чуть ли не за один вечер, говорят, что все сразу же получают серьезную зарплату, уютный офис и счастливое будущее. На этом слишком радужном фоне за Python прочно закрепился имидж языка для новичков, где достаточно написать пару for и def, объявить несколько переменных — и вот ты уже профессиональный Python-разработчик уровня middle. А потом «перерастаешь» и идешь писать на «серьезном языке».Это все какой-то интернет-морок, потому что Python, как и любой другой язык, требует освоения, изучения, времени.В самом языке у нас целая куча всяких знаний, нюансов, сложностей и декларативных частей. Судите сами:У нас есть протокол итерации. Конечно, можно сказать, что итератор — не совсем часть функционального программирования, что итераторы есть и в других языках. Но я хочу подчеркнуть, что итераторы — это не совсем просто, особенно вначале, особенно когда нам нужно реализовывать свои.Декораторы. Здесь тебе и функции высшего порядка, замыкания, три уровня вложенности для параметризированных декораторов — слишком много всего для такого «простого» языка. Я сам по началу долгое время не понимал паттерн «декоратор», с трудом писал их. И я знаю, что у многих с этим есть проблемы, это видно как минимум на собеседованиях.Метаклассы. Классы, создающие классы? Функция type, она же класс? А тип type — тоже type? Все классы создаются type? Популярный сценарий применения — ORM? Можно, пожалуйста, мне другой «простой» язык?Библиотека functools в полном составе :)Генераторы. Можно спрашивать на собеседованиях: «А для чего вы используете генераторы?» — и услышать очень много разных ответов. Некоторые вообще не понимают, зачем они нужны. А ведь генератор поддерживает протокол итерации, в него можно слать значения, yield from — ну, сами все понимаете.Контроль зависимостей. Venv, virtualenv, pipenv, poetry, pip tools, pdm, pipx, pyproject и так далее.Инфраструктура вокруг Python. Крайне полезно для работы было бы знать, что такое pypa, pypi, psf — а это ещё отдельный пласт знаний.PEP. Иногда спрашиваю людей: «А что такое PEP?», — и самый частый ответ: «Стандарт кода». Даже опытные разработчики помнят PEP8 в лучшем случае.Около сотни магических методов. Хорошо было бы в них просто ориентироваться.Могущественная интроспекция и «мета-язычные» вещи. Runpy, importlib, trace, traceback, gc, inspect, sys, typing.get_type_hints, typing.get_origin, typing.get_args.Встроенные классы ошибок, методов и типов. Можете выполнить и получите 156: import builtins; len(dir(builtins)). Это количество встроенных классов ошибок, методов, типов. Желательно помнить многое из этого.Новые вещи. Оператор «морж», pattern matching.Конкурентное выполнение. Threading с кучей нюансов, multiprocessing, subprocess, свежий communicating sequential processes «паттерн» (PEP 554).GIL. Комментарии не нужны, но они будут (я не умею лаконично, помогите).Особые языковые возможности. Dict, list, set comprehensions, generator expressions, например.Асинхронность. Это отдельный «подсет» языка, где уже целая своя вселенная с кучей пакетов и даже отдельными (не встроенными в язык) концепциями, вроде structured concurrency.Аннотации типов. Или аннотированный Python — тоже, считай, свой мини-«подсет» языка, вводящий новый понятийный аппарат, и новый вид типизации — структурную саб-типизацию, ближайшим аналогом которой из мира динамической типизации можно было бы назвать утиную. Динамическая типизация. Строгая (местами «обходимая» полимформизмом, или неявным приведением int к float), утиная типизация, typing.Protocol (про него выше), gradual типизация. Скажите мне — это правда так просто?Типичный диалог (а иногда и монолог) в интернетеИ я бы мог ещё продолжать какое-то время. Этот список я составил не для того, чтобы отпугнуть от языка, но мне хочется продемонстрировать, что Python — не такой простой язык программирования. Действительно, на нём легко начать, у него низкий порог первоначального входа. Но это вовсе не язык только лишь для новичков, такое отношение к нему порождает безответственный подход к разработке. Язык взял много хорошего от других, он не простой, со своими плюсами и минусами, очень красивыми местами и не самыми лучшими. И всё это требует освоения.GIL Можем сказать, что мы снова про скорость. Когда заходит разговор о тредах и Python, о скорости и Python, сразу на сцену выползает наш любимый и обожаемый GIL.Ограничения GIL известны всем — в Python при попытке распаралеллить любую CPU-bound нагрузку с помощью тредов, разработчики неизбежно сталкиваются с тем, что в один момент у всегда будет работать только один поток. Тема абсолютно выдающаяся и имеющая за собой такой поток реминисценций, что про это даже немного неловко говорить, — но поговорить нам, всё-таки, в рамках статьи нужно.Издалека проблема GIL — и сложная, и, как кажется, практически нерешаемая (мы все помним, что на этот счет сказали Гвидо, Дэвид Бизли; UPD: тут недавно вышла статья, говорят о nogil… но давайте не будем об этом). В современных бэкендах и Python накал страстей вокруг GIL сильно стих — веб-сервисы очень часто по своей природе могут быть асинхронными, а для этого у нас есть множество чудесных фреймворков, вроде FastAPI, поэтому ограничения GIL нас не так сильно касаются. При этом горизонтально мы масштабируемся процессами, как в синхронных бэкендах, так и в асинхронных. У многих есть кубер — и там мы масштабируемся тоже процессами, только над ними стоит абстракция уровнем выше — pod.Изрядно поношенный GILНекоторые разработчики спрашивают, зачем нам тогда треды? В современном Python их используют для одной интересной штуки — на них можно делать wannabe-асинхронный код, при этом не делая асинхронный код, как ни странно. То есть можно написать на тредах что-то, что занимается интенсивной i/o-нагрузкой. И тогда внезапно все станет асинхронным за счет того, что GIL отпускается на i/o-операциях. Это важное примечание, так как это сложный и неочевидный нюанс работы GIL — он описан в документации, но кто же её читает — шутка :). Но благодаря ему мы можем получить преимущество даже в синхронном коде, а также не блокировать петлю событий, когда у нас возникает потребность вызвать синхронное i/o в петле событий (вспомним про запись файлов, привет экзекьюторам).Python — «неказистый язык для набрасывания прототипов на Django»Частое мнение в интернете — Python годен только для быстрого, реактивного набрасывания прототипов на Django, а все остальное удел «серьезных языков» (тм). И это ещe одно, с моей точки зрения, не очень корректное суждение. Конечно, на Django, на Python быстро набрасывают прототипы — я не буду отрицать очевидного. Но на языке делают не только это, описанное — лишь малая часть возможностей экосистемы Python. Однако быстро прототипирующие люди, использующие язык одним способом, почему-то взяли на себя прерогативу оценивать позиционирование языка по своей сфере. И с этим я не согласен. В качестве одного из контраргументов можно привести наш выдающийся тулсет для написания тестов, который не мог возникнуть в языке для быстрого набрасывания прототипов за ненадобностью.Pytest — это прекрасный фреймворк для написания тестов и яркий представитель этого тулсета. Он позволяет выстраивать очень сложные тестовые сценарии с инверсией зависимостей всего с помощью двух инструментов — fixtures и parametrize. С их помощью можно делать очень сложные тесты — подробно изучать производительность и бизнес-логику системы.В тестировании, помимо Pytest, у нас есть Hypothesis — отличный фреймворк для fuzzy, property тестирования. Фреймворк настолько впечатляет, что недавно, на Python Language Summit 2021, стало известно, что его используют core developer'ы и с помощью него нашли ошибки в PEG парсере самого Python.Использовать его просто, а результатом являются такие тесты, о которых я раньше только мечтал. В итоге из одного теста у нас получается целая последовательность, которая имеет различные статические проверки, динамические, а также случайно сгенерированные:from hypothesis import given\n",
      "from hypothesis.strategies import text\n",
      "\n",
      "\n",
      "def encode(input_string: str) -> list:\n",
      "    \"\"\"Это просто синтетический пример.\"\"\"\n",
      "    count: int = 1\n",
      "    prev: str = \"\"\n",
      "    lst: list = []\n",
      "    character: str\n",
      "    for character in input_string:\n",
      "        if character != prev:\n",
      "            if prev:\n",
      "                lst.append((prev, count))\n",
      "            count = 1\n",
      "            prev = character\n",
      "        else:\n",
      "            count += 1\n",
      "    lst.append((character, count))\n",
      "    return lst\n",
      "\n",
      "\n",
      "def decode(lst: list) -> str:\n",
      "    \"\"\"Это просто синтетический пример (2).\"\"\"\n",
      "    q: str = \"\"\n",
      "    character: str\n",
      "    count: int\n",
      "    for character, count in lst:\n",
      "        q += character * count\n",
      "    return q\n",
      "\n",
      "\n",
      "@given(text())\n",
      "def test_decode_inverts_encode(s):\n",
      "    \"\"\"И здесь мы получаем совершенно невероятный объем тестов. При том, \n",
      "    что повесили 1 декоратор.\"\"\"\n",
      "    assert decode(encode(s)) == s\n",
      "Для Python существует библиотека для мутационного тестирования mutmut. Если вы вдруг с ней не сталкивались — посмотрите, это очень интересный инструмент. С его помощью можно проверять ваши тесты через небольшие изменения в коде — я бы назвал это мета-тестированием. Вообще, в Python есть огромное количество вспомогательных библиотек для тестирования — Faker, Factory boy, Mixer, Seed, куча расширений для Pytest — например, для параллелизации. Поэтому говорить, что Python можно использовать только для быстрого прототипирования на Django — немного некорректно.БезопасностьКак-то раз нам написали комментарий, что за Java и C# стоят крупные компании, бизнес, а за Python только Гвидо и никакой ответственности нет. Видимо, это означало, что языком пользоваться страшно и от этого он абсолютно несерьезен.Мне кажется, этот вопрос довольно сложный. Крупные проблемы языковой среды — не такой уж и частый сценарий. Но вряд ли при серьезных проблемах разработчики на Java или C# моментально получат их решение.А ещё можно вспомнить, что современный софт пишется с использованием огромного количества Open Source библиотек, которые пишутся сторонними разработчиками, лежат в открытом доступе — и баги там случаются часто, а гарантий их исправления нет и быть не может. Тогда как серьезные проблемы Python, в случае их возникновения, точно будут исправлены. К тому же у Python «сжался» релизный график.Поэтому проблемы у многих языков, как я считаю, общие, — и выделять энтерпрайз-языки отдельно, ставить их выше — не совсем правильно.ПопулярностьПопулярность языка программирования часто оценивают на результатах рейтингов. Поскольку вокруг них ходит огромное количество споров, я решил проанализировать почти все известные рейтинги по популярности языков.У нас есть аналитика GitHub, здесь Python сразу за JavaScriptGoogle-тренды довольно наглядно показывают, что Python\n",
      "очень долго рос все эти годы и умудрился своей неспешной змеиной «походкой»\n",
      "обогнать мощных конкурентовРейтинг PYPL (некоторые говорят, что он не очень объективен)Любимый многими RedMonk. Кто-то считает его довольно объективнымМенее известный рейтинг IEEE Spectrum, который\n",
      "агрегирует данные из 11 источников, включая (но не только)\n",
      "некоторые из описанных выше и нижеStackoverflow говорит, что по популярности Python сейчас третийВот и всем известный и бесконечно критикуемый TIOBEПричин роста много, но кроме известных, для себя я предпочитаю выделять такую: одна из сильнейших черт Python в том, что он по пути вбирал в себя огромное количество разных парадигм и подходов, зачастую беря из них если не лучшие, то как минимум хорошие куски. Именно поэтому он умудряется быть таким универсальным и при этом довольно дружелюбным. Хм, дружелюбный сосед-питон? (шутка, предоставлена ЗАО «бумер-кринж»).И о минусахЧто? Секундочку, ведь я только что активно поддерживал Python, а теперь начинаю перечислять минусы? Дело в том, что мне не хочется выглядеть как фанатик в розовых очках. Поэтому я просто назову уже не совсем типовые вещи, но тоже часто встречающиеся:GIL всё ещё нас беспокоит. В итоге I/O-bound с небольшим количеством CPU-bound может привести к тому, что некоторые сигналы будут не доставлены.У нас нет оптимизации хвостовой рекурсии (а нужна ли она?). Это большой топик.Некоторые не любят пробелы… вспомним Whython. Я до сих пор не понимаю, в чем проблема — я везде код форматирую пробелами, а в Python они дают возможность избежать написания «лишних» скобок. Но не признать того, что есть недовольные, нельзя.Lambda — есть разработчики, которые хотели бы видеть их как функции. Тогда как в Python они устроены в качестве выражений.Python 2 и 3. Это неактуальный топик на текущий день, и Python 3 принес столько невероятных вещей, что о второй версии говорить нет желания. Но стоит признать — это было больно, и многие ещё помнят, насколько болезненно дался переход. Некоторых это отвращает от языка: подсознательно ты ждешь Python 3 vs 4.Специфический подход к ООП.Смешение парадигм даётся не всегда просто.Я не буду сейчас пытаться разбираться ещё и в этих пунктах, но это всегда повод для отдельной дискуссии. Возможно — в комментариях.Небольшие итогиЯ хочу сказать, что Python, на мой взгляд, надежен для большинства кейсов, с которыми мы сталкиваемся в бэкенд-разработке. С поправкой на хайлоад, конечно. Python используется большинством авторитетных компаний, таких как Google и Яндекс. При этом популярность языка продолжает расти, а так же Гвидо недавно на Python Language Summit 2021 обещал, что с поддержкой Microsoft он сделает Python быстрее раз в пять, во многом с помощью jit. Может быть из конца списка языков в бенчмарках мы переместимся в середину, и уж там то точно заживем.В Python фантастические веб-фреймворки. Можем вспомнить Django или FastAPI — это выдающиеся фреймворки, со своими плюсами и минусами. Современный Python можно брать для написания быстрых, хороших и серьезных бэкендов. В Python-среде много крутых разработчиков и приятных людей!Python жутко популярен и продолжает расти. Потенциал его роста не исчерпан.Для меня Python — язык для написания бэкенда номер один. Язык, с которого я начинаю любой бекенд в 2021 и только в каких-то исключительных ситуациях беру другие.А ещё на Python очень приятно писать код — и этого совершенно не стоит стесняться.Эта статья — расширенная версия доклада с IT-конференции Райффайзенбанка <code/R>. Здесь много новых подробностей, но если хотите услышать часть голосом — смотрите видео.     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Мы в Evrone часто сталкиваемся с легендой, что для задачи, которая встает перед программистами, есть какой-то волшебный, лучший инструмент. К примеру, если вы хотите сделать что-нибудь бэкендовое, вам обязательно нужен PHP. А если вы хотите создать крутой современный фронтенд, то без JavaScript вам делать нечего. Если же вы такой хипстер, что хотите быстро делать фулстек фичи, то вам просто необходим Ruby. И, наконец, если у вас ML, artificial intelligence, big data или просто вам на бэкенде нужен какой-то клей, чтобы работать с данными, то вам жизненно необходим Python. Python самый популярный язык программирования в миреНачнем с того, что произошло 40 лет назад, когда Гвидо ван Россум только закончил учиться на профессионального автора языков программирования (!), и на фултайм работе делал язык программирования ABC. В 1986 году Гвидо завершил работу над ним, и вместе с тем самым Таненбаумом, который написал толстую книжку, начал делать операционную систему Amoeba. Гвидо надо было делать для неё скрипты, и он как профессиональный разработчик языков программирования, начал делать новый язык для себя. Чтобы создавать скрипты как клей для доступа к к системным библиотекам, написанным на С. У команды, которая работала над Амебой, была добрая традиция называть программы для этой операционной системы в честь телешоу. Так что язык Python не просто так называется в честь телешоу «Монти Пайтон». В 2000 году Гвидо присоединился к команде Zope, которая делала пайтоновские фреймворк, инфраструктуру и т.д.. Это можно считать стартом мировой популярности Python. Python лучший язык для обученияНоги Python растут из языка ABC, который пытались сделать для обучения новичков. Но это было 40 лет назад, с первого раза у Гвидо не получилось, зато неплохо получилось со второго.Когда Python был создан, его разработчики написали Zen of Python — манифест из 10 пунктов, который подчеркивал основные ценности языка. Например, что «simple is better than complex, but complex is better than complicated», разумные принципы программирования. Python считается очень хорошим языком для обучения, лучшим, чем JavaScript, потому что он придерживается принципа «минимального удивления». Новые разработчики минимально удивляются поведению кода, который они написали. Python логичен, предсказуем и большинство штук в нем нужно делать, руководствуясь еще одним принципом из Zen of Python: «явное лучше неявного».К сожалению, у такого подхода есть обратная сторона. Software complexity problem, с которой боремся мы, программисты. Python как язык похож на кочан капусты, и его простота, это лишь внешний слой. Но чем глубже разработчик погружается, тем больше всякого разного предлагает язык программирования. В глубине его сокрыт инструментарий, чтобы делать действительно большие проекты. Там есть метапрограммирование, декораторы, куча специализированного синтаксиса, огромная стандартная библиотека. Python, который используется в больших проектах на миллионы строк кода, это не тот Python, которому обучают на курсах «войти в IT». Язык прост в начале, но чем больше мы используем этот инструмент, тем больше разных фишечек он нам предоставляет, чтобы мы могли создавать сложный софт. Python очень хорошо документированКогда я в 90-х начинал писать код, для меня золотым стандартом документации был MSDN, который действительно объяснял, как правильно программировать под Windows. Другой пример хорошей документации — это Qt, там было множество примеров, после несколько сотен страниц становилось понятно, что вообще делать. Сейчас документация Python вобрала в себя все лучшее за последние 30 лет. Python документирован очень хорошо. Не только сам язык, его синтаксис, семантика, но и стандартная библиотека. На официальном сайте огромное количество примеров в разделе документации. Новый разработчик прочтет там не только «что», но и «как». А опытный разработчик найдет нюансы использования с примерами, сможет сразу посмотреть, как правильно. Python имеет в комплекте «батарейки»В стандартной библиотеке Python более 200 модулей для того, чтобы работать с данными, работать с форматами файлов, отсылать и принимать почту. Огромное количество штук Python может сделать «из коробки».У «батареек» есть обратная сторона. Стандартная библиотека очень быстро устаревает и, к примеру, та ее часть, которая связана с сетевыми запросами, уже много лет назад была заменена мега популярной библиотекой Requests.Недавно в комьюнити Python была неприятная ситуация, когда один из очень известных контрибьюторов сделал библиотеку для того, чтобы декораторами просто и читаемо описывать структуры данных. Дальше он поговорил с Гвидо, в стандартной библиотеке сделали простенькую альтернативу dataclasses. После чего автор изначальной библиотеки начал получать сообщения типа: «Нафига ты этот велосипед изобретаешь, если в стандартной библиотеке все есть?». Но в стандартной библиотеке это очень простенькая штучка, а он делает полнофункциональную.Есть когнитивное искажение, что если что-то есть в стандартной библиотеке, то не надо изобретать велосипед. Поэтому наличие этой огромной стандартной библиотеки в какой-то мере тормозит развитие Python. Она маскирует наличие альтернатив. Условно, когда мы в гугле ищем, как распаковать zip-архив в Go, то мы сразу видим библиотеки, которые делают это правильно. С Python мы сразу видим стандартную библиотеку. Даже если эта часть устарела на 10-20-30 лет, альтернативы еще надо поискать.Python очень популярен в наукеЕсть мнение, которое я часто встречал в аналитике и у знакомых питонистов. Возможно, Python популярен в науке  не потому, что он делает для науки что-то особо крутое. Возможно, он популярен, потому что ученые — это не программисты. Гвидо сделал Python как клей для доступа ко всяким системным штукам. Python с самого начала  хорошо мог в C, C++, Fortran — а это как раз то, что в те времена использовали ученые для работы с данными и экспериментами. Ученым нравилась простота изучения Python, им дальше простого первого слоя языка копать не было необходимости. А когда ученые начали использовать язык, он проник в образование и получилось самоподдерживающееся пророчество: учёные используют Python, обучают своих студентов, те из них, кто остается в образовании, тоже используют Python в обучении. Так за 30 лет Python очень надежно проник в науку. Синтаксис многих популярных решений для Python, например, numpy, близок математикам. Сейчас считается, что Python это инструмент по умолчанию для ML и Data Science, что это привычный инструмент для математиков, ведь за десятилетия этого самоподдерживающегося пророчества выросла огромная экосистема для работы с данными и ML: TensorFlow, AirFlow, Pandas и множество других систем.Python медленныйPython интерпретируемый язык программирования. Но сейчас интерпретируемость это совсем не то, что было 30 лет назад. Тогда говорили, что Basic интерпретируемый, и подразумевали, что есть некая программа «basic», которая берет текст программы на языке программирования Basic, построчно читает, разбирает и выполняет.С современными языками программирования это, конечно, не так. Все современные языки программирования компилируемые. Просто некоторые, например, Java, сперва компилируется в байт-код, а потом виртуальная машина Java этот байт-код компилирует в машинный код. Или, как в случае С или Rust, у них нет виртуальной машины, семантика чуть беднее, поэтому они сразу компилируются из текста в машинный код. Python компилируется из текста в байт-код и далее виртуальная машина Python, программа python.exe, этот байткод выполняет. Байт-код — это маленькие инструкции, каждая из которых делает какую-то часть работы программы. Для Python они довольно хорошо оптимизированы, как и его виртуальная машина. Она не компилирует машинный код, потому что пока есть виртуальная машина, язык может обеспечить гораздо более богатую семантику, чем то что нам предлагает, к примеру, JavaScript. Виртуальная машина сама по себе медленная, это не машинные коды, это все-таки программа, которая выполняет байт-коды. А еще есть Global Interpreter Lock (GIL). Виртуальная машина Python предпочитает одновременно выполнять байткод python только в одном потоке. Это придумано для того, чтобы можно было сделать очень быстрый сборщик мусора и защитить структуры данных в памяти от конкурентного использования.Это не значит, что Python не может выполнять код параллельно. Он не может свой байт-код выполнять параллельно, а чужой нативный код может. Как только байткод начал делать что-нибудь системное, например, читать из файла, или писать в сетку, он поднимает GIL, и какой-то другой поток может начать выполнять высокоуровневую логику байткода. Плюс, стандартная библиотека позволяет хорошо раскладывать Python на процессы. Но все это не мешает ему быть в худшем случае раз в сто-двести медленнее, чем С. Python популярен в вебеPython не самое популярное решение для веба, однако многие питонисты делают фулстек веб-приложения, и еще большее количество программистов делают на Python свои API. Почему так, почему не PHP? Потому что в конце нулевых Python как раз начал замещать PHP и Perl, вышел фреймворк Django, который вдохновился очень популярным тогда фреймворком Ruby on Rails. Считается, что Django взял лучшие моменты из Rails и добавил традиционно хорошую документацию. А Гвидо с 2005 года начал работать в Google над App Engine. И это считается одной из ключевых вех в популярности Python, потому что язык стал пиарить Google, как язык «по умолчанию» чтобы делать веб-приложения для Google App Engine. Python занял нишу между PHP и Java, когда PHP уже слишком мало, а Java это слишком много.За Django последовали другие фреймворки, но в начале десятых случился современный фронтенд, к нам пришли React, Angular, Vue и сожрали фронтендовую часть фулстека. Но в 2019 году Python немного отыгрался FastAPI, о котором поговорим чуть ниже.У Python беда с управлением зависимостями Десятки лет назад библиотеки для Python жили на сайте Vaults of Parnassus. Каждая библиотека была архивом с текстовичком о том, как ее добавлять свою программу. В 1998 году разработчики в версии 1.6 языка добавили в стандартную библиотеку модуль distutils, который ввёл понятие distribution package — zip-архив, который можно скачивать и устанавливать в виде install package. С установкой приняли спорное решение, которое во многом прокляло зависимости Python. Авторы посмотрели на огромный архив Vaults of Parnassus и решили, что заставлять каждого разработчика приводить библиотеки к общему виду никто не будет. Придумали, что в install package будет файл setup.py, и его просто будут запускать. А в файл setup.py разаработчики зависимостей сами напишут какой-то код, который будет ставить зависимость, как написано в текстовичке. Копировать нужные файлы в какие-то директории, вносить необходимые изменения. А дальше то, что поставилось, уже можно использовать в своем коде. Главная проблема была в том, что setup.py мог выполнять любой Python-код, и поэтому когда сейчас мы делаем pip install, начинают выполняться все setup.py файлы, и мы не представляем себе, какие зависимости поставятся, что поменяется, и что вообще произойдет.Сообщество Python пытается побороть эти сложности, но язык редко определяет работу с зависимостями. Например, npm это не часть JavaScript, Maven это не часть стандарта Java, gem до недавнего времени не был частью Ruby, composer не часть PHP. Так выглядит экосистема работы с зависимостями Python в 2022 году:Это популярные тулзы, которые сейчас используются, чтобы управлять зависимостями в Python. И, на секундочку, у Python есть логотип сложности управления зависимостями — Утконос. Представляете, у языка есть логотип того, насколько сложно управлять зависимостями!Python взял лучшее из статического и динамического типизированияТрадиционно типы считают крутым инструментом, чтобы сократить число ошибок в программах. И Википедия согласна, что типы — это чтобы было меньше багов. Динамическое и статическое типизирование, это про то, кто расставляет типы, «капканы для ошибок». В случае динамического типизирования типы расставляет язык программирования, в случае статического — программист.У динамического типизирования традиционным плюсом было то, что так как язык за нас расставляет типы, мы можем со страшной скоростью писать код. Минусом было то, что капканы срабатывают, как правило, не сразу, а на этапе выполнения. Иногда капканы могут вообще не поставиться.В статически типизированном языке код пишется медленнее, потому что нужно руками расставлять капканы. Rust в этом плане особенно показателен. Например, я расставил три капкана, которые мне интересны, и говорю ему, ну ты там остальные сам выведи, а Rust такой: «Я не понимаю, что там, давай, ты мне уточнишь». Я смотрю, и тоже не понимаю, что там. Ну и дальше идет получасовая сессия в попытке натянуть код на глобус. Зато при статическом типизировании капканы срабатывают сразу.Python ввёл Gradual Typing — лучшее из обоих миров, когда мы в начале фигачим без типов (их расставляет язык), а потом, когда код стабилизируется, добавляем типы только там, где надо. Это позволяет иметь в программах существенно меньше багов и высокую скорость разработки одновременно.Современный Python часто использует типы не только для того, чтобы расставлять капканы для ошибок, но и не по назначению. Например, pydantic, популярная сейчас библиотека для работы с данными. То, что после двоеточия, это типы, и они используются в качестве DSL, чтобы описать данные. Мегапопулярный FastAPI теперь точно так же использует типы, чтобы описывать API. Как в Ruby DSL использовался 20 лет назад, чтобы писать лаконичный, идиоматичный код, так сейчас программисты на Python научились использовать типы, чтобы писать такой же лаконичный и идиоматичный код.Какой Python сейчас,…Python поддерживается крупнейшими мировыми IT-компаниями. Он уже развивается не одним человеком, много лет избираемый steering council из нескольких core-разработчиков работает над языком. У Python стабильный цикл релизов поддержки, end of life, security фиксов. В Python регулярно появляются новые фичи, в 3.10 появился, например, pattern matching, что очень круто. Разработчики Python стараются демонтировать устаревшую стандартную библиотеку, изучают возможности отказа от GIL, сейчас есть даже работающая версия, которая убирает GIL и при этом работает быстрее. Внедряется асинхронность, Django и Flask не так давно перешли на асинхронные рельсы, и активно делаются решения, которые на одном ноутбуке позволяют легко держать 10 тысяч асинхронных подключений в секунду. И, конечно же, идет битва с зависимостями. Есть pipenv, которое пытается как npm в NodeJS, чтобы npm install, или как gem в Ruby, gem install, чтобы одной командой все работало. … и каким он станет в будущемЕсть две ключевые идеи, которыми я бы хотел завершить свой рассказ. Во-первых, сайт Real Python, который содержит исчерпывающую, крутейшую документацию о Python. Это сотни обучающих статей, каждую из которых писали несколько разработчиков и редактировали несколько редакторов. Наверно, лучшее по обучению, что сейчас есть в мире, и при том бесплатно. Они зарабатывают деньги на видеоуроках, но тексты бесплатны.Во-вторых, Python из-за его легкости в изучении, распространении в образовании и сильной позиции в современной разработке, стал де-факто языком для того чтобы «войти в IT». И многие HR говорят, что это очень сильно размывает пул разработчиков. Из сотни откликов на вакансию питониста людей с опытом больше пяти лет не так много, а большинство окончили трехмесячные курсы и хотят войти в IT. Есть опасение, что вот эта мегапопулярность может серьезно пошатнуть позиции Python. Или нет?    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Привет, Хаброжители!\n",
      "\r\n",
      "Компьютер способен решить практически любую задачу, если ему дать правильные инструкции. С этого и начинается программирование. Даниэль Зингаро создал книгу для начинающих, чтобы вы сразу учились решать интересные задачи, которые использовались на олимпиадах по программированию, и развивали мышление программиста.\n",
      "\r\n",
      "В каждой главе вам даются задания, собственные решения можно выложить на сайт и получить оценку профи. Вы на практике освоите основные возможности, функции и методы языка Python и получите четкое представление о структурах данных, алгоритмах и других основах программирования. Дополнительные упражнения потребуют от вас усилий, вы должны будете самостоятельно изучить новые понятия, а вопросы с несколькими вариантами ответов заставят задуматься об особенностях работы каждого фрагмента кода.\n",
      "\r\n",
      "Вы узнаете, как:\n",
      "\n",
      "запускать программы на Python, работать со строками и использовать переменные;\n",
      "писать программы, принимающие решения;\n",
      "повысить эффективность кода с помощью циклов while и for;\n",
      "использовать множества, списки и словари для организации, сортировки и поиска данных;\n",
      "разрабатывать программы с использованием функций и методики нисходящего проектирования;\n",
      "создавать алгоритмы поиска и использовать нотацию «О большое» для разработки более эффективного кода.\n",
      "\r\n",
      "К концу книги вы не только овладеете Python, но и научитесь тому типу мышления, который необходим для решения задач. Языки программирования приходят и уходят, а подходы к решению проблем останутся с вами навсегда!\n",
      "\n",
      "\n",
      "Для кого предназначена книга\n",
      "Эта книга для всех, кто хочет научиться писать компьютерные программы для решения своих задач.\n",
      "\r\n",
      "Во-первых, вы, возможно, слышали о языке программирования Python и хотите научиться писать код на нем. В следующем разделе я объясню, почему именно Python отлично подходит в качестве первого языка программирования для изучения. Здесь вы много узнаете о Python и позже сможете прочесть более сложные книги об этом языке.\n",
      "\r\n",
      "Во-вторых, если вы не слышали о Python или просто хотите узнать, что такое программирование, не волнуйтесь, вам эта книга тоже подойдет! Она научит вас понимать суть программирования. У программистов есть свои методы разбивать задачи на небольшие части и находить их решения с помощью кода. На этом уровне не имеет значения, какой именно язык программирования используется, потому что мышление программиста не привязано к какому-либо определенному языку.\n",
      "\r\n",
      "И в-третьих, вам может быть интересно изучить какой-нибудь другой язык программирования, например C ++, Java, Go или Rust. Многое из того, что вы узнаете в ходе изучения Python, будет полезно при усвоении других языков программирования. Кроме того, Python сам по себе заслуживает изучения. А почему именно он — об этом дальше.\n",
      "\n",
      "\n",
      "Приступим к работе\r\n",
      "Программирование — это написание кода для решения некоторой задачи. Этим мы и займемся. Поэтому не будем сначала изучать концепции Python, а затем формулировать задачи. Напротив, я сформулирую задачу, а уже на ее основе стану вводить концепции, которые нужно изучить.\n",
      "\r\n",
      "В этой главе мы решим две задачи: определим количество слов в строке (этим же занимаются функции подсчета слов в текстовом редакторе) и вычислим объем конуса. Для этого вам придется познакомиться с несколькими концепциями Python. В какой-то момент вы можете почувствовать, что вам нужно больше подробностей, чтобы полностью понять приведенный материал и то, как все это сочетается друг с другом при разработке программы на Python. Не волнуйтесь: в следующих главах мы вернемся к наиболее важным понятиям и подробно остановимся на них.\n",
      "\n",
      "Что мы будем делать\r\n",
      "Как упоминалось во введении, мы будем решать задачи из области соревновательного программирования с использованием языка Python. Все представленные задачи по соревновательному программированию можно найти на одном из сайтов с задачами для программистов. Я предполагаю, что вы следовали инструкциям, приведенным во введении: установили Python и зарегистрировались на сайтах.\n",
      "\r\n",
      "Мы напишем программы для решения всех приведенных задач. У каждой из них есть входные данные (ввод) определенного типа, которые программа будет получать, и ожидаемые выходные данные (вывод), тоже определенного типа. Будем считать, что программа правильно решает задачу, если может принимать любые допустимые входные данные и выдает в ответ правильные выходные данные.\n",
      "\r\n",
      "В целом возможных входных данных могут быть миллионы или миллиарды. Каждый вариант входных данных называется экземпляром задачи. Например, в первой задаче, которую мы решим, входные данные — это строка текста, например hello there или bbaabbb aa abab. Наша цель — подсчитать и вывести количество слов в строке. Одна из самых крутых вещей в программировании заключается в том, что зачастую небольшой объем универсального кода позволяет решить бесконечное количество типовых задач. И не имеет значения, будет в строке два слова, три или 50 — программа всегда будет делать это правильно.\n",
      "\r\n",
      "Наши программы будут выполнять три задачи.\n",
      "\n",
      "Чтение входных данных. Необходимо определить конкретный экземпляр задачи, которую требуется решить, поэтому сначала мы считываем предоставленные входные данные.\n",
      "Обработка. Мы обрабатываем входные данные, чтобы определить правильные выходные данные.\n",
      "Запись вывода. Решив задачу, выдаем желаемый вывод.\n",
      "\r\n",
      "Границы между шагами бывают размытыми — иногда нам, возможно, придется чередовать обработку с получением результата. Но все равно работа в целом делится на эти три этапа.\r\n",
      "Вы, вероятно, ежедневно пользуетесь программами, работающими в соответствии с моделью «ввод — обработка — вывод». Рассмотрим программу-калькулятор: вы вводите формулу (входные данные), программа обрабатывает числа (обработка), а затем отображает ответ (выходные данные). Или вспомним о поисковых системах в Интернете: вы вводите поисковый запрос (входные данные), поисковая система определяет наиболее релевантные результаты (обработка) и отображает их (выходные данные).\n",
      "\r\n",
      "Сравните приведенные примеры программ с интерактивными программами, которые также выполняют ввод, обработку и вывод. Например, для набора текста этой книги я использую текстовый редактор. Когда я набираю символ, редактор в ответ добавляет его в мой документ. И мне не нужно печатать сразу весь документ, чтобы увидеть результат, — редактор интерактивно отображает его по мере печатания. В этой книге мы не будем писать интерактивные программы, но если после изучения этой книги вы заинтересуетесь их созданием, то радуйтесь: Python определенно подходит для этого.\n",
      "\r\n",
      "Тексты всех задач можно найти здесь и на сайте. Однако они не всегда совпадают, потому что я переписал их ради единообразия внутри книги. Но не волнуйтесь: в моих формулировках вы найдете ту же информацию, что и в официальной постановке задачи.\n",
      "\n",
      "Оболочка Python\r\n",
      "Для каждой задачи из книги мы хотим написать программу и сохранить ее в отдельном файле. Но для начала неплохо было бы знать, какую программу писать! Для решения многих задач вам нужно будет изучить пару новых функций Python.\n",
      "\r\n",
      "Лучший способ поэкспериментировать с функциями Python — использовать оболочку Python. Это интерактивная среда, в которой можно ввести Python и нажать клавишу Enter, а Python в ответ выведет результат. Как только вы получите достаточно знаний, чтобы решить текущую задачу, можно будет перестать работать с оболочкой и начать писать решение в текстовом файле.\r\n",
      "Для начала создайте на рабочем столе новую папку и назовите ее programming. Будем использовать ее для хранения результатов работы по ходу изучения книги.\n",
      "\r\n",
      "Сейчас мы откроем эту папку и запустим оболочку Python. Чтобы запустить оболочку Python в своей операционной системе, выполните следующие действия.\n",
      "\n",
      "Windows\n",
      "\r\n",
      "Если вы работаете в Windows, сделайте следующее.\r\n",
      "1. Удерживая нажатой клавишу Shift, щелкните правой кнопкой мыши на папке с вашими программами, например programming.\r\n",
      "2. В появившемся меню выберите пункт Open PowerShell window here (Открыть окно PowerShell здесь). Если в контекстном меню нет такого пункта, выберите Open command window here (Открыть окно команд здесь).\r\n",
      "3. В появившемся окне вы увидите строку, которая заканчивается знаком «больше» (>). Это приглашение операционной системы, и теперь она ждет, когда вы наберете команду. Вводить здесь нужно именно команды операционной системы, а не код Python. Обязательно нажимайте клавишу Enter после каждой команды.\r\n",
      "4. Вы находитесь в папке programming. Можете ввести команду dir (от англ. directory — «папка»), если хотите посмотреть содержимое папки. Впрочем, пока вы не увидите никаких файлов, потому что мы их еще не создавали.\r\n",
      "5. Введите команду python для запуска оболочки Python.\n",
      "\r\n",
      "Запустив оболочку Python, вы увидите что-то вроде этого:\n",
      "\n",
      "Python 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:30:23)\n",
      "[MSC v.1928 32 bit (Intel)] on win32\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>>\r\n",
      "Здесь важно, чтобы в первой строке была указана версия Python не ниже 3.6. Если у вас установлена более старая версия, особенно 2.x, или Python не загружается вообще, установите последнюю версию Python, следуя инструкциям, приведенным во введении.\r\n",
      "Внизу этого окна вы увидите приглашение вида >>>. Здесь мы будем писать код Python. Никогда не вводите символы >>> сами. Закончив писать программу, вы можете нажать сочетание клавиш Сtrl+Z, а затем Enter, чтобы выйти.\n",
      "\n",
      "macOS\n",
      "\r\n",
      "В macOS сделайте следующее.\r\n",
      "1. Откройте приложение Terminal. Для этого нажмите сочетание клавиш Command+Пробел, введите слово terminal, а затем дважды щелкните на результате.\r\n",
      "2. В открывшемся окне вы увидите строку, оканчивающуюся символом доллара ($). Это приглашение операционной системы, теперь можете ввести команду. Здесь вводятся именно команды операционной системы, а не код Python. Обязательно нажимайте клавишу Enter после каждой команды.\r\n",
      "3. Можете ввести команду ls, чтобы получить список файлов, находящихся в текущей папке. Вы увидите свой рабочий стол.\r\n",
      "4. Введите команду cd Desktop, чтобы перейти в папку рабочего стола. Команда cd означает change directory, то есть «перейти в другую папку».\r\n",
      "5. Введите команду cd programming, чтобы перейти в папку programming.\r\n",
      "6. Теперь введите команду python3, чтобы запустить оболочку Python (можно попробовать ввести просто python, но в результате может запуститься старая версия Python 2, которая не подходит для работы с этой книгой).\n",
      "\r\n",
      "Когда вы запустите оболочку Python, появится что-то вроде этого:\n",
      "\n",
      "Python 3.9.2 (default, Mar 15 2021, 17:23:44)\n",
      "[Clang 11.0.0 (clang-1100.0.33.17)] on darwin\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>>\r\n",
      "Здесь важно, что в первой строке указана версия Python не ниже 3.6. Если у вас более старая версия, особенно 2.x, или Python не загружается вообще, установите последнюю версию Python, следуя инструкциям, данным во введении.\n",
      "\r\n",
      "Внизу этого окна увидите приглашение >>>. Здесь вы набираете код Python. Никогда не вводите символы >>> самостоятельно. Если закончили программирование, можете нажать сочетание клавиш Ctrl+D, чтобы выйти.\n",
      "\n",
      "Linux\n",
      "\r\n",
      "Порядок действий для Linux таков.\r\n",
      "1. Щелкните правой кнопкой мыши на папке programming.\r\n",
      "2. В появившемся меню выберите пункт Open in Terminal (Открыть в терминале) (а можно сначала открыть консоль и перейти в папку programming, если вам так удобнее).\r\n",
      "3. В открывшемся окне вы увидите строку, оканчивающуюся символом доллара ($). Это приглашение операционной системы, теперь можете ввести команду. Здесь вводятся именно команды операционной системы, а не код Python. Обязательно нажимайте клавишу Enter после каждой команды.\r\n",
      "4. Вы находитесь в папке programming. Можете ввести команду ls, чтобы просмотреть ее содержимое. Но пока никаких файлов в ней быть не должно, потому что мы их еще не создавали.\r\n",
      "5. Теперь введите команду python3, чтобы запустить оболочку Python (можно попробовать ввести просто python, но в результате может запуститься старая версия Python 2, которая не подходит для работы с этой книгой).\n",
      "\r\n",
      "Когда вы запустите оболочку Python, появится что-то вроде этого:\n",
      "\n",
      "Python 3.9.2 (default, Feb 20 2021, 20:57:50)\n",
      "[GCC 7.5.0] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>>\r\n",
      "Важно, чтобы версия Python в первой строке была не ниже 3.6. Если у вас более старая версия, особенно 2.x, или Python не загружается вообще, установите последнюю версию Python, следуя инструкциям, приведенным во введении.\n",
      "\r\n",
      "Внизу этого окна вы увидите приглашение Python вида >>>. Здесь мы будем писать код Python. Никогда не вводите символы >>> сами. Закончив писать программу, можете нажать сочетание клавиш Сtrl+D, чтобы выйти.\n",
      "\n",
      "Задача 1. Количество слов\r\n",
      "Пришло время для нашей первой задачи! На Python напишем небольшую программу для подсчета слов. Вы узнаете, как считывать входные данные пользователя, обрабатывать их для решения задачи и выводить результат. А также узнаете, как управлять текстом и числами в своих программах, использовать встроенные операции Python и сохранять промежуточные результаты на пути к решению.\n",
      "\r\n",
      "Это задача с сайта DMOJ, код dmopc15c7p2.\n",
      "\n",
      "Постановка задачи\n",
      "\r\n",
      "Подсчитайте количество слов во входных данных. В этой задаче словом будет считаться любая последовательность строчных букв. Например, «привет» — это слово, но и всякие бессвязные наборы букв вроде «ыфафыва» тоже считаются словами.\n",
      "\n",
      "Входные данные\n",
      "\r\n",
      "Входными данными будет одна строка текста, состоящая из строчных букв и пробелов. Между каждой парой слов будет ровно один пробел, при этом перед первым словом и после последнего слова их не будет.\n",
      "\r\n",
      "Максимальная длина строки — 80 символов.\n",
      "\n",
      "Выходные данные\n",
      "\r\n",
      "Количество слов в строке.\n",
      "\n",
      "Строки\r\n",
      "Фундаментальным строительным блоком программ на Python являются значения. У каждого значения есть тип, определяющий операции, которые над ним можно проводить. В задаче подсчета слов мы работаем со строкой текста. В Python текст хранится как строковое значение, поэтому нам нужно побольше узнать о том, что это значит. Чтобы решить задачу, требуется узнать количество слов в тексте, а для обозначения количества следует узнать о числовых значениях. Начнем со строк.\n",
      "\n",
      "Представление строк\n",
      "\r\n",
      "Строка — это тип в Python, который используется для работы с текстом. Чтобы записать строковое значение, нужно поместить его в одинарные кавычки. В оболочке Python введите следующее:\n",
      "\n",
      ">>> 'hello'\n",
      "'hello'\n",
      ">>> 'a bunch of words'\n",
      "'a bunch of words'\r\n",
      "Оболочка Python повторяет за мной все, что я ввел.\n",
      "\r\n",
      "А что, если в строке будет одинарная кавычка как ее часть?\n",
      "\n",
      ">>> 'don't say that'\n",
      "     File \"<stdin>\", line 1\n",
      "        'don't say that'\n",
      "               ^\n",
      "SyntaxError: invalid syntax\r\n",
      "Одиночная кавычка в слове don't завершает строку. Остальная часть уже не войдет в строку, поэтому наш ввод вызывает синтаксическую ошибку. Синтаксическая ошибка означает, что мы нарушили правила Python и написали неправильный код Python.\n",
      "\r\n",
      "Чтобы исправить положение, можем воспользоваться двойными кавычками, которые тоже подходят для разделения строк:\n",
      "\n",
      ">>> \"don't say that\"\n",
      "\"don't say that\"\r\n",
      "Но если в рассматриваемой строке нет одинарных кавычек, то применять двойные кавычки без причины я не буду.\n",
      "\n",
      "Строковые операторы\n",
      "\r\n",
      "Именно строки подойдут для хранения текста, количество слов в котором мы хотим подсчитать. Чтобы считать слова или вообще что-либо делать со строками, придется научиться работать с ними.\n",
      "\r\n",
      "Над строками можно выполнять множество операций. Для некоторых из них используются специальные символы, вставляемые между операндами. Например, оператор + служит для конкатенации строк:\n",
      "\n",
      ">>> 'hello' + 'there'\n",
      "'hellothere'\r\n",
      "Ах да, нам же нужен пробел между словами. Добавим его в конец первой строки:\n",
      "\n",
      ">>> 'hello ' + 'there'\n",
      "'hello there'\r\n",
      "А еще есть оператор *, который размножает строку на указанное количество раз:\n",
      "\n",
      ">>> '-' * 30\n",
      "'------------------------------'\r\n",
      "Здесь 30 — это целое число. О числах подробнее поговорим в ближайшее время.\n",
      "\n",
      "ПРОВЕРИМ ЗНАНИЯ\r\n",
      "Что выведет следующий код?\n",
      "\n",
      ">>> '' * 3\n",
      "А. ''''''\n",
      "Б. ''\n",
      "В. Этот код вызывает синтаксическую ошибку (недопустимый код Python).\n",
      "\n",
      "\n",
      "Ответ\n",
      "Б. '' — это пустая строка — строка без символов. Трехкратное повторение пустой строки остается пустой строкой!\n",
      "\n",
      "Строковые методы\n",
      "\r\n",
      "Метод — это операция, выполняемая над определенным типом значений. У строк, в частности, много методов. Например, есть метод с именем upper, который превращает все буквы строки в прописные:\n",
      "\n",
      ">>> 'hello'.upper()\n",
      "'HELLO'\r\n",
      "Информация, которую мы получаем от метода, называется возвращаемым значением метода. Например, в предыдущем примере мы могли бы сказать, что метод upper вернул строку 'HELLO'.\n",
      "\r\n",
      "Выполнение метода над значением называется вызовом метода. Вызывается он с помощью точечной нотации (.) между значением и именем метода. Также нужно после имени метода указать круглые скобки. У некоторых методов, например у upper, эти скобки остаются пустыми.\n",
      "\r\n",
      "А для некоторых методов можно передать в скобках информацию. Остальные методы требуют информации и без нее просто не работают. Информация, которую мы включаем при вызове метода, называется его аргументами.\n",
      "\r\n",
      "Например, у строк есть метод strip. Если вызывать его без аргументов, он удаляет из строки все начальные и конечные пробелы:\n",
      "\n",
      ">>> ' abc'.strip()\n",
      "'abc'\n",
      ">>> ' abc '.strip()\n",
      "'abc'\n",
      ">>> 'abc'.strip()\n",
      "'abc'\r\n",
      "Но мы также можем передать ему в качестве аргумента строку. Если сделать это, аргумент скажет методу, какие именно символы нужно удалить слева и справа:\n",
      "\n",
      ">>> 'abc'.strip('a')\n",
      "'bc'\n",
      ">>> 'abca'.strip('a')\n",
      "'bc'\n",
      ">>> 'abca'.strip('ac')\n",
      "'b'\r\n",
      "Поговорим еще об одном строковом методе — count. Мы передаем ему строковый аргумент, и он сообщает, сколько вхождений этого аргумента найдено в строке:\n",
      "\n",
      ">>> 'abc'.count('a')\n",
      "1\n",
      ">>> 'abc'.count('q')\n",
      "0\n",
      ">>> 'aaabcaa'.count('a')\n",
      "5\n",
      ">>> 'aaabcaa'.count('ab')\n",
      "1\r\n",
      "Если вхождения строки-аргумента перекрываются, учитывается только первое:\n",
      "\n",
      ">>> 'ababa'.count('aba')\n",
      "1\r\n",
      "В отличие от прочих методов, которые я описал, метод count пригодится для поставленной задачи с подсчетом слов.\n",
      "\r\n",
      "Сами подумайте: строка состоит из нескольких слов. Обратите внимание на то, что после каждого слова стоит пробел. Фактически, если бы вам нужно было подсчитать количество слов вручную, пробелы могли бы подсказать, где заканчивается каждое слово. А что, если мы посчитаем количество пробелов в строке? Для этого можем передать методу count символ пробела. Это выглядит так:\n",
      "\n",
      ">>> 'this is a string with a few words'.count(' ')\n",
      "7\r\n",
      "Мы получили значение 7. Это не соответствует количеству слов, ведь их в строке восемь, но значение близко. Почему мы получили 7 вместо 8?\n",
      "\r\n",
      "Причина в том, что пробел стоит после каждого слова, кроме последнего. Это значит, что при подсчете пробелов не учитывается последнее слово. Чтобы внести поправки, нам нужно научиться обращаться с числами.\n",
      "\n",
      "Целые числа и числа с плавающей точкой\r\n",
      "Любое выражение состоит из значений и операторов. Рассмотрим, как писать числовые значения и комбинировать их с операторами.\n",
      "\r\n",
      "В Python есть два разных типа для работы с числами: целые числа (без дробной части) и числа с плавающей точкой (с дробной частью).\n",
      "\r\n",
      "Целочисленные значения пишутся без десятичной точки. Вот несколько примеров:\n",
      "\n",
      ">>> 30\n",
      "30\n",
      ">>> 7\n",
      "7\n",
      ">>> 1000000\n",
      "1000000\n",
      ">>> -9\n",
      "-9\r\n",
      "Значение само по себе — простейший вид выражения.\n",
      "\r\n",
      "Привычные нам математические операторы позволяют работать с целыми числами. Оператор + выполняет сложение, — — вычитание, * — умножение. С их помощью можно писать более сложные выражения.\n",
      "\n",
      ">>> 8 + 10\n",
      "18\n",
      ">>> 8 - 10\n",
      "-2\n",
      ">>> 8 * 10\n",
      "80\r\n",
      "Обратите внимание на пробелы вокруг операторов. Хотя для Python выражения 8+10 и 8 + 10 одинаковы, написание с пробелами упрощает чтение.\n",
      "\r\n",
      "В Python есть два оператора деления! Оператор // выполняет целочисленное деление, при котором остаток отбрасывается, а результат округляется вниз:\n",
      "\n",
      ">>> 8 // 2\n",
      "4\n",
      ">>> 9 // 5\n",
      "1\n",
      ">>> -9 // 5\n",
      "-2\r\n",
      "Если хотите получить остаток от деления, используйте оператор деления по модулю, который обозначается символом %. Например, деление 8 на 2 выполняется без остатка:\n",
      "\n",
      ">>> 8 % 2\n",
      "0\r\n",
      "При делении 8 на 3 в остатке 2:\n",
      "\n",
      ">>> 8 % 3\n",
      "2\r\n",
      "Оператор /, в отличие от //, не округляет результат:\n",
      "\n",
      ">>> 8 / 2\n",
      "4.0\n",
      ">>> 9 / 5\n",
      "1.8\n",
      ">>> -9 / 5\n",
      "-1.8\r\n",
      "И вот тут-то полученные результаты уже не являются целыми числами! Они пишутся через дробную точку и принадлежат другому типу Python, называемому float (число с плавающей точкой). Чтобы писать значения с плавающей точкой, нужно добавить собственно точку:\n",
      "\n",
      ">>> 12.5 * 2\n",
      "25.0\r\n",
      "Но пока вернемся к целым числам, а о числах с плавающей точкой поговорим в задаче про объем конуса позже в этой главе.\n",
      "\r\n",
      "Когда мы используем в выражении несколько операторов, Python определяет порядок их применения согласно правилам приоритета. У каждого оператора есть приоритет. Ровно так же, как делали вы, решая примеры в школьных тетрадях, Python сперва выполняет операции умножения и деления (у них более высокий приоритет), а потом сложения и вычитания (более низкий приоритет):\n",
      "\n",
      ">>> 50 + 10 * 2\n",
      "70\r\n",
      "Опять же, как и при вычислениях на бумаге, операции в круглых скобках имеют наивысший приоритет. Зная это, мы можем заставить Python выполнять операции в любом нужном нам порядке:\n",
      "\n",
      ">>> (50 + 10) * 2\n",
      "120\r\n",
      "Программисты часто добавляют в выражения круглые скобки, даже если это особо и не требуется. Дело в том, что в Python довольно много операторов и в процессе написания легко запутаться в приоритетах, что чревато ошибками, а это плохо.\n",
      "\r\n",
      "Вы можете спросить: а есть ли у целых чисел и чисел с плавающей точкой методы, как у строк. Есть! Но они не так уж и полезны. Например, есть метод, который сообщает нам, сколько памяти компьютера выделено на целое число. Чем оно больше, тем больше памяти ему требуется:\n",
      "\n",
      ">>> (5).bit_length()\n",
      "3\n",
      ">>> (100).bit_length()\n",
      "7\n",
      ">>> (99999).bit_length()\n",
      "17\r\n",
      "В этом случае вокруг чисел нужны круглые скобки, иначе точечная нотация будет воспринята как десятичная точка и мы получим синтаксическую ошибку.\n",
      "\n",
      "Переменные\n",
      "\r\n",
      "Теперь вы умеете писать строковые и числовые значения. Неплохо бы еще получить возможность где-то хранить результаты вычислений, чтобы можно было воспользоваться ими позже. В задаче о подсчете слов было бы удобно где-нибудь хранить строку слов и подсчитанное количество слов в ней.\n",
      "\n",
      "Оператор присваивания\n",
      "\r\n",
      "Переменная — это имя, которое ссылается на некоторое значение. Всякий раз, когда мы позже используем имя переменной, оно будет заменяться значением, на которое ссылается эта переменная. Чтобы она ссылалась на значение, применяется оператор присваивания. Он состоит из переменной, знака равенства (=) и выражения. Python вычисляет выражение и заставляет переменную ссылаться на результат. Пример оператора присваивания:\n",
      "\n",
      ">>> dollars = 250\n",
      "Имя dollars всегда заменяется числом 250:\n",
      ">>> dollars\n",
      "250\n",
      ">>> dollars + 10\n",
      "260\n",
      ">>> dollars\n",
      "250\r\n",
      "Переменная одновременно ссылается только на одно значение. Как только мы укажем оператор присваивания и назначим ей другое значение, старое будет забыто:\n",
      "\n",
      ">>> dollars = 250\n",
      ">>> dollars\n",
      "250\n",
      ">>> dollars = 300\n",
      ">>> dollars\n",
      "300\r\n",
      "В программе можно завести сколько угодно переменных. В больших программах обычно существуют сотни переменных. Пример использования двух переменных:\n",
      "\n",
      ">>> purchase_price1 = 58\n",
      ">>> purchase_price2 = 9\n",
      ">>> purchase_price1 + purchase_price2\n",
      "67\r\n",
      "Обратите внимание: я выбрал такие имена переменных, чтобы из них было понятно, что в них хранится. Эти две переменные, например, хранят стоимости двух покупок. Было бы проще ввести имена переменных p1 и p2, но, читая код через несколько дней, мы, вероятно, уже не поймем, что они означают!\n",
      "\r\n",
      "Можно поместить в переменные строки:\n",
      "\n",
      ">>> start = 'Monday'\n",
      ">>> end = 'Friday'\n",
      ">>> start\n",
      "'Monday'\n",
      ">>> end\n",
      "'Friday'\r\n",
      "Можно использовать их так же, как и переменные, которые ссылаются на числа, — в более крупных выражениях:\n",
      "\n",
      ">>> start + '-' + end\n",
      "'Monday-Friday'\r\n",
      "Имена переменных Python должны начинаться со строчной буквы и могут содержать другие буквы, цифры и символы подчеркивания для разделения слов.\n",
      "\n",
      "Изменение значений переменных\n",
      "\r\n",
      "Предположим, у нас есть переменная dollars, которая ссылается на значение:\n",
      "\n",
      ">>> dollars = 250\r\n",
      "Теперь мы хотим увеличить значение так, чтобы переменная dollars стала равна 251. Не сработает:\n",
      "\n",
      ">>> dollars + 1\n",
      "251\n",
      "\r\n",
      "Результат равен 251, но это значение пропало, нигде не сохранившись:\n",
      "\n",
      ">>> dollars\n",
      "250\r\n",
      "Нам нужен оператор присваивания, который позволяет сохранить результат:\n",
      "\n",
      ">>> dollars = dollars + 1\n",
      ">>> dollars\n",
      "251\n",
      ">>> dollars = dollars + 1\n",
      ">>> dollars\n",
      "252\r\n",
      "Учащиеся часто путают оператор присваивания = с символом равенства. Не повторяйте этой ошибки! Оператор присваивания — это команда, заставляющая переменную ссылаться на значение выражения, а не утверждение о том, что два значения равны.\n",
      "\n",
      "ПРОВЕРИМ ЗНАНИЯ\r\n",
      "Каким окажется значение переменной у после выполнения приведенного фрагмента кода?\n",
      "\n",
      ">>> x = 37\n",
      ">>> y = x + 2\n",
      ">>> x = 20\n",
      "А. 39\n",
      "Б. 22\n",
      "В. 35\n",
      "Г. 20\n",
      "Д. 18\n",
      "\n",
      "\n",
      "Ответ\n",
      "A. Присваивание значения у выполняется лишь раз и дает значение 39. Выражение x = 20 меняет значение переменной x и на значение y никак не влияет.\n",
      "\n",
      "Подсчет слов с использованием переменной\r\n",
      "Давайте резюмируем все, что вы уже знаете о решении задачи подсчета слов.\n",
      "\n",
      "Вы узнали о строках и о том, что их можно применять для хранения какого-то текста.\n",
      "Узнали, что у строк есть метод count, который позволит подсчитать количество пробелов между словами в строке. Получим значение на единицу меньше нужного.\n",
      "Вы узнали о целых числах и об операторе сложения, который можно использовать для увеличения числа на 1.\n",
      "Узнали о переменных и об операторе присваивания, которые помогают сохранять значения.\n",
      "\r\n",
      "Обобщив все это, можно сделать переменную ссылкой на строку и затем подсчитать количество слов:\n",
      "\n",
      ">>> line = 'this is a string with a few words'\n",
      ">>> total_words = line.count(' ') + 1\n",
      ">>> total_words\n",
      "8\r\n",
      "Переменные line и total_words здесь не нужны, можно обойтись без них:\n",
      "\n",
      ">>> 'this is a string with a few words'.count(' ') + 1\n",
      "8\r\n",
      "Применение переменных для сбора промежуточных результатов — хорошая практика для сохранения читабельности кода. Как только наши программы станут длиннее нескольких строк, без переменных будет уже не обойтись.\n",
      "\n",
      "Чтение ввода от пользователя\r\n",
      "Одна из проблем с написанным ранее кодом заключается в том, что он работает только с конкретной строкой, которую мы ввели. Он сообщает, что в этой строке столько-то слов, и на этом все. Чтобы узнать, сколько слов в другой строке, нам придется заменить текущую строку на новую. Если мы хотим решить задачу о подсчете слов, нужна программа, которая сможет работать с любой строкой, переданной ей в качестве входных данных.\n",
      "\r\n",
      "Чтобы считать строку ввода, нужна функция input. Функции похожи на методы: мы вызываем их, передав какие-то аргументы, а они возвращают значение. Различие между методом и функцией заключается в том, что в функциях не используется точечная нотация. Вся информация передается в них через аргументы.\n",
      "\r\n",
      "Далее приведен пример вызова функции ввода с последующим вводом информации, в данном случае слова testing:\n",
      "\n",
      ">>> input()\n",
      "testing\n",
      "'testing'\r\n",
      "Введя функцию input() и нажав клавишу Enter, вы не получите приглашение для ввода. Вместо этого Python ждет, когда вы что-нибудь напечатаете на клавиатуре и нажмете клавишу Enter. Затем функция input возвратит введенную вами строку. Как обычно, если мы нигде не сохраним эту строку, она будет потеряна. Воспользуемся оператором присваивания, чтобы сохранить введенное значение:\n",
      "\n",
      ">>> result = input()\n",
      "testing\n",
      ">>> result\n",
      "'testing'\n",
      ">>> result.upper()\n",
      "'TESTING'\r\n",
      "Обратите внимание, что в последней строке я использовал метод upper на значении, возвращаемом input. Так делать можно, потому что input возвращает строку, а upper — строковый метод.\n",
      "\n",
      "Вывод результата\r\n",
      "Вы уже успели убедиться в том, что ввод выражений в оболочке Python приводит к выводу результата:\n",
      "\n",
      ">>> 'abc'\n",
      "'abc'\n",
      ">>> 'abc'.upper()\n",
      "'ABC'\n",
      ">>> 45 + 9\n",
      "54\r\n",
      "Это подарок от встроенной оболочки Python. Предполагается, что если вы набираете выражение, то, вероятно, захотите увидеть его значение. Но при запуске программы Python вне оболочки Python этого удобства нет. Вместо этого мы должны явно вызывать функцию print всякий раз, когда хотим вывести что-нибудь на экран. Функция print также работает из оболочки:\n",
      "\n",
      ">>> print('abc')\n",
      "abc\n",
      ">>> print('abc'.upper())\n",
      "ABC\n",
      ">>> print(45 + 9)\n",
      "54\r\n",
      "Обратите внимание на то, что строки, выводимые с помощью функции print, не заключаются в кавычки. Это удобно, так как мы, скорее всего, не захотим задействовать кавычки при общении программы с пользователями!\n",
      "\r\n",
      "Приятная особенность функции print заключается в том, что вы можете передать ей сколько угодно аргументов и все они будут выведены и разделены пробелами:\n",
      "\n",
      ">>> print('abc', 45 + 9)\n",
      "abc 54\n",
      "\n",
      "Об авторах\n",
      "Об авторе\n",
      "\r\n",
      "Доктор Даниэль Зингаро — адъюнкт-профессор информатики и преподаватель в Университете Торонто, обладатель множества наград. Его основная область исследований — образование в сфере информатики и вопросы, связанные с тем, как студенты осваивают (пусть порой и плохо) информатику. Даниэль написал книгу Algorithmic Thinking1 (No Starch Press, 2021), которая помогает студентам изучать и использовать алгоритмы и структуры данных.\n",
      "\n",
      "О научном редакторе\n",
      "\r\n",
      "Люк Савчак — редактор-фрилансер и программист-любитель. Ему нравится превращать прозу в стихи, он написал пособие по нарезанию нужного количества кусочков торта и заумную версию Boggle, разобраться в которой сможет только преподаватель математики. Сейчас он преподает французский и английский языки на окраине Торонто. Люк также пишет стихи и сочиняет музыку для фортепиано, чем с удовольствием зарабатывал бы себе на жизнь, если бы мог. Его сайт: https://sawczak.com/.\n",
      "\r\n",
      "Более подробно с книгой можно ознакомиться на сайте издательства:\r\n",
      "» Оглавление\r\n",
      "» Отрывок\n",
      "\r\n",
      "По факту оплаты бумажной версии книги на e-mail высылается электронная книга.\r\n",
      "Для Хаброжителей скидка 25% по купону — Python\n",
      "\n",
      "P.S.\r\n",
      "На сайте издательства продолжается осенняя распродажа.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " В сети часто возникают разговоры на тему «Python best IDE — does it really exist?». Для примера — недавний Q&A в группе Python community на LinkedIn (к сожалению, просмотр возможен только для пользователей группы). Если вкратце, то мир вертится вокруг:\n",
      "\n",
      "\n",
      "Eclipse + PyDev — похоже, что самое популярное из свободных решений.\n",
      "Komodo IDE — по отзывам отличное средство, но не free.\n",
      "WingIDE — лучший autocompletion для Python, но опять-таки non-free.\n",
      "Eric — IDE на Qt и QScintilla, пробовал только на Windows, видно недопробовал — показалась неудобной. Если у кого есть полезная информация и ссылки — поделитесь, пожалуйста, в комментариях.\n",
      "NetBeans 6.5 — готовится к выходу встроенная поддержка Python в этой IDE версии 6.5, ждем с нетерпением!\n",
      "Vim — для фанатов Vim ничего лучше быть, понятно, не может — после обработки напильником, правда.\n",
      "Emacs — аналогично Vim — для пользователей, которые много времени проводят в Emacs, он дает лучший user-experience при разработке на любом языке.\n",
      "\n",
      "\r\n",
      "Если первые четыре решения предоставляют IDE для Python прямо из коробки, то последние два требуют определенных настроек. Для Vim не знаю, но для Emacs надо потратить не меньше 6 часов поисков по интернету и экспериментов перед тем как будет получена удобная и (очень) функциональная среда для Python.\n",
      "\r\n",
      "Что до меня, то весьма долгое время я работал с Eclipse + PyDev — вполне работоспособное решение. Но после окончательного переезда с Windows в Ubuntu (всем ставить шрифты Liberation в Убунте!), я решил таки завершить начатое уже давно — начать использовать Emacs не только как GTD-органайзер, IRC-клиент и редактор «когда не хочется запускать Eclipse».\n",
      "\r\n",
      "История данных шести часов под хабракатом (история предполагает, что читающие хоть немного знакомы с Emacs и Python, а также носит Ubuntu(Debian)-specific оттенок, когда дело касается установки пакетов).\n",
      "\n",
      "\n",
      "\n",
      "Emacs Python-Mode\r\n",
      "Emacs версий 21 и старше имеют встроенный режим программирования на Python, называемый по имени файла модуля python.el. Существует и независимый проект, который дает примерно ту же функциональность. Сравним оба решения.\n",
      "\n",
      "Встроенный python.el\r\n",
      "Хорошая новость для тех, кому просто нужен удобный и функциональный текстовый редактор для python-скриптов – можно ничего, что описано ниже не делать, python.el обеспечивает хороший комплект возможностей прямо из коробки:\n",
      "\n",
      "\n",
      "Синтаксическая подсветка кода.\n",
      "Автоматическое расставление отступов.\n",
      "Возможность запуска внутреннего (в Emacs) интерпретатора.\n",
      "Запуск редактируемого кода во внутреннем интерпретаторе (как всего буфера, так и выделенной его части).\n",
      "Какой-никакой completion.\n",
      "Удобные функции комментирования/раскомментирования кода.\n",
      "Взаимодействие со справочной системой Python.\n",
      "\n",
      "\n",
      "Независимый python-mode.el\r\n",
      "Походив некоторое время по просторам интернета на тему «python-mode.el vs. python.el», я пришел к выводу, что большая часть сообщества пользуется модулем python-mode.el. Основные причины этого вполне себе изложены в этой ветке на форуме velocityreviews. Из основных можно выделить:\n",
      "\n",
      "\n",
      "Плохая «встроенность» внутреннего интерпретора у python.el.\n",
      "Отсутствие обозревателя классов у python.el.\n",
      "«Совсем не тот look&feel как у старого доброго python-mode.el» — сообщество придирчиво :)\n",
      "\n",
      "\r\n",
      "Собственно различий не так уж и много и сразу они не будут заметны — нужно какое-то время поработать там и там, чтобы их обнаружить. Но я все-таки поставил python-mode.el: sudo aptitude install python-mode и делов — даже .emacs править не надо.\n",
      "\r\n",
      "Итак, считаем, что режим выбрали, переходим дальше после пары скриншотов Emacs в Python-Mode:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Наблюдательные, наверное, заметили, что у меня в Emacs ttf-шрифты? Об этом будет небольшой следующий пост. Также можно заметить, что ошибки подсвечиваются на лету.\n",
      "\n",
      "Подсветка ошибок на лету — pylint\r\n",
      "У Emacs есть отличный режим flymake-mode, который позволяет подсвечивать на лету все ошибки компиляции (а также предупреждения и сообщения о плохом стиле программирования). Беда в том, что по умолчанию Python не поддерживается. Исправляем это упущение:\n",
      "\r\n",
      "В консоли:\n",
      "sudo aptitude install pylint pymacs\n",
      "\r\n",
      "Эти пакеты требуются для flymake в python-mode. pylint — для проверки кода на ошибки, pymacs обеспечивает двусторонее взаимодействие между Python и EmacsLISP'ом.\n",
      "\r\n",
      "Создайте следующий скрипт в вашем PATH (я создал в ~/bin), назовите его epylint:\n",
      "\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import re\n",
      "import sys\n",
      "\n",
      "from subprocess import *\n",
      "\n",
      "p = Popen(\"pylint -f parseable -r n --disable-msg-cat=C,R %s\" %\n",
      "          sys.argv[1], shell = True, stdout = PIPE).stdout\n",
      "\n",
      "for line in p.readlines():\n",
      "    match = re.search(\"\\\\[([WE])(, (.+?))?\\\\]\", line)\n",
      "    if match:\n",
      "        kind = match.group(1)\n",
      "        func = match.group(3)\n",
      "\n",
      "        if kind == \"W\":\n",
      "            msg = \"Warning\"\n",
      "        else:\n",
      "            msg = \"Error\"\n",
      "\n",
      "        if func:\n",
      "            line = re.sub(\"\\\\[([WE])(, (.+?))?\\\\]\",\n",
      "                          \"%s (%s):\" % (msg, func), line)\n",
      "        else:\n",
      "            line = re.sub(\"\\\\[([WE])?\\\\]\", \"%s:\" % msg, line)\n",
      "\n",
      "        print line,\n",
      "\n",
      "    p.close()\n",
      "\n",
      "\r\n",
      "Данный скрипт будет проверять наши исходники на ошибки вызовом pylint'а.\n",
      "\r\n",
      "Далее, в вашем .emacs (обычно ~/.emacs, но у упорядоченных людей имеется небольшая иерархия файлов, вроде programming.el, python.el, common.el, которые подгружаются в .emacs c помощью load-file) добавляем строки:\n",
      "\n",
      "\n",
      "(when (load \"flymake\" t)\n",
      "  (defun flymake-pylint-init ()\n",
      "    (let* ((temp-file (flymake-init-create-temp-buffer-copy\n",
      "                       'flymake-create-temp-inplace))\n",
      "           (local-file (file-relative-name\n",
      "                        temp-file\n",
      "                        (file-name-directory buffer-file-name))))\n",
      "      (list \"epylint\" (list local-file))))\n",
      "\n",
      "  (add-to-list 'flymake-allowed-file-name-masks\n",
      "               '(\"\\\\.py\\\\'\" flymake-pylint-init)))\n",
      "\n",
      "(add-hook 'python-mode-hook 'flymake-mode)\n",
      "\n",
      "\r\n",
      "Последняя строчка необязательна — она включает режим flymake по умолчанию для всех новых буферов, использующих python-mode. Если ее не написать, для включения flymake нужно будет выполнять M-x flymake-mode в нужном буфере.\n",
      "\r\n",
      "Все, подсветка ошибок должна заработать.\n",
      "\n",
      "Рефакторинг и completion Python-кода\r\n",
      "Идем на SourceForge-хостинг проектов Rope и Ropemacs, качаем rope и ropemacs, устанавливаем — sudo python setup.py install — оба пакета.\n",
      "\r\n",
      "После этого нужно добавить в .emacs-файл строки:\n",
      "\n",
      "\n",
      "(require 'pymacs)\n",
      "(pymacs-load \"ropemacs\" \"rope-\")\n",
      "\n",
      "\r\n",
      "Возможно, что вы заметили в процессе, что оба пакета rope и ropemacs — обычные python-модули. А расширение функционала Emacs с помощью ropemacs происходит через pymacs — который связывает EmacsLISP и Python.\n",
      "\r\n",
      "Смотрим результат (обратите внимание на меню — там появился пункт «Rope», через который доступны большинство функций ropemacs):\n",
      "\r\n",
      "Completion (срабатывает по нажатию M-/):\n",
      "\n",
      "\r\n",
      "Рефакторинг (на примере Rename):\n",
      "\n",
      "\r\n",
      "На самом деле это не лучший completion, который можно получить в Emacs для Python, но давайте в следующий раз :) Обещаю рассказать про свои дальнейшие изыскания.\n",
      "\n",
      "Для новичков в Emacs\r\n",
      "Я привык к Emacs'у, его сочетаниям клавиш и общей парадигме — поэтому со всем тем, что у получилось в итоге, мне работать в удовольствие — пальцы летают по клавиатуре, о мыши совсем забыл, минибуфер рулит… Однако у меня есть серьезные опасения, что у тех, кто Emacs'ом пока не пользовался, но хотел бы, возникнут серьезные проблемы — это как после Windows пересесть на Linux, очень похожие ощущения. Кнопки странные, курсор двигать слишком мудрено, какие-то буферы, окна… Именно для таких читателей пишу — если правда хочется попробовать Emacs (не зря же о нем столько разговоров) — пройдите сначала Emacs tutorial, потом поиграйте с ним с недельку по вечерам и уже потом запускайте его в свой рабочий процесс — отвыкнуть будет сложно. Я сам скептически относился к этому «редактору» полгода назад — а сейчас он у меня работает от включения до выключения компьютера, заменяя с десяток программ (органайзер, дневник, IDE для разных языков, просто редактор, jabber и IRC-клиент, средства для написания и публикования документации в html и не только, файловый менеджер) — причем он не навязывается, он просто удобней их.\n",
      "\r\n",
      "Всем спасибо за внимание, надеюсь, информация будет полезной для вас. И да — это моя первая публикация на хабре, если есть оплошности — пишите в комментариях, учту на будущее.\n",
      "\n",
      "Источники и полезные ссылки (многие из EmacsWiki по Python-Mode)\n",
      "EmacsWiki по Python-Mode\n",
      "Выбор между python.el и python-mode.el\n",
      "Bazaar-репозитарий проекта python-mode.el\n",
      "Включение completion в python-mode\n",
      "Использование C-c! для запуска ipython вместо python\n",
      "Нажимаем F1 для проверки кода pylint'ом\n",
      "Отличный code completion и рефакторинг python-кода\n",
      "Отладка python-кода в Emacs: PdbNotes\n",
      "Python completion с pycomplete\n",
      "И еще про completion\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем доброго {daytime}!\n",
      "\r\n",
      "Сегодня пришла пора рассказать вам о фундаментальной проблеме перекодировки при взаимодействии проекта собранного на MS Visual C++ на платформе Windows и наиболее приятной скриптовой обвязки для языка C++, благодаря библиотеке Boost.Python, собственно написанной для языка Python.\n",
      "\r\n",
      "Вы ведь хотите использовать для вашего приложения на C++ под ОС Windows хорошую скриптовую обвязку на последней версии Python 3.x, либо вы хотите использовать для вашего приложения на Python максимально ускоренный участок кода вашего модуля, переписанный на C++. В обоих случаях, если вы знаете оба языка как минимум хорошо, вам стоит это прочитать.\n",
      "\r\n",
      "Не буду вас утомлять многочасовыми выкладками о проблеме перекодировки текста в принципе. Все мы знаем что эта проблема не новая, везде и всюду решается по разному, в большинстве случаев никак, то есть перекладывается на плечи программиста.\n",
      "\r\n",
      "В языке Python, начиная с версии 3.0 принято решение считать строками только сам текст. Не важно как сам текст закодирован, а закодирован он в Юникоде, само понятие строки навсегда оторвано от его кодировки. То есть нет никакой возможности понять какое число соответствует символу в строке иначе, как перекодировать его в массив байт, указав кодировку.\n",
      "\"Привет!\".encode('cp1251')\n",
      "\r\n",
      "Пример выше показывает, что сама строка «Привет!» останется таковой как вы её набрали, вне зависимости от того, смотрят на неё в России, США или в Китае, на Windows, Linux или MacOS, она останется строкой «Привет!». Декодировав её в массив байт методом строки str.encode( encoding ) мы всегда получим одно и то же значение элементов массива байт, вне зависимости от того в какой точке земного шара мы находимся и какой платформой мы пользуемся. И это замечательно!\r\n",
      "Однако вернёмся на Землю. Есть такая ОС Windows…\n",
      "\r\n",
      "Вся проблема заключается в замечательной среде разработки MS Visual Studio. А более всего она замечательна тем, что все строки в C++ в ней гарантированно в кодировке кодовой страницы Windows. То есть для России все строки всегда будут в 'cp1251'. И всё бы ничего, но для вывода на веб-страницу, сохранения в XML, вывода в интернациональную БД и прочего данная кодировка не подходит. Предлагаемый Microsoft вариант со строками вида L«Привет» приемлем чуть более, но мы ведь знаем как замечательно в C++ работается с такими строками. Кроме того мы будем исходить из того, что к нам попал проект уже с кучей строк в виде cp1251. Гигабайты кода, работающие с std::string и char* и работающие с ними прекрасно: быстро и качественно.\n",
      "\r\n",
      "Если вы идёте со стороны Python в C++, просто помните, что строки Python отлично конвертируются в char* используя внутреннюю память Python, поскольку все строки в Python 3.x как минимум в UTF-8 уже хранятся и за ними зорко следят GC и счётчик ссылок. Поэтому опять же: не надо этого UCS-2 от Microsoft выдаваемого за Юникод, используйте обычные строки. Ну и кроме того, помните, что локальная для России БД вашей компании не скажет вам спасибо за удвоенный размер данных, при переходе с WIN-1251 на UTF-8, поскольку наверняка под завязку набита кириллицей.\r\n",
      "В общем проблема обозначена.\n",
      "\n",
      "Теперь решение.\r\n",
      "У вас уже наверняка есть последняя версия Python 3.x (в настоящий момент это — Python 3.3), если ещё нет, ставьте последний отсюда: www.python.org/download/releases\r\n",
      "Также у вас наверняка стоит MS Visual Studio (в настоящий момент последняя — это VS2012, но всё нижесказанное будет верно и для предыдущей версии VS2010).\r\n",
      "Для связки ваших классов на C++ с Python потребуется библиотека Boost.Python. Поставляется в составе уже почти стандартной библиотеки Boost: www.boost.org (в настоящий момент последней версией является 1.52, но проверено и верно вплоть до 1.44).\r\n",
      "К сожалению, в отличие от всего остального, Boost.Python нужно собрать. Если он у вас ещё не собран вместе с остальными библиотеками собрать только Boost.Python можно следующей командой Boost.Build (в более старых версиях через bjam):\n",
      "b2 --with-python --build-type=complete\r\n",
      "Если вы выкачали Python 3.x для x64, то нужно указать ещё и address-model=64.\r\n",
      "Более подробно в документации Boost.Build.\r\n",
      "В результате в {BoostDir}\\stage\\lib\\ у вас должна появится куча библиотек вида boost-python*. Они нам уже вот-вот понадобятся!..\n",
      "\r\n",
      "Итак собственно воспроизводим проблему. Пишем простой класс:\n",
      "    class MY_EXPORT Search\n",
      "    {\n",
      "    public:\n",
      "        static string That( const string& name );\n",
      "    };\n",
      "\r\n",
      "С вот такой реализацией его единственного метода:\n",
      "    string Search::That( const string& name )\n",
      "    {\n",
      "        if( name == \"Это Я!\" )\n",
      "            return \"Я\";\n",
      "        else\n",
      "            throw runtime_error( \"Я ничего не нашёл!\" );\n",
      "    }\n",
      "\r\n",
      "В реальности всё значительно сложнее: у вас скорее всего запись из БД с полями кириллицей, да и сами значения тоже кириллицей, и всё в кодировке Windows-1251. Но нам чтобы отладится хватит этого тестового примера. Здесь есть конвертация строк туда и обратно из С++ и даже передача исключений в Python.\n",
      "\r\n",
      "Используя Boost.Python обернём нашу маленькую библиотеку:\n",
      "BOOST_PYTHON_MODULE( my )\n",
      "{\n",
      "    class_<Search>( \"Search\" )\n",
      "        .def( \"That\", &Search::That )\n",
      "        .staticmethod( \"That\" )\n",
      "    ;\n",
      "}\n",
      "\r\n",
      "Не забываем про зависимости от Boost и исходной библиотеки в настройках проекта!\r\n",
      "Полученную библиотеку переименовываем в my.pyd (да-да, просто меняем расширение).\n",
      "\r\n",
      "Пробуем поработать с ней из Python. Можно прямо из консоли, если нет под рукой IDE вроде Eclipse+PyDev, просто импортируем и используем в две строки:\n",
      "import my\n",
      "my.Search.That( \"Это Я!\" )\n",
      "\r\n",
      "Не забываем, что это всё-таки .dll и ей наверняка требуется .dll исходной библиотеки с классом Search, кроме того новой библиотеке-обёртке потребуется .dll Boost.Python соответствующей сборки из {BoostDir}\\stage\\lib\\, например для MS VS2012 и Boost 1.52 для сборки Debug (Multi-thread DLL) это boost_python-vc110-mt-gd-1_52.dll.\r\n",
      "Если непонятно чего не хватает вашей .dll посмотрите её зависимости с помощью того же Dependency Walker: www.dependencywalker.com — просто откройте depends.exe вашу .dll с библиотекой-обёрткой.\r\n",
      "Итак вам удалось импортировать библиотеку my и выполнить my.Search.That( \"Это Я!\" )\n",
      "\n",
      "Если всё хорошо, вы увидите пришедшее исключение из С++ с пустым текстом. То есть мало того, что мы не попали в нужную ветку if, так ещё и текст исключения перекодировался не так, как мы его отправили!\n",
      "\r\n",
      "Если вы присоединитесь к процессу Python через \"Attach to process\" из MS Visual Studio, то увидите что в Search::That( const string& name ) приходит name в UTF-8. Boost.Python не знает о том в какой кодировке отдавать строку, поэтому отдаёт по умолчанию в UTF-8. \r\n",
      "Само собой наш код в Visual Studio полностью ориентирован на Windows-1251, поэтому понять что «Р­С‚Рѕ РЇ!» на самом деле «Это Я!» он также не может. Получаем разговор слепого с глухим. По той же причине не видно текста исключения пришедшего из C++ в Python.\n",
      "\n",
      "Ну что же, будем исправлять.\r\n",
      "Первое что приходит в голову: унаследовать/завернуть исходный класс в другой, который умеет перекодировать.\r\n",
      "Ага, теперь посмотрим на остальные классы, сиротливо шаркающих ножкой в ожидании своей очереди. Вы готовы потратить полжизни? Даже если это не так, первые же замеры производительности покажут насколько вы не правы оборачивая потомков. Ну и в конце у вас будут адовые проблемы при попытке достать обёрнутые классы обратно в объекты C++. Они у вас будут, поверьте! Мы строим мост по которому будем ходить в обе стороны, и обёртки классов должны напрямую ссылаться на методы и свойства нужного класса. Смотри в сторону extract<T&>(obj) из boost::python на стороне C++.\n",
      "\r\n",
      "Анализируем всё, что делается в Boost.Python когда строка путешествует между C++ и Python. Видим несколько замечательных мест в которых используются функции PyUnicode_AsString и PyUnicode_FromString. Немного зная родное для Python API для чистого Си (если не знаем, то читая документацию) понимаем, что это и есть корень всех зол. Boost.Python отлично различает Python 2 и 3 версий, но понять самостоятельно что строку юникода нужно преобразовать в строку закодированную кодовой страницей файловой системы он не может, однако предоставляет для этого альтернативные функции, которые предлагается использовать самостоятельно:\n",
      "\n",
      "PyUnicode_DecodeFSDefault — перекодирует строку в кодировке файловой системы (в нашем случае это как раз Windows-1251) и возвращает уже готовый объект строки, отлично подходит вместо PyUnicode_FromString в {BoostDir}\\libs\\python\\src\\ в файлах str.cpp и converter\\builtin_converters.cpp.\n",
      "\n",
      "PyUnicode_DecodeFSDefaultAndSize – то же самое, но с указанием размера строки. Подходит в качестве замены аналогичной PyUnicode_FromStringAndSize в тех же файлах.\n",
      "\n",
      "PyUnicode_EncodeFSDefault — наоборот принимает объект строки из питона и перекодирует, возвращает результат в виде массива байт (объекта PyBytes), из массива байт уже после этого можно вытянуть обычную сишную строку функцией PyBytes_AsString. Требуется для обратного преобразования вместо функции PyUnicode_AsUTF8String, а в паре \n",
      "PyBytes_AsString( PyUnicode_EncodeFSDefault(obj) ) заменяют макрос _PyUnicode_AsString( obj ) делающий фактически то же, но без переконвертации.\n",
      "\n",
      "Итак, мы вооружены до зубов, знаем врага в лицо, осталось только его найти и обезвредить!\n",
      "\r\n",
      "Нам нужны файлы использующие PyUnicode_* в коде {BoostDir}\\libs\\python\\src\\ и заголовочных файлах внутри {BoostDir}\\boost\\python\\, кроме того, открою сразу тайну, нам потребуется ещё поправить исключения в файле error.cpp.\n",
      "\r\n",
      "В общем список следующий:\n",
      "builtin_converters.cpp — правим преобразования строк из Python в С++ и обратно\n",
      "builtin_converters.hpp — надо поправить макрос преобразования в заголовочном файле\n",
      "str.cpp — правим обёртку в C++ над классом Python str (обычная строка питона в C++).\n",
      "errors.cpp – правим передачу текста исключения из C++ в Python\n",
      "\r\n",
      "Изменений немного, они точечные, все перечислены ниже, в архиве приложенном к статье лежат патчи и отчёты об изменениях, как правило все изменения не превышают одной строки кода, чаще даже одной инструкции вызова, суммарно их ровно 13 на 4 файла. Вы ведь не суеверны, нет?..\n",
      "\r\n",
      "После всех правок собираем только Boost.Python уже упомянутой выше командой:\n",
      "b2 --with-python --build-type=complete \r\n",
      "(Добавьте обязательно address-model=64 если сборка для x64, т. е. и ваш проект, и Python 3.x установленный на вашей машине собраны под 64-разрядную архитектуру адресации.)\n",
      "\r\n",
      "После того как Boost.Python собран, соберите заново свой проект с обновлённой библиотекой, обновились не только .lib и .dll, но и один заголовочный файл.\r\n",
      "Не забудьте подменить старый и унылые .dll на свежесобранные. Вы ведь наверняка не забудете их скопировать, так ведь?!\n",
      "\n",
      "Момент истины!\n",
      "import my\n",
      "res = my.Search.That( 'Это Я!' )\n",
      "print( res )\n",
      "\r\n",
      "Всё тот же код теперь возвращает то, что и ожидалось: строку 'Я'.\r\n",
      "Вполне себе кириллица, очень даже Юникод, если Python 3 считает этот объект строкой!\r\n",
      "Теперь проверим как там придёт наше исключение:\n",
      "import my\n",
      "res = my.Search.That( 'Это Я!' )\n",
      "print( res )\n",
      "try:\n",
      "    my.Search.That( 'Это кто-то другой!' )\n",
      "except RuntimeError as e:\n",
      "    print( e )\n",
      "\r\n",
      "Наше исключение приходит замечательно, с нужным текстом, в виде RuntimeError — стандартного исключения Python.\r\n",
      "Бонусом мы получили то, что на стороне C++ создавая объекты boost::python::str мы их сразу конвертируем в Юникод, что очень поможет нам когда мы на стороне C++ захотим какой-нибудь аттрибут объекта Python названного кириллицей:\n",
      "object my = import( \"my\" );\n",
      "object func = my.attr( str(\"Функция\") )\n",
      "int res = extract<int>( func( x * x ) );\n",
      "\r\n",
      "Теперь в MS Visual C++ не будет никаких проблем с таким кодом. Всё отлично выцепится, позовётся и вернёт всё что надо.\r\n",
      "Ну и раз уж речь зашла о вызове из C++ кода на Python, стоит упомянуть о том, как ловить оттуда исключения.\r\n",
      "Все исключения из Python на уровне C++ будут ловиться типом error_already_set& всё из того же boost::python. Выцепить текст, тип и стэк исключения не представляется сложным и подробно описано вот здесь: wiki.python.org/moin/boost.python/EmbeddingPython — раздел Extracting Python Exceptions. В подавляющем большинстве случаев ничего большего нежели забрать текст исключения и не понадобится, если конечно вы не придумали своей специфической логики исключений. Но в этом случае вам лучше написать свой транслятор исключений, а это уже совсем другая история…\n",
      "\n",
      "Итого\r\n",
      "Мы подружили родной код MS Visual C++ с обычным кодом Python с помощью небольшого патча Boost.Python, фактически не меняя код, просто подменив в нескольких местах вызов одних функций API питона на другие, выполняющие дополнительную перекодировку. Поскольку всё сделано через API самого Python, он сам позаботится о памяти выделяемой для объектов, никаких std::string и прочих ужасов обращения к Heap через замечательные мьютексы, которые Microsoft так старательно заложила в механизм new своей стандартной библиотеки. Нет! Ничего такого! Всё за нас сделает Python, нам лишь надо было ему немного помочь.\r\n",
      "Простые смертные всё так же могут писать код в Visual Studio не задумываясь о кодировках. Возможно даже и не зная о них. В принципе узкому специалисту в области той же транспортной части (протоколы, пакеты данных и т. п.) знать об этом не так уж и обязательно.\r\n",
      "Особо пытливые могут замерить потери на перекодировке, они разумеется есть. По моим замерам, они настолько незначительные, что однажды переписав код очень медленной генерации веб-страницы с C++ на один join+format в Python ускорил его почти на 10%. Это с учётом перекодировки с вышеприведёнными правками. Соответственно можете представить незначительность подобных потерь, если в коде на C++ просто собиралась достаточно большая строка (даже с предварительным reserve).\r\n",
      "По стабильности, уже полгода минимум как оболочка построенная на данных изменениях благополучно крутится на рабочих сайтах (правда версии Boost намного старше текущей). На сегодняшний день всё перекодируется стабильно, нареканий не вызывает, и не вызывало.\n",
      "\n",
      "Обещанный архив с изменениями\r\n",
      "Здесь собраны отчёты и патчи по изменениям в файлах библиотеки Boost.Python:\n",
      "www.2shared.com/file/NFvkxMzL/habr_py3_cxx.html\n",
      "\r\n",
      "Также прилагается бонусом маленький архив с тестовым проектом (собран под x64):\n",
      "www.2shared.com/file/FRboyHQv/pywrap.html\n",
      "\n",
      "Ссылки на полезное\r\n",
      "Ссылка на документацию Python 3. Раздел Си-API перекодировки из кодовой страницы файловой системы и обратно:\n",
      "docs.python.org/3/c-api/unicode.html?highlight=pyunicode#file-system-encoding\n",
      "\r\n",
      "Ссылка на документацию Boost.Python:\n",
      "www.boost.org/doc/libs/1_52_0/libs/python/doc    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Все более или менее знающие Python разработчики знают про такую жуткую вещь как GIL. Глобальный блокировщик всего процесса до тех пор пока Python выполняется в одном из потоков. Он даёт потоко-защищённость методами сравнимыми с садизмом, поскольку любая неявная блокировка в многопоточном приложении смерти подобна, всё что опиралось на параллельное выполнение, умирает в мучениях, раз за разом натыкаясь на блокировку GIL.\r\n",
      "Известно что по сей день из-за этого скорбного факта программисты на C++ используют Python-обёртки по большей части лишь в однопоточных приложениях, а программисты на Python пытаются всех убедить, что им и так неплохо живётся.\r\n",
      "Казалось бы, если поток порождён в C++, он не знает ни о каком GIL, используй Python без блокировок и радуйся. Радость разработчика однако закончится уже на втором потоке запросившем область глобальных переменных без блокировки.\r\n",
      "Однако есть путь ведущий к светлому будущему!\r\n",
      "Этот путь был изначально в таком языке как Perl, он же поддерживается в Си-API языка Python и я ума не приложу почему подобный механизм не включен в один из стандартных модулей Python! Способ по сути сводит использование различных под-интерпретаторов Python в разных потоках, причём используя свой GIL для каждого(!!!) без всякого шаманства и магии, просто последовательно вызвав несколько функций и стандартного набора Си-API языка Python!\n",
      "\n",
      "Честная многопоточность в Python\r\n",
      "Всё нижеперечисленное основывается на новом GIL, введённом в Python 3.2, отлажено и работает на Python 3.3. Однако для более ранних версий, том же Python 2.7, предлагается использовать то же API, само поведение GIL не так важно, как его запуск из порождённого интерпретатора.\r\n",
      "Итак начнём, нам потребуется основной поток, в котором мы просто всё проинициализируем и запустим некоторое количество потоков, работающих с различным функционалом Python, как родным питоновским, так и написанном на C++. Всё будем делать из C++, будем работать с библиотеками boost::python и boost::thread. Если у вас ещё нет библиотеки BOOST, либо вы используете чистый Си вместо C++, это не страшно, основная работа здесь идёт на Си, а BOOST используется лишь для наглядности и простоты разработки, всё то же самое можно сделать на чистом Си, используя API Python и API ОС для работы с потоками.\r\n",
      "В начале работы с Python нужно проинициализировать интерпретатор, включить механизм GIL и разрешить многопоточность в GIL, сохранив состояние главного потока:\n",
      "        Py_Initialize();        // инициализация интерпретатора Python \n",
      "        PyEval_InitThreads();   // инициализация потоков в Python и механизма GIL\n",
      "\n",
      "        mGilState = PyGILState_Ensure();     // забираем себе GIL сразу для настройки многопоточности\n",
      "        mThreadState = PyEval_SaveThread();  // сохраняем состояние главного потока и отпускаем GIL\n",
      "\n",
      "        // здесь GIL разблокирован для основного интерпретатора Python и ждёт блокировки из других потоков\n",
      "\r\n",
      "Разумеется инициализация подразумевает и освобождение ресурсов, удобнее всего завести класс с конструктором и деструктором, где деструктор восстанавливает состояние потока, освобождает GIL и завершает работу интерпретатора (включая работу под-интерпретаторов):\n",
      "        // здесь GIL должен быть разблокирован для основного интерпретатора\n",
      "\n",
      "        PyEval_RestoreThread( mThreadState );   // восстанавливаем состояние главного потока и забираем себе GIL\n",
      "        PyGILState_Release( mGilState );        // отпускаем блокировку GIL с сохранённым состоянием\n",
      "\n",
      "        Py_Finalize();  // завершает работу как основного интерпретатора, со всеми под-интерпретаторами Python\n",
      "\r\n",
      "Пока что всё очевидно для всех кто когда-либо работал с GIL из Си-API языка Python. Для основного потока всего лишь требуется выполнять роль диспетчера, не блокируя GIL и не мешая остальным потокам делать свою работу. Вот так примерно должен выглядеть класс целиком:\n",
      "class PyMainThread  // специальный класс для основного потока\n",
      "{\n",
      "public:\n",
      "    PyMainThread()  // нужно создать экземпляр класса в самом начале работы\n",
      "    {\n",
      "        Py_Initialize();        // инициализация интерпретатора Python \n",
      "        PyEval_InitThreads();   // инициализация потоков в Python и механизма GIL\n",
      "\n",
      "        mGilState = PyGILState_Ensure();     // забираем себе GIL сразу для настройки многопоточности\n",
      "        mThreadState = PyEval_SaveThread();  // сохраняем состояние главного потока и отпускаем GIL\n",
      "\n",
      "        // здесь GIL разблокирован для основного интерпретатора Python и ждёт блокировки из других потоков\n",
      "    }\n",
      "\n",
      "    ~PyMainThread() // по завершении работы нужно корректно освободит ресурсы интерпретатора\n",
      "    {\n",
      "        // здесь GIL должен быть разблокирован для основного интерпретатора\n",
      "\n",
      "        PyEval_RestoreThread( mThreadState );   // восстанавливаем состояние главного потока и забираем себе GIL\n",
      "        PyGILState_Release( mGilState );        // отпускаем блокировку GIL с сохранённым состоянием\n",
      "\n",
      "        Py_Finalize();  // завершает работу как основного интерпретатора, со всеми под-интерпретаторами Python\n",
      "    }\n",
      "\n",
      "private:\n",
      "    PyGILState_STATE mGilState;     // сохранённое состояние GIL\n",
      "    PyThreadState* mThreadState;    // сохранённое состояние основного потока\n",
      "};\n",
      "\r\n",
      "Собственно работа в функции main() или её аналоге сводится к следующей схеме:\n",
      "    PyMainThread main_thread; // начальная инициализация интерпретатора в главном потоке\n",
      "\n",
      "    boost::thread_group group;\n",
      "\n",
      "    // порождаем потоки, каждый выполняет свою работу, взаимодействуя с Python без общего GIL\n",
      "    for( int id = 1; id <= THREAD_NUM; ++id)\n",
      "        group.create_thread( ThreadWork(id) );\n",
      "\n",
      "    group.join_all();\n",
      "\r\n",
      "Всё, закончили с примитивщиной, народ жаждет магии… ах да, я обещал что её не будет.\n",
      "\n",
      "Работа в каждом потоке\r\n",
      "Если мы сейчас попробуем просто сделать в каждом порождённом потоке time.sleep(1) мы получим падение уже на втором потоке.\r\n",
      "Нас спасёт волшебная функция Py_NewInterpreter (!!!), в которой всё бы хорошо, но её использование требует блокировки GIL (!) и это было бы страшно, если бы не тот факт, что GIL приходит и уходит, а под-интерпретатор останется. И уже в нём можно блокировать его GIL сколько угодно, у него самого потоков будет ровно 1 — тот в котором его и создали:\n",
      "        mMainGilState = PyGILState_Ensure();    // забираем блокировку основного интерпретатора\n",
      "        mOldThreadState = PyThreadState_Get();  // сохраняем текущее состояние порождённого потока\n",
      "        mNewThreadState = Py_NewInterpreter();  // создаём в потоке под-интерпретатор\n",
      "        PyThreadState_Swap( mNewThreadState );  // с этого момента для потока актуален уже под-интерпретатор\n",
      "\n",
      "        mSubThreadState = PyEval_SaveThread();  // не забываем освободить предыдущую блокировку GIL\n",
      "        mSubGilState = PyGILState_Ensure();     // и заблокировать GIL уже для под-интерпретатора\n",
      "\r\n",
      "Это также лучше всего сделать в конструкторе специального класса, а деструкторе соответственно следующий код:\n",
      "        PyGILState_Release( mSubGilState );         // разблокируем GIL для под-интерпретатора\n",
      "        PyEval_RestoreThread( mSubThreadState );    // восстанавливаем блокировку и состояние потока для основного интерпретатора\n",
      "        Py_EndInterpreter( mNewThreadState );       // завершаем работу под-интерпретатора\n",
      "        PyThreadState_Swap( mOldThreadState );      // возвращаем состояние потока для основного интерпретатора\n",
      "        PyGILState_Release( mMainGilState );        // освобождаем блокировку GIL для основного интерпретатора\n",
      "\r\n",
      "Код всего класса приведён ниже:\n",
      "class PySubThread   // класс для работы в каждом порождённом потоке\n",
      "{\n",
      "public:\n",
      "    PySubThread()   // нужно для инициализации под-интерпретатора Python в самом начале работы потока\n",
      "    {\n",
      "        mMainGilState = PyGILState_Ensure();    // забираем блокировку основного интерпретатора\n",
      "        mOldThreadState = PyThreadState_Get();  // сохраняем текущее состояние порождённого потока\n",
      "        mNewThreadState = Py_NewInterpreter();  // создаём в потоке под-интерпретатор\n",
      "        PyThreadState_Swap( mNewThreadState );  // с этого момента для потока актуален уже под-интерпретатор\n",
      "\n",
      "        mSubThreadState = PyEval_SaveThread();  // не забываем освободить предыдущую блокировку GIL\n",
      "        mSubGilState = PyGILState_Ensure();     // и заблокировать GIL уже для под-интерпретатора\n",
      "    }\n",
      "\n",
      "    ~PySubThread()  // по завершении работы потока нужно корректно освободить ресурсы под-интепретатора Python\n",
      "    {\n",
      "        PyGILState_Release( mSubGilState );         // разблокируем GIL для под-интерпретатора\n",
      "        PyEval_RestoreThread( mSubThreadState );    // восстанавливаем блокировку и состояние потока для основного интерпретатора\n",
      "        Py_EndInterpreter( mNewThreadState );       // завершаем работу под-интерпретатора\n",
      "        PyThreadState_Swap( mOldThreadState );      // возвращаем состояние потока для основного интерпретатора\n",
      "        PyGILState_Release( mMainGilState );        // освобождаем блокировку GIL для основного интерпретатора\n",
      "    }\n",
      "\n",
      "private:\n",
      "    PyGILState_STATE mMainGilState;     // состояние GIL основного интерпретатора Python\n",
      "    PyThreadState* mOldThreadState;     // состояние текущего потока для основного интерпретатора\n",
      "    PyThreadState* mNewThreadState;     // состояние потока для порождённого под-интерпретатора\n",
      "    PyThreadState* mSubThreadState;     // сохранённое состояние потока при разблокировке GIL\n",
      "    PyGILState_STATE mSubGilState;      // состояние GIL для порождённого под-интерпретатора Python\n",
      "};\n",
      "\r\n",
      "Как видите работа по инициализации в каждом потоке уже не столь тревиальна и примитивна, как в основном потоке. Однако мы имеем полный PROFIT для каждого потока. Пусть в каждом отдельном под-интерпретаторе нам придётся заново импортировать модули, однако мы получаем почти полную изолированность данных Python для каждого потока!\n",
      "\n",
      "Тестируем результат\r\n",
      "Итак, давайте проверим, насколько мы правы. Давайте для полноты ощущений заведём свой модуль на Python написанный на C++ и предоставляющи функцию аналог time.sleep:\n",
      "#include <boost/python.hpp>\n",
      "#include <boost/thread.hpp>\n",
      "\n",
      "using namespace boost::python;\n",
      "using namespace boost::this_thread;\n",
      "using namespace boost::posix_time;\n",
      "\n",
      "void wait( double sec ) // функция ожидания, аналог стандарному time.sleep в Python\n",
      "{\n",
      "    int msec = static_cast<int>( sec * 1000 );  // переводим в миллисекунды\n",
      "    sleep( millisec( msec ) );                  // спим указанный в секундах период\n",
      "}\n",
      "\n",
      "BOOST_PYTHON_MODULE( ctimer ) // используем boost::python и создаём модуль ctimer\n",
      "{\n",
      "    def( \"wait\", wait, args(\"sec\") );   // в Python это будет ctimer.wait(sec)\n",
      "}\n",
      "\r\n",
      "Собираем DLL и переименовываем в модуль Python ctimer.pyd, если мы под Windows. Полученный модуль ctimer подкладываем для выполнения основного приложения. Будем использовать ctimer.wait наряду со стандартным time.sleep.\r\n",
      "Заводим класс-функтор для работы в каждом отдельном потоке:\n",
      "class ThreadWork    // тестовый класс-функтор для передачи в экземпляр boost::thread\n",
      "{\n",
      "public:\n",
      "    ThreadWork( int id )    // сохраним порядковый номер запущенного потока\n",
      "        : mID( id )\n",
      "    {\n",
      "    }\n",
      "\n",
      "    void operator () ( void )   // собственно выполняемая работа в каждом потоке\n",
      "    {\n",
      "        cout << \"Thread#\" << mID << \" <= START\" << endl;\n",
      "\n",
      "        PySubThread sub_thread; // здесь мы порождаем под-интерпретатор Python\n",
      "\n",
      "        for( int rep = 1; rep <= REPEAT_TIMES; ++rep )\n",
      "        {\n",
      "            // работаем с модулем написаном на Python\n",
      "            cout << \"Thread#\" << mID << \" <= Repeat#\" << rep << \" <= import time; time.sleep(pause)\" << endl;\n",
      "\n",
      "            object time = import( \"time\" );     // import time\n",
      "            time.attr( \"sleep\" )( PAUSE_SEC );  // time.sleep(pause)\n",
      "\n",
      "            // работаем с модулем написаном на C++\n",
      "            cout << \"Thread#\" << mID << \" <= Repeat#\" << rep << \" <= import ctimer; ctimer.wait(pause)\" << endl;\n",
      "\n",
      "            object ctimer = import( \"ctimer\" ); // import ctimer\n",
      "            ctimer.attr( \"wait\" )( PAUSE_SEC ); // ctimer.wait(pause)\n",
      "        }\n",
      "\n",
      "        cout << \"Thread#\" << mID << \" <= END\" << endl;\n",
      "\n",
      "        // здесь под-интерпретатор Python завершит свою работу\n",
      "    }\n",
      "\n",
      "private:\n",
      "    int mID;    // порядковый номер при запуске потоков\n",
      "};\n",
      "\r\n",
      "Запускаем приложение и радуемся! Потоки параллельно работают с модулями на Python, в каждом потоке отдельно болтаются маленькие под-питончики, которые заблокировали каждый свой GIL и совершенно свободно работают все вместе не мешая друг другу.\r\n",
      "Ура, товарищи!\n",
      "\r\n",
      "Ссылка на проект MSVS 2012 с исходниками (целых два .cpp файла) и собранными DLL и EXE для Python 3.3 x64 можно скачать здесь (290 KB)\n",
      "\n",
      "Полезные ссылки\n",
      "Работа с под-интерпретатором Python\n",
      "API для работы с потоками, интерпретатором и GIL\n",
      "Документация по Boost.Python\n",
      "Документация по Boost.Thread    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Данная статья не является продолжением повествования об обёртках C++ API. Никаких обёрток сегодня не будет. Хотя по логике это третья часть данного повествования.\r\n",
      "Сегодня будет море крови, расчленение существующих типов и магическое превращение их в привычные аналоги в другом языке.\r\n",
      "Речь не пойдёт о существующей конвертации между строками, нет, мы напишем свои конвертеры.\r\n",
      "Мы превратим привычный datetime.datetime питона в boost::posix_time::ptime библиотеки Boost и обратно, да чёрт с ним, мы вообще всю библиотеку datetime превратим в бустовые типы! А чтобы не было скучно, принесём в жертву встроенный класс массива байт Python 3.x, для него как раз ещё нет конвертера в Boost.Python, а потом зверски используем конвертацию массива байт в новом конвертере питоновского uuid.UUID в boost::uuids::uuid. Да, конвертер можно использовать в конвертере!\r\n",
      "Жаждешь крови, Колизей?!..\n",
      "\n",
      "Оглавление\n",
      "\n",
      "Объединяя C++ и Python. Тонкости Boost.Python. Часть первая\n",
      "Объединяя C++ и Python. Тонкости Boost.Python. Часть вторая\n",
      "Конвертация типов в Boost.Python. Делаем преобразование между привычными типами C++ и Python\n",
      "Путешествие исключений между C++ и Python или «Туда и обратно»\n",
      "\n",
      "Вместо введения\r\n",
      "Если кто не заметил, Boost.Python делает огромную работу, превращая кучу скаляров в объекты классов Python соответствующего типа. Если вы хотите сравнить, пишите на чистом Си, используйте напрямую C-API, дайте ему посношать свой мозг. Потратьте кучу времени, чтобы понять комфорт современных технологий, удобство мягкого кресла, необходимость горячей ванной и пульта дистанционного управления для телевизора. Любители деревянных лавок, мытья в проруби и лучины, пусть и дальше занимаются лубочным творчеством.\r\n",
      "Так вот, есть такое понятие: built-in converters в Boost.Python — встроенные конвертеры типов из Python в C++ и обратно, которые частично реализованы в $(BoostPath)\\libs\\python\\src\\converter и $(BoostPath)\\boost\\python\\converter. Их много, они решают где-то около 95% проблем при работе со встроенными типами Python и C++, есть конвертация строк, не идеальная конечно, но если в C++ мы работаем с UTF-8 строками или wide-строками, то всё будет быстро, качественно и незаметно, в смысле удобно в использовании.\r\n",
      "Почти всё что не делается встроенными конвертерами, решается обёртками ваших классов. Boost.Python предлагает поистине чудовищно простой путь, описывать обёртки классов, в виде мета-языка, который даже выглядит как класс Python:\n",
      "class_<Some>( \"Some\" )\n",
      "    .def( \"method_A\", &Some::method_A, args( \"x\", \"y\", \"z\" ) )\n",
      "    .def( \"method_B\", &Some::method_B, agrs( \"u\", \"v\" ) )\n",
      ";\n",
      "\r\n",
      "Всё здорово, но есть одно но…\r\n",
      "… одно большое и замечательное но: и C++, и Python — языки со своими библиотеками. В C++ \n",
      "#include <boost/date_time.hpp>\n",
      "#include <boost/uuid/uuid.hpp>\n",
      "\r\n",
      "является де-факто аналогом в Python:\n",
      "import datetime\n",
      "import uuid\n",
      "\r\n",
      "Так вот, очень многое в вашем C++ коде уже может быть завязано именно на работу например с классом boost::gregorian::date, а в Python в свою очередь многое завязано на класс datetime.date, его аналог. Работать в Python с обёрткой класса boost::gregorian::date, обёрнутому со всеми методами, перегрузкой операторов и попытка воткнуть его экземпляры вместо привычного datetime.date — это я даже не знаю как называется, это не костыль, это танцы с гранатой. И эта граната рванёт, господа присяжные заседатели. На стороне Python нужно работать со встроенной библиотекой даты и времени.\r\n",
      "Если вы читаете это, и смотрите на свой код, где вы через extract достаёте в C++ поля питоновского datetime, то нечего глупо улыбаться, всё описанное абзацем выше относится к вам в не меньше степени. Даже если у вас в C++ свой мега-класс даты/времени, то лучше написать конвертер типа, чем выцеплять их по одному полю в каком-то велосипедном методе.\r\n",
      "В общем, если на стороне Python свой тип, а на стороне С++ свой устоявшийся тип, реализующий базовую логику с аналогичной функционально составляющей, то вам нужен конвертер.\r\n",
      "Вам действительно нужен конвертер.\n",
      "\n",
      "Что такое есть конвертер\r\n",
      "Конвертер — это некое зарегистрированное в Boost.Python преобразование из типа C++ в тип Python или обратно. На стороне C++ вы пользуетесь привычными типами, в полной уверенности, что в Python это будет соответствующий тип. Собственно конвертеры обычно пишут в обе стороны, но написать преобразование из C++ в Python на порядок проще, сами увидите. Всё дело в том, что создание экземпляра в C++ требует памяти, что является зачастую нетревиальной задачей. Создание объекта в Python задача крайне простая, поэтому начнём с преобразования из C++ в Python.\n",
      "\n",
      "Конвертация типа из C++ в Python\r\n",
      "Для конвертации из C++ в Python вам потребуется структура у которой есть статический метод convert, принимающий ссылку на тип в C++ и возвращающий PyObject*, общий тип для любого объекта использующегося в C-API языка Python и в качестве начинки объекта boost::python::object.\r\n",
      "Давайте сразу заведём шаблонную структуру, поскольку мы хотим массовой бойни:\n",
      "template< typename T >\n",
      "struct type_into_python\n",
      "{\n",
      "    static PyObject* convert( T const& );\n",
      "};\n",
      "\r\n",
      "Всё что потребуется — реализовать например для типа boost::posix_time::ptime метод специализации шаблонной структуры:\n",
      "template<> PyObject* type_into_python<ptime>::convert( ptime const& ); \n",
      "\r\n",
      "и зарегистрировать конвертер при объявлении модуля внутри BOOST_PYTHON_MODULE:\n",
      "    to_python_converter< ptime, type_into_python<ptime> >();\n",
      "\r\n",
      "Ну хорошо, раз уж я сказал Аз, давайте скажу вам и Буки. Реализация конвертера для boost::posix_time::ptime будет выглядеть приблизительно вот так:\n",
      "PyObject* type_into_python<ptime>::convert( ptime const& t )\n",
      "{\n",
      "    auto d = t.date();\n",
      "    auto tod = t.time_of_day();\n",
      "    auto usec = tod.total_microseconds() % 1000000;\n",
      "    return PyDateTime_FromDateAndTime( d.year(), d.month(), d.day(), tod.hours(), tod.minutes(), tod.seconds(), usec );\n",
      "}\n",
      "\n",
      "Важно! При регистрации модуля нам обязательно нужно подключить datetime через C-API:\n",
      "    PyDateTime_IMPORT;\n",
      "    to_python_converter< ptime, type_into_python<ptime> >();\n",
      "\r\n",
      "Без строки PyDateTime_IMPORT ничего не взлетит.\n",
      "\r\n",
      "Нам в общем повезло в том, что в C-API языка Python есть готовая функция по созданию PyObject* на новый datetime.datetime по его параметрам, по сути аналог конструктора класса datetime. И не повезло, что в Boost такое «забавное» API для класса ptime. Класс получился не совсем самостоятельным, приходится выцеплять из него дату и время, находящиеся там отдельными компонентами, причём время представлено в виде time_duration — аналог не столько datetime.time, сколько скорее datetime.timedelta! Это в общем-то не позволит взаимооднозначно представить типы библиотеки datetime в C++.Ну и совсем неприятно то, что boost::posix_time::time_duration не предоставляет прямого доступа к микросекундам и миллисекундам. Вместо этого приходится либо «хитро» работать с методом fractional_seconds(), либо тупо сделать страшное — взять модуль total_microseconds() % 1000000. Что хуже — я ещё не решил, мне вообще не нравится как сделан time_duration. Мы из него за это сделаем класс datetime.time, а другой схожий класс datetime.timedelta мы пока трогать не будем.\n",
      "\n",
      "Конвертация из Python в C++\r\n",
      "Хе-хе, друзья мои, это реально сложный пункт. Запаситесь валидолом, пристегните ремни.\r\n",
      "Всё вроде бы точно так же: делаем шаблон структуры с двумя методами convertible и construct — возможность конвертации и конструктор типа в C++. Собственно всё равно как называются методы, главное сослаться на них при регистрации, удобнее всего это делать в конструкторе нашей шаблонной структуры:\n",
      "template< typename T >\n",
      "struct type_from_python\n",
      "{\n",
      "    type_from_python()\n",
      "    {\n",
      "        converter::registry::push_back( convertible, construct, type_id<T>() );\n",
      "    }\n",
      "\n",
      "    static void* convertible( PyObject* );\n",
      "    static void construct( PyObject*, converter::rvalue_from_python_stage1_data* );\n",
      "};\n",
      "\r\n",
      "Собственно при объявлении модуля будет достаточно вызвать конструктор данной структуры. Ну и, разумеется, нужно реализовать данные методы для каждого конвертируемого типа, например для ptime:\n",
      "template<> void* type_from_python<ptime>::convertible( PyObject* );\n",
      "template<> void  type_from_python<ptime>::construct( PyObject*, converter::rvalue_from_python_stage1_data* );\n",
      "\r\n",
      "Давайте посмотрим сразу на реализацию метода проверки на конвертабельность и метода конструирования ptime:\n",
      "void* type_from_python<ptime>::convertible( PyObject* obj )\n",
      "{\n",
      "    return PyDateTime_Check( obj ) ? obj : nullptr;\n",
      "}\n",
      "\n",
      "void type_from_python<ptime>::construct( PyObject* obj, converter::rvalue_from_python_stage1_data* data )\n",
      "{\n",
      "    auto storage = reinterpret_cast< converter::rvalue_from_python_storage<ptime>* >( data )->storage.bytes;\n",
      "    date date_only( PyDateTime_GET_YEAR( obj ), PyDateTime_GET_MONTH( obj ), PyDateTime_GET_DAY( obj ) );\n",
      "    time_duration time_of_day( PyDateTime_DATE_GET_HOUR( obj ), PyDateTime_DATE_GET_MINUTE( obj ), PyDateTime_DATE_GET_SECOND( obj ) );\n",
      "    time_of_day += microsec( PyDateTime_DATE_GET_MICROSECOND( obj ) );\n",
      "    new(storage) ptime( date_only, time_of_day );\n",
      "    data->convertible = storage; \n",
      "}\n",
      "\r\n",
      "С методом convertible всё ясно: ты datetime — проходи, нет — nullptr и на выход.\r\n",
      "А вот метод construct будет таким же зубодробительным для абсолютно каждого типа!\r\n",
      "Даже если у вас свой тип MyDateTime, вам придётся его создавать по месту через размещающий new там, где вам дадут его разместить! Видите вот этот забавный оператор:\n",
      "    new(storage) ptime( date_only, time_of_day );\n",
      "\r\n",
      "Это размещающий new. Он создаёт ваш новый объект в указанном месте. Это самое место нам нужно ещё вычислить, нам предлагают следующий путь получения искомого указателя:\n",
      "    auto storage = reinterpret_cast< converter::rvalue_from_python_storage<ptime>* >( data )->storage.bytes;\n",
      "\r\n",
      "Я не буду это комментировать. Просто запомните.\r\n",
      "Всё остальное — дополнительные вычисления для вызова вполне понятного конструктора несамостоятельного класса ptime.\r\n",
      "Не забудьте в конце заполнить ещё одно поле:\n",
      "    data->convertible = storage;\n",
      "\r\n",
      "Опять же не знаю как это помягче назвать, просто помните, что это важно и поле нужно заполнить. Думайте об этом как о неприятной мелочи перед всеобщим счастьем.\r\n",
      "Примеры как это делает кто-то кроме меня можно посмотреть здесь, здесь и здесь на сайте Boost.Python в разделе FAQ.\n",
      "\n",
      "Преобразование типов datetime в <boost/date_time.hpp> и обратно\r\n",
      "Итого, для date и time по отдельности всё довольно просто. Благодаря нашей шаблонной структуре нам осталось лишь добавить реализацию для date и time_duration следующих методов специализаций наших шаблонных структур:\n",
      "template<> PyObject* type_into_python<date>::convert( date const& );\n",
      "template<> void*     type_from_python<date>::convertible( PyObject* );\n",
      "template<> void      type_from_python<date>::construct( PyObject*, converter::rvalue_from_python_stage1_data* );\n",
      "\n",
      "template<> PyObject* type_into_python<time_duration>::convert( time_duration const& );\n",
      "template<> void*     type_from_python<time_duration>::convertible( PyObject* );\n",
      "template<> void      type_from_python<time_duration>::construct( PyObject*, converter::rvalue_from_python_stage1_data* );\n",
      "\r\n",
      "Задача несложная, сводится к разбиению предыдущих методов на пары для даты и времени по отдельности.\r\n",
      "Для boost::gregorian::date и datetime.date:\n",
      "PyObject* type_into_python<date>::convert( date const& d )\n",
      "{\n",
      "    return PyDate_FromDate( d.year(), d.month(), d.day() );\n",
      "}\n",
      "\n",
      "void* type_from_python<date>::convertible( PyObject* obj )\n",
      "{\n",
      "    return PyDate_Check( obj ) ? obj : nullptr;\n",
      "}\n",
      "\n",
      "void type_from_python<date>::construct( PyObject* obj, converter::rvalue_from_python_stage1_data* data )\n",
      "{\n",
      "    auto storage = reinterpret_cast< converter::rvalue_from_python_storage<date>* >( data )->storage.bytes;\n",
      "    new(storage) date( PyDateTime_GET_YEAR( obj ), PyDateTime_GET_MONTH( obj ), PyDateTime_GET_DAY( obj ) );\n",
      "    data->convertible = storage; \n",
      "}\n",
      "\r\n",
      "И для boost::posix_time::time_duration и datetime.time:\n",
      "PyObject* type_into_python<time_duration>::convert( time_duration const& t )\n",
      "{\n",
      "    auto usec = t.total_microseconds() % 1000000;\n",
      "    return PyTime_FromTime( t.hours(), t.minutes(), t.seconds(), usec );\n",
      "}\n",
      "\n",
      "void* type_from_python<time_duration>::convertible( PyObject* obj )\n",
      "{\n",
      "    return PyTime_Check( obj ) ? obj : nullptr;\n",
      "}\n",
      "\n",
      "void type_from_python<time_duration>::construct( PyObject* obj, converter::rvalue_from_python_stage1_data* data )\n",
      "{\n",
      "    auto storage = reinterpret_cast< converter::rvalue_from_python_storage<time_duration>* >( data )->storage.bytes;\n",
      "    time_duration* t = new(storage) time_duration( PyDateTime_TIME_GET_HOUR( obj ), PyDateTime_TIME_GET_MINUTE( obj ), PyDateTime_TIME_GET_SECOND( obj ) );\n",
      "    *t += microsec( PyDateTime_TIME_GET_MICROSECOND( obj ) );\n",
      "    data->convertible = storage; \n",
      "}\n",
      "\r\n",
      "Регистрация всего этого добра в нашем модуле будет выглядеть примерно так:\n",
      "BOOST_PYTHON_MODULE( ... )\n",
      "{\n",
      "    ...\n",
      "    PyDateTime_IMPORT;\n",
      "\n",
      "    to_python_converter< ptime, type_into_python<ptime> >();\n",
      "    type_from_python< ptime >();\n",
      "\n",
      "    to_python_converter< date, type_into_python<date> >();\n",
      "    type_from_python< date >();\n",
      "\n",
      "    to_python_converter< time_duration, type_into_python<time_duration> >();\n",
      "    type_from_python< time_duration >();\n",
      "    ...\n",
      "}\n",
      "\n",
      "\n",
      "Проверяем работу с конвертацией даты и времени\r\n",
      "Пора проверить в деле нашу мегаконвертацию, заведём всякие ненужные функции которые на входе принимают дату/время и на выходе тоже возвращают дату/время.\n",
      "ptime tomorrow();\n",
      "ptime day_before( ptime const& the_moment );\n",
      "\n",
      "date last_day_of_this_month();\n",
      "date year_after( date const& the_day );\n",
      "\n",
      "time_duration delta_between( ptime const& at, ptime const& to );\n",
      "time_duration plus_midday( time_duration const& the_moment );\n",
      "\r\n",
      "Объявим их в нашем модуле, чтобы вызывать из Python:\n",
      "    def( \"tomorrow\", tomorrow );\n",
      "    def( \"day_before\", day_before, args( \"moment\" ) );\n",
      "    def( \"last_day_of_this_month\", last_day_of_this_month );\n",
      "    def( \"year_after\", year_after, args( \"day\" ) );\n",
      "    def( \"delta_between\", delta_between, args( \"at\", \"to\" ) );\n",
      "    def( \"plus_midday\", plus_midday, args( \"moment\" ) );\n",
      "\r\n",
      "Путь эти наши функции делают следующее (хотя на самом деле это уже не важно, важны типы на входе/выходе):\n",
      "ptime tomorrow()\n",
      "{\n",
      "    return microsec_clock::local_time() + days( 1 );\n",
      "}\n",
      "\n",
      "ptime day_before( ptime const& that )\n",
      "{\n",
      "    return that - days( 1 );\n",
      "}\n",
      "\n",
      "date last_day_of_this_month()\n",
      "{\n",
      "    date today = day_clock::local_day();\n",
      "    date next_first_day = (today.month() == Dec) ? date( today.year() + 1, 1, 1 ) : date( today.year(), today.month() + 1, 1 );\n",
      "    return next_first_day - days( 1 );\n",
      "}\n",
      "\n",
      "date year_after( date const& the_day )\n",
      "{\n",
      "    return the_day + years( 1 );\n",
      "}\n",
      "\n",
      "time_duration delta_between( ptime const& at, ptime const& to )\n",
      "{\n",
      "    return to - at;\n",
      "}\n",
      "\n",
      "time_duration plus_midday( time_duration const& the_moment )\n",
      "{\n",
      "    return time_duration( 12, 0, 0 ) + the_moment;\n",
      "}\n",
      "\r\n",
      "В частности вот такой вот несложный скрипт (на Python 3.x):\n",
      "from someconv import *\n",
      "from datetime import *\n",
      "# test datetime.datetime <=> boost::posix_time::ptime\n",
      "t = tomorrow(); print( 'Tomorrow at same time:', t )\n",
      "for _ in range(3): t = day_before(t); print( 'Day before that moment:', t )\n",
      "# test datetime.date <=> boost::gregorian::date\n",
      "d = last_day_of_this_month(); print( 'Last day of this month:', d )\n",
      "for _ in range(3): d = year_after(d); print( 'Day before that day:', d )\n",
      "# test datetime.time <=> boost::posix_time::time_duration\n",
      "at = datetime.now()\n",
      "to = at + timedelta( seconds=12*60*60 )\n",
      "dt = delta_between( at, to )\n",
      "print( \"Delta between '{at}' and '{to}' is '{dt}'\".format( at=at, to=to, dt=dt ) )\n",
      "t0 = time( 6, 30, 0 )\n",
      "t1 = plus_midday( t0 )\n",
      "print( t0, \"plus midday is:\", t1 )\n",
      "\r\n",
      "Должен отработать корректно и завершиться примерно вот с выводом корректных дат и времён. Тестовый скрипт разумеется будет приложен. (Вывод не пишу, чтобы не палиться во сколько это было написано!)\r\n",
      "Можете в принципе не стесняться и писать свои тестовые функции, они все отработают как надо, если вы всё сделали правильно.\r\n",
      "В крайнем случае в конце выложу ссылку на проект вместе с тестовым скриптом.\n",
      "\n",
      "Байтовый массив в виде вектора байт в C++\r\n",
      "Вообще говоря приведённый ниже пример вреден чрезвычайно. Стандартный шаблон std::vector по типу с разрядностью ниже int будет крайне неэффективным. Проигрыш при копировании и, как следствие, при vector::resize() будет катастрофичен, просто потому, что копирование будет производиться поэлементно. Со всеми включенными оптимизациями это приведёт к потерям до 170% при простом копировании в сравнении с memcpy() (замерялось в Release-сборке MSVS v10). Что не особо приятно для частоиспользуемого фрагмента кода. Особенно когда копирования не видно, а иногда неявно происходит resize(). Возникают «занятные» проседания по производительности, в том смысле что будет чем заняться, отлавливая тормоза в большой системе.\n",
      "\r\n",
      "Пример ниже чисто академический, если вам где-либо нужна маниакальная оптимизация кода и вы именно за этим пишете часть кода модуля на С++. Если же вам побоку на производительность, смело можете использовать данное преобразование.\r\n",
      "Для Python 2.x данный раздел неактуален в принципе. Тогда байтовые массивы назывались строками. Куда интереснее будет почитать про работу с unicode и преобразование его в стандартную строку C++ здесь в PyWiki.\r\n",
      "Зато для Python 3.x данное преобразование позволит сократить громадный кусок кода c кучей C-API до использования обычного vector (byte — беззнаковое 8-битное целое — uint8_t).\n",
      "\r\n",
      "Итак, снова используем наши замечательные шаблонные структуры и радуемся:\n",
      "typedef uint8_t byte;\n",
      "typedef vector<byte> byte_array;\n",
      "...\n",
      "template<> PyObject* type_into_python<byte_array>::convert( byte_array const& );\n",
      "template<> void*     type_from_python<byte_array>::convertible( PyObject* );\n",
      "template<> void      type_from_python<byte_array>::construct( PyObject*, converter::rvalue_from_python_stage1_data* );\n",
      "\r\n",
      "Всё так же добавляем в объявление нашего модуля регистрацию конвертеров:\n",
      "BOOST_PYTHON_MODULE( ... )\n",
      "{\n",
      "    ...\n",
      "    to_python_converter< byte_array, type_into_python<byte_array> >();\n",
      "    type_from_python< byte_array >();\n",
      "}\n",
      "\r\n",
      "И простейшая реализация, используем просто знание C-API объекта PyBytes и работаем с методами std::vector:\n",
      "PyObject* type_into_python<byte_array>::convert( byte_array const& ba )\n",
      "{\n",
      "    const char* src = ba.empty() ? \"\" : reinterpret_cast<const char*>( &ba.front() );\n",
      "    return PyBytes_FromStringAndSize( src, ba.size() );\n",
      "}\n",
      "\n",
      "void* type_from_python<byte_array>::convertible( PyObject* obj )\n",
      "{\n",
      "    return PyBytes_Check( obj ) ? obj : nullptr;\n",
      "}\n",
      "\n",
      "void type_from_python<byte_array>::construct( PyObject* obj, converter::rvalue_from_python_stage1_data* data )\n",
      "{\n",
      "    auto storage = reinterpret_cast< converter::rvalue_from_python_storage<byte_array>* >( data )->storage.bytes;\n",
      "    byte* dest; Py_ssize_t len;\n",
      "    PyBytes_AsStringAndSize( obj, reinterpret_cast<char**>( &dest ), &len );\n",
      "    new(storage) byte_array( dest, dest + len );\n",
      "    data->convertible = storage; \n",
      "}\n",
      "\r\n",
      "Вряд ли потребуются дополнительные комментарии, за знаниями C-API объекта PyBytes отправлю вот сюда.\n",
      "\n",
      "Преобразуем uuid.UUID в boost::uuids::uuid и обратно\r\n",
      "Вы будете смеятся, но мы до того упростили себе работу, создав те два шаблона в самом начале, что опять же всё сведётся к реализации тройки методов:\n",
      "using namespace boost::uuids;\n",
      "...\n",
      "template<> PyObject* type_into_python<uuid>::convert( uuid const& );\n",
      "template<> void*     type_from_python<uuid>::convertible( PyObject* );\n",
      "template<> void      type_from_python<uuid>::construct( PyObject*, converter::rvalue_from_python_stage1_data* );\n",
      "\r\n",
      "Привычно добавляем в объявление модуля две новых строчки — регистрацию конвертации туда и обратно:\n",
      "    to_python_converter< uuid, type_into_python<uuid> >();\n",
      "    type_from_python< uuid >();\n",
      "\r\n",
      "А теперь самое интересное, C-API тут нам не поможет, скорее помешает, проще всего действовать через boost::python::import собственно модуля Python «uuid» и класса «UUID» этого же модуля.\n",
      "static object py_uuid = import( \"uuid\" );\n",
      "static object py_uuid_UUID = py_uuid.attr( \"UUID\" );\n",
      "\n",
      "PyObject* type_into_python<uuid>::convert( uuid const& u )\n",
      "{\n",
      "    return incref( py_uuid_UUID( object(), byte_array( u.data, u.data + sizeof(u.data) ) ).ptr() );\n",
      "}\n",
      "\n",
      "void* type_from_python<uuid>::convertible( PyObject* obj )\n",
      "{\n",
      "    return PyObject_IsInstance( obj, py_uuid_UUID.ptr() ) ? obj : nullptr;\n",
      "}\n",
      "\n",
      "void type_from_python<uuid>::construct( PyObject* obj, converter::rvalue_from_python_stage1_data* data )\n",
      "{\n",
      "    auto storage = reinterpret_cast< converter::rvalue_from_python_storage<uuid>* >( data )->storage.bytes;\n",
      "    byte_array ba = extract<byte_array>( object( handle<>( borrowed( obj ) ) ).attr( \"bytes\" ) );\n",
      "    uuid* res = new(storage) uuid;\n",
      "    memcpy( res->data, &ba.front(), ba.size() );\n",
      "    data->convertible = storage;\n",
      "}\n",
      "\r\n",
      "Уж извините, что использовал глобальные переменные, обычно это делается в синглтоне с Py_Initialize() и Py_Finalize() в конструкторе и деструкторе соответственно. Но раз уж тут у нас пример чисто учебный и используется пока только из Python, то можно обойтись таким быдлоподходом, ещё раз простите, но так код понятнее.\n",
      "\r\n",
      "Поскольку поведение в этих методах сильно отличается от всего вышеописанного, надо подробнее описать, что собственно происходит.\r\n",
      "В py_uuid мы сохранили объект подключенного модуля uuid из стандартной библиотеки Python.\r\n",
      "В py_uuid_UUID мы сохранили объект класса uuid.UUID. Именно сам класс как таковой. Применение скобок к данному объекту приведёт к вызову конструктора и созданию объекта данного типа. Что мы впоследствии и сделаем. Однако сам этот класс как таковой нам ещё пригодиться для метода convertible — проверки типа аргумента, является ли объект UUID'ом.\n",
      "\r\n",
      "В сторону Python из C++ всё понятно — просто вызываем конструктор, в первый параметр передаём None (дефолтный конструктор boost::python::object создаст как раз None), во второй уходит наш байтовый массив из предыдущего раздела. Если у вас Python 2.x код немного поменяется и упроститься, там достаточно передать строку и сделать вид, что это байтовый массив.\n",
      "\r\n",
      "При проверке Python-объекта на конвертабельность нам здорово помогает функция PyObject_IsInstance().\r\n",
      "Указатель PyObject* типа uuid.UUID берём с помощью метода ptr() класса boost::python::object. Вот тут нам и пригодился объект класса как таковой. По факту классы в Python такие же объекты. И это здорово. Спасибо за столь логичный и понятный язык.\n",
      "\r\n",
      "Вот код преобразования из Python в C++ уже ничего не понятно, что происходит на этой строчке:\n",
      "    byte_array ba = extract<byte_array>( object( handle<>( borrowed( obj ) ) ).attr( \"bytes\" ) );\n",
      "\r\n",
      "Здесь на самом деле всё предельно просто. Из объекта uuid.UUID пришедшего как PyObject* мы создаём полноценный boost::python::object. Обратите внимание на конструкцию handle<>( borrowed( obj ) ) — здесь очень важно не потерять вызов borrowed, иначе наш свежий object грохнет в деструкторе переданный объект.\r\n",
      "Итак, мы получили из PyObject* объект boost::python::object по ссылке на аргумент типа uuid.UUID. Берём у нашего объекта атрибут bytes, вытаскиваем из него byte_array через extract. Всё, у нас есть содержимое.\r\n",
      "Любители сделать всё через сериализацию-десериализацию могут поиспражняться через преобразование в строку и обратно. Всякий lexical_cast() им в помощь и камень на шею. Помните, что создание строк и сериализация в C++ по сути очень дорогая операция.\r\n",
      "Пользователи Python 2.x сразу же получат байты в виде строки. Такие уж раньше были строки, как и в C/C++, по сути через char*.\r\n",
      "В общем дальше всё просто, заполняем массив, уж извините за небезопасное копирование, и передаём заполненный объект обратно в C++.\n",
      "\n",
      "Проверяем работу преобразований массива байт и UUID\r\n",
      "Давайте заведём ещё несколько функций гоняющих туда-сюда наши типы между C++ и Python:\n",
      "byte_array string_to_bytes( string const& src );\n",
      "string bytes_to_string( byte_array const& src );\n",
      "\n",
      "uuid random_uuid();\n",
      "byte_array uuid_bytes( uuid const& src );\n",
      "\r\n",
      "Опишем их в нашем модуле для вызова из Python:\n",
      "BOOST_PYTHON_MODULE( someconv )\n",
      "{\n",
      "    ...\n",
      "    def( \"string_to_bytes\", string_to_bytes, args( \"src\" ) );\n",
      "    def( \"bytes_to_string\", bytes_to_string, args( \"src\" ) );\n",
      "    def( \"random_uuid\", random_uuid );\n",
      "    def( \"uuid_bytes\", uuid_bytes, args( \"src\" ) );\n",
      "    ...\n",
      "}\n",
      "\r\n",
      "Собственно поведение их не столь важно, однако давайте честно опишем их реализацию для наглядности результата:\n",
      "byte_array string_to_bytes( std::string const& src )\n",
      "{\n",
      "    return byte_array( src.begin(), src.end() );\n",
      "}\n",
      "\n",
      "string bytes_to_string( byte_array const& src )\n",
      "{\n",
      "    return string( src.begin(), src.end() );\n",
      "}\n",
      "\n",
      "uuid random_uuid()\n",
      "{\n",
      "    static random_generator gen_uuid;\n",
      "    return gen_uuid();\n",
      "}\n",
      "\n",
      "byte_array uuid_bytes( uuid const& src )\n",
      "{\n",
      "    return byte_array( src.data, src.data + sizeof(src.data) );\n",
      "}\n",
      "\r\n",
      "В общем и целом такой тестовый скрипт (на Python 3.x):\n",
      "from someconv import *\n",
      "from uuid import *\n",
      "...\n",
      "# test bytes <=> std::vector<uint8_t>\n",
      "print( bytes_to_string( b\"I_must_be_string\" ) )\n",
      "print( string_to_bytes( \"I_must_be_byte_array\" ) )\n",
      "print( bytes_to_string( \" - Привет!\".encode() ) )\n",
      "print( string_to_bytes( \" - Пока!\" ).decode() )\n",
      "print( bytes_to_string( string_to_bytes( \" - Ну пока!\" ) ) )\n",
      "# test uuid.UUID <=> boost::uuids::uuid\n",
      "u = random_uuid()\n",
      "print( 'Generated UUID (C++ module):', uuid_bytes(u) )\n",
      "print( 'Generated UUID (in Python): ', u.bytes)\n",
      "\r\n",
      "Должен корректно отработать и выдать результат что-то вроде:\n",
      "I_must_be_string\n",
      "b'I_must_be_byte_array'\n",
      " - Привет!\n",
      " - Пока!\n",
      " - Ну пока!\n",
      "Generated UUID (C++ module): b'\\xf1B\\xdb\\xa9<lL\\x9d\\x9a\\xfd\\xf3\\xe9\\x9f\\xa6\\x9aT'\n",
      "Generated UUID (in Python):  b'\\xf1B\\xdb\\xa9<lL\\x9d\\x9a\\xfd\\xf3\\xe9\\x9f\\xa6\\x9aT'\n",
      "\r\n",
      "Кстати, если вы для проверки возьмёте и удалите borrowed из ковертации UUID из Python в C++, то свалитесь ровно на последней строчке, так как объект будет уже уничтожен и не у чего будет брать свойство bytes.\n",
      "\n",
      "Итого\r\n",
      "Мы научились не только писать конвертеры, но и обобщать их, сводить трудовые затраты при их написании к минимуму и использовать один из другого. Собственно мы уже знаем что это, как этим пользоваться и где оно жизненно необходимо.\r\n",
      "Ссылка на проект лежит здесь (~207 KB). Проект MSVS v11, настроен на сборку с Python 3.3 x64.\n",
      "\n",
      "Полезные ссылки\n",
      "Документация Boost.Python\n",
      "Как написать конвертер строки\n",
      "Преобразование Unicode в Python 2.x\n",
      "Преобразование массивов между C++ и Python\n",
      "Ещё вариант конвертации даты/времени    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " В этой главе сказа про дружбу C++ и Python будет на удивление мало использования Boost.Python. Передача исключений туда и обратно является по сути слабым местом данной библиотеки. Будем обходиться родным API языка Python, а где это возможно использовать Boost.Python.\r\n",
      "Тем не менее Boost.Python создаёт окружение, в котором исключения из C++ попадают в Python в виде стандартного RuntimeError, а обратно из Python генерируется исключение C++ типа error_already_set, что означает «тебе что-то прилетело, сходи сам почитай что там». И вот здесь нам как раз будет не лишним использовать C-API языка Python, чтобы вычитать необходимую информацию об исключении и преобразовать в соответствующий класс сообразно логике приложения.\r\n",
      "К чему такие сложности? — Дело в том, что в Python, в отличие от C++, кроме текста исключения и его типа приходит ещё и traceback — стек до места возникновения исключения. Давайте немного расширим стандартный std::exception дополнительным параметром для этого stacktrace, а заодно напишем конвертер исключений туда и обратно из классов C++ в классы исключений Python.\n",
      "\n",
      "В предыдущих сериях\n",
      "1. Boost.Python. Введение. Обёртки C++ в Python.\n",
      "2. Boost.Python. Обёртка специфичных классов C++.\n",
      "3. Boost.Python. Создание конвертеров типов между C++ и Python.\n",
      "\n",
      "Введение\r\n",
      "Допустим имеется некая иерархия исключений, которые вам нужно взаимооднозначно представлять в виде соответствующего класса в C++ и в Python при обработке исключения. Это особенно актуально, если вы объединяете логику приложения на C++ и сложной скриптовой обвязки на Python, либо если пишете модуль для Python на C++ со сложной логикой. Рано или поздно мы упираемся в обработку исключения пришедшего из C++ в Python или наоборот. \n",
      "\r\n",
      "Конечно в большинстве случаев вам будет достаточно стандартного механизма Boost.Python преобразования исключения из C++ в Python в виде RuntimeException с текстом пришедшим из exception::what(). На стороне С++ нужно отлавливать исключение типа error_already_set и использовать родное API языка Python, но зато можно вычитать не только тип и текст исключения, но и traceback — фактически историю исключения.\r\n",
      "Но обо всём по порядку.\n",
      "\n",
      "Путешествие маленького исключения из C++ в Python\r\n",
      "Итак, вы написали модуль на C++ с использованием Boost.Python, подключили его в коде на Python через обычный import и используете одну из обёрток функций или методов. Допустим в коде на C++ кидается самое обычное исключение через столь же обычный throw. В коде на Python вы получите RuntimeException с текстом, полученным из exception::what() если это исключение порождено от std::exception.\r\n",
      "Если вас устраивает то, что вы кроме текста исключения ничего не получите, то можно даже больше ничего и не делать. Однако если вам нужно будет отловить исключение строго определённого класса ошибок, то нужно будет немного поработать.\n",
      "\r\n",
      "На самом деле Boost.Python предоставляет возможность зарегистрировать свою трансляцию исключения покидающего родные пенаты модуля, написанного на C++. Всё что нужно сделать: в функции объявления модуля вызвать шаблонную функцию boost::python::register_exception_translator<T,F>(F), где T — тип исключения в C++, а F — функция, принимающая ссылку на исключение данного типа и каким-то образом исполняющая свой долг по передаче исключения нужного типа во внешний код уже в Python. В общем виде примерно так:\n",
      "class error : public exception { ... };\n",
      "...\n",
      "\n",
      "void translate_error( error const& );\n",
      "...\n",
      "\n",
      "BOOST_PYTHON_MODULE( ... )\n",
      "{\n",
      "    ...\n",
      "    register_exception_translator<error>( translate_error );\n",
      "}\n",
      "\n",
      "...\n",
      "void translate_error( error const& e )\n",
      "{\n",
      "    PyErr_SetString( PyExc_Exception, e.what() );\n",
      "}\n",
      "\r\n",
      "Здесь мы использовали стандартное исключение типа Exception встроенное в Python, но можете использовать абсолютно любое исключение: стандартное, внешнее подключенное через import с получением PyObject* через object::ptr() или даже своё собственное созданное прямо тут же на месте через PyErr_NewException.\n",
      "\r\n",
      "Давайте для полноты ощущений добавим ещё пару классов, которые будут отдаваться как аналог ZeroDivisionError и ValueError и для полного счастья унаследуем их от нашего error, назовём их соответственно zero_division_error и value_error:\n",
      "class error : public exception\n",
      "{\n",
      "public:\n",
      "    error();\n",
      "    error( string const& message );\n",
      "    error( string const& message, string const& details );\n",
      "\n",
      "    virtual const char* what() const;\n",
      "\n",
      "    virtual string const& get_message() const;\n",
      "    virtual string const& get_details() const;\n",
      "\n",
      "    virtual const char* type() const;\n",
      "\n",
      "private:\n",
      "    string m_message;\n",
      "    string m_details;\n",
      "};\n",
      "\n",
      "class value_error : public error\n",
      "{\n",
      "public:\n",
      "    value_error();\n",
      "    value_error( string const& message );\n",
      "    value_error( string const& message, string const& details );\n",
      "\n",
      "    virtual const char* type() const;\n",
      "};\n",
      "\n",
      "class zero_division_error : public error\n",
      "{\n",
      "public:\n",
      "    zero_division_error();\n",
      "    zero_division_error( string const& message );\n",
      "    zero_division_error( string const& message, string const& details );\n",
      "\n",
      "    virtual const char* type() const;\n",
      "};\n",
      "\r\n",
      "Поле m_details нам потребуется на обратном пути из Python в C++, для сохранения traceback например. А метод type() понадобится для отладки чуть позже. Простая и понятная иерархия.\n",
      "\r\n",
      "Зарегистрируем для наших исключений функции-трансляторы в Python:\n",
      "void translate_error( error const& );\n",
      "void translate_value_error( value_error const& );\n",
      "void translate_zero_division_error( zero_division_error const& );\n",
      "...\n",
      "BOOST_PYTHON_MODULE( ... )\n",
      "{\n",
      "    ...\n",
      "    register_exception_translator<error>( translate_error );\n",
      "    register_exception_translator<value_error>( translate_value_error );\n",
      "    register_exception_translator<zero_division_error>( translate_zero_division_error );\n",
      "}\n",
      "...\n",
      "void translate_error( error const& e )\n",
      "{\n",
      "    PyErr_SetString( PyExc_Exception, e.what() );\n",
      "}\n",
      "\n",
      "void translate_value_error( value_error const& e )\n",
      "{\n",
      "    PyErr_SetString( PyExc_ValueError, e.what() );\n",
      "}\n",
      "\n",
      "void translate_zero_division_error( zero_division_error const& e )\n",
      "{\n",
      "    PyErr_SetString( PyExc_ZeroDivisionError, e.what() );\n",
      "}\n",
      "\r\n",
      "Отлично, осталось только завести на стороне C++ тестовые функции, которые и будут кидаться этими исключениями:\n",
      "double divide( double a, double b )\n",
      "{\n",
      "    if( abs( b ) < numeric_limits<double>::epsilon() )\n",
      "        throw zero_division_error();\n",
      "    return a / b;\n",
      "}\n",
      "\n",
      "double to_num( const char* val )\n",
      "{\n",
      "    double res;\n",
      "    if( !val || !sscanf( val, \"%LG\", &res ) )\n",
      "        throw value_error();\n",
      "    return res;\n",
      "}\n",
      "\n",
      "void test( bool val )\n",
      "{\n",
      "    if( !val )\n",
      "        throw error( \"Test failure.\", \"test\" );\n",
      "}\n",
      "\r\n",
      "Почему бы и нет, эти функции не хуже любых других и кидают ровно то, что нам надо.\r\n",
      "Оборачиваем их:\n",
      "...\n",
      "BOOST_PYTHON_MODULE( python_module )\n",
      "{\n",
      "    register_exception_translator<error>( translate_error );\n",
      "    register_exception_translator<value_error>( translate_value_error );\n",
      "    register_exception_translator<zero_division_error>( translate_zero_division_error );\n",
      "\n",
      "    def( \"divide\",  divide, args( \"a\", \"b\" ) );\n",
      "    def( \"to_num\",  to_num, args( \"val\" ) );\n",
      "    def( \"test\",    test,   args( \"val\" ) );\n",
      "}\n",
      "...\n",
      "\r\n",
      "Ну что ж, собираем наш модуль, выполняем import python_module, вызываем наши функции с нужными параметрами, получаем нужные исключения (скрипт на Python 3.x):\n",
      "import python_module as pm\n",
      "\n",
      "try:\n",
      "    res = pm.divide( 1, 0 )\n",
      "except ZeroDivisionError:\n",
      "    print( \"ZeroDivisionError - OK\" )\n",
      "except Exception as e:\n",
      "    print( \"Expected ZeroDivisionError, but exception of type '{t}' with text: '{e}'\".format(t=type(e),e=e) )\n",
      "else:\n",
      "    print( \"Expected ZeroDivisionError, but no exception raised! Result: {r}\".format(r=res) )\n",
      "\n",
      "try:\n",
      "    res = pm.to_num( 'qwe' )\n",
      "except ValueError:\n",
      "    print( \"ValueError - OK\" )\n",
      "except Exception as e:\n",
      "    print( \"Expected ValueError, but exception of type '{t}' with text: '{e}'\".format(t=type(e),e=e) )\n",
      "else:\n",
      "    print( \"Expected ValueError, but no exception raised! Result: {r}\".format(r=res) )\n",
      "\n",
      "try:\n",
      "    res = pm.test( False )\n",
      "except Exception as e:\n",
      "    if type(e) is Exception:\n",
      "        print( \"Exception - OK\" )\n",
      "    else:\n",
      "        print( \"Exception of type '{t}', expected type 'Exception', message: '{e}'\".format(t=type(e),e=e) )\n",
      "else:\n",
      "    print( \"Expected Exception, but no exception raised! Result: {r}\".format(r=res) )\n",
      "\r\n",
      "Вывод скрипта:\r\n",
      "ZeroDivisionError — OK\r\n",
      "ValueError — OK\r\n",
      "Exception — OK\r\n",
      "Пока всё хорошо. Поехали в обратную сторону.\n",
      "\n",
      "Приключения нашего исключения по пути из Python в C++\r\n",
      "Давайте выделим в отдельный проект типы исключений и тестовые функции и будем собирать из них отдельную динамически подключаемую библиотеку error_types. Модуль для Python будем собирать отдельно в проекте python_module.\r\n",
      "А теперь заведём приложение на C++ где будем ловить исключения из Python, назовём его catch_exceptions.\r\n",
      "Всё что нужно, это подключить наш модуль через import(«python_module»), потом получить доступ к функциям модуля через attr(«divide»), attr(«to_num»), attr(«test»). Будем вызывать их, они породят исключения на уровне C++ кода, пройдут в интерпретатор Python и пробросятся дальше в приложение на C++, вызвав исключение error_already_set — исключение библиотеки Boost.Python заготовленное как раз для таких случаев.\n",
      "\r\n",
      "Сам по себе объект типа error_already_set, важно просто поймать исключение. В общем случае обработка такого исключения выглядит так:\n",
      "catch( error_already_set const& )\n",
      "{\n",
      "    PyObject *exc, *val, *tb;\n",
      "    PyErr_Fetch( &exc, &val, &tb );\n",
      "    PyErr_NormalizeException( &exc, &val, &tb );\n",
      "\n",
      "    handle<> hexc(exc), hval( allow_null( val ) ), htb( allow_null( tb ) );\n",
      "\n",
      "    throw error( extract<string>( !hval ? str( hexc ) : str( hval ) ) );\n",
      "}\n",
      "\r\n",
      "Так мы получим исключение всегда одного и того же типа, но хотя бы сможем извлечь текст исключения. Но нам ведь приходит тип исключения в переменную exc, сам объект исключения в переменной val и даже объект со стеком произошедшего исключения в переменной tb. Давайте преобразуем произошедшее исключение в zero_division_error и value_error, если пришло ZeroDivisionError или ValueError соответственно.\n",
      "\r\n",
      "Стоп! Не всем всё понятно, что это за две функции, почему всё PyObject*, откуда исключения в C-API если в Си их нет, давайте подробнее.\r\n",
      "Да, в чистом Си нет исключений, зато в Python они есть и его API предоставляет возможность вытянуть информацию о произошедшем исключении. В Python C-API все значения и типы, да вообще почти всё, представляется в виде PyObject*, поэтому исключение E типа T — это пара значений типа PyObject*, добавим к этому ещё и PyObject* для traceback — сохранённого стека, где произошло исключение.\r\n",
      "Вытянуть информацию о произошедшем исключении можно функцией PyErr_Fetch, после чего информацию об исключении можно нормализовать (если не хотите работать в внутренним представлением в виде tuple) функцией PyErr_NormalizeException.\r\n",
      "После вызова пары этих функций мы заполним три значения типа PyObject*, соответственно: класс исключения, экземпляр (объект) исключения и стек (traceback) сохранённый в момент генерации исключения.\n",
      "\r\n",
      "Далее куда удобнее работать с Boost.Python, оборачиваем PyObject* в boost::python::handle<>, который совместим с любым объектом библиотеки Boost.Python, нам как раз нужен boost::python::str. После преобразования к аналогу строки питона в Boost.Python мы можем вытянуть стандартную родную строку std::string языка C++. При желании можно вытянуть и обычный const char*.\n",
      "\r\n",
      "С первыми двумя параметрами всё понятно, они прекрасно приводятся к строке, а вот с traceback ещё придётся преобразовать к читабельному виду. Проще всего это сделать при помощи модуля traceback, передав наши три параметра в функцию format_exception. Функция traceback.format_exception( exc, val, tb ) вернёт нам массив строк в виде стандартного list языка Python, который замечательно джойнится в одну большую толстую строку. \r\n",
      "На стороне C++, используя Boost.Python, это будет выглядеть примерно так:\n",
      "    ...\n",
      "    format_exception = import( \"traceback\" ).attr( \"format_exception\" );\n",
      "    return extract<string>( str( \"\" ).join( format_exception( exc, val, tb ) ) );\n",
      "}\n",
      "\r\n",
      "Можно сделать вспомогательную функцию для генерации строки из исключения. Проблема однако в том, что вызов import() в функции каждый раз будет приводить к недешёвому вызову, поэтому объект получаемый из import( «traceback» ).attr( «format_exception» ) лучше всего сохранить результат функции в отдельный object, также нам понадобится сохранить результат import(«python_module»). Учитывая то, что это потребуется сделать где-то между Py_Initialize() и Py_Finalize(), то ничего лучше полей синглтона для хранения таких переменных в голову не приходит.\n",
      "\n",
      "Работа с Python API через синглтон\r\n",
      "Итак, давайте заведём синглтон, это усложнит приложение, но несколько упростит код и позволит корректно инициализировать работу с интерпретатором, сохранить все вспомогательные объекты и корректно всё завершить:\n",
      "class python_interpreter\n",
      "{\n",
      "public:\n",
      "    static double divide( double, double );\n",
      "    static double to_num( string const& );\n",
      "    static void   test( bool );\n",
      "\n",
      "    static string format_error( handle<> const&, handle<> const&, handle<> const& );\n",
      "\n",
      "private:\n",
      "    object m_python_module;\n",
      "    object m_format_exception;\n",
      "\n",
      "    python_interpreter();\n",
      "    ~python_interpreter();\n",
      "\n",
      "    static python_interpreter& instance();\n",
      "\n",
      "    object& python_module();\n",
      "    string  format_error( object const&, object const&, object const& );\n",
      "};\n",
      "\r\n",
      "Конструктор будет инициализировать работу с интерпретатором, а деструктор зачищать сохранённые поля и де-инициализировать работу с интерпретатором, методы python_module и format_error импортируют соответствующие модули лишь однажды:\n",
      "python_interpreter::python_interpreter()\n",
      "{\n",
      "    Py_Initialize();\n",
      "}\n",
      "\n",
      "python_interpreter::~python_interpreter()\n",
      "{\n",
      "    m_python_module = object();\n",
      "    m_format_exception = object();\n",
      "\n",
      "    Py_Finalize();\n",
      "}\n",
      "\n",
      "double python_interpreter::divide( double a, double b )\n",
      "{\n",
      "    return extract<double>( instance().python_module().attr(\"divide\")( a, b ) );\n",
      "}\n",
      "\n",
      "double python_interpreter::to_num( string const& val )\n",
      "{\n",
      "    return extract<double>( instance().python_module().attr(\"to_num\")( val ) );\n",
      "}\n",
      "\n",
      "void python_interpreter::test( bool val )\n",
      "{\n",
      "    instance().python_module().attr(\"test\")( val );\n",
      "}\n",
      "\n",
      "string python_interpreter::format_error( handle<> const& exc, handle<> const& msg, handle<> const& tb )\n",
      "{\n",
      "    return instance().format_error( object(exc), object(msg), object(tb) );\n",
      "}\n",
      "\n",
      "python_interpreter& python_interpreter::instance()\n",
      "{\n",
      "    static python_interpreter single;\n",
      "    return single;\n",
      "}\n",
      "\n",
      "object& python_interpreter::python_module()\n",
      "{\n",
      "    if( m_python_module.is_none() )\n",
      "        m_python_module = import( \"python_module\" );\n",
      "    return m_python_module;\n",
      "}\n",
      "\n",
      "string python_interpreter::format_error( object const& exc, object const& val, object const& tb )\n",
      "{\n",
      "    if( m_format_exception.is_none() )\n",
      "        m_format_exception = import( \"traceback\" ).attr( \"format_exception\" );\n",
      "    return extract<string>( str( \"\" ).join( m_format_exception( exc, val, tb ) ) );\n",
      "}\n",
      "\r\n",
      "Итого мы получили готовый механизм, применимый для любого приложения на C++, использующего Python как мощный вспомогательный функционал с кучей библиотек.\r\n",
      "Пора проверить наш механизм исключений!\n",
      "\n",
      "Проверка механизма трансляции исключений из Python в C++\r\n",
      "Заведём вспомогательную функцию:\n",
      "void rethrow_python_exception()\n",
      "{\n",
      "    PyObject *exc, *val, *tb;\n",
      "    PyErr_Fetch( &exc, &val, &tb );\n",
      "    PyErr_NormalizeException( &exc, &val, &tb );\n",
      "\n",
      "    handle<> hexc(exc), hval( allow_null( val ) ), htb( allow_null( tb ) );\n",
      "\n",
      "    string message, details;\n",
      "    message = extract<string>( !hval ? str( hexc ) : str( hval ) );\n",
      "    details = !tb ? extract<string>( str( hexc ) ) : python_interpreter::format_error( hexc, hval, htb );\n",
      "\n",
      "    if( PyObject_IsSubclass( exc, PyExc_ZeroDivisionError ) )\n",
      "        throw zero_division_error( message, details );\n",
      "    else if( PyObject_IsSubclass( exc, PyExc_ValueError ) )\n",
      "        throw value_error( message, details );\n",
      "    else\n",
      "        throw error( message, details );\n",
      "}\n",
      "\r\n",
      "Тогда механизм обработки исключений сведётся к следующей схеме для каждого тестируемого метода как например для divide:\n",
      "        try\n",
      "        {\n",
      "            try\n",
      "            {\n",
      "                python_interpreter::divide( 1, 0 );\n",
      "            }\n",
      "            catch( error_already_set const& )\n",
      "            {\n",
      "                rethrow_python_exception();\n",
      "            }\n",
      "        }\n",
      "        catch( error const& e )\n",
      "        {\n",
      "            output_error( e );\n",
      "        }\n",
      "\r\n",
      "Здесь output_error простейшая функция, которая выводит информацию об исключении, например вот так:\n",
      "void output_error( error const& e )\n",
      "{\n",
      "    cerr << \"\\nError type: \" << e.type() << \"\\nMessage: \" << e.get_message() << \"\\nDetails: \" << e.get_details() << endl;\n",
      "}\n",
      "\r\n",
      "Здесь как раз нам пригодился виртуальный метод type() который мы завели в базовом классе error.\n",
      "\r\n",
      "Заводим подобные секции для to_num и для test, а ещё проверим что придёт если просто выполнить в Python строку «1 / 0» через exec:\n",
      "        try\n",
      "        {\n",
      "            try\n",
      "            {\n",
      "                exec( \"1 / 0\" );\n",
      "            }\n",
      "            catch( error_already_set const& )\n",
      "            {\n",
      "                rethrow_python_exception();\n",
      "            }\n",
      "        }\n",
      "        catch( error const& e )\n",
      "        {\n",
      "            output_error( e );\n",
      "        }\n",
      "\r\n",
      "Запускаем…\r\n",
      "Вывод должен быть примерно таким:\n",
      "\r\n",
      "Error type: zero_division_error\r\n",
      "Message: Division by zero!\r\n",
      "Details: <class 'ZeroDivisionError'>\n",
      "\r\n",
      "Error type: value_error\r\n",
      "Message: Inappropriate value!\r\n",
      "Details: <class 'ValueError'>\n",
      "\r\n",
      "Error type: error\r\n",
      "Message: Test failure.\r\n",
      "Details: <class 'Exception'>\n",
      "\r\n",
      "Error type: zero_division_error\r\n",
      "Message: division by zero\r\n",
      "Details: Traceback (most recent call last):\r\n",
      " File \"\", line 1, in ZeroDivisionError: division by zero\n",
      "\n",
      "Эпилог\r\n",
      "Итого: мы получили механизм взаимнооднозначного преобразования исключений из Python в C++ и обратно.\r\n",
      "Минус заметен сразу же — это разные абсолютно не связанные между собой сущности. Это связано с тем, что С++ класс не может быть унаследован от класса из Python, равно как и наоборот. Есть вариант с «инкапсуляцией» нужного класса исключения в обёртке класса C++ для Python, но это всё так же будут разные классы, которые просто преобразуются в соответствующие каждый в своём языке.\r\n",
      "Если у вас сложная иерархия исключений в C++, наиболее простой способ, завести себе аналог в Python в отдельном .py модуле, потому как создавать через PyErr_NewException, а затем где-то хранить довольно накладно и не добавит читаемости коду.\r\n",
      "Не знаю как вы, а я с нетерпением жду, когда Boost.Python обзаведётся приличным транслятором исключений, или хотя бы аналогом boost::python::bases для наследования обёртки от класса Python. В целом Boost.Python отличная библиотека, но данный аспект прибавляет геммороя при разборе исключений из Python на стороне C++. Трансляция в Python через регистрацию функции-транслятора register_exception_translator<E,F>(F) выглядит вполне удачной и позволяет преобразовать исключение типа A в C++, в совершенно другой класс B на стороне Python, но хотя бы автоматически.\r\n",
      "В принципе не обязательно реагировать на error_already_set именно так как описано выше, можете сами выбрать себе рецептуру поведения вашего приложения, используя Python API для обработки исключений из Python.\n",
      "\r\n",
      "Ссылка на проект находится здесь (~223 KB). Проект MSVS v11 настроен на сборку с Boost 1.52 и Python 3.3 x64.\n",
      "\n",
      "Полезные ссылки\n",
      "PyWiki — Извлечение исключения на стороне C++\n",
      "Обработка исключений средствами Python C-API\n",
      "Регистрация транслятора исключений из C++ в Python    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Иногда полезно держать несколько версий python на одной машине. Допустим для разработки двух проектов нам необходима вторая и третья ветка python. Или вы поддерживаете проект который использует старую версию python.\n",
      "\n",
      "Обычно для этого мы используем виртуальное окружение virtualenv или же обертку для него virtualenvwrapper. Об этом я рассказывать не буду, так как есть уже много подобных статей, да и в документациях к самим утилитам все очень хорошо объяснено. Достаточно только забить virtualenv или virtualenvwrapper в поисковик.\n",
      "Но в дополнение к ним я хочу рассказать в этой статье про менеджер версий python. Кому любопытно прошу под кат.\n",
      "\n",
      "Чтобы использовать несколько версий python, можно установить их вручную или воспользоваться менеджер версий. Таких есть два: pythonbrew(который более не развивается) и pyenv. Оба менеджера не поддерживают windows(pythonbrew, pyenv) так что питонистам пишущим на этой платформе, придется пока разруливать все руками, либо сделать свою утилиту для смены путей до нужных версий. Кто как справляется с данной ситуацией можете оставлять в комментариях.\n",
      "Так как pythonbrew более не поддерживается в этой статье он рассмотрен не будет.\n",
      "\n",
      "P.S. В статье приведены примеры проверенные для OS Ubuntu 12.04. При попытке повторить их, делайте поправки относительно своего дистрибутива.\n",
      "\n",
      "Ручной способ\n",
      "Для того чтобы работать с несколькими версиями питона, можно установить необходимые версии в указанный префикс. Например чтобы не мудрить с правами, установим дополнительно 2 версии python(2.7.6 и 3.3.2) в директорию пользователю:\n",
      "2.7.6\n",
      "$ mkdir -p ~/python/src/ && cd ~/python/src/\n",
      "$ wget http://www.python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz\n",
      "$ tar -xf ~/python/src/Python-2.7.6.tar.xz && cd ./Python-2.7.6\n",
      "$ ./configure --prefix=$HOME/python/2.7.6/\n",
      "$ make && make install\n",
      "\n",
      "для 3.3.2 делаем аналогичные операции:\n",
      "$ wget http://www.python.org/ftp/python/3.3.2/Python-3.3.2.tar.xz ~/python/src/\n",
      "$ tar -xf ~/python/src/Python-3.3.2.tar.xz && cd ./Python-3.3.2\n",
      "$ ./configure --prefix=$HOME/python/3.3.2/\n",
      "$ make && make install\n",
      "\n",
      "Теперь можно создать виртуальное окружение чтобы использовать эти версии:\n",
      "$ virtualenv -p ~/python/2.7.6/bin/python env && . ./env/bin/activate\n",
      "\n",
      "или через virtualenvwrapper:\n",
      "$ mkvirtualenv -p ~/python/2.7.6/bin/python evnwrapper\n",
      "\n",
      "Собственно на основании такого способа описана статья по созданию мультихостинга.\n",
      "Далее если вам необходимо использовать какую-то из этих версий как python по умолчанию, то вам необходимо добавить в переменную окружения путь до интерпретатора python.\n",
      "$ echo 'export PATH=~/python/2.7.6/bin/' >> ~/.bashrc\n",
      "\n",
      "Соответственно вместо bashrc вы ставите bash_profile, zshrc, kshrc, profile в зависимости от вашей командной оболочки.\n",
      "$ . ~/.bashrc\n",
      "\n",
      "И по необходимости можно установить pip, предварительно установив setuptools.\n",
      "$ wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python\n",
      "$ wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py -O - | python\n",
      "\n",
      " Фух, ну вроде бы все. А теперь о том как можно сделать это проще использую менеджер версий python.\n",
      "\n",
      "PyEnv\n",
      "В общем если вы достаточно ленивы, то можно не делать всего того что описано выше а воспользоваться утилитой pyenv, которая упростит вам данное взаимодействие с окружением и путями.\n",
      "\n",
      "Так в чем же заключается особенность этой утилиты? Вот что она может со слов автора проекта:\n",
      "\n",
      "Let you change the global Python version on a per-user basis.\n",
      "Provide support for per-project Python versions.\n",
      "Allow you to override the Python version with an environment variable.\n",
      "Search commands from multiple versions of Python at a time. This may be helpful to test across Python versions with tox.\n",
      "\n",
      "По умолчанию все версии Python будут доступны в ~/.pyenv/versions/. Изменять версии Python можно как в глобальном контексте так и в локальном(например под конкретный проект).\n",
      "\n",
      "Как ставить pyenv хорошо описывается в инструкции. Так же у автора есть скрипт который по мимо самой pyenv ставит еще и дополнительные плагины, в том числе и для virtualenv. Есть возможность установить плагин и для virtualenvwrapper.\n",
      "\n",
      "Перед установкой необходимо поставить некоторые зависимости:\n",
      "# apt-get install make libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev\n",
      "\n",
      "Прежде чем начать установку, убедитесь, что у вас установлен git:\n",
      "# apt-get install git\n",
      "\n",
      "Далее устанавливаем по инструкции:\n",
      "$ git clone git://github.com/yyuu/pyenv.git ~/.pyenv\n",
      "\n",
      "Или так:\n",
      "$ curl https://raw.github.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash\n",
      "\n",
      "Во втором случае установка произойдет с дополнительными плагинами.\n",
      "Далее, для того чтобы все заработало, дополним наш bashrc и перезагрузим оболочку:\n",
      "$ echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc\n",
      "$ echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc\n",
      "$ echo 'eval \"$(pyenv init -)\"' >> ~/.bashrc\n",
      "$ . ~/.bashrc\n",
      "\n",
      "Для обновления утилиты или смены ее версии используем git.\n",
      "ИнструкцияДля управления версиями pyenv необходимо перейти в директорию с утилитой:\n",
      "$ cd ~/.pyenv\n",
      "\n",
      "Для просмотра доступных версий:\n",
      "$ git tag\n",
      "\n",
      "для смены версии\n",
      "$ git checkout <version>\n",
      "\n",
      "для обновления\n",
      "$ git pull\n",
      "\n",
      "\n",
      "Пример использования\n",
      "~ $ pyenv install 2.7.5\n",
      "~ $ pyenv install 3.3.2\n",
      "~ $ pyenv rehash\n",
      "~ $ pyenv versions\n",
      "* system\n",
      "  2.7.5\n",
      "  3.3.2\n",
      "~ $ pyenv global 2.7.5\n",
      "~ $ python --version\n",
      "Python 2.7.5\n",
      "~ $ cd projects/\n",
      "~/projects $ pyenv local 3.3.2\n",
      "~/projects $ python --version\n",
      "Python 3.3.2\n",
      "~/projects $ cd test_prj/\n",
      "~/projects/test_prj $ python --version\n",
      "Python 3.3.2\n",
      "~/projects/test_prj $ cd ..\n",
      "~/projects $ pyenv local --unset\n",
      "~/projects $ python --version\n",
      "Python 2.7.5\n",
      "\n",
      "\n",
      "В добавок ко всему все довольно подробно и детально расписано у автора проекта в его репозиториях на github.\n",
      "\n",
      "Виртуальное окружение\n",
      "Все, а дальше как хотите. Если вы используете 3 ветку python то для создания виртуального окружения можно воспользоваться утилитой venv которая работает из коробки. Про это есть статья на хабре. Если вы больше привыкли к virtualenv или ее обертке virtualenvwrapper то тут есть два варианта: либо поставить плагин к pyenv, или использовать их к той версии python c которой вы работаете. Соответственно если выбрать первый вариант, то созданные вами окружения будут добавлены к вашим версиям python и доступны через команду:\n",
      "$ pyenv versions\n",
      "\n",
      "Добавить плагин легко, просто клонируем его из репозитория pyenv-virtualenv или pyenv-virtualenvwrapper:\n",
      "$ mkdir -p ~/.pyenv/plugins\n",
      "$ git clone git://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv\n",
      "$ git clone git://github.com/yyuu/pyenv-virtualenvwrapper.git ~/.pyenv/plugins/pyenv-virtualenvwrapper\n",
      "\n",
      "Пример использования можно посмотреть в документации для pyenv-virtualenv и pyenv-virtualenvwrapper.\n",
      "Все, а дальше пользуйтесь, как вам привычнее.\n",
      "\n",
      "Пример использования\n",
      "$ pyenv versions\n",
      "* system\n",
      "  2.7.5\n",
      "  3.3.2\n",
      "\n",
      "$ mkdir -p ~/test_project/prj_for_2.7.5 && cd ~/test_project/prj_for_2.7.5\n",
      "$ pyenv virtualenv 2.7.5 my-virtualenv-2.7.5\n",
      "$ pyenv local my-virtualenv-2.7.5\n",
      "$ pip install django==1.4\n",
      "$ pip freeze\n",
      "Django==1.4\n",
      "wsgiref==0.1.2\n",
      "$ python --version\n",
      "Python 2.7.5\n",
      "\n",
      "$ mkdir -p ~/test_project/test_project && cd ~/test_project/test_project\n",
      "$ pyenv virtualenv 3.3.2 my-virtualenv-3.3.2\n",
      "$ pyenv local my-virtualenv-3.3.2\n",
      "$ pip install django==1.5\n",
      "$ pip freeze\n",
      "Django==1.5\n",
      "$ python --version\n",
      "Python 3.3.2\n",
      "\n",
      "Теперь находясь в директории проекта можно запускать скрипт от нужной версии python не прилагая никаких действий. pyenv создает в директории файл .python-version который содержит в себе информацию о том какую версию python с каким окружение использовать для данного проекта.\n",
      "\n",
      "Полезные ссылки\n",
      "github.com/utahta/pythonbrew\n",
      "github.com/yyuu/pyenv\n",
      "github.com/yyuu/pyenv-installer\n",
      "github.com/yyuu/pyenv-virtualenv\n",
      "github.com/yyuu/pyenv-virtualenvwrapper\n",
      "docs.python.org/dev/library/venv.html\n",
      "www.virtualenv.org/en/latest\n",
      "virtualenvwrapper.readthedocs.org/en/latest    \n",
      " Всем известно, что мне не нравится третья версия Python и то, в каком направлении развивается этот язык программирования. За последние несколько месяцев я получил много писем с вопросами о моём видении развития Python и решил поделиться своими мыслями с сообществом, чтобы, по возможности, дать пищу для размышлений будущим разработчикам языка.\n",
      "\n",
      "Можно сказать совершенно точно: Python не является идеальным языком программирования. На мой взгляд, основные проблемы вытекают из особенностей интерпретатора и мало связаны с самим языком, однако все эти нюансы интерпретатора постепенно становятся частью самого языка, и поэтому они так важны.\n",
      "\n",
      "Я хочу начать наш разговор с одной странности интерпретатора (слоты) и закончить его самой большой ошибкой архитектуры языка. По сути, эта серия постов является исследованием решений, заложенных в архитектуре интерпретатора, и их влияния как на интерпретатор, так и на сам язык. Я считаю, что с точки зрения общего дизайна языка такие статьи будут выглядеть гораздо интереснее, чем просто высказывание мыслей по улучшению Python.\n",
      "\n",
      "Язык и реализацияЭтот раздел был добавлен мной после написания всей статьи. На мой взгляд, некоторые разработчики упускают из виду факт взаимосвязи Python как языка и CPython как интерпретатора, и считают, что они независимы друг от друга. Да, существует спецификация языка, однако во многих случаях она либо описывает работу интерпретатора, либо попросту умалчивает некоторые моменты.\n",
      "\n",
      "При таком подходе неявные детали реализации интерпретатора напрямую влияют на архитектуру языка, и даже заставляют другие реализации языка Python перенимать некоторые вещи. Так, например, PyPy ничего не знает о слотах (насколько мне известно), однако вынужден работать так, будто слоты являются его частью.\n",
      "\n",
      "Слоты (Slots)На мой взгляд, одной из самых больших проблем языка является идиотская система слотов. Я говорю не о конструкции __slots__, я имею в виду слоты внутреннего типа для специальных методов. Эти слоты являются «особенностью» языка, которую многие упускают из виду, потому что мало кому приходится с этим сталкиваться. При этом само существование слотов является самой большой проблемой языка Python.\n",
      "\n",
      "Итак, что такое слот? Это побочный эффект внутренней реализации интерпретатора. Каждый программист, пишущий на Python, знает о «магических методах», таких как __add__: эти методы начинаются и заканчиваются двумя символами подчёркивания, между которыми заключено их название. Каждый разработчик знает, что если написать в коде a + b, то интерпретатором будет вызвана функция a.__add__(b).\n",
      "\n",
      "К сожалению, это неправда.\n",
      "\n",
      "На самом деле Python так не работает. Python внутри устроен совсем не так (по крайней мере в текущей версии). Вот как работает интерпретатор:\n",
      "\n",
      "Когда создаётся объект, интерпретатор находит все дескрипторы класса и ищет магические методы, такие как __add__.\n",
      "Для каждого найденного специального метода интерпретатор помещает ссылку на дескриптор в специально отведённый слот объекта, например, магический метод __add__ связан с двумя внутренними слотами: tp_as_number->nb_add и tp_as_sequence->sq_concat.\n",
      "Когда интерпретатор хочет выполнить a + b, он вызовет что-то вроде TYPE_OF(a)->tp_as_number->nb_add(a, b) (на самом деле там всё сложнее, потому что у метода __add__ есть несколько слотов).\n",
      "\n",
      "Операция a + b должна представлять собой нечто вроде type(a).__add__(a, b), однако, как мы увидели из работы со слотами, это не совсем верно. Вы можете легко убедиться в этом сами, переопределив метод метакласса __getattribute__, и попытавшись реализовать собственный метод __add__, — вы заметите, что он никогда не будет вызван.\n",
      "\n",
      "На мой взгляд, система слотов просто абсурдна. Она представляет собой оптимизацию для работы с некоторыми типами данных (например, integer), однако не несёт абсолютно никакого смысла для других объектов.\n",
      "\n",
      "Чтобы продемонстрировать это, я написал такой бессмысленный класс (x.py):\n",
      "\n",
      "class A(object):\n",
      "    def __add__(self, other):\n",
      "        return 42\n",
      "\n",
      "Поскольку мы переопределили метод __add__, интерпретатор поместит в слот его. Но давайте проверим, насколько он быстрый? Когда мы выполним операцию a + b, мы задействуем систему слотов, и вот результаты профилирования:\n",
      "\n",
      "$ python3 -mtimeit -s 'from x import A; a = A(); b = A()' 'a + b'\n",
      "1000000 loops, best of 3: 0.256 usec per loop\n",
      "\n",
      "Если же мы выполним операцию a.__add__(b), то система слотов использована не будет, и вместо этого интерпретатор обратится к словарю экземпляра класса (где он ничего не найдёт) и, далее, к словарю самого класса, где искомый метод будет обнаружен. Вот как выглядят замеры:\n",
      "\n",
      "$ python3 -mtimeit -s 'from x import A; a = A(); b = A()' 'a.__add__(b)'\n",
      "10000000 loops, best of 3: 0.158 usec per loop\n",
      "\n",
      "Вы можете в это поверить? Вариант без использования слотов оказался быстрее варианта со слотами. Магия? Я не до конца уверен в причинах такого поведения, однако так продолжается уже давно, очень давно. По факту, классы старого типа (которые не имели слотов) работали гораздо быстрее классов нового типа и имели больше возможностей.\n",
      "\n",
      "Больше возможностей, спросите вы? Да, потому что классы старого типа могли делать так (Python 2.7):\n",
      "\n",
      ">>> original = 42\n",
      ">>> class FooProxy:\n",
      "...  def __getattr__(self, x):\n",
      "...   return getattr(original, x)\n",
      "...\n",
      ">>> proxy = FooProxy()\n",
      ">>> proxy\n",
      "42\n",
      ">>> 1 + proxy\n",
      "43\n",
      ">>> proxy + 1\n",
      "43\n",
      "\n",
      "Сегодня, несмотря на более сложную, чем в Python 2, систему типов, мы имеем меньше возможностей. Код, приведённый выше не может быть выполнен с использованием классов нового типа. На самом деле всё ещё хуже, если принять во внимание то, насколько легковесными были классы старого типа:\n",
      "\n",
      ">>> import sys\n",
      ">>> class OldStyleClass:\n",
      "...  pass\n",
      "...\n",
      ">>> class NewStyleClass(object):\n",
      "...  pass\n",
      "...\n",
      ">>> sys.getsizeof(OldStyleClass)\n",
      "104\n",
      ">>> sys.getsizeof(NewStyleClass)\n",
      "904\n",
      "\n",
      "Откуда взялась система слотов?Всё вышесказанное поднимает вопрос о том, откуда вообще взялись слоты. Насколько я могу судить, так повелось издавна. Когда изначально создавался интерпретатор Python, встроенные типы (например, строки) были реализованы как глобальные статические структуры, что повлекло за собой необходимость содержать им в себе все эти специальные методы, которые объект должен иметь. Это было до появления метода __add__ как такового. Если мы обратимся к самой ранней версии Python 1990 года, то увидим, как были реализованы объекты в то время.\n",
      "\n",
      "Вот, например, как выглядел integer:\n",
      "\n",
      "static number_methods int_as_number = {\n",
      "    intadd, /*tp_add*/\n",
      "    intsub, /*tp_subtract*/\n",
      "    intmul, /*tp_multiply*/\n",
      "    intdiv, /*tp_divide*/\n",
      "    intrem, /*tp_remainder*/\n",
      "    intpow, /*tp_power*/\n",
      "    intneg, /*tp_negate*/\n",
      "    intpos, /*tp_plus*/\n",
      "};\n",
      "\n",
      "typeobject Inttype = {\n",
      "    OB_HEAD_INIT(&Typetype)\n",
      "    0,\n",
      "    \"int\",\n",
      "    sizeof(intobject),\n",
      "    0,\n",
      "    free,       /*tp_dealloc*/\n",
      "    intprint,   /*tp_print*/\n",
      "    0,          /*tp_getattr*/\n",
      "    0,          /*tp_setattr*/\n",
      "    intcompare, /*tp_compare*/\n",
      "    intrepr,    /*tp_repr*/\n",
      "    &int_as_number, /*tp_as_number*/\n",
      "    0,          /*tp_as_sequence*/\n",
      "    0,          /*tp_as_mapping*/\n",
      "};\n",
      "\n",
      "Как мы видим, даже в самой первой версии Python уже существовал метод tp_as_number. К сожалению, некоторые старые версии Python (в частности, интерпретатор) были утеряны вследствие повреждения репозитория, поэтому обратимся к чуть более поздним версиям, чтобы увидеть, как были реализованы объекты. Так выглядел код функции add в 1993 году:\n",
      "\n",
      "static object *\n",
      "add(v, w)\n",
      "    object *v, *w;\n",
      "{\n",
      "    if (v->ob_type->tp_as_sequence != NULL)\n",
      "        return (*v->ob_type->tp_as_sequence->sq_concat)(v, w);\n",
      "    else if (v->ob_type->tp_as_number != NULL) {\n",
      "        object *x;\n",
      "        if (coerce(&v, &w) != 0)\n",
      "            return NULL;\n",
      "        x = (*v->ob_type->tp_as_number->nb_add)(v, w);\n",
      "        DECREF(v);\n",
      "        DECREF(w);\n",
      "        return x;\n",
      "    }\n",
      "    err_setstr(TypeError, \"bad operand type(s) for +\");\n",
      "    return NULL;\n",
      "}\n",
      "\n",
      "Так когда же появились методы __add__ и другие? Я думаю, что они появились в версии 1.1. Мне удалось скомпилировать Python 1.1 на OS X 10.9:$ ./python -v\n",
      "Python 1.1 (Aug 16 2014)\n",
      "Copyright 1991-1994 Stichting Mathematisch Centrum, Amsterdam\n",
      "\n",
      "Конечно, эта версия не стабильна, и не всё работает как нужно, однако получить представление о Python тех дней можно. Например, существовала огромная разница между реализацией объектов на C и на Python:\n",
      "\n",
      "$ ./python test.py\n",
      "Traceback (innermost last):\n",
      "  File \"test.py\", line 1, in ?\n",
      "    print dir(1 + 1)\n",
      "TypeError: dir() argument must have __dict__ attribute\n",
      "\n",
      "\n",
      "Мы видим, что тогда не было никакой интроспекции для встроенных типов, таких как integer. По факту метод __add__ поддерживался исключительно для пользовательских классов:\n",
      "\n",
      ">>> (1).__add__(2)\n",
      "Traceback (innermost last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "TypeError: attribute-less object\n",
      "\n",
      "Вот такое наследие досталось нам сегодня в Python. Основной принцип архитектуры объектов Python не изменился, но многие и многие годы они подвергались многочисленным доработкам, изменениям и рефакторингу.\n",
      "\n",
      "Современный PyObjectСегодня многие поспорят с утверждением, что между встроенными типами данных Python, реализованными на C, и объектами, реализованными на чистом Python, разница незначительна. В Python 2.7 эта разница особенно явно проявляется в том, что метод __repr__ предоставляется соответствующим классом class для типов, реализованных в Python, и, соответственно, type для встроенных объектов, реализованных на C. Это различие, на самом деле, указывает на размещение объекта: статически (для type) или динамически в «куче» (для class). На практике эта разница не имела никакого значения, и в Python 3 она полностью исчезла. Специальные методы размещаются в слотах и наоборот. Казалось бы, разницы между классами Python и C больше нет.\n",
      "\n",
      "Однако разница всё ещё есть, и очень заметная. Давайте разберёмся.\n",
      "\n",
      "Как известно, классы в Python «открыты». Это означает, что вы можете «заглянуть» в них, увидеть содержимое, которое в них хранится, добавить или удалить методы даже после того, как объявление класса будет выполнено. Но такая гибкость не предусмотрена для встроенных классов интерпретатора. Почему так?\n",
      "\n",
      "Нет никаких технических ограничений, чтобы добавить новый метод к, скажем, объекту dict. Причина, по которой интерпретатор не позволяет вам это сделать, имеет мало общего со здравомыслием разработчика, всё дело в том, что встроенные типы данных не располагаются в «куче». Чтобы оценить глобальные последствия этого, нужно сначала понять, как Python запускает интерпретатор.\n",
      "\n",
      "Чёртов интерпретаторЗапуск интерпретатора в Python является очень дорогим процессом. Когда вы запускаете исполняемый файл, вы приводите в действие сложный механизм, который умеет делать чуть более, чем всё. В числе прочих вещей инициализируются встроенные типы данных, механизмы импорта модулей, импортируются некоторые обязательные модули, производится работа с операционной системой для настроийки работы с сигналами и параметрами командной строки, настраивается внутреннее состояние интерпретатора и т.д. И только после окончания всех этих процессов интерпретатор запускает ваш код и завершает свою работу. Так Python работает на протяжении вот уже 25 лет.\n",
      "\n",
      "Вот как это выглядит в псевдокоде:\n",
      "\n",
      "/* вызывается единожды */\n",
      "bootstrap()\n",
      "\n",
      "/* эти три строки могут быть вызваны в цикле, если хотите */\n",
      "initialize()\n",
      "rv = run_code()\n",
      "finalize()\n",
      "\n",
      "/* вызывается единожды */\n",
      "shutdown()\n",
      "\n",
      "Проблема в том, что у интерпретатора есть огромное количество глобальных объектов, и по факту у нас есть один интерпретатор. Гораздо лучше, с точки зрения архитектуры, было ты инициализировать интерпретатор и запускать его как-то так:\n",
      "\n",
      "interpreter *iptr = make_interpreter();\n",
      "interpreter_run_code(iptr):\n",
      "finalize_interpreter(iptr);\n",
      "\n",
      "Так работают другие динамические языки программирования, например, Lua, JavaScript и т.д. Ключевая особенность в том, что у вас может быть два интерпретатора, и в этом заключается новая концепция.\n",
      "\n",
      "Кому вообще может быть нужно несколько интерпретаторов? Вы удивитесь, но даже для Python это нужно, или, по крайней мере, может быть полезно. Среди существующих примеров можно назвать приложения со встроенным Python, такие как веб-приложения на mod_python, — им определённо нужно запускаться в изолированном окружении. Да, в Python есть субинтерпретаторы, но они работают внутри основного интерпретатора, и только потому, что в Python так много всего завязано на внутреннее состояние. Самая большая часть кода для работы с внутренним состоянием Python является одновременно самой спорной: global interpreter lock (GIL). Python работает в концепции одного интерпретатора потому, что существует огромное количество данных, используемых всеми субинтерпретаторами совместно. Всем им нужна блокировка (lock) для единоличного доступа к этим данным, поэтому эта блокировка реализована в интерпретаторе. О каких данных идёт речь?\n",
      "\n",
      "Если вы посмотрите на код, приведённый выше, то увидите все эти огромные структуры, объявленные как глобальные переменные. По факту интерпретатор использует эти структуры напрямую в коде на Python с помощью макроса OB_HEAD_INIT(&Typetype), который задаёт этим структурам необходимые для работы интерпретатора заголовки. Например, там есть подсчёт количества ссылок на объект.\n",
      "\n",
      "Теперь вы видите к чему всё идёт? Эти объекты используются совместно всеми субинтерпретаторами. А теперь представьте, что мы можем изменить любой из этих объектов в коде Python: две совершенно независимые и никак не связанные между собой программы на Python, которые ничто не должно связывать, смогут влиять на состояние друг друга. Представьте себе, например, что JavaScript код во вкладке с Facebook может изменить реализацию встроенного объекта array, и во вкладке с Google эти изменения немедленно начали бы работать.\n",
      "\n",
      "Это архитектурное решение 1990 года, которое до сих пор продолжает влиять на современную версию языка.\n",
      "\n",
      "С другой стороны, неизменяемость встроенных типов в целом была благосклонно воспринята сообществом разработчиков Python, поскольку проблемы изменяемых типов данных достаточно хорошо известны на примере других языков программирования, и мы, будем откровенны, не так уж и много потеряли.\n",
      "\n",
      "Однако есть ещё кое-что.\n",
      "\n",
      "Что такое VTable?Итак, в Python встроенные (реализованные на C) типы данных практически неизменяемы. Чем же ещё они отличаются? Ещё одно различие заключается в «открытости» классов Python. Методы классов, реализованных на языке программирования Python, являются «виртуальными»: не существует «настоящей» таблицы виртуальных методов, как в C++, и все методы хранятся в словаре класса, из которого делается выборка с помощью поискового алгоритма. Последствия очевидны: когда вы наследуетесь от какого-либо объекта и переопределяете его метод, велика вероятность что и другой метод будет опосредованно затронут, потому что он вызывается в процессе.\n",
      "\n",
      "Хорошим примером являются коллекции, которые содержат удобные для работы функции. Так, словари в Python имеют два метода для получения объекта: __getitem__() и get(). Когда вы создаёте класс в Python, вы обычно реализуете один метод через другой, возвращая, например, return self.__getitem__(key) из функции get(key).\n",
      "\n",
      "Для типов, реализуемых в интерпретаторе, всё иначе. Причина, опять же, заключается в разнице между слотами и словарями. Скажем, вы хотите создать словарь в интерпретаторе, и одним из условий является переиспользование существующего кода, поэтому вы хотите вызвать __getitem__ из get. Как вы поступите?\n",
      "\n",
      "Метод Python в C — это просто функция со специфической сигнатурой, и это первая проблема. Главной задачей функции является обработка параметров из кода на Python и конвертирование их во что-то, что можно будет использовать на уровне C. Как минимум, вам нужно преобразовать аргументы вызова функции из кортежа или словаря Python (args и kwargs) в локальные переменные. Обычно делают так: сначала dict__getitem__ просто парсит аргументы, а затем происходит вызов dict_do_getitem с актуальными параметрами. Видите, что происходит? dict__getitem__ и dict_get обе вызывают dict_get, которая является внутренней статической функцией, и вы не можете ничего с этим поделать.\n",
      "\n",
      "Не существует хорошего способа обойти это ограничение, и причина заключается в системе слотов. У интерпретатора не существует нормального способа сделать вызов через vtable, и причина этому — GIL. Словарь (dict) общается с «внешним миром» через API с помощью атомарных операций, и это полностью теряет весь смысл, когда такие вызовы происходят через vtable. Почему? Да потому что такой вызов может не попасть на уровень Python, и тогда он не будет обработан через GIL, и это сразу приведёт к огромным проблемам.\n",
      "\n",
      "Представьте себе страдания при переопределении в классе, отнаследованном от словаря, внутренней функции dict_get, в которой запускается lazy import. Вы выкидываете все ваши гарантии в окно. Но опять же, быть может, нам уже давно следовало бы это сделать?\n",
      "\n",
      "ЗаключениеВ последние годы прослеживается явная тенденция усложнения языка Python. Я хотел бы увидеть обратное.\n",
      "\n",
      "Я хочу, чтобы внутренняя архитектура его интерпретатора основывалась на независимых друг от друга субинтерпретаторах с локальными базовыми типами данных, аналогично тому, как это работает в JavaScript. Это открыло бы широчайшие возможности для встраивания и многопоточности, основанной на обмене сообщениями. Процессоры больше не станут быстрее.\n",
      "\n",
      "Вместо слотов и словарей в роли vtable, давайте поэкспериментируем просто со словарями. Язык Objective-C полностью основан на обмене сообщениями, и это играет решающую роль в скорости его работы: я вижу, что обработка вызовов в Objective-C происходит гораздо быстрее, чем в Python. То, что строки являются внутренним типом в Python делает их сравнение быстрым. Я готов поспорить, что предлагаемый подход будет не хуже, и даже если это немного замедлит работу внутренних типов, в итоге должна получиться гораздо более простая архитектура, которую проще будет оптимизировать.\n",
      "\n",
      "Вам стоит изучить исходный код Python, чтобы увидеть, как много лишнего кода требуется для работы системы слотов, это просто невероятно! Я убеждён, что это была плохая идея, и нам следовало бы давно от неё отказаться. Отказ от слотов пойдёт на пользу даже PyPy, поскольку я уверен, что его авторам приходится лезть из кожи вон, чтобы их интерпретатор работал в режиме совместимости с CPython.\n",
      "\n",
      "Перевёл Dreadatour, текст читал %username%.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " На Хабре уже есть статьи о NoSQL СУБД Tarantool и о том, как его используют в Mail.Ru Group (и не только). Однако нет рецептов того, как работать с Tarantool на Python. В своей статье я хочу рассказать о том, как мы готовим Tarantool Python в своих проектах, какие проблемы и сложности при этом возникают, плюсы, минусы, подводные камни и, конечно же, «в чем фишка». Итак, обо всем по порядку.\n",
      "\n",
      "\n",
      "\r\n",
      "Tarantool представляет собой Application Server для Lua. Он умеет хранить данные на диске, обеспечивает быстрый доступ к ним. Tarantool используется в задачах с большими потоками данных в единицу времени. Если говорить о цифрах, то это десятки и сотни тысяч операций в секунду. Например, в одном из моих проектов генерируется более 80 000 запросов в секунду (выборка, вставка, обновление, удаление), при этом нагрузка равномерно распределяется по 4 серверам с 12 инстансами Tarantool. Не все современные СУБД готовы работать с такими нагрузками. Кроме того, при таком количестве данных, очень дорого ожидание выполнения запроса, поэтому сами программы должны быстро переключаться от одной задачи к другой. Для эффективной и равномерной загрузки CPU сервера (всех его ядер) как раз нужен Tarantool и асинхронные приемы в программировании.\n",
      "\n",
      "Как работает коннектор tarantool-python?\r\n",
      "Прежде чем говорить об асинхронном коде на Python, нужно хорошее понимание того, как обычный синхронный код на Python взаимодействует с Tarantool. Я буду использовать версию Tarantool 1.6 под CentOS, его установка проста и тривиальна и подробно расписана на сайте проекта, там же можно найти обширный user guide. Хочу заметить, что в последнее время, с появлением хорошей документации, стало гораздо проще разбираться в запуске и использовании инстанса Tarantool. Ещё на Хабре совсем недавно появилась полезная статья «Tarantool 1.6 — давай начнем».\n",
      "\r\n",
      "Итак, Tarantool установлен, запущен и готов к работе. Для работы с Python 2.7 берём из pypi коннектор tarantool-python:\n",
      "\n",
      "$ pip install tarantool-python\r\n",
      "Этого пока достаточно для решения наших задач. А какие они? В одном из моих проектов появилась необходимость «сложить» поток данных в Tarantool для дальнейшей их обработки, при этом размер одной пачки данных составляет примерно 1,5 КБайт. Прежде, чем приступать к решению задачи, следует хорошо изучить вопрос и провести испытания выбранных подходов и инструментов. Скрипт для тестирования производительности выглядит элементарно и пишется за пару минут:\n",
      "\n",
      "import tarantool\n",
      "import string\n",
      "\n",
      "mod_len = len(string.printable)\n",
      "data = [string.printable[it] * 1536 \n",
      "        for it in range(mod_len)]\n",
      "\n",
      "tnt = tarantool.connect(\"127.0.0.1\", 3301)\n",
      "\n",
      "for it in range(100000):\n",
      "    r = tnt.insert(\"tester\", (it, data[it % mod_len]))\n",
      "\r\n",
      "Всё просто: в цикле последовательно делаем 100 тысяч вставок в Tarantool. На моей виртуальной машине этот код выполняется в среднем за 32 секунды, то есть порядка трёх тысяч вставок в секунду. Программа несложная, и если полученной производительности хватает, то можно более ничего не делать, ведь, как известно, «преждевременная оптимизация — зло». Однако для нашего проекта этого оказалось мало, к тому же сам Tarantool может показать гораздо более лучшие результаты.\n",
      "\n",
      "Профилируем код\r\n",
      "Прежде чем предпринимать необдуманные шаги, попробуем внимательно изучить наш код и то, как он работает. Спасибо моему коллеге Dreadatour за его цикл статей о профилировании Python-кода.\n",
      "\r\n",
      "Перед запуском профайлера полезно понять, как работает программа, всё-таки лучший инструмент для профилирования — голова разработчика. Сам скрипт прост, изучать там особо нечего, попробуем «копнуть глубже». Если заглянуть в реализацию драйвера коннектора, то можно понять, что запрос упаковывается при помощи библиотеки msgpack, отправляется в сокет при помощи вызова sendall, а затем из сокета вычитывается длина ответа и сам ответ. Уже интереснее. Сколько же операций с сокетом Tarantool будет сделано в результате выполнения этого кода? В нашем случае для одного запроса tnt.insert будет сделан один вызов socket.sendall (отправили данные) и два вызова socket.recv (получили длину ответа и сам ответ). Метод «пристального взгляда» говорит, что для вставки ста тысяч записей будет сделано 200k + 100k = 300k read/write системных вызовов. И профайлер (я использовал cProfile и kcachegrind для интерпретации результатов) подтверждает наши умозаключения:\n",
      "\n",
      "\n",
      "\r\n",
      "Что можно поменять в этой схеме? В первую очередь, конечно, хочется уменьшить количество системных вызовов, то есть операций с сокетом Tarantool. Это можно сделать, сгруппировав запросы tnt.insert в «пачку», и вызывать socket.sendall для всех запросов сразу. Точно также можно вычитывать из сокета «пачку» ответов от Tarantool за один socket.recv. При обычном, классическом стиле программирования это сделать не так просто: нужен буфер для данных, задержка, чтобы накапливать данные в буфер, и еще нужно без задержек по очереди возвращать результаты запросов. А что делать, если запросов было много и вдруг стало очень мало? Снова возникнут задержки, которые мы стремимся избежать. В общем, нужен принципиально новый подход, но самое главное — хочется оставить код исходной задачи таким же простым, каким он и был изначально. На помощь для решения нашей задачи приходят асинхронные фреймворки.\n",
      "\n",
      "Gevent и Python 2.7\r\n",
      "Мне приходилось иметь дело с несколькими асинхронными фреймворками: twisted, tornado, gevent и прочие. На Хабре уже не раз поднимался вопрос сравнения и бенчмарков этих инструментов, например: раз и два.\n",
      "\r\n",
      "Мой выбор упал на gevent. Основная причина заключается в эффективности работы с I/O-операциями и простоте написания кода. Хороший туториал по использованию этой библиотеки можно найти здесь. А в этом туториале есть классический пример быстрого краулера:\n",
      "\n",
      "import time\n",
      "import gevent.monkey\n",
      "gevent.monkey.patch_socket()\n",
      "\n",
      "import gevent\n",
      "import urllib2\n",
      "import json\n",
      "\n",
      "def fetch(pid):\n",
      "    url = 'http://json-time.appspot.com/time.json'\n",
      "    response = urllib2.urlopen(url)\n",
      "    result = response.read()\n",
      "    json_result = json.loads(result)\n",
      "    return json_result['datetime']\n",
      "\n",
      "def synchronous():\n",
      "    for i in range(1,10):\n",
      "        fetch(i)\n",
      "\n",
      "def asynchronous():\n",
      "    threads = []\n",
      "    for i in range(1,10):\n",
      "        threads.append(gevent.spawn(fetch, i))\n",
      "    gevent.joinall(threads)\n",
      "\n",
      "t1 = time.time()\n",
      "synchronous()\n",
      "t2 = time.time()\n",
      "print('Sync:', t2 - t1)\n",
      "\n",
      "t1 = time.time()\n",
      "asynchronous()\n",
      "t2 = time.time()\n",
      "print('Async:', t2 - t1)\n",
      "\r\n",
      "На моей виртуальной машине для этого теста получились такие результаты:\n",
      "\n",
      "Sync: 1.529\n",
      "Async: 0.238\n",
      "\r\n",
      "Неплохой прирост производительности! Чтобы заставить работать синхронный код асинхронно с помощью gevent, понадобилось обернуть вызов фукции fetch в gevent.spawn, как бы распараллелив скачивание самих URL-ов. Потребовалось также выполнить monkey.patch_socket(), после чего все вызовы для работы сокетами становятся кооперативными. Таким образом, пока один URL скачивается и программа ждёт ответа от удалённого сервиса, движок gevent переключается на другие задачи и вместо бесполезного ожидания пытается скачивать другие доступные документы. В недрах Python все gevent threads исполняются последовательно, но за счет того, что нет ожиданий (системных вызовов wait), итоговый результат получается быстрее.\r\n",
      "Выглядит неплохо, а самое главное — такой подход очень хорошо подошел и для нашей задачи. Однако драйвер tarantool-python не умеет из коробки работать с gevent, и мне пришлось поверх него написать коннектор gtarantool.\n",
      "\n",
      "Gevent и Tarantool\r\n",
      "Коннектор gtarantool работает с gevent и Tarantool 1.6 и уже сейчас доступен на pypi:\n",
      "\n",
      "$ pip install gtarantool\r\n",
      "Тем временем, новое решение нашей задачи принимает такой вид:\n",
      "\n",
      "import gevent\n",
      "import gtarantool\n",
      "\n",
      "import string\n",
      "\n",
      "mod_len = len(string.printable)\n",
      "data = [string.printable[it] * 1536\n",
      "        for it in range(mod_len)]\n",
      "cnt = 0\n",
      "\n",
      "def insert_job(tnt):\n",
      "    global cnt\n",
      "    \n",
      "    for i in range(10000):\n",
      "        cnt += 1\n",
      "        tnt.insert(\"tester\", (cnt, data[it % mod_len]))\n",
      "\n",
      "\n",
      "tnt = gtarantool.connect(\"127.0.0.1\", 3301)\n",
      "\n",
      "jobs = [gevent.spawn(insert_job, tnt)\n",
      "        for _ in range(10)]\n",
      "\n",
      "gevent.joinall(jobs)\n",
      "\r\n",
      "Что изменилось по сравнению с синхронным кодом? Мы разделили вставку 100k записей между десятью асинхронными «зелеными» потоками, каждый из которых делает в цикле около 10k вызовов tnt.insert, и все это — через один коннект к Tarantool. Время выполнения программы сократилось до 12 секунд, что почти в 3 раза эффективнее синхронной версии, а количество вставок данных в БД выросло до 8 тысяч в секунду. Почему же такая схема работает быстрее? В чем фишка?\n",
      "\r\n",
      "Коннектор gtarantool внутри использует буфер запросов в сокет Tarantool и отдельные «зеленые потоки» чтения/записи в этот сокет. Пробуем посмотреть на результаты в профайлере (на этот раз я использовал Greenlet Profiler — это адаптированный профайлер yappi для greenlets):\n",
      "\n",
      "\r\n",
      "Анализируя результаты в kcachegrind мы видим, что количество вызовов socket.recv уменьшилось со 100k до 10k, а количество вызовов socket.send упало с 200k до 2,5k. Собственно, это и делает работу с Tarantool более эффективной: меньше тяжёлых системных вызовов за счёт более лёгких и «дешёвых» гринлетов. А самое главное и приятное то, что код исходной программы остался, фактически, «синхронным». В нём нет никаких уродливых twisted-callbacks.\n",
      "\r\n",
      "Этот подход мы успешно используем в своем проекте. В чем еще профит:\n",
      "\n",
      "Мы отказались от fork-ов. Можно использовать несколько Python-процессов, а в каждом процессе использовать один коннект gtarantool (или пул соединений).\n",
      "Внутри greenlets переключение происходит гораздо быстрее и эффективнее, чем переключение между Unix-процессами.\n",
      "Уменьшение количества процессов позволило сильно сократить потребление памяти.\n",
      "Уменьшение количества операций с Tarantool-сокетом повысило эффективность работы с самим Tarantool, он стал потреблять меньше CPU.\n",
      "\n",
      "А что же с Python 3 и Tarantool?\r\n",
      "Одно из отличий между различными асинхронными фреймворками — способность работать под Python 3. Например, gevent его не поддерживает. Более того, библиотека tarantool-python тоже не будет работать под Python 3 (ещё не успели портировать). Ну как же так?\n",
      "\r\n",
      "Путь джедая тернист. Очень хотелось сравнить асинхронную работу с tarantool из-под второй и третьей версии Python, и тогда я принял решение переписать всё на Python 3.4. После Python 2.7 было немного непривычно писать код:\n",
      "\n",
      "не работает print “foo”\n",
      "все строки – это объекты класса str\n",
      "нет типа long\n",
      "…\n",
      "\r\n",
      "Но привыкание прошло успешно, и теперь я стараюсь сразу писать код для Python 2.7 так, чтобы он без изменений работал и на Python 3.\n",
      "\r\n",
      "Коннектор tarantool-python пришлось немного доработать:\n",
      "\n",
      "StandartError заменил на Exception\n",
      "basestring заменил на str\n",
      "xrange заменил на range\n",
      "long — удалил\n",
      "\r\n",
      "Получился форк синхронного коннектора, работающий под Python 3.4. После тщательной проверки этот код, возможно, будет влит в основную ветку библиотеки, а пока установить его можно прямо с Гитхаба:\n",
      "\n",
      "$ pip install git+https://github.com/shveenkov/tarantool-python.git@for_python3.4\r\n",
      "Первые результаты бенчмарков не вызвали восторга. Обычный синхронный вариант вставки 100k записей, размером в 1,5 Кбайт стал выполняться в среднем чуть больше минуты — в два раза дольше, чем такой же код под второй версией Python! На помощь снова приходит профилирование:\n",
      "\n",
      "\r\n",
      "Ого! Ну и откуда взялись 400k вызовов socket.recv? А откуда 200k вызовов socket.sendall? Пришлось снова погрузиться в код коннектора tarantool-python: как оказалось, это результат работы Python-строк и байт в качестве ключей dict. Для примера можно сравнить такой код:\n",
      "\r\n",
      "Python 3.4:\n",
      ">>> a=dict()\n",
      ">>> a[b\"key\"] = 1\n",
      ">>> a[\"key\"]\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyError: 'key'\n",
      "\r\n",
      "Python 2.7:\n",
      ">>> a=dict()\n",
      ">>> a[b\"key\"] = 1\n",
      ">>> a[\"key\"]\n",
      "1\n",
      "\r\n",
      "Подобные мелочи представляют собой яркий пример сложности портирования кода на Python 3, и даже тесты тут не всегда помогают, ведь формально всё работает, однако работает в два раза медленнее, а для наших реалий это существенная разница. Чиним код, добавляем «пару байт» в коннектор (ссылка на изменения в коде коннектора, а также еще одно изменение) — есть результат!\n",
      "\n",
      "\r\n",
      "Ну что же, теперь неплохо! Синхронный вариант коннектора начал справляться с задачей в среднем за 35 секунд, что чуть медленней Python 2.7, но с этим уже можно жить.\n",
      "\n",
      "Переходим на asyncio в Python 3\r\n",
      "Asyncio – это корутины для Python 3 «из коробки». Есть документация, есть примеры, есть готовые библиотеки для asyncio и Python 3. На первый взгляд всё достаточно сложно и запутанно (по крайней мере, по сравнению с gevent), однако при дальнейшем рассмотрении всё становится на свои места. И вот, после некоторых усилий, я написал версию коннектора к Tarantool для asyncio – aiotarantool.\n",
      "\r\n",
      "Этот коннектор так же доступен через pypi:\n",
      "\n",
      "$ pip install aiotarantool\r\n",
      "Хочу отметить, что код нашей исходной задачи на asyncio стал немного сложнее её первоначальной версии. Появились конструкции yield from, появились декораторы @asyncio.coroutine, но в целом он мне нравится, а отличий от gevent не так много:\n",
      "\n",
      "import asyncio\n",
      "import aiotarantool\n",
      "import string\n",
      "\n",
      "mod_len = len(string.printable)\n",
      "data = [string.printable[it] * 1536\n",
      "        for it in range(mod_len)]\n",
      "cnt = 0\n",
      "\n",
      "@asyncio.coroutine\n",
      "def insert_job(tnt):\n",
      "    global cnt\n",
      "    \n",
      "    for it in range(10000):\n",
      "        cnt += 1\n",
      "        args = (cnt, data[it % mod_len])\n",
      "        yield from tnt.insert(\"tester\", args)\n",
      "\n",
      "\n",
      "loop = asyncio.get_event_loop()\n",
      "tnt = aiotarantool.connect(\"127.0.0.1\", 3301)\n",
      "\n",
      "tasks = [asyncio.async(insert_job(tnt))\n",
      "         for _ in range(10)]\n",
      "\n",
      "loop.run_until_complete(asyncio.wait(tasks))\n",
      "loop.close()\n",
      "\r\n",
      "Этот вариант справляется с задачей, в среднем, за 13 секунд (получается порядка 7,5k вставок в секунду), что чуть медленнее версии на Python 2.7 и gevent, но гораздо лучше всех синхронных версий. В aiotarantool есть одно небольшое, но очень важное отличие от других библиотек, доступных на asyncio.org: вызов tarantool.connect делается вне asyncio.event_loop. На самом деле этот вызов не создает настоящего коннекта: он будет сделан позже, внутри одной из корутин при первом вызове yield from tnt.insert. Такой подход мне показался проще и удобнее при программировании на asyncio.\n",
      "\r\n",
      "По традиции, результаты профилирования (я использовал yappi профайлер, но есть подозрение, что он не совсем правильно считает количество вызовов функций при работе с asyncio):\n",
      "\n",
      "\r\n",
      "В результате мы видим 5k вызовов StreamReader.feed_data и StreamWriter.write, что, несомненно, гораздо лучше, чем 200k вызовов socket.recv и 100k вызовов socket.sendall в синхронном варианте.\n",
      "\n",
      "Сравнение подходов\r\n",
      "Привожу результаты сравнения рассмотренных вариантов работы с Tarantool. Код бенчмарков можно найти в каталоге tests библиотек gtarantool и aiotrantool. В бенчмарке выполняется вставка, поиск, изменение и удаление 100 000 записей размером 1,5 Кбайт. Каждый тест запускался десять раз, в таблицах приведено среднее округлённое значение, поскольку важны не точные цифры (они зависят от конкретного железа), а их соотношение.\n",
      "\r\n",
      "Сравниваются:\n",
      "\n",
      "синхронный tarantool-python под Python 2.7;\n",
      "синхронный tarantool-python под Python 3.4;\n",
      "асинхронный вариант с gtarantool под Python 2.7;\n",
      "асинхронный вариант с aiotarantool под Python 3.4.\n",
      "\r\n",
      "Время выполнения теста, в секундах (меньше — лучше):\n",
      "\n",
      "Операция\r\n",
      "(100k записей)\n",
      "tarantool-python\r\n",
      "2.7\n",
      "tarantool-python\r\n",
      "3.4\n",
      "gtarantool\r\n",
      "(gevent)\n",
      "aiotarantool\r\n",
      "(asyncio)\n",
      "\n",
      "\n",
      "insert\n",
      "34\n",
      "38\n",
      "11\n",
      "13\n",
      "\n",
      "\n",
      "select\n",
      "23\n",
      "23\n",
      "10\n",
      "13\n",
      "\n",
      "\n",
      "update\n",
      "34\n",
      "33\n",
      "10\n",
      "14\n",
      "\n",
      "\n",
      "delete\n",
      "35\n",
      "35\n",
      "10\n",
      "13\n",
      "\n",
      "Количество операций в секунду (больше — лучше):\n",
      "\n",
      "Операция\r\n",
      "(100k записей)\n",
      "tarantool-python\r\n",
      "2.7\n",
      "tarantool-python\r\n",
      "3.4\n",
      "gtarantool\r\n",
      "(gevent)\n",
      "aiotarantool\r\n",
      "(asyncio)\n",
      "\n",
      "\n",
      "insert\n",
      "3000\n",
      "2600\n",
      "9100\n",
      "7700\n",
      "\n",
      "\n",
      "select\n",
      "4300\n",
      "4300\n",
      "10000\n",
      "7700\n",
      "\n",
      "\n",
      "update\n",
      "2900\n",
      "3000\n",
      "10000\n",
      "7100\n",
      "\n",
      "\n",
      "delete\n",
      "2900\n",
      "2900\n",
      "10000\n",
      "7700\n",
      "\n",
      "Производительность gtarantool немного лучше, чем у aiotarantool. Мы используем gtarantool уже давно, это проверенное решение на больших нагрузках, однако gevent не поддерживается в Python 3. Кроме того, следует помнить, что gevent — сторонняя библиотека, требующая компиляции при установке. Asyncio привлекает своей скоростью и новизной, она идёт в Python 3 «из коробки», и в ней отсутствуют «костыли» в виде «monkey.patch». Но под реальной нагрузкой aiotarantool в нашем проекте пока ещё не работал. Всё впереди!\n",
      "\n",
      "Выжимаем из сервера максимум\r\n",
      "Для максимально эффективного использования ресурсов нашего сервера попробуем немного усложнить код нашего бенчмарка. Сделаем одновременное удаление, вставку, изменение и выборку данных (достаточно распространённый профиль нагрузки) в одном Python-процессе, а самих таких процессов создадим несколько, скажем, 22 (магическое число). Если всего ядер на машине 24, то одно ядро оставим системе (на всякий случай), одно ядро отдаем на откуп Tarantool (ему хватит!), а оставшиеся 22 забираем под процессы Python. Сравнение будем делать и на gevent, и на asyncio, код бенчмарков можно найти здесь для gtarantool, и здесь для aiotarantool.\n",
      "\r\n",
      "Очень важно наглядно и красиво отобразить результаты для последующего их сравнения. Самое время оценить возможности новой версии Tarantool 1.6: фактически он является интерпретатором Lua, а это значит, что мы можем запускать абсолютно любой код Lua прямо в базе. Пишем простейшую программу, и наш Tarantool уже умеет отправлять любую свою статистику в graphite. Добавляем код в наш init-скрипт запуска Tarantool (в реальном проекте, конечно, такие вещи лучше выносить в отдельный модуль):\n",
      "\n",
      "fiber = require('fiber')\n",
      "socket = require('socket')\n",
      "log = require('log')\n",
      "\n",
      "local host = '127.0.0.1'\n",
      "local port = 2003\n",
      "\n",
      "fstat = function()\n",
      "    local sock = socket('AF_INET', 'SOCK_DGRAM', 'udp')\n",
      "    while true do\n",
      "        local ts = tostring(math.floor(fiber.time()))\n",
      "        info = {\n",
      "            insert = box.stat.INSERT.rps,\n",
      "            select = box.stat.SELECT.rps,\n",
      "            update = box.stat.UPDATE.rps,\n",
      "            delete = box.stat.DELETE.rps\n",
      "        }\n",
      "\n",
      "        for k, v in pairs(info) do\n",
      "            metric = 'tnt.' .. k .. ' ' .. tostring(v) .. ' ' .. ts\n",
      "            sock:sendto(host, port, metric)\n",
      "        end\n",
      "\n",
      "        fiber.sleep(1)\n",
      "        log.info('send stat to graphite ' .. ts)\n",
      "    end\n",
      "end\n",
      "\n",
      "fiber.create(fstat)\n",
      "\r\n",
      "Запускаем Tarantool и автоматически получаем графики со статистикой. Круто? Мне очень понравилась эта фича!\n",
      "\r\n",
      "Теперь проведем два бенчмарка: в первом будем выполнять одновременное удаление, вставку, изменение и выборку данных. Во втором бенчмарке будем выполнять только выборку. На всех графиках по оси абсцисс — время, а по оси ординат — количество операций в секунду:\n",
      "\n",
      "\n",
      "gtarantool (insert, select, update, delete):\n",
      "\n",
      "\n",
      "aiotarantool (insert, select, update, delete):\n",
      "\n",
      "\n",
      "gtarantool (select only):\n",
      "\n",
      "\n",
      "aiotarantool (select only):\n",
      "\n",
      "\r\n",
      "Напомню, что процесс Tarantool использовал только одно ядро. В первом бенчмарке загрузка CPU (этого ядра) при этом составляла 100%, а во втором тесте процесс Tarantool утилизировал своё ядро только на 60%.\n",
      "\r\n",
      "Полученные результаты позволяют нам сделать вывод о том, что рассмотренные в статье приёмы подходят для работы с большими нагрузками в наших проектах.\n",
      "\n",
      "Выводы\r\n",
      "Примеры в статье носят, конечно, искусственный характер. Настоящие задачи немного сложнее и разнообразнее, но их решения в общем случае выглядят именно так, как показано в приведённом коде. Где можно применить такой подход? Там, где требуется «много-много, очень много запросов в секунду»: в этом случае для эффективной работы с Tarantool понадобится асинхронный код. Корутины эффективны там, где есть ожидание событий (системных вызовов), и классический пример такой задачи — краулер.\n",
      "\r\n",
      "Писать код на asyncio или gevent не так сложно, как кажется, но обязательно нужно уделять много внимания профилированию кода: часто асинхронный код работает вовсе не так, как ожидалось.\n",
      "\r\n",
      "Tarantool и его протокол очень хорошо подходят для работы с асинхронным стилем разработки. Стоит только погрузиться в мир Tarantool и Lua, и можно бесконечно удивляться их мощным возможностям. Код на Python может эффективно работать с Tarantool, а в Python 3 заложен хороший потенциал для разработки на корутинах asyncio.\n",
      "\r\n",
      "Надеюсь, что материал этой статьи принесет пользу сообществу и пополнит базу знаний о Tarantool и об асинхронном программировании. Думаю, что asyncio и aiotarantool дойдут до использования в продакшен и в наших проектах, и мне будет чем ещё поделиться с читателями Хабра.\n",
      "\n",
      "Ccылки, которые использовались при написании статьи:\n",
      "\n",
      "tarantool.org\n",
      "habrahabr.ru/company/mailru/blog/252065 — Tarantool 1.6 от первого лица\n",
      "habrahabr.ru/post/254533 — Tarantool 1.6 — давай начнем\n",
      "emptysqua.re/blog/greenletprofiler — профайлер для gevent\n",
      "code.google.com/p/yappi — еще один профайлер\n",
      "habrahabr.ru/company/mailru/blog/202832 — статья о профилировании кода на Python\n",
      "docs.python.org/3/library/asyncio.html — документация на asyncio\n",
      "asyncio.org — примеры готовых библиотек\n",
      "www.gevent.org — gevent\n",
      "\r\n",
      "Ну и, конечно, версии коннекторов для Tarantool:\n",
      "\n",
      "github.com/shveenkov/aiotarantool\n",
      "github.com/shveenkov/gtarantool\n",
      "\r\n",
      "Самое время попробовать их у себя в деле!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " В процессе изучения языка мы обычно пользуемся ПК для работы с соответствующими приложениями, средами, программами, читаем книги, используем массовые онлайн курсы. Сейчас, когда смартфоны с нами всегда и везде, грех не воспользоваться их возможностями для усвоения знаний по программированию или их усовершенствования.В процессе изучения языка программирования, в частности Python, я считаю, что нужна погружаться в него полностью. Лозунг: «Ни дня без кода!» я дополняю, ну если ни кодить, то хотя бы почитать об этом. Для того чтобы не выходить из ритма в условиях, когда нужно в жизни сделать многое, но некогда, а хотелось бы учить Python, мобильный приложения помогут не тратить зря время в транспорте, во время ожиданий и т.д.Предлагаю список приложений из Google Play для изучения Python на Android-устройствах, которые помогут не только получить знания, но и проверить свой уровень по Python.Три из ниже перечисленных приложений я обязательно советую студентам при изучении Python: Learn Python, Quiz&Learn Python и Python Challenge. Ну теперь подробнее.\n",
      "  1. Learn Python Рейтинг 4,8 на 20.10.2015\n",
      "Одно из лучших, симпатичное приложение: интерфейс сразу порадовал. Содержит короткие уроки и тесты, контролирующие процесс изучения Python. Есть элемент азарта – набираете очки, за прохождение занятий и тестов. По окончании курса можете получить сертификат. Приложение охватывает следующие темы: основы Python, типы данных, управляющие структуры, функции и модули, работа с файлами, функциональное программирование, объектно-ориентированное программирование, регулярные выражения. \n",
      "2. QPython — Python for Android  Рейтинг 4,4 на 19.10.2015\n",
      "QPython это скрипт, который запускает Python на Android устройствах, позволяет выполнить сценарии и проекты. QPython содержит интерпретатор Python, консоль, редактор, и SL4A-библиотеку (позволяет не всегда создание и запуск скриптов, написанных на различных языках сценариев прямо на Android-устройствах. SL4A предназначен для разработчиков и все ещё находится на стадии альфа-тестирования — ru.wikipedia.org/wiki/SL4A) для Android. Таким образом, приложение предлагает комплект разработчика, который позволяет легко создавать проекты и сценарии Python на Android-устройствах. Основные характеристики \n",
      "поддержка программирования Python на Android, в том числе веб-приложений, игр и SL4A-программирования и т.д. \n",
      " выполнение Python сценариев / проектов на устройствах Android \n",
      " можно выполнить Python код & файлы с QRCode \n",
      " QEdit позволяет легко создавать/ редактировать на Python скрипты / проекты \n",
      " включает в себя множество полезных библиотек Python\n",
      " поддержка pip (система управления пакетами используется для установки и управления программных пакетов, написанных на Python en.wikipedia.org/wiki/Pip_(package_manager))\n",
      " поддержка SL4A программирование для доступа к функциям Android: сеть, Bluetooth, GPS и др.\n",
      "Есть также приложение QPython3, которое в отличие от QPython имеет поддержку python3. \n",
      "3. Python Documentation Рейтинг 4,4 на 19.10.2015\n",
      "Удобное и стильное приложение с документацией по Python 3.5 на английском языке, полностью Offline. Имеет улучшенный поиск, простую навигацию, мобильный формат чтения, удобный интерфейс. В содержание документации входят такие разделы: \n",
      "Что нового в Python? \n",
      " Учебник Python \n",
      " библиотеке ссылок Python \n",
      "справочник по языку Python \n",
      " Python настройка и использовании \n",
      " Python HOWTOs \n",
      " Расширения и вложения \n",
      " Python / C API \n",
      " Установка Python модулей \n",
      "Деинсталяция Python модулей\n",
      "FAQs\n",
      "\n",
      " 4. Quiz&Learn Python Рейтинг 4.1 на 19.10.2015\n",
      "Приложение Quiz&Learn Python для тестирования и улучшения знаний и понятий Python (версия 2.7). Вопросы варьируются от основ программирования на языке Python до очень конкретных, возможно, неожиданных способов написания кода. В зависимости от прогресса вопросы могут усложнятся. Чем быстрее вы отвечаете, тем больше очков вы получите. Можно удалить два неправильных ответа, пропустить вопрос, остановить таймер, или отладить код. Сайт разработчиков mobileicecube.com/quiz-learn-python.\n",
      "5. Python interview questions Рейтинг 4,0 на 20.10.2015\n",
      "Интересное приложение. Помогает проверить знании языка Python в виде интервью.\n",
      "6. Python Challenge Рейтинг 3,9 на 19.10.2015\n",
      "Это приложение представляет собой тест на английском языке для пользователей, которые хотели бы узнать и испытать свои знания по программированию на Python. Приложение имеет два основных режима: Challenge Mode и Practice Mode. Challenge Mode состоит из 20 вопросов. Очки начисляются на основе учета времени, необходимого для решения каждого вопроса. Тест заканчивается, когда пользователь выбирает неправильный ответ или завершает все 20 вопросов.В Practice Mode вопросы сортируются по разным темам. Пользователь может сам их выбрать. Предлагается 10 вопросов. Пользователь может продолжать тест, даже если он даст не правильный ответ. Все вопросы и ответы будут показаны в конце тренировки.\n",
      "7. Python Guide Рейтинг 3,9 на 19.10.2015\n",
      "Это приложение представляет собой краткое руководство для Python. Руководство будет особенно полезно для новичков, которые хотят ознакомиться с правилами синтаксиса Python. Руководство охватывает основы программирования Python, так что пользователи будут иметь достаточно знаний, чтобы создать какое-нибудь простое и продуктивное приложение. Руководство состоит из следующих тем: переменные, условия, функции, циклы, списки, строки, словари. Небольшой список. Хотелось бы большего.\n",
      "8. Python Programming in a day Рейтинг 3,0 на 20.10.2015\n",
      "Разработчики приложения советуют нам пропустить длинные, сложные книги по Phyton. Для того, чтобы научиться программировать на Phyton 3.0 быстрее они предлагают короткое и лаконичное приложение, которое научит всему необходимому для Phyton программирования. Эта книга написана для людей, которые не имеют никакого знания в программировании или являются новичками. Она фокусирует на самых важных понятиях с примерами.  \n",
      "9. Learn Python & Python Django   Рейтинг 3,0 на 20.10.2015\n",
      "Это приложение наконец-то отличается от предыдущих тем, что затрагивает не только сам Python, но и Django. Это по сути обучалка на английском языке по Python и Django на платформе Udemy. В ее содержание входят такие темы: Python и Hello World, обзор и история Python, функции, классы, базы данных, модули и пакеты, JSON, установка Django, интерфейс администратора, язык шаблонов Django и др… В приложении 18 лекций, более 4-х часов высококачественного контента, сообщество, видео и аудио лекции, презентации, статьи, можно сохранить курсы для просмотра в автономном режиме. \n",
      "10. Учебник Python  Рейтинг 4,0 на 20.10.2015\n",
      "Сборник тьюториалов по Python 2 и 3. Расскажет о истории и философии Python, как установить Python, работать с числами и строками в Python, о типах данных и переменных, функциях и т.д… Мне не понравился тем, что немного не подстроен под возможности мобильных телефонов – шрифты кое-где маленькие.Название на русском меня воодушевило, что тьюториал мог быть на русском языке – ан нет!В общем, я не нашла приложений на русском. \n",
      "Еще можно назвать приложения такие как:\n",
      "Dive Into Python 3 — книга Марка Пилгрима, рейтинг 3,8, имеет проблемы с растягивание интерфейса.\n",
      "Python For Android  — Python IDE, рейтинг 3,3 из-за навязчивой рекламы\n",
      "Python Tutorial — учебник по программированию на Python 2.6, рейтинг 3,8\n",
      "Python Programming Tutorial – учебник по Python 2.7, рейтинг 3,6\n",
      "    \n",
      " Аннотация\n",
      "В этой статье мы представляем методологию для начального освоения научной информатики, базирующейся на моделировании в обучении. Мы предлагаем многофазные системы массового обслуживания, как базис для изучаемых объектов. Мы используем Python и параллельные вычисления для реализации моделей, с предоставлением программного кода и результатов стохастического моделирования.\n",
      "\n",
      "1. Ввведение и предистория\n",
      "В нашем исследовании мы понимаем значение термина “научная информатика” как использование компьютеров для анализа и решения научных и инженерных проблем. Мы отличаем их от простых численных вычислений. Использование научной информатики в обучении всегда является сложной задачей, как для учащегося, так и для преподавателя. Такой процесс изучения имеет дело с многими техническими и междисциплинарными вопросам, а также требует синхронизации математических знаний с информатикой. Для преодоления этих сложностей мы предлагаем набор обучающих принципов и методологию, которая основывается на конструктивистском подходе к обучению и обеспечивает соответствующий структурный базис для преподавателя. Всё это даёт возможность учащимся провести серию вычислительных экспериментов с компьютерными моделями. Такой подход связан со знанием математики и программирования, которые, в свою очередь, преподаются в процессе основного учебного курса и тесно с ним связаны. Мы рассмотрим раздел вычислительной статистики, как вступительный раздел научной информатики и как возможную область применения данного исследования. Ниже представлена предыстория данной методологии.\n",
      "\n",
      "1.1. Научная информатика\n",
      "Карниадакс и Кирби II дали определение “компьютерной информатики, как `сердца` имитационных исследований”. Авторы предлагают “целостный подход численных алгоритмов, современные методы программирования и параллельные вычисления … Часто такие концепции и подобный инструментарий переодически изучаются в различных, смежных по тематике, курсах и учебниках, и взаимосвязь между ними становится сразу очевидна. Необходимость интеграции концепций и инструментов обычно становится явной после завершения курса, например в процессе первой поствузовой работы или при написании тезисов к диссертации, тем самым заставляя учащегося синтезировать понимание трёх независимых областей в одно, для получения требуемого решения. Хотя этот процесс, несомненно, очень ценен, на него уходит много времени, и, во многих случаях, он может не дать эффективного сочетания концепций и инструментов. С педагогической точки зрения, для усиления понимания тем научной информатики, целостный интегрированный подход может стимулировать учащегося сразу к нескольким дисциплинам. На рисунке 1 представлено определение научной информатики как пересечение численной математики, информатики и моделирования [16].\n",
      "\n",
      "Рис. 1. Научная информатика.\n",
      "\n",
      "1.2. Конструктивизм в обучении\n",
      "Кейн и Кейн в их фундаментальных исследованиях [6] предложили основные принципы конструктивизма в обучении. Одним из важнейших для нас является следующий: “Мозг обрабатывает части и целое одновременно”.\n",
      "\n",
      "Таким образом, хорошо организованный обучающий процесс демонстрирует лежащие в основе детали и идеи. Используя подход, базирующийся на моделировании, после создания имитационной модели становятся очевидными цели исследования. Это позволяет нам наблюдать результаты и формировать соответствующие выводы.\n",
      "\n",
      "1.3. Обучение на основе моделирования: почему модели?\n",
      "Гиббонс вводит, базирующуюся на моделировании, программу обучения в 2001 [9]. Выделяя следующие основополагающие принципы:\n",
      "\n",
      "Обучающийся получает опыт путём взаимодействия с моделями;\n",
      "Обучающийся решает научные и инженерные проблемы путём экспериментов с моделью;\n",
      "Рассмотрение и постановка проблем;\n",
      "Определение конкретных учебных целей;\n",
      "Представление всей необходимой информации в контексте решения.\n",
      "\n",
      "Миллард и др. [30] предлагают модель облегчённого обучения используя “интерактивное моделирование”. Авторы представляют современные компьютерные технологии, базирующиеся на “многообещающей методологии” основанной на “динамике системы”. “Практический опыт включает конструирование интерактивных … моделей, а также их использование для проверки гипотез и экспериментов”.\n",
      "\n",
      "Лерер и Шаубле [25] заостряют внимание на экспериментах с различными представлениями модели: “Обучение студента усиливается, когда у студента есть возможность создания и пересмотра нескольких вариантов моделей, а затем сравнения адекватности описания этих различных моделей”.\n",
      "\n",
      "1.4. Научная информатика в основе образования: эксперименты с моделями\n",
      "Сюэ [40] предлагает “реформы преподавания в обучении на основе “научной информатики” путём моделирования и имитаций”. Он советует “… использовать моделирование и имитации для решения актуальных проблем программирования, моделирования и анализа данных…”. Обучение, базирующееся на моделировании, используется в математическом образовании. Множество моделей построено с использованием программного обеспечения «Geogebra» [33]. Модели играют главную роль в Научном Образовании [7,18]. \n",
      "\n",
      "1.5. Стохастическое моделирование систем массового обслуживания\n",
      "Мы предлагаем использование систем массового обслуживания в связи с простотой их начальных определений и из-за широких возможностей моделирования и имитаций. Теория Массового Обслуживания хорошо известна и моделирование Cистем Массового Обслуживания (СМО) широко используется в науке [4,19] и образовании [13,36]. Мультифазные системы массового обслуживания являются хорошей платформой для экспериментов учащегося, как и использование параллельных вычислений. Также существует ряд интересных теоретических результатов для изучения и исследования [12].\n",
      "\n",
      "1.6. Python в образовании, основанном на научной информатике\n",
      "Python один из популярнейших языков программирования учёных и педагогов [21–23]. Python широко используется в промышленных научных вычислениях [14]. Лангтанген докладывает о долгосрочном опыте использования Python как первичного языка для обучения Научной Информатике в Университете Осло [24]. Python продвигается как первый язык для изучения программирования [38], а также для углублённого изучения вычислительных методов [3,20,34].\n",
      "\n",
      "2. Основы\n",
      "Прежде, чем приступить к моделированию, определим ключевые подходы, которые мы будем использовать в процессе. В этой главе затронем вопросы генерация случайных чисел и вероятностных распределений, стохастического моделирования. Рассмотрим элементарную теорию вероятностей. Основной задачей этих экспериментов будет экспериментальное доказательство Центральной Предельной Теоремы. Модели и эксперименты с этими моделями проясняют принцип генераторов псевдо- и квази-случайных чисел, а также понимание экспоненциального распределения. Это может обеспечить основу для более детальных экспериментов с моделями СМО. \n",
      "\n",
      "2.1. Случайные величины и распределения\n",
      "Все элементы теории вероятностей традиционно считаются трудными для понимания и всегда находятся в сфере интересов международных образовательных учреждениях [15]. В тоже время эти вопросы занимают немаловажную роль и в научных исследованиях [10]. Подход, базирующийся на моделировании, делает более легким понимание этого материала. Модель, которую рассмотрим в этой статье, является простой моделью бросания одного или нескольких игральных кубиков, начиная с одного и заканчивая несколькими.\n",
      "\n",
      "Задача этих вводных экспериментов является довольно сложной. Мы не только рассмотрим вероятностные распределения, но также затронем моделирование и параллельные вычисления. Мы также сделаем один шаг вперёд в научном исследовании: экспериментально докажем Центральную Предельную Теорему.\n",
      "\n",
      "Мы начнём с генерации случайных чисел (не затрагивая распределения). Затем объясним равномерно распределённые случайные величины. Обсуждения об истинной случайности и квази случайности представлены авторами [26, 35]. Для продвинутых учеников будет представлен ряд экспериментов с генератором псевдослучайных величин Python. На начальном этапе, для наглядности изучения, будем повышать количество испытаний, наблюдая результат моделирования. На последующем этапе мы перейдём к более сложным экспериментам и параллельным вычислениям. Будем использовать для моделирования модуль случайных величин Python, а для параллельных вычислений — библиотеку mpi4py. Модуль случайных величин Python основан на псевдослучайном генераторе чисел для различных распределений. Для примера: random.randint(a,b) возвращает случайное целое число N, где a ≤ N ≤ b и random.expovariate(lambd) возвращает экспоненциально распределенные случайные величины с параметром ‘lambd’. Для получения более подробной информации обратитесь к документации Python. Программирование модели подбрасывания кубика представлено на рис.2. \n",
      "\n",
      "import pylab\n",
      "import random\n",
      "\n",
      "number_of_trials =100\n",
      "## Here we simulate the repeated throwing of a single six-sided die\n",
      "list_of_values = []\n",
      "for i in range(number_of_trials):\n",
      "    list_of_values.append(random.randint( 1,6))\n",
      "\n",
      "print \"Trials =\", number_of_trials, \"times.\"\n",
      "print \"Mean =\", pylab.mean(list_of_values)\n",
      "print \"Standard deviation =\", pylab.std(list_of_values)\n",
      "\n",
      "pylab.hist(list_of_values, bins=[0.5,1.5,2.5,3-5,4.5,5.5,6.5])\n",
      "pylab.xlabel('Value')\n",
      "pylab.ylabel('Number of times')\n",
      "pylab.show()\n",
      "\n",
      "Рис. 2. Программирование модели подбрасывания одного кубика на Python\n",
      "\n",
      "Результаты моделирования подбрасывания одного кубика представлены на рисунке 3. \n",
      "\n",
      "\n",
      "Рис. 3. Результаты моделирования подбрасывания одного кубика\n",
      "\n",
      "Далее рассмотрим случай подбрасывания двух игральных кубиков. Главная идея на данном этапе – это объяснение ЦПТ с помощью эксперимента с разным количеством кубиков. Рисунок 4 демонстрирует эту идею.\n",
      "\n",
      "\n",
      "Рис. 4. Сравнение функций плотности вероятности\n",
      "\n",
      "Процесс изучения продолжается путём изменения кода для моделирования подбрасывания двух кубиков таким образом, чтобы начать рассматривать случай с несколькими кубиками. Код аналогичен коду с одним кубиком, за исключением двух строк кода, представленных ниже:\n",
      "\n",
      "...\n",
      "list_of_values.append(random.randint(1, 6) + random.randint(1, 6))\n",
      "...\n",
      "pylab.hist(list_of_values, pylab.arange(1.5, 13.5, 1.0))\n",
      "...\n",
      "\n",
      "Результат вычислений в случае двух кубиков представлен на рисунке 5.\n",
      "\n",
      "\n",
      "Рис. 5. Случай двух кубиков\n",
      "\n",
      "Теперь можем рассмотреть нормальное распределение. Задача, на данном этапе, показать как предыдущий случай с несколькими кубиками коррелирует с нормальным распределением. Следующая задача познакомит нас со средней величиной и среднеквадратическим отклонением. Код остаётся такой же, как в случае одного кубика, за исключением инструкции, приведённой ниже: \n",
      "\n",
      "...\n",
      "list_of_values.append(random.normalvariate(7, 2.4))\n",
      "...\n",
      "\n",
      "Результаты моделирования для нормального распределения представлены на рисунке 6.\n",
      "\n",
      "\n",
      "Рис. 6. Результат моделирования для нормального распределения\n",
      "\n",
      "Финальным шагом является демонстрация экспоненциального распределения. Экспоненциальное распределение используется для моделирования распределения (длительности) интервалов между моментами поступления требований в системах разного типа. Результаты их моделирования представлены на рис 7 и 8.\n",
      "\n",
      "import pylab\n",
      "import random\n",
      "\n",
      "number_of_trials = 1000\n",
      "number_of_customer_per_hour = 10\n",
      "\n",
      "## Here we simulate the interarrival time of the customers\n",
      "\n",
      "list_of_values = []\n",
      "for i in range(number of trials):\n",
      "    list_of_values.append(random.expovariate(number_of_customer_per_hour))\n",
      "\n",
      "mean=pylab.mean(list_of_values)\n",
      "std=pylab.std(list_of_values)\n",
      "print \"Trials =\", number_of_trials, \"times\"\n",
      "print \"Mean =\", mean\n",
      "print \"Standard deviation =\", std\n",
      "\n",
      "pylab.hist(list_of_values,20)\n",
      "pylab.xlabel('Value')\n",
      "pylab.ylabel('Number of times')\n",
      "pylab.show()\n",
      "\n",
      "Рис. 7. Модель Python для экспоненциального распределения\n",
      "\n",
      "\n",
      "Рис. 8. Результат моделирования для экспоненциального распределения\n",
      "\n",
      "2.2. Стохастическое моделирование\n",
      "Стохастическое моделирование является важным элементом научной информатики. Мы сосредоточимся на методе Монте-Карло [10,11,27]. После того, как модель была построена, мы можем генерировать случайные переменные и экспериментировать с различными параметрами системы. В рамках этой статьи ключевым моментом экспериментов Монте-Карло является повторения испытаний много раз с целью накопления и интегрирования результатов. Простейшее приложение было описано в предыдущем разделе. Увеличивая количество испытаний мы будем увеличивать точность результатов моделирования.\n",
      "\n",
      "Здесь учащийся должен провести определённое количество экспериментов используя эту простую модель путём увеличения количества испытаний. За счёт увеличения числа кубиков и числа испытаний, учащийся столкнётся с относительно долгим временем вычисления. Это прекрасный повод для использования параллельных вычислений. Модель Python для нескольких игральных кубиков представлена на рисунке 9. И результаты моделирования представлены на рисунке 10. Следующим шагом будет рассмотрение более общих вопросов, связанных с различными системами массового обслуживания. Краткое введение в классификацию СМО представлено в следующей части этой статьи. Начнём изучение с системы М/М/1 системы и более сложных систем массового обслуживания. Основные понятия стохастических процессов будут подробно освещены в этой части статьи. В качестве возможного примера можно предложить задачу исследования выходного потока. Докажем, что вывод М/М/1 системы является Пуассоновским потоком. Таким образом собранные данные будут представлены в виде построенной выходной эмпирической гистограммы.\n",
      "\n",
      "import pylab\n",
      "import random\n",
      "\n",
      "number_of_trials = 150000\n",
      "number_of_dice = 200\n",
      "\n",
      "## Here we simulate the repeated throwing\n",
      "## of a number of single six-sided dice\n",
      "list_of_values = []\n",
      "for i in range(number_of_trials):\n",
      "   sum=0\n",
      "   for j in range(number_of_dice): sum+=random.randint(1,6)\n",
      "   list_of_values.append(sum)\n",
      "\n",
      "mean=pylab.mean(list_of_values)\n",
      "std=pylab.std(list_of_values)\n",
      "print \"Trials =\", number_of_trials, \"times\"\n",
      "print \"Mean =\", mean\n",
      "print \"Standard deviation =\", std\n",
      "\n",
      "pylab.hist(list_of_values,20)\n",
      "pylab.xlabel('Value')\n",
      "pylab.ylabel('Number of times')\n",
      "pylab.show()\n",
      "\n",
      "Рис. 9. Модель моделирования Python для расширенного нормального распределения\n",
      "\n",
      "\n",
      "Рис. 10. Результат моделирования для расширенного нормального распределения\n",
      "\n",
      "3. Многофазные системы массового обслуживания и стохастическое моделирование\n",
      "Ниже приведено вводное описание СМО, учитывающие нюансы моделирования и стохастики. \n",
      "\n",
      "3.1. Системы Массового Обслуживания\n",
      "Простая система массового обслуживания состоит из одного обслуживающего устройства, которое обрабатывает прибывающие заявки. Общая схема простой системы массового обслуживания представлена на рисунке 11. В общем, СМО состоит из одного или нескольких обслуживающих устройств, которые обрабатывают прибывающие заявки. Также возможен вариант одного или нескольких этапов обслуживания с одним или несколькими обслуживающими устройствами в каждой фазе. Приходящие клиенты, которые обнаружили все сервера занятыми, присоединяются к одному или нескольким очередям перед обслуживающими устройствами. Существует множество приложений, с помощью которых можно моделировать СМО, такие как производственные системы, системы связи, системы технического обслуживания и другие. Общую СМО можно охарактеризовать тремя основными компонентами: поток поступления заявок, процесс обслуживания и метод обслуживания очереди. Поступление заявок может приходить из нескольких ограниченных или неограниченных источников.\n",
      "\n",
      "\n",
      "Рис. 11. Простая СМО.\n",
      "\n",
      "Процесс поступления заявок описывает как заявки приходят в систему. Определим \n",
      " как временной интервал между прибытием заявок между  и -ой заявкой, ожидаемое (среднее) время между поступлениями заявок как  и частоту поступления заявок как \n",
      "\n",
      "Также определим  как количество обслуживающих устройств. Механизм обслуживания определяется этим числом. Каждое обслуживающее устройство имеет свою собственную очередь, а также вероятностное распределение времени обслуживания заявки.\n",
      "\n",
      "Определим  как время обслуживание -ой заявки,  как среднее время обслуживания заявки и  как скорость обслуживания заявки.\n",
      "\n",
      "Правило, которое использует обслуживающее устройство для выбора следующей заявки из очереди называется дисциплиной очереди СМО. Наиболее частые дисциплины очереди: Приоритетная – клиенты обслуживаются в порядке их важности. FIFO – первый пришёл первый обслужен; LIFO – стек, последний пришёл первым обслужен. Расширенная классификация систем по Кендаллу использует 6 символов: A/B/s/q/c/p, где А – распределение интервалов между приходящими заявками, В – распределение интервалов обслуживания, s – количество серверов, q – дисциплин обслуживания (опущена для FIFO), с – вместимость системы (опущено для бесконечных очередей), p – количество возможных заявок (опущено для открытых систем) [17,37]. Для примера М/М/1 описывает Пуассоновский поток на входе, одно экспоненциальное обслуживающее устройство, одну бесконечную FIFO очередь, и бесконечное число заявок. \n",
      "\n",
      "СМО используются для моделирования и исследования различных областей науки и техники. Для примера: мы можем моделировать и изучать производственные или транспортные системы с использованием теории массового обслуживания. Причём запросы на обслуживание рассматриваются в качестве заявок, а процедуры технического обслуживания в качестве механизма обслуживания. Следующий пример такой: вычислительные машины (терминальные запросы и серверные ответы соответственно), компьютерные многодисковые системы памяти (запросы на запись/чтение данных, общий дисковый контроллер), транкинговая радиосвязь (телефонные сигналы, повторители), компьютерные сети (запросы, каналы) [39]. В биологии можно использовать теорию массового обслуживания для моделирования энзимных систем (белки, общие энзимы). В биохимии можно использовать модель сети массового обслуживания для изучения регуляторной цепи ЛАК оперона.\n",
      "\n",
      "3.2. Почему многофазные?\n",
      "Рассмотрим многофазную СМО, которая состоит из нескольких обслуживающих устройств, которые соединены последовательно и имеют неограниченное количество заявок. Время между заявками и время обработки независимы и экспоненциально распределены. Очередь бесконечна с дисциплиной обслуживания FIFO. Многофазная СМО естественным образом отражает топологию многоядерных компьютерных систем. Как мы увидим в дальнейшем, каждая модель может быть легко написана на языке программирования, изучена и модифицирована. Модель также позволяет провести сравнительное исследование различных подходов многопроцессорной обработки. Модель мультифазной СМО представлена на рисунке 12.\n",
      "\n",
      "\n",
      "Рис. 12. Мультифазная СМО.\n",
      "\n",
      "3.3. Теоретическая основа\n",
      "В случае статистического моделирования, мы всегда сталкиваемся с проблемой верификации компьютерного кода. Всегда остаётся открытым вопрос ошибок в нашей программе или алгоритме. Модель не полностью является аналитической и каждый раз запуская программу мы имеем различные данные на входе/выходе. Таким образом, для проверки корректности кода или алгоритма, необходимы различные подходы (от того, который мы используем в случае полностью детерминированных входных данных). Для решения этого вопроса мы должны применить теоретические результаты некоторых исследований, которые можно найти в научной литературе. Эти результаты дают нам базу для для верификации и анализа выходных данных, а также для решения проблемы корректности результатов моделирования [31,32]. \n",
      "\n",
      "Мы будем исследовать время пребывания заявки в многофазной СМО. Обозначим  как время пребывания заявки в системе,  как время обслуживания n-ой заявки j-ой фазы. Рассмотрим  как  для k-ой фазы.\n",
      "\n",
      "Существует такая константа  такая, что\n",
      "\n",
      "\n",
      "\n",
      "Теорема. Если условия (1) и (2) выполнены, тогда \n",
      "\n",
      "\n",
      "\n",
      "3.4. Статистическое моделирование\n",
      "После того как модель построена, мы могли бы провести ряд экспериментов с этой моделью. Это позволит изучить некоторые характеристики системы. Мы можем эмитировать случайные величины с ожидаемым среднем значением и посчитать (используя рекуррентное уравнение, представленное ниже) нужные значения для изучения. Эти значения также будут случайными (мы имеем есть стохастичность входных данных нашей модели – случайное временн между приходом заявок и случайное время обслуживания). В итоге, мы можем вычислить некоторые параметры этих случайных величин (переменных): среднее значение и вероятностное распределение. Мы называем этот метод статистическим моделированием из-за случайности, представленной в модели. Если нужны более точные результаты, мы должны повторить эксперименты с нашей моделью и затем интегрировать результаты, то вычислить интегральные характеристики: средние значение или среднеквадратическое отклонение. Это называется методом Монте Карло и он был описан в статье немного выше. \n",
      "\n",
      "3.5. Рекуррентное уравнение\n",
      "Для разработки алгоритма моделирования ранее описанной СМО, нужно разобрать некоторые математические конструкции. Основная задача – исследование и вычисление времени пребывания заявки с номером n в многофазной СМО, состоящей из  фаз. Мы можем привести следующие рекуррентное уравнение [12], обозначим:  — время прибытия -ой заявки;  как время обслуживания -ой заявки -ой фазы; . Следующее рекуррентное уравнение справедливо для времени ожидания  для -ой заявки -ой фазы:\n",
      "\n",
      "\n",
      "Предположение. Рекуррентное уравнение для расчёта времени пребывания заявки в многофазной СМО.\n",
      "\n",
      "Доказательство. Это правда, что если время , тогда время ожидания в -ой фазе -ой заявки является 0. В этом случае , время ожидания в -ой фазе -ой заявки  и . Принимая во внимание два вышеупомянутых случая, мы в итоге имеем предполагаемый результат.\n",
      "\n",
      "Теперь мы можем начать реализовывать необходимые алгоритмы на основе всех полученных теоретических результатов.\n",
      "\n",
      "4. Python для многопроцессорной обработки\n",
      "Python как язык программирования очень популярен среди учёных и педагогов и может быть очень привлекательным для решения научно-ориентированных задач [3]. Python предоставляет мощную платформу для моделирования и имитаций, включая графические утилиты, широкое количество математических и статистических пакетов, а также пакетов для мультипроцессорной обработки. Для уменьшения времени выполнения, необходимо совместить код Python и Си. Всё это даёт нам мощную платформу моделирования для получения статистических данных и обработки результатов. Ключевые концепции в Python, которые также является важными в моделировании, это: декораторы, сопрограммы, yield выражения, многопроцессорная обработка и очереди. Очень хорошо рассмотрены эти моменты у Бизли в его книге [2]. Не смотря на это, существуют несколько путей организации межпроцессного взаимодействия, и мы начнём с использования очередей, потому что это очень естественно в свете изучения СМО.\n",
      "\n",
      "Приведём ниже простой пример преимущества использования многопроцессорной обработки для увеличения эффективности и результативности программного кода. Изучающий может улучшить результаты моделирования, путём использования параллельных вычислений на суперкомпьютерах или кластерных системах [28, 29]. С одной стороны, многопроцессорность позволит нам сопоставить мультифазную модель с ресурсами многоядерного процессора, а с другой стороны, мы можем использовать мультипроцессорность чтобы выполнить ряд параллельных испытаний Монте – Карло. Мы рассмотрим два этих подхода в следующем разделе. Для мотивированных учеников далее будет представлено краткое введение в мультипроцессорную обработку с помощью Python.\n",
      "\n",
      "Мы начнём с использования модуля mpi4py. Это важно для представления главной идеи о том, как работает MPI. Он просто копирует предоставленную программу одному из процессорных ядер, определяемых пользователем, и интегрирует результаты после использования метода gather(). Пример Python кода (рис. 13) и результаты моделирования (рис. 14) представлены ниже.\n",
      "\n",
      "#!/usr/bin/python\n",
      "import pylab\n",
      "import random \n",
      "import numpy as np \n",
      "from mpi4py import MPI\n",
      "\n",
      "dice=200 \n",
      "trials= 150000\n",
      "\n",
      "rank = MPI.COMM_WORLD.Get_rank()\n",
      "size = MPI.COMM WORLD.Get_size()\n",
      "name = MPI.Get_processor_name()\n",
      "\n",
      "random.seed(rank)\n",
      "\n",
      "## Each process - one throwing of a number of six-sided dice\n",
      "\n",
      "values= np.zeros(trials)\n",
      "\n",
      "for i in range(trials):\n",
      "    sum=0\n",
      "    for j in range(dice): sum+=random.randint(l,6)\n",
      "    values[i]=sum\n",
      "\n",
      "\n",
      "data=np.array(MPI.COMM_WORLD.gather(values, root=0))\n",
      "if rank == 0:\n",
      "    data=data.flatten()\n",
      "    mean=pylab.mean(data)\n",
      "    std=pylab.std(data)\n",
      "\n",
      "    print \"Number of trials =\", size*trials, \"times.\"\n",
      "    print \"Mean =\", mean\n",
      "    print \"Standard deviation =\", std\n",
      "\n",
      "    pylab.hist(data,20) pylab.xlabel('Value') pylab.ylabel('Number of times')     \n",
      "    pylab.savefig('multi_dice_mpi.png')\n",
      "\n",
      "Рис. 13. Python модель для расширенного нормального распределения с применением MPI.\n",
      "\n",
      "\n",
      "Рис. 14. Нормальное распределение с применением MPI.\n",
      "\n",
      "5. Образовательный подход, основанный на моделировании\n",
      "Многофазные СМО дают нам ядро для разработки соответствующего подхода, основанного на моделировании. Такой подход включает в себя базовые понятия, описанные в предыдущих разделах, а также более сложные теоретические результаты и методы. Основные идеи носят стохастический характер: случайные величины, случайные числовые распределения, генераторы случайных чисел, Центральная Предельная Теорема; конструкции программирования Python:\n",
      "декораторы, сопрограммы и yeild выражения. Более сложные результаты включают следующее теоретические понятия: время пребывания заявки в системе, рекуррентное уравнение для вычисления времени пребывания заявки в СМО, методы стохастического моделирования и мультипроцессорные технологии. На рисунке 15 представлена главная схема, описывающая образовательную основу.\n",
      "\n",
      "\n",
      "Рис. 15. Образовательный подход, основанный на моделировании\n",
      "\n",
      "Все эти теоретические и программные структуры дают учащемуся проводить эксперимент с различными моделями многофазных СМО. Цель таких экспериментов двояка. Во-первых, это даёт учащимися понять следующую последовательность, которая важна в любых научных исследованиях: необходимые для изучения теоретические факты, математические модели, программные конструкции, компьютерные модели, стохастические модели и наблюдение результатов моделирования. Это даст учащемуся полную картину общих научных исследований (рис 16).\n",
      "\n",
      "\n",
      "Рис. 16. Область научных исследований\n",
      "\n",
      "Такой подход позволит глубже понять стохастическое моделирование и базовые программные конструкции, такие как многопроцессорная обработка и параллельное программирование. Эти положения имеют первостепенное значение в области научных вычислений.\n",
      "\n",
      "5.1. Эксперименты с моделями\n",
      "В этом разделе мы рассмотрим три компьютерные модели многофазных СМО. Все эти модели различны по своим философским и ключевым особенностям. Несмотря на то, что целью экспериментов является создание статистической модели и исследование главных параметров многофазных систем, концептуальные идеи этих моделей совершенно различны. Сравнение этих основных идей поможет ученику понять фундаментальные положения, которые лежат в основе параллельных вычислений, многопроцессорного статистического и имитационного моделирования.\n",
      "\n",
      "Первая модель, представленная нами, основана на записи в реальном времени и мы называем это имитационной моделью. Она использует мультипроцессорный модуль Python. Точность этой модели зависит от точности и разрешающей способности метода времени time(). Она может быть довольно низкой, в случае различных операционных систем общего назначения, и довольно высокой, в случае систем реального времени. Учащийся может изменять эту модель используя ранее предложенное рекуррентное уравнение (для вычисления времени пребывания заявки в системе) и сравнивать результаты в обоих случаях.\n",
      "\n",
      "Следующая модель вычисляет время пребывания заявки в системе и основана на стохастическоом моделировании. Модель не использует много-процессорность напрямую. Мультипроцессорная обработка эмулируется при помощи использования yield выражениями Python.\n",
      "\n",
      "Последняя модель представлена здесь с использованием Python MPI mpi4py модулем. Здесь используется настоящий MPI (мультипроцессорный) подход для статистического моделирования и можем, за счёт этого, увеличивается количество испытаний в методе Монте Карло.\n",
      "\n",
      "В общем, задача учащегося заключается в создании серии экспериментов с предоставленными моделями и получение экспериментального доказательства закона повторного логарифма (law of the iterated logarithm) для времени пребывания заявки в многофазной СМО.\n",
      "\n",
      "5.2. Имитационная модель, использующая мультипроцессорный сервис\n",
      "Ниже представлена имитационная модель. Основной вопрос для изучения – это разница между имитационной моделью и статистической моделью. Другой важный вопрос это корректность и точность имитационной модели. Также важным является вопрос правильности и точности представленной модели. Учащийся может изучить и сравнить результаты моделирования, зависящие от различных параметров, таких как интервал и частота обработки, количество заявок и количество обслуживающих узлов. Общая схема модели представлена на рисунке 17. \n",
      "\n",
      "\n",
      "Рис. 17. Имитационная модель\n",
      "\n",
      "Программный код код состоит из двух главных частей. Первый предназначен непосредственно для расчётов, а следующий – для построения результатов. Модуль для вычисления содержит три главные функции: producer() – для получения заявок и помещения их в первую очередь; server() – для обслуживания заявок; consumer() – для получения результатов. Эта программная модель основана на реальной симуляции и не использует математические выражения для вычислений. Её точность зависит от точности временного модуля Python и, как правило, зависит от операционной системы. Вычисление работы обслуживающих устройств распределяются между различными процессами внутри мультипроцессорной системы. Компьютерный код для реализации, указанной выше модели, представлен на рисунке 18. \n",
      "\n",
      "import multiprocessing\n",
      "import time \n",
      "import random \n",
      "import numpy as np\n",
      "\n",
      "def server(input_q,next_q,i):\n",
      "    while True:\n",
      "        item = input_q.get()\n",
      "        if i==0:item.st=time.time() ## start recording time\n",
      "                                                    ## (first phase)\n",
      "        timc.sleep(random.expovariate(glambda|i]))\n",
      "##stop recording time (last phase)\n",
      "        if i==M-1 :item.st=time.time()-item.st\n",
      "        next_q.put(item)\n",
      "        input_q.task_done()\n",
      "    print(\"Server%d stop\" % i) ##will be never printed why?\n",
      "\n",
      "def producer(sequence,output_q): \n",
      "    for item in sequence:\n",
      "        time.sleep(random.expovariate(glambda[0]))\n",
      "        output_q.put(ilem)\n",
      "\n",
      "def consumer(input_q):\n",
      "    \"Finalizing procedures\"\n",
      "    ## start recording processing time\n",
      "    ptime=time.time()\n",
      "    in_seq=[]\n",
      "    while True:\n",
      "        item = input_q.get()\n",
      "        in_scq+=[item]\n",
      "        input_q.task_done()\n",
      "        if item.cid == N-1:\n",
      "            break\n",
      "    print_results(in_seq)\n",
      "    print(\"END\")\n",
      "    print(\"Processing time sec. %d\" %(time.time()-ptime))\n",
      "    ## stop recording processing time\n",
      "    printf(\"CPU used %d\" %(multiprocessing.cpu_count()))\n",
      "\n",
      "def print_resulls(in_seq):\n",
      "    \"Output rezults\"\n",
      "    f=open(\"out.txt\",\"w\")\n",
      "    f.write(\"%d\\n\" % N)\n",
      "    for l in range(M):\n",
      "        f.write(\"%d%s\" % (glambda[t],\",\"))\n",
      "    f.write(\"%d\\n\" % glambda[M])\n",
      "\n",
      "    for t in range(N-1):\n",
      "        f.write(\"%f%s\" % (in_seq[t].st,\",\"))\n",
      "    f.write(\"%f\\n\" % (in_seq[N-1].st))\n",
      "    f.close()\n",
      "\n",
      "    class Client(object):\n",
      "    \"Class client\"\n",
      "    def __init__(self,cid,st):\n",
      "        self.cid=cid ## customer id\n",
      "        self.st=st ## sojourn time of the customer\n",
      "\n",
      "###GLOBALS\n",
      "N=100 ## total number of customers arrived\n",
      "M=5 ## number of servers\n",
      "### glambda - arrival + servicing frequency \n",
      "### = customers/per time unit\n",
      "glambda=np.array([30000]+[i for i in np.linspace(25000,5000,M)])\n",
      "\n",
      "###START\n",
      "if __name__ == \"__main__\":\n",
      "\tall_clients=[Client(num,0) for num in range(0,N)]\n",
      "        q=[multiprocessing.JoinableQueue() for i in range(M+1)]\n",
      "\n",
      "        for i in range(M):\n",
      "            serv = multiprocessing.Process(target=server,args=(q[i],q[i+1],i))\n",
      "            serv.daemon=True\n",
      "            serv.start()\n",
      "\n",
      "        cons = multiprocessing.Process(target=consumer,args=(q[M],))\n",
      "        cons.start()\n",
      "\n",
      "### start 'produsing' customers \n",
      "producer(all_clients,q[0])\n",
      "\n",
      "for i in q: i.join()\n",
      "\n",
      "Рис. 18. Python код для имитационной модели, использующей мультипроцессорный сервис.\n",
      "\n",
      "Вопросы для изучения:\n",
      "\n",
      "Как глобальные переменные предоставляются процессам и разделяются между ними?\n",
      "Как завершаться процессы, связанные с различными обслуживающими устройствами?\n",
      "Как передаётся информационный поток между различными процессами?\n",
      "Что насчёт корректности модели?\n",
      "Что насчёт эффективности модели. Сколько времени потребуется для обмена информацией различным процессам?\n",
      "\n",
      "Теперь мы можем распечатать результаты, используя модуль matplotlib и можем визуально анализировать результаты, после предоставления диаграммы. Мы можем увидеть (рисунок 19) что модели необходимо дальнейшее совершенствование. Таким образом, мы можем перейди к более мощной модели.\n",
      "\n",
      "\n",
      "Рис. 19. Результаты моделирования имитационной модели мультипроцессорный сервис. \n",
      "\n",
      "5.3. Единичный процесс статистической модели\n",
      "Главной особенностью статистической модели является следующее: теперь мы используем рекуррентное уравнение для точного вычисления времени пребывания заявки в системе; мы обрабатываем все данные в единственном процессе используя специфическую сопрограммную функцию Python; мы осуществляем определённое число моделирований по методу Монте-Карло для лучшей достоверности расчётов. Эта модель даёт нам “точные” расчёты времени пребывания заявки в системе. Главная схема модели представлена на рисунке 20. Учащийся может изучить различия между имитационной и статистической моделью.\n",
      "\n",
      "\n",
      "Рис. 20. Единичный процесс статистической модели\n",
      "\n",
      "Программный код для реализации указанной выше модели представлен на рисунке 21. Результаты симуляции представлены на рисунке 22.\n",
      "\n",
      "#!/usr/bin/python\n",
      "import random\n",
      "import time\n",
      "import numpy as np\n",
      "from numpy import linspace\n",
      "\n",
      "def coroutine(func):\n",
      "    del start(*args,**kwargs):\n",
      "        g = func(*args,**kwargs)\n",
      "        g.next()\n",
      "        return g\n",
      "    return start\n",
      "\n",
      "def print_header():\n",
      "    \"Output rezults - header\"\n",
      "    f=open(\"out.txt\",\"w\") \n",
      "    f.write(\"%d\\n\" % N)\n",
      "    ##number of points in printing template \n",
      "    f.write(\"%d\\n\" % TMPN) \n",
      "    for t in range(M):\n",
      "        f.write(\"%d%s\" % (glambda[t],\",\")) \n",
      "    f.write(\"%d\\n\" % glambda[M]) \n",
      "    f.close()\n",
      "\n",
      "def print_results(in_seq):\n",
      "    \"Output rezults\"\n",
      "    f=open(\"out.txt\",\"a\")\n",
      "    k=()\n",
      "    for i in range(N-2):\n",
      "        if in_seq[i].cid==template[k]:\n",
      "            f.write(\"%f%s\" % (in_seq[i].st,\",\")) \n",
      "            k+=1\n",
      "    f.write(\"%f\\n\" % (in_seq[N-1 ].st))\n",
      "    f.close()\n",
      "\n",
      "coroutine \n",
      "def server(i):\n",
      "\n",
      "    ST=0 ##sojourn time for the previous client\n",
      "    item=None\n",
      "    while True:\n",
      "        item = (yield item) ##get item \n",
      "        if item == None:    ##new Monte Carlo iteration \n",
      "            ST=0 \n",
      "            continue\n",
      "        waiting_time=max(0.0,ST-item.st-item.tau) \n",
      "item.st+=random.expovariate(glambda[i+1])+waiting_time\n",
      "ST=item.st\n",
      "\n",
      "def producer(): \n",
      "    results=[]\n",
      "    i=0\n",
      "    while True:\n",
      "        if i == N: break\n",
      "        c=Client(i,0.,0.)\n",
      "        if i!=0: c.tau=random.expovariate(glambda[0]) \n",
      "        i+= 1\n",
      "        for s in p: c=s.send(c) \n",
      "        results+=[c]\n",
      "    for s in p: c=s.send(None) ##final signal \n",
      "    return results\n",
      "\n",
      "class Client(object):\n",
      "    def __init__(self,cid,st,tau):\n",
      "        self.cid=cid \n",
      "        self.st=st \n",
      "        self.tau=tau \n",
      "    def params(self): \n",
      "        return (self.cid,self.st,self.tau)\n",
      "\n",
      "stt=time.time()\n",
      "\n",
      "N=1000000   ## Clients \n",
      "M=5\t            ## Servers\n",
      "\n",
      "## Input/sevice frequency\n",
      "glambda= [30000]+[i for i in linspace(25000,5000,M)]\n",
      "MKS=20     ## Monte Carlo simulation results\n",
      "\n",
      "## Number of points in the printing template \n",
      "TMPN=N/10000\n",
      "\n",
      "##printing template\n",
      "template= map(int,linspace(0,N-1,TMPN)) \n",
      "\n",
      "print_header()\n",
      "\n",
      "p=[]\n",
      "for i in range(M):p +=[server(i)]\n",
      "for i in range! MKS):\n",
      "    print_results(producer()) \n",
      "    print(\"Step=%d\" % i)\n",
      "\n",
      "sys.stdout.write(\"Processing time:%d\\n\" % int(time.time()-stt))\n",
      "\n",
      "Рис. 21. Python код для единичного процесса статистической модели\n",
      "\n",
      "\n",
      "Рис. 22. Результаты моделирования для единичного процесса статистической модели\n",
      "\n",
      "5.4. Статистическая модель на MPI\n",
      "Следующим шагом развития нашей модели является использование Python MPI модуля – mpi4py. Это позволяет нам работать с большим количеством симуляций Монте Карло и использовать кластер для запуска и тестирования модели. Следующим шагом должно являться дальнейшее совершенствование модели на основе использования языка программирования Си, “реального” MPI или SWIG (https://ru.wikipedia.org/wiki/SWIG) технологии для Python. Эта модель в практически идентична предыдущей модели с той лишь разницей, что используется mpi4py для мультипроцессорной обработки и интеграции результатов (рисунок 23). \n",
      "\n",
      "\n",
      "Рис. 23. Статистическая модель MPI\n",
      "\n",
      "В дополнении к предыдущей модели, должны быть импортированы несколько дополнительных модулей. Функция print_results() также должна быть переписана, так как на данном этапе у нас больше испытаний. Мы должны также переписать главную часть программы. На рисунке 24 мы предоставили только ту часть программного кода, которая отличается от кода предыдущей модели. Результаты моделирования представлены на рисунке 25.\n",
      "\n",
      "....................\n",
      "import sys\n",
      "from mpi4py import MPI\n",
      "....................\n",
      "def print_results(in_seq):\n",
      "    \"Output rezults\"\n",
      "    f=open(\"out.txt\",\"a\")\n",
      "    for m in range(int(size)): \n",
      "        for j in range(MKS):\n",
      "            for i in range(TMPN-l):\n",
      "                f.write(\"%f%s\" % (in_seq[m][i+j*TMPN].st,\",\")) \n",
      "            f.write(\"%f\\n\" % (in_seq[m][(TMPN-l)+j*TMPN].st))\n",
      "    f.close()\n",
      "....................\n",
      "stt=time.time() #start time for the process\n",
      "\n",
      "rank = MPI.COMM_WORLD.Get_rank()\n",
      "size = MPI.COMM_WORLD.Get_size()\n",
      "name = MPI.Get_processor_name()\n",
      "\n",
      "N= 10**3 ## Clients \n",
      "M=5\t       ## Servers\n",
      "## Input/sevice frequency\n",
      "glambda=[30000]+[i for i in linspace(25000,5000,M)]\n",
      "## Number of Monte-Carlo simulations for this particuar process\n",
      "MKS=20\n",
      "TMPN=200 ## Number of points in printing templat\n",
      "template= map(int,linspace(0,N-1,TMPN)) ## points for printing\n",
      "p=[]\n",
      "results=[] ## this process results \n",
      "total_results=[] ## overall results \n",
      "for i in range(M):p +=[server(i)] \n",
      "for i in range(MKS):results+=producer()\n",
      "\n",
      "total_results=MPI.COMM_WORLD.gather(results,0) \n",
      "random.seed(rank)\n",
      "\n",
      "if rank == 0: \n",
      "    print_header() \n",
      "    print_results(total_results)\n",
      "    sys.stdout.write(\"Processing time: %d\\n\" % int(time.time()-stt))\n",
      "\n",
      "Рис. 24. Python код для статистической модели на основе MPI\n",
      "\n",
      "\n",
      "Рис. 25. Результаты моделирования статистической модели MPI\n",
      "\n",
      "6. Выводы\n",
      "В этой статье были рассмотрены несколько моделей для обучения на основе моделирования. Эти модели дают возможность учащемуся провести серию экспериментов и повысить понимание дисциплины Научной Информатики. Есть несколько уровней сложности представленных моделей и экспериментов с такими моделями. Первый уровень – базовый. Он подводит нас к пониманию случайных величин, а также даёт первичное понимание области научных исследований. Следующий уровень является более сложным и даёт более глубокое понимание параллельного программирования и стохастического моделирования. Соответсвующие теоретические знания представлены, и, по необходимости, могут быть использованы в качестве дополнительного материала. Это всё предоставляет базовый инструментарий для введения в Научную Информатику. И, в завершении, мы хотели бы дать рекомендации для дальнейшего изучения и совершенствования моделей. \n",
      "\n",
      "6.1. Линейность модели и статистические параметры СМО\n",
      "Модель мультифазной СМО, представленная в этой статье, не линейна [12]. Это становится очевидно из рекуррентного уравнения, так как она содержит нелинейную математическую функцию max. Если мы хотим получить правильные результаты моделирования, особенно в случае расчёта статистических параметров СМО, мы должны использовать частично линейную модель для расчёта. Эта особенно важно для ненагруженных транспортных систем, так как в противном случае мы можем получить довольно большую ошибочную разницу в расчётах. \n",
      "\n",
      "6.2. Расширения модулей Python и параллельное программирование с Си\n",
      "Для умелых учеников, может быть интересноым продолжить улучшение эффективности программного кода. Это можно сделать путём расширения модулей Python с имплементированными Си функциями используя технологию SWING. Возможно улучшить расчёты кода и ускорить вычисления используя Cython, языка программирования Си, “реальные” MPI технологиии HTC (высокопроизводительные вычисления) в кластерных системах [5, 28, 29].\n",
      "\n",
      "6.3. Эффективность программных решений и дальнейшие разработки\n",
      "В этом разделе учащийся может изучить эффективность различных программных решений. Этот топик важен для любых программных моделей, которые основаны на параллельных вычислениях. Учащийся может изучить эффективность различных программных моделей и попытаться шаг за шагом улучшать алгоритмы. Ключевым моментом здесь является исследование соотношения количества информационных потоков и вычислений для различных программных процессов. Так соотношение имеет важное значение при построении наиболее эффективной разработки программ с параллельными вычислениями. Другой интересный топик – это изучение возможности преобразование алгоритмической структуры на кластерную HTC структуру.\n",
      "\n",
      "В качестве дополнительной задачи для проведения исследований авторы рассматривают моделирование СМО, которая должна быть смоделирована и проанализирована. Сравнительно сложный характер СМО и соответсвующий вид приложений требуют применения более обширных техник программирования. Таким образом, появляется хорошая базовая платформа для внедрения таких общих концепций программирования, как наследование, инкапсуляция и полиморфизм. С другой стороны базовые теоретические концепции информатики также необходимо осветить. Помимо всего этого, статистическое и имитационное моделирование СМО требуют более продвинутых знаний в области теории вероятностей, использования большего количества вычислительных ресурсов и предоставления реальной научной вычислительной среды, а также хорошей мотивации продвинутого учащегося. \n",
      "\n",
      "Литература \n",
      "Полный список использованной литературы[1]  A. Arazi, E. Ben-Jacob and U. Yechiali, Bridging genetic net- works and queueing theory, Physica A: Statistical Mechanics and Its Applications 332 (2004), 585–616. \n",
      "[2]  D.M. Beazley, Python Essential Reference, Addison-Wesley Professional, 2009. \n",
      "[3]  J. Bernard, Use Python for scientific computing, Linux Journal 175 (2008), 7. \n",
      "[4] U.N. Bhat, An Introduction to Queueing Theory Modeling and Analysis in Applications, Birkhäuser, Boston, MA, 2008.\n",
      "[5] K.J. Bogacev, Basics of Parallel Programming, Binom, Moscow, 2003.\n",
      "[6] R.N. Caine and G. Caine, Making Connections: Teaching andthe Human Brain, Association for Supervision and Curriculum Development, Alexandria, 1991.\n",
      "[7] J. Clement and M.A. Rea, Model Based Learning and Instruction in Science, Springer, The Netherlands, 2008.\n",
      "[8] N.A. Cookson, W.H. Mather, T. Danino, O. Mondragón- Palomino, R.J. Williams, L.S. Tsimring and J. Hasty, Queue- ing up for enzymatic processing: correlated signaling through coupled degradation, Molecular Systems Biology 7 (2011), 1. [9] A.S. Gibbons, Model-centered instruction, Journal of Structural Learning and Intelligent Systems 4 (2001), 511–540. [10] M.T. Heath, Scientific Computing an Introductory Survey, McGraw-Hill, New York, 1997.\n",
      "[11] A. Hellander, Stochastic Simulation and Monte Carlo Meth- ods, 2009.\n",
      "[12] G.I. Ivcenko, V.A. Kastanov and I.N. Kovalenko, Queuing System Theory, Visshaja Shkola, Moscow, 1982.\n",
      "[13] Z.L. Joel, N.W. Wei, J. Louis and T.S. Chuan, Discrete–event \n",
      "simulation of queuing systems, in: Sixth Youth Science Con- ference, Singapore Ministry of Education, Singapore, 2000, pp. 1–5. \n",
      "[14] E. Jones, Introduction to scientific computing with Python, in: SciPy, California Institute of Technology, Pasadena, CA, 2007, p. 333. \n",
      "[15] M. Joubert and P. Andrews, Research and developments in probability education internationally, in: British Congress for Mathematics Education, 2010, p. 41. \n",
      "[16] G.E. Karniadakis and R.M. Kyrby, Parallel Scientific Comput- ing in C++ and MPI. A Seamless Approach to Parallel Al- gorithms and Their Implementation, Cambridge Univ. Press, 2003. \n",
      "[17] D.G. Kendall, Stochastic processes occurring in the theory of queues and their analysis by the method of the imbedded Markov chain, The Annals of Mathematical Statistics 1 (1953), 338–354. \n",
      "[18] M.S. Khine and I.M. Saleh, Models and modeling, cognitive tools for scientific enquiry, in: Models and Modeling in Science Education, Springer, 2011, p. 290. \n",
      "[19] T. Kiesling and T. Krieger, Efficient parallel queuing system simulation, in: The 38th Conference on Winter Simulation, Winter Simulation Conference, 2006, pp. 1020–1027. \n",
      "[20] J. Kiusalaas, Numerical Methods in Engineering with Python, Cambridge Univ. Press, 2010. \n",
      "[21] A. Kumar, Python for Education. Learning Maths & Science Using Python and Writing Them in LATEX, Inter University Accelerator Centre, New Delhi, 2010. \n",
      "[22] H.P. Langtangen, Python Scripting for Computational Science, Springer-Verlag, Berlin, 2009. \n",
      "[23] H.P. Langtangen, A Primer on Scientific Programming with Python, Springer-Verlag, Berlin, 2011. \n",
      "[24] H.P. Langtangen, Experience with using Python as a primary language for teaching scientific computing at the University of Oslo, University of Oslo, 2012. \n",
      "[25] R. Lehrer and L. Schauble, Cultivating model-based reason- ing in science education, in: The Cambridge Handbook of the Learning Sciences, Cambridge Univ. Press, 2005, pp. 371–388. \n",
      "[26]  G. Levy, An introduction to quasi-random numbers, in: Nu- merical Algorithms, Group, 2012. \n",
      "[27]  J.S. Liu, Monte Carlo Strategies in Scientific Computing, Har- vard Univ., 2001. \n",
      "[28]  V.E. Malishkin and V.D. Korneev, Parallel Programming of Multicomputers, Novosibirsk Technical Univ., Novosibirsk, 2006. \n",
      "[29]  N. Matloff, Programming on Parallel Machines: GPU, Multi- core, Clusters and More, University of California, 2012. \n",
      "[30]  M. Milrad, J.M. Spector and P.I. Davidsen, Model facilitated  ",
      "learning, in: Instructional Design, Development and Evalua-  ",
      "tion, Syracuse Univ. Press, 2003. \n",
      "[31]  S. Minkevicˇius, On the law of the iterated logarithm in multi- phase queueing systems, Informatica II (1997), 367–376. \n",
      "[32]  S. Minkevicˇius and V. Dolgopolovas, Analysis of the law of the iterated logarithm for the idle time of a customer in multiphase  ",
      "queues, Int. J. Pure Appl. Math. 66 (2011), 183–190. \n",
      "[33]  Model-Centered Learning, Pathways to mathematical under- standing using GeoGebra, in: Modeling and Simulations for Learning and Instruction, Sense Publishers, The Netherlands,  ",
      "2011. \n",
      "[34] C.R. Myers and J.P. Sethna, Python for education: Computa- tional methods for nonlinear systems, Computing in Science & Engineering 9 (2007), 75–79. \n",
      "[35] H. Niederreiter, Random Number Generation and Quasi- Monte Carlo Methods, SIAM, 1992. \n",
      "[36] F.B. Nilsen, Queuing systems: Modeling, analysis and simu- lation, Department of Informatics, University of Oslo, Oslo, 1998. \n",
      "[37] R.P. Sen, Operations Research: Algorithms and Applications, PHI Learning, 2010. \n",
      "[38] F. Stajano, Python in education: Raising a generation of native speakers, in: 8th International Python Conference, Washing- ton, DC, 2000, pp. 1–5. \n",
      "[39] J. Sztrik, Finite-source queueing systems and their applica- tions, Formal Methods in Computing 1 (2001), 7–10. \n",
      "[40] L. Xue, Modeling and simulation in scientific computing ed- ucation, in: International Conference on Scalable Computing and Communications, 2009, pp. 577–580.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Совсем недавно произошёл релиз минималистичного Alpine Linux 3.8. Очень часто данный linux образ используют в докере, собирая очень компактные окружения для runtime.\n",
      "\r\n",
      "Сегодняшняя статья будет рассмотрена в срезе использования runtime системы в докере для Python 3.6.X версий, с различным составом пакетов pip. А так же мы соберём самый новый Python 3.7 в Alpine.\n",
      "\r\n",
      "В конце статьи будет представлен размер образа image, занимаемый на диске, в зависимости от состава пакетов pip и произведено сравнение между дистрибутивами Alpine 3.8, Debian 9, Fedora 28.\n",
      "\r\n",
      "Итак, приступим: для тестирования дистрибутивы выбраны. Будем собирать следующие docker images:\n",
      "\n",
      "Система, ее обновление. И Python3 с обновлённым pip (10 версии)\n",
      "п.1 + tornado cython\n",
      "п.2 + numpy-scipy\n",
      "п.3 + pillow bokeh pandas websocket-client\n",
      "\r\n",
      "В результате даных заливок, мы получим различные версии: Python без пакетов, Python с web сервером, Python с пакетами для обработки многопоточных математических вычислений, Python с «графическим» стеком и работы с данными.\n",
      "\r\n",
      "Итак, результирующие файлы для Debian и Fedora будут выглядеть у нас так:\r\n",
      "Debian\n",
      "FROM debian\n",
      " \n",
      "RUN apt-get update -y && apt-get install python3-pip -y && pip3 install pip --upgrade && apt-get clean\n",
      " \n",
      "RUN pip3 install cython tornado websocket-client pytest numpy pandas scipy bokeh pillow\n",
      "\n",
      "\r\n",
      "Fedora\n",
      "FROM fedora\n",
      "  \n",
      "RUN dnf update -y && dnf install libstdc++ -y && dnf clean all && pip3 install --upgrade pip && python3 --version\n",
      "  \n",
      "RUN pip3 install cython tornado websocket-client pytest numpy pandas scipy bokeh pillow\n",
      "\r\n",
      "А вот с Alpine 3.8 пока заминка. Официально на момент написания статьи он ещё на вышел, а посмотреть, то хочется:-). Поэтому нам понадобиться их образ системы:\n",
      "dl-cdn.alpinelinux.org/alpine/v3.8/releases/x86_64\r\n",
      "И мы соберём свой Alpine from Sratch:\n",
      "github.com/gliderlabs/docker-alpine/tree/master/versions/library-3.8/x86_64\n",
      "\r\n",
      "Выкачиваем файловую систему Alpine 3.8:\r\n",
      "curl dl-cdn.alpinelinux.org/alpine/v3.8/releases/x86_64/alpine-minirootfs-3.8.0_rc10-x86_64.tar.gz >alpine3.8.tar.gz\n",
      "\r\n",
      "Создаём свой докер файл:\n",
      "FROM scratch\n",
      "ADD alpine3.8.tar.gz /\n",
      "\n",
      "ENV RELEASE=\"v3.8\"\n",
      "ENV MIRROR=\"http://dl-cdn.alpinelinux.org/alpine\"\n",
      "ENV PACKAGES=\"alpine-baselayout,busybox,alpine-keys,apk-tools,libc-utils\"\n",
      "ENV TAGS=(alpine:3.8)\n",
      "\n",
      "\r\n",
      "Затем копипастим и добавляем в этот файл борку Python 3.6 со страницы github.com/docker-library/python/blob/master/3.6/alpine3.7/Dockerfile\n",
      "не забыв удалить или закомментировать строку FROM alpine:3.7\n",
      "\r\n",
      "И пробуем создать образ с Alpine 3.8 и Python на борту:\n",
      "\n",
      "$docker build -t alpine3.8 .\n",
      "#результат работы (последние строки)\n",
      "(1/2) Purging .fetch-deps (0)\n",
      "(2/2) Purging libressl (2.7.4-r0)\n",
      "Executing busybox-1.28.4-r0.trigger\n",
      "OK: 33 MiB in 45 packages\n",
      "+ python get-pip.py --disable-pip-version-check --no-cache-dir 'pip==10.0.1'\n",
      "Collecting pip==10.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/74/ecd13431bcc456ed390b44c8a6e917c1820365cbebcb6a8974d1cd045ab4/pip-10.0.1-py2.py3-none-any.whl (1.3MB)\n",
      "Collecting setuptools\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/e1/820d941153923aac1d49d7fc37e17b6e73bfbd2904959fffbad77900cf92/setuptools-39.2.0-py2.py3-none-any.whl (567kB)\n",
      "Collecting wheel\n",
      "  Downloading https://files.pythonhosted.org/packages/81/30/e935244ca6165187ae8be876b6316ae201b71485538ffac1d718843025a9/wheel-0.31.1-py2.py3-none-any.whl (41kB)\n",
      "Installing collected packages: pip, setuptools, wheel\n",
      "Successfully installed pip-10.0.1 setuptools-39.2.0 wheel-0.31.1\n",
      "+ pip --version\n",
      "pip 10.0.1 from /usr/local/lib/python3.6/site-packages/pip (python 3.6)\n",
      "+ find /usr/local -depth '(' '(' -type d -a '(' -name test -o -name tests ')' ')' -o '(' -type f -a '(' -name '*.pyc' -o -name '*.pyo' ')' ')' ')' -exec rm -rf '{}' +\n",
      "+ rm -f get-pip.py\n",
      " ---> f7439dca5f31\n",
      "Removing intermediate container e1fcd1c74873\n",
      "Step 17/17 : CMD python3\n",
      " ---> Running in a4dc6dfa5184\n",
      " ---> 6924b206c6b9\n",
      "Removing intermediate container a4dc6dfa5184\n",
      "Successfully built 6924b206c6b9\n",
      "\n",
      "\r\n",
      "Результаты первого шага установка только Python (docker images --all):\n",
      "\n",
      "Debian 9 / 513 MB\n",
      " Fedora 28 / 387 MB \n",
      " Alpine 3.8 / 82.2 MB \n",
      "\n",
      "\n",
      "Шаг 2. Установка cython и tornadoНачинаем добавлять пакеты pip. Первым установим cython и tornado. Для Debian и Fedora пакеты ставятся без ошибок, а вот Alpine падает с ошибкой:\n",
      "unable to execute 'gcc': No such file or directory\n",
      "    error: command 'gcc' failed with exit status 1\r\n",
      "Придется гуглить и потом уже добавлять библиотеки сборки в Alpine, чтобы pip успешно собрал их из исходного текста. Затем запускать сборку докера снова, затем опять искать зависимости, читать форумы stackoverflow и issues в github и ждать и ждать и ждать.\n",
      "\n",
      "\r\n",
      "Поскольку в следующих шагах мы начнём добавлять математические и графические библиотеки в наш образ runtime Python, и чтобы слишком не увеличивать текст данной статьи, я приведу финальные зависимости для Alpine linux:\n",
      "         apk add --no-cache --virtual .build-deps  \\\n",
      "                gfortran \\\n",
      "                build-base \\\n",
      "                openblas-dev \\\n",
      "                bzip2-dev \\\n",
      "                coreutils \\\n",
      "                dpkg-dev dpkg \\\n",
      "                expat-dev \\\n",
      "                gcc \\\n",
      "                gdbm-dev \\\n",
      "                libc-dev \\\n",
      "                libffi-dev \\\n",
      "                libnsl-dev \\\n",
      "                libressl \\\n",
      "                libressl-dev \\\n",
      "                libtirpc-dev \\\n",
      "                linux-headers \\\n",
      "                make \\\n",
      "                ncurses-dev \\\n",
      "                pax-utils \\\n",
      "                readline-dev \\\n",
      "                sqlite-dev \\\n",
      "                tcl-dev \\\n",
      "                tk \\\n",
      "                tk-dev \\\n",
      "                xz-dev \\\n",
      "                zlib-dev \\\n",
      "                libxml2-dev \\\n",
      "                libxslt-dev \\\n",
      "                musl-dev \\\n",
      "                libgcc \\\n",
      "                curl \\\n",
      "                jpeg-dev \\\n",
      "                zlib-dev \\\n",
      "                freetype-dev \\\n",
      "                lcms2-dev \\\n",
      "                openjpeg-dev \\\n",
      "                tiff-dev \\\n",
      "                tk-dev \\\n",
      "                tcl-dev \\\n",
      "        && ln -s /usr/include/locale.h /usr/include/xlocale.h\n",
      "\n",
      "Debian 9 / 534 MB\n",
      " Fedora 28 / 407 MB \n",
      " Alpine 3.8 / 144 MB \n",
      "\n",
      "\n",
      "Шаг 3. Добавляем математику numpy scipy\n",
      "\n",
      "Debian 9 / 763 MB\n",
      " Fedora 28 / 626 MB \n",
      " Alpine 3.8 / 404 MB MB \n",
      "\n",
      "\n",
      "Шаг 4. Добавляем графический стек websocket-client pytest pandas bokeh pillow\n",
      "\n",
      "Dockerfile-Alpine\n",
      "FROM scratch\n",
      "ADD alpine3.8.tar.gz /\n",
      "CMD [\"/bin/sh\"]\n",
      "\n",
      "ENV RELEASE=\"v3.8\"\n",
      "ENV MIRROR=\"http://dl-cdn.alpinelinux.org/alpine\"\n",
      "ENV PACKAGES=\"alpine-baselayout,busybox,alpine-keys,apk-tools,libc-utils\"\n",
      "#ENV BUILD_OPTIONS=(-b -s -t UTC -r $RELEASE -m $MIRROR -p $PACKAGES)\n",
      "ENV TAGS=(alpine:3.8)\n",
      "\n",
      "#\n",
      "# NOTE: THIS DOCKERFILE IS GENERATED VIA \"update.sh\"\n",
      "#\n",
      "# PLEASE DO NOT EDIT IT DIRECTLY.\n",
      "#\n",
      "#https://github.com/docker-library/python/blob/master/3.6/alpine3.7/Dockerfile\n",
      "# ensure local python is preferred over distribution python\n",
      "ENV PATH /usr/local/bin:$PATH\n",
      "\n",
      "# http://bugs.python.org/issue19846\n",
      "# > At the moment, setting \"LANG=C\" on a Linux system *fundamentally breaks Python 3*, and that's not OK.\n",
      "ENV LANG C.UTF-8\n",
      "\n",
      "# install ca-certificates so that HTTPS works consistently\n",
      "# the other runtime dependencies for Python are installed later\n",
      "RUN apk add --no-cache ca-certificates\n",
      "\n",
      "ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D\n",
      "ENV PYTHON_VERSION 3.6.6\n",
      "\n",
      "RUN set -ex \\\n",
      "    && apk add --no-cache --virtual .fetch-deps \\\n",
      "\tgnupg \\\n",
      "\tlibressl \\\n",
      "\ttar \\\n",
      "\txz \\\n",
      "    \\\n",
      "    && wget -O python.tar.xz \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz\" \\\n",
      "    && wget -O python.tar.xz.asc \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc\" \\\n",
      "    && export GNUPGHOME=\"$(mktemp -d)\" \\\n",
      "    && gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$GPG_KEY\" \\\n",
      "    && gpg --batch --verify python.tar.xz.asc python.tar.xz \\\n",
      "    && rm -rf \"$GNUPGHOME\" python.tar.xz.asc \\\n",
      "    && mkdir -p /usr/src/python \\\n",
      "    && tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \\\n",
      "    && rm python.tar.xz \\\n",
      "    \\\n",
      "    && apk add --no-cache --virtual .build-deps  \\\n",
      "\tbzip2-dev \\\n",
      "\tcoreutils \\\n",
      "\tdpkg-dev dpkg \\\n",
      "\texpat-dev \\\n",
      "\tgcc \\\n",
      "\tgdbm-dev \\\n",
      "\tlibc-dev \\\n",
      "\tlibffi-dev \\\n",
      "\tlibnsl-dev \\\n",
      "\tlibressl \\\n",
      "\tlibressl-dev \\\n",
      "\tlibtirpc-dev \\\n",
      "\tlinux-headers \\\n",
      "\tmake \\\n",
      "\tncurses-dev \\\n",
      "\tpax-utils \\\n",
      "\treadline-dev \\\n",
      "\tsqlite-dev \\\n",
      "\ttcl-dev \\\n",
      "\ttk \\\n",
      "\ttk-dev \\\n",
      "\txz-dev \\\n",
      "\tzlib-dev \\\n",
      "# add build deps before removing fetch deps in case there's overlap\n",
      "    && apk del .fetch-deps \\\n",
      "    \\\n",
      "    && cd /usr/src/python \\\n",
      "    && gnuArch=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" \\\n",
      "    && ./configure \\\n",
      "\t--build=\"$gnuArch\" \\\n",
      "\t--enable-loadable-sqlite-extensions \\\n",
      "\t--enable-shared \\\n",
      "\t--with-system-expat \\\n",
      "\t--with-system-ffi \\\n",
      "\t--without-ensurepip \\\n",
      "    && make -j \"$(nproc)\" \\\n",
      "# set thread stack size to 1MB so we don't segfault before we hit sys.getrecursionlimit()\n",
      "# https://github.com/alpinelinux/aports/commit/2026e1259422d4e0cf92391ca2d3844356c649d0\n",
      "\tEXTRA_CFLAGS=\"-DTHREAD_STACK_SIZE=0x100000\" \\\n",
      "    && make install \\\n",
      "    \\\n",
      "    && runDeps=\"$( \\\n",
      "\tscanelf --needed --nobanner --format '%n#p' --recursive /usr/local \\\n",
      "\t    | tr ',' '\\n' \\\n",
      "\t    | sort -u \\\n",
      "\t    | awk 'system(\"[ -e /usr/local/lib/\" $1 \" ]\") == 0 { next } { print \"so:\" $1 }' \\\n",
      "    )\" \\\n",
      "    && apk add --virtual .python-rundeps $runDeps \\\n",
      "    && apk del .build-deps \\\n",
      "    \\\n",
      "    && find /usr/local -depth \\\n",
      "\t\\( \\\n",
      "\t    \\( -type d -a \\( -name test -o -name tests \\) \\) \\\n",
      "\t    -o \\\n",
      "\t    \\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n",
      "\t\\) -exec rm -rf '{}' + \\\n",
      "    && rm -rf /usr/src/python\n",
      "\n",
      "# make some useful symlinks that are expected to exist\n",
      "RUN cd /usr/local/bin \\\n",
      "    && ln -s idle3 idle \\\n",
      "    && ln -s pydoc3 pydoc \\\n",
      "    && ln -s python3 python \\\n",
      "    && ln -s python3-config python-config\n",
      "\n",
      "# if this is called \"PIP_VERSION\", pip explodes with \"ValueError: invalid truth value '<VERSION>'\"\n",
      "ENV PYTHON_PIP_VERSION 10.0.1\n",
      "\n",
      "RUN set -ex; \\\n",
      "    \\\n",
      "    apk add --no-cache --virtual .fetch-deps libressl; \\\n",
      "    \\\n",
      "    wget -O get-pip.py 'https://bootstrap.pypa.io/get-pip.py'; \\\n",
      "    \\\n",
      "    apk del .fetch-deps; \\\n",
      "    \\\n",
      "    python get-pip.py \\\n",
      "\t--disable-pip-version-check \\\n",
      "\t--no-cache-dir \\\n",
      "\t\"pip==$PYTHON_PIP_VERSION\" \\\n",
      "    ; \\\n",
      "    pip --version; \\\n",
      "    \\\n",
      "    find /usr/local -depth \\\n",
      "\t\\( \\\n",
      "\t    \\( -type d -a \\( -name test -o -name tests \\) \\) \\\n",
      "\t    -o \\\n",
      "\t    \\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n",
      "\t\\) -exec rm -rf '{}' +; \\\n",
      "    rm -f get-pip.py\n",
      "\n",
      "         apk add --no-cache --virtual .build-deps  \\\n",
      "                gfortran \\\n",
      "                build-base \\\n",
      "                openblas-dev \\\n",
      "                bzip2-dev \\\n",
      "                coreutils \\\n",
      "                dpkg-dev dpkg \\\n",
      "                expat-dev \\\n",
      "                gcc \\\n",
      "                gdbm-dev \\\n",
      "                libc-dev \\\n",
      "                libffi-dev \\\n",
      "                libnsl-dev \\\n",
      "                libressl \\\n",
      "                libressl-dev \\\n",
      "                libtirpc-dev \\\n",
      "                linux-headers \\\n",
      "                make \\\n",
      "                ncurses-dev \\\n",
      "                pax-utils \\\n",
      "                readline-dev \\\n",
      "                sqlite-dev \\\n",
      "                tcl-dev \\\n",
      "                tk \\\n",
      "                tk-dev \\\n",
      "                xz-dev \\\n",
      "                zlib-dev \\\n",
      "                libxml2-dev \\\n",
      "                libxslt-dev \\\n",
      "                musl-dev \\\n",
      "                libgcc \\\n",
      "                curl \\\n",
      "                jpeg-dev \\\n",
      "                zlib-dev \\\n",
      "                freetype-dev \\\n",
      "                lcms2-dev \\\n",
      "                openjpeg-dev \\\n",
      "                tiff-dev \\\n",
      "                tk-dev \\\n",
      "                tcl-dev \\\n",
      "        && ln -s /usr/include/locale.h /usr/include/xlocale.h \\\n",
      "        && pip install cython tornado websocket-client pytest numpy pandas scipy bokeh pillow \\\n",
      "        && apk del .build-deps \\\n",
      "       && apk add --no-cache  libstdc++ openblas zlib jpeg openjpeg tiff tk tcl musl libxml2 libxslt xz zlib libstdc++ openblas \\\n",
      "    && pip list\n",
      "\n",
      "CMD [\"python3\"]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Debian 9 / 905 MB\n",
      " Fedora 28 / 760 MB \n",
      " Alpine 3.8 / 650 MB \n",
      "\n",
      "\r\n",
      "В качестве бонуса, попробуем в Alpine 3.8 скомпилировать ещё не вышедший для докера Python 3.7.\n",
      "Новая версия Python 3.7 представлена 27 июня 2018 года\n",
      "\r\n",
      "Код компиляции возьмём из:\n",
      "github.com/docker-library/python/tree/bbbc37fff3411a34deef30dd9b34dc938fe7b134/3.7-rc/alpine3.7\n",
      "\n",
      "Завершение компиляции 3.7Package          Version\n",
      "---------------- -------\n",
      "atomicwrites     1.1.5  \n",
      "attrs            18.1.0 \n",
      "bokeh            0.13.0 \n",
      "Cython           0.28.3 \n",
      "Jinja2           2.10   \n",
      "MarkupSafe       1.0    \n",
      "more-itertools   4.2.0  \n",
      "numpy            1.14.5 \n",
      "packaging        17.1   \n",
      "pandas           0.23.1 \n",
      "Pillow           5.1.0  \n",
      "pip              10.0.1 \n",
      "pluggy           0.6.0  \n",
      "py               1.5.4  \n",
      "pyparsing        2.2.0  \n",
      "pytest           3.6.2  \n",
      "python-dateutil  2.7.3  \n",
      "pytz             2018.4 \n",
      "PyYAML           4.1    \n",
      "scipy            1.1.0  \n",
      "setuptools       39.2.0 \n",
      "six              1.11.0 \n",
      "tornado          5.0.2  \n",
      "websocket-client 0.48.0 \n",
      "wheel            0.31.1 \n",
      " ---> 3d18b8c27cd9\n",
      "Removing intermediate container f546e004b79f\n",
      "Step 18/18 : CMD python3\n",
      " ---> Running in 23c5aea50a0d\n",
      " ---> d5385e425064\n",
      "Removing intermediate container 23c5aea50a0d\n",
      "Successfully built d5385e425064\n",
      "\n",
      "real    41m17,619s\n",
      "\n",
      "\r\n",
      "Размер Alpine 3.8 с Python 3.7 с текущим списком пакетов pip 656 MB\n",
      "\n",
      "Итоги\r\n",
      "Python\n",
      "\n",
      "Debian 9 / больше в 6.24х / +430 Mb\n",
      "Fedora 28 / больше в 4,7х / +304 Mb\n",
      "\r\n",
      "Python tornado cython\n",
      "\n",
      "Debian 9 / больше в 3,71х / +390 Mb\n",
      "Fedora 28 / больше в 2,82x / +263 Mb\n",
      "\n",
      "\r\n",
      "Python tornado cython numpy scipy\n",
      "\n",
      "Debian 9 / больше в 1,88 раз / +359 Mb\n",
      "Fedora 28 / больше в 1.54 раз / +222 Mb\n",
      "\n",
      "\r\n",
      "Python tornado cython numpy scipy websocket-client pytest pandas bokeh pillow\n",
      "\n",
      "Debian 9 / больше в 1,39 раз / +255 Mb\n",
      "Fedora 28 / больше в 1.16 раз / +110 Mb\n",
      "\n",
      "При использовании пустого runtime Python, дистрибутив Alpine linux лидер по минимальному размеру. При увеличени количества библиотек pip до tornado+cython+numpy+scipy Alpine все ещё дает заметную экономию в размере на жёстком диске. Одако как только в пакетах появляются графические утилиты для работы с данными для Python, разница практически исчезает.\n",
      "\r\n",
      "При большом количестве графических пакетов, оптимальнее выбрать дистрибутив Fedora, чем заниматься компиляцией пакетов в Alpine (компиляция может длиться 1-2 часа), и в результате получить экономию в один или два десятка процентов места на жёстком диске.\n",
      "\n",
      "UPDATE1: Тестирование проводилось на Fedora Atomic Host: release 28 (Twenty Eight), Version: 2018.5\n",
      "\n",
      "UPDATE2: Проверка занимаемого места на диске, была проведена по данному билду hub.docker.com/r/flytrue/python-runtime-docker/tags\n",
      "$docker pull flytrue/python-runtime-docker:alpine-full\n",
      "$docker save flytrue/python-runtime-docker:alpine-full -o alpine-full.tar\n",
      "$ls -lh alpine-full.tar \n",
      "-rw-------. 1 fedora fedora 631M июн 29 08:38 alpine-full.tar\n",
      "$ docker images --all|grep alpine\n",
      "docker.io/flytrue/python-runtime-docker   alpine-full         f37154658671        19 hours ago        650 MB\n",
      "      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Какой образ для Python вы используете \n",
      "            76.16%\n",
      "           Официальный образ Python из https://hub.docker.com/ \n",
      "            115\n",
      "           \n",
      "            0.66%\n",
      "           Неофициальный образ Python из https://hub.docker.com/ \n",
      "            1\n",
      "           \n",
      "            23.18%\n",
      "           Собираю сам \n",
      "            35\n",
      "            \n",
      "       Проголосовал 151 пользователь. \n",
      "\n",
      "       Воздержались 85 пользователей. \n",
      "      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. На основе какой системы собраны ваши Docker образы для Python \n",
      "            62.55%\n",
      "           Alpine \n",
      "            147\n",
      "           \n",
      "            2.98%\n",
      "           Arch linux \n",
      "            7\n",
      "           \n",
      "            20.43%\n",
      "           Debian \n",
      "            48\n",
      "           \n",
      "            2.13%\n",
      "           Fedora \n",
      "            5\n",
      "           \n",
      "            0.43%\n",
      "           SUSE, openSUSE, SLES \n",
      "            1\n",
      "           \n",
      "            34.04%\n",
      "           Ubuntu \n",
      "            80\n",
      "           \n",
      "            9.79%\n",
      "           RHEL, Oracle Linux, CentOS, Scientific Linux \n",
      "            23\n",
      "           \n",
      "            6.38%\n",
      "           Не знаю \n",
      "            15\n",
      "            \n",
      "       Проголосовали 235 пользователей. \n",
      "\n",
      "       Воздержался 101 пользователь. \n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Собрались мы с Олегом Буниным (olegbunin) и Валентином Домбровским поговорить про то, как к осени вместе подготовим классную конференцию про Python, и записали видео.\n",
      "\r\n",
      "Под катом наша беседа в текстовом виде. В частности, ответ на самый главный вопрос, зачем вообще нужны конференции. И хочу заметить, не для того, чтобы чему-нибудь научиться – учатся сейчас в интернете (например на Хабре :).\n",
      "\r\n",
      "Мы немного поговорили о трендах в экосистеме Python, о темах будущей конференции и организационных фишках. Кстати, обсуждение тем открытое, и каждый может предложить нам позвать конкретного спикера или подсказать направление. Посмотрите, что у нас уже есть, и включайтесь.\n",
      "\n",
      "\n",
      "Олег: Всем привет. Поговорим об этой осени. Сегодня у нас в гостях Валентин и Григорий, которые кое-что знают про Python [питон]. Расскажите всем!\n",
      "\n",
      "Григорий: Во-первых, он Пайтон.\n",
      "\n",
      "Олег: Простите, расскажите нам про Пайтон.\n",
      "\n",
      "Валентин: Привет, Олег. С нами еще замечательный Олег Бунин. Как говорится, я не знаю кто эти люди, но интервью у них берет Олег. Да, мы с Гришей являемся евангелистами сообщества Moscow Python, которое появилось в 2012 году под именем Moscow Django (это такой фреймворк на Python). Шесть лет назад в марте 2012 года у нас появилась идея запустить сообщество для разработчиков. Идея возникла из-за того, что я был директором по маркетингу студии разработки, которая как раз-таки занималась разработкой на Django. В качестве идеи для продвижения этой студии мы решили запустить сообщество.\n",
      "\r\n",
      "У нас была небольшая компания, которая называлась Sevenquark. Мы объединились с сообществом для стартапов Greenfield Project и 1 марта 2012 года провели свое первое мероприятие, на котором присутствовало 20 человек. Эта идея довольно быстро прижилась, и мы разрослись. За эти шесть лет выросли до сообщества, в котором мы насчитываем более 5000 человек.\n",
      "\n",
      "Олег: По-моему, это сейчас самый популярный язык программирования.\n",
      "\n",
      "Григорий: Один из самых популярных.\n",
      "\n",
      "Олег: После Java. И Perl :)\n",
      "\n",
      "Григорий: Очень трудно сравнивать, потому что есть JavaScript, кровавая enterprise Java, есть С#, есть всякие интересные вещи типа Go.\n",
      "\n",
      "Но, конечно, позиции Python очень сильны.\n",
      "\n",
      "Валентин: И в последнее время они усиливаются с ростом популярности машинного обучения и искусственного интеллекта. Об этом мы сможем говорить позднее, когда будем говорить об этих технологиях на Python. За шесть лет мы выросли в сообщество, которое провело уже 58 митапов. Мы проводим их ежемесячно. На каждый митап к нам приходит порядка двухсот человек. Встречи проходят в крупнейших компаниях: Яндекс, Rambler, Mail.ru Group, Saran и так далее.\n",
      "\n",
      "Олег: Что еще делает сообщество, кроме встреч?\n",
      "\n",
      "Валентин: Наше сообщество состоит из 5000 человек, как я уже сказал, мы их считаем по списку рассылки, включая наших замечательных евангелистов.\n",
      "\n",
      "Олег: Это же самое большое русскоязычное сообщество.\n",
      "\n",
      "Григорий: Есть немножко.\n",
      "\n",
      "Валентин: Я думаю — да, мы самое крупное и самое активное IT-сообщество. Те вещи, которые мы делаем, довольно таки уникальные. По крайней мере, я пока не знаю тех, кто делает то же, что и мы. У сообщества Moscow Python есть пять евангелистов: ваш покорный слуга, Григорий Петров, Михаил Корнеев, Илья Лебедев и Владимир Филонов. Вместе мы делаем различные активности помимо митапов. Это Learn Python — курс для тех, кто хочет научиться Python с нуля. Мы уже провели девять наборов и с сентября набираем десятый. И конференции, которые мы начали делать с 2016 года.\n",
      "\n",
      "Олег: Зачем конференции? Собирались митапами и собирались бы.\n",
      "\n",
      "Валентин: Родилась идея — из того состава докладчиков, специалистов по Python отобрать лучших из лучших и сделать то, что мы называем мегамитап. Это митап длиной в целый день, на котором выступают лучшие докладчики, рассказывают о своих практиках, технологиях и передают свои знания сообществу. Идея была сделать это отчетным событием нашего сообщества, где мы рассказываем, как мы развиваемся и что планируем сделать.\n",
      "\n",
      "Олег: Собрание акционеров.\n",
      "\n",
      "Григорий: Что-то вроде. Когда на митап приходит больше четырехсот человек, возникает резонный вопрос: раз нас столько собралось — надо фигачить конференцию.\n",
      "\n",
      "Олег: Они получают приглашения от вас на митапы? Вы каждый месяц проводите по встрече.\n",
      "\n",
      "Валентин: Да. Сейчас, кроме месяца, когда проводится конференция, это октябрь, мы действительно ежемесячно проводим митапы, не делая перерывов.\n",
      "\n",
      "Наш план — проводить 11 митапов в год.\n",
      "\n",
      "Олег: В 2016 году была первая конференция. Расскажите про нее. Получилось или нет то, что вы хотели? Что пошло дальше? Куда решили развивать?\n",
      "\n",
      "Валентин: В 2016 году мы собрали первую конференцию в гостиничном комплексе Измайлово. Для первого опыта это было неплохо. Тогда представили 16 докладов в два потока.\n",
      "\n",
      "Олег: Это один день был, да?\n",
      "\n",
      "Валентин: Да. Формат конференции и содержание докладов отвечали потребностям аудитории. Людям понравилось, но, возможно, было некоторое снисхождение, потому что это был первый опыт. Мы старались это подчеркивать на конференции: «Сейчас будет много косяков и из-за этого будет особенно интересно», — и косяки были.\n",
      "\n",
      "Олег: Это мы, как организаторы конференции, видим косяк. В большинстве случаев посетителям он незаметен.\n",
      "\n",
      "Валентин: Именно это я и хотел сказать. Даже если это как-то касается публики, люди чаще всего обращают на это мало внимания. Поэтому было довольно душевно. После мероприятия мы посидели в пивном баре Круг. Всё закончилось, но мы решили продолжать и получили позитивные отклики.\n",
      "\n",
      "Олег: Сколько собрали участников в 2017 году?\n",
      "\n",
      "Валентин: В 2017 году было порядка двухсот человек. Мы постарались расширить, увеличить масштаб мероприятия. Проводили мы его на площадке Технополис. Случайно так совпало, что в этом же году Яндекс проводила Yet another Conference. Поэтому мы решили, что это подтверждение того, что площадка довольно качественная. Мне кажется, что там было довольно фотогеничное пространство, красивый зал. Мы пригласили иностранных докладчиков. У нас выступали четыре иностранных докладчика с тремя докладами и два провели workshop — такая интересная схема. Это было снова однодневное мероприятие на два потока докладов. Затем мы снова решили продолжать.\n",
      "\n",
      "Олег: Что с планами на этот год? Будет Moscow Python? Я немножко лукавлю, задавая этот вопрос, конечно.\n",
      "\n",
      "Григорий: Конечно же, будет.\n",
      "\n",
      "Олег: Да, конечно же, будет. В этом году мы будем делать Moscow Python Conf ++ вместе с сообществом. Мы — такая машинка по организации конференции. За последние 10 лет мы научились организовывать классные конференции, но ничего не знаем про Python. Ребята всё знают про Python. Мы решили объединиться и сделать Мега событие, которое будет в разы больше, чем в прошлом году.\n",
      "\n",
      "Поставить самим себе новую планку, взять ее и сделать очень интересно.\n",
      "\r\n",
      "Я немного расскажу про технические подробности, а потом попытаю ребят про содержание, про самую вкуснятину. Это будет 2 дня 22 и 23 октября в центре Москвы на хорошей, милой площадке Инфопространство, с которой мы стартовали HighLoad++ 11 лет назад. Два полных дня, несколько треков с докладами, митапы и другие различные активности в течение дня. Обязательная вечеринка в первый день. Пару лет назад мы начали их делать всегда и нам очень-очень нравится. Мы умеем их организовывать так, чтобы интересно было пообщаться, подружиться, чтобы вечеринка тоже была полезной. Это касается организационной части.\n",
      "\n",
      "Григорий: Чем конференция отличается от митапа, школы, образовательных курсов?\n",
      "\n",
      "Валентин: Я перебью немного. Помимо того, что Григорий евангелист Moscow Python, он еще руководитель Программного комитета.\n",
      "\n",
      "Олег: Поэтому все шишки на него.\n",
      "\n",
      "Григорий: Это был неожиданный поворот, но я попробую его развернуть в благоприятную для себя сторону. Конференция — это особый формат. В интернете гуляет поверье: зачем в современном мире ходить на конференцию, ведь всё можно узнать в Google? Зашёл на сайт и прочитал официальную документацию. Но это, как бумажные книги.\n",
      "\r\n",
      "Как бы безумно это не звучало, но если ты хочешь узнать что-то большое и сложное, имеет смысл найти бумажную книжку, где человек потратил несколько лет на то, чтобы собрать все эти знания в одном месте.\n",
      "\n",
      "Бумажная книжка, конечно, не научит тебя пользоваться последней версией TensorFlow, потому что она успеет устареть. Но бумажная книга позволит тебе выучить психологию, биологию или японский язык гораздо лучше, чем документация в интернете. У книжек своя ниша, у конференции тоже своя ниша.\n",
      "\r\n",
      "Конференцию мы делаем не для того, чтобы кого-то чему-то научить. Гости приходят на конференцию не для того, чтобы слушать доклады, как ни странно. Сейчас я вам открою тайну. Я сделал много конференций и много участвовал. На конференцию приходят в первую очередь, для того чтобы общаться. Это специальное место, где с докладами выступают спикеры, у которых серьезный опыт в программировании, которые работают в крутейших компаниях. Они пришли на конференцию, чтобы отвечать на вопросы.\n",
      "\r\n",
      "Кто такие гости конференции? Кто вообще идет на IT-конференцию? Я сейчас говорю про Moscow Python Conf ++. Это разработчик, который профессионально применяет Python в своей работе или хочет зачем-то применять. У него интересные ситуации, какие-то вопросы, сложности. Каждый день на работе он что-то делает по 8 часов, и что-то получается, а что-то — нет. Не всё из этого можно нагуглить. Он читает программу конференции и видит: «Вот этот человек работает в Google с machine learning, да еще по той теме, чем я занимаюсь последние два года. Если я приду на конференцию, послушаю его доклад, потом подойду к нему и задам вопрос, то мне ответит топовый специалист в этой области». Все прекрасно понимают, что если написать ему e-mail — там очередь. А вот если прийти на конференцию, то можно пообщаться с лидерами индустрии, с такими же разработчиками, которые используют эту же технологию каждый свой рабочий день. Это на порядок эффективнее, чем общаться в Facebook и задавать вопросы на Stackoverflow и Reddit. На Stackoverflow могут ответить на какой-то технический вопрос из серии «Как сделать такую штуку?» Там никто не будет делиться практическим опытом, потому что большинство опытных разработчиков всё-таки работают. При всей моей любви к Stackoverflow, я им много пользуюсь в своей работе, но всё равно топовых экспертов там мало. Они работают на своей работе, несколько раз в год летают на конференции. Именно там их можно поймать и узнать, что и как они делают.\n",
      "\n",
      "Валентин: Друзья, пишите в комментариях темы или спикеров, которых вы хотели бы видеть на конференции. Мы, как сообщество разработчиков, постараемся учесть ваши пожелания и составить программу, отвечающую вашим требованиям.\n",
      "\n",
      "Григорий: Еще у нас есть специальный Google Doc, который можно комментировать. Мы, программный комитет, уже набрейнштормили кучу тем и полсотни спикеров. Милости просим дописывать туда, кого бы вам было интересно видеть на конференции. Но не просто: «А-а-а, Гвидо ван Россум — живой!», а тех, кому вы хотите задать вопросы, темы, по которым у вас есть проблемы в работе, ответы на которые помогут вам что-то сделать круто.\n",
      "\n",
      "Олег: Гриша правильно говорит про возможность общения на конференции с людьми, которых сложно выцепить в обычной жизни. Мы, как организаторы, это прекрасно понимаем, и у нас на данный момент есть сложившийся самый оптимальный с нашей точки зрения формат проведения конференции. Для того чтобы организовать такое общение, фасилитировать его и подстегнуть, мы придумали много различных действий, организационных фишек.\n",
      "\r\n",
      "Во-первых, это поток митапов. Митап на конференции — это очень небольшая камерная встреча 10-15 человек, посвященная очень узкой конкретной теме. Наши докладчики с удовольствием ею пользуются. Например, для того чтобы ответить на вопросы после доклада, увести аудиторию и рассмотреть какой-то узкий сегмент от доклада в кругу тех, для кого это очень важно, для кого обсудить именно конкретное применение Phyton, какой-то библиотеки или чего-то еще очень важно именно здесь, сейчас и с этим человеком.\n",
      "\r\n",
      "Во-вторых, эта выставка. У нас на выставке не работают маркетологи. Мы стараемся минимизировать работу HR и максимизировать работу технических специалистов, чтобы вы могли, придя на выставку, задать вопрос и получить конкретный ответ.\n",
      "\r\n",
      "В-третьих, вечеринка, на которой остаются все докладчики в том числе.\n",
      "\r\n",
      "В-четвертых, после того, как доклад закончен, возле каждого зала расположена дискуссионная зона с флипчартом, где докладчик продолжает отвечать на вопросы, на которые не успел ответить в рамках самого доклада.\n",
      "\r\n",
      "Ты говорил, что вы набрейнштромили 50 тем уже. Да?\n",
      "\n",
      "Григорий: 50 потенциальных спикеров. Тем — несколько десятков. Python — достаточно уникальный язык тем, что он действительно общего назначения.\n",
      "\n",
      "На Python делают всё: веб-приложения, сюрприз — десктоп приложения, еще больший сюрприз — мобильные приложения, математику, тестирование, автоматизацию.\n",
      "\r\n",
      "Сейчас на Python делают Machine Learning, Big Data, Artificial Intelligence. Топовый фреймворк для работы с Machine Learning TensorFlow стоит на плечах Python.\n",
      "\n",
      "Валентин: Тем, кто хочет остановить восстание машин, нужно остановить развитие Python.\n",
      "\n",
      "Григорий: Наша основная задача, как программного комитета — сделать программу разнообразной. Я постоянно помню о том, что на конференцию идут те, кто хочет задавать вопросы. Я хочу, чтобы как можно больше разработчиков могли найти в программе одного-двух спикеров по тем темам, где им есть что спросить.\n",
      "\n",
      "Олег: Правильно ли я понимаю, как строится программа? Phyton и какая-то практическая область применения. Так?\n",
      "\n",
      "Григорий: Не только практические области применения, но и какие-то части экосистемы. У Python, например, есть злободневные вопросы. Как раз то, что ты помогаешь сделать конференцию двухдневной в несколько потоков, поможет нам раскрыть такие темы, как «Миграция с 2.7 на 3.х» — это просто один из столпов Python. Как 10 лет назад начали мигрировать, так до сих пор и недовымигрировали. У Python это даже ещё более ядрёнее, чем у Ruby в свое время.\n",
      "\r\n",
      "Это вопросы управления зависимостями, потому что, когда мы берем новую машинку и говорим: «Python install вот эти зависимости». С шансами 90% оно взорвется с ошибкой «не могу собрать что-то там».\n",
      "\r\n",
      "Это практические применения, безусловно: «Python и Tensorflow», «Python и работа с базами данных», «Phyton и работа с Docker, Kubernetes», «Python и deploy», «Python и мобильная разработка».\n",
      "\r\n",
      "Какие-то специфичные для языка вещи. Например, «Будущий Python» или «Что использовать в 2018 году на бэкенде: Python или JavaScript?» Наконец, такие флеймообразующие темы, как, например, «Python for Web с помощью такого горячего пирожка, как WebAssembly».\n",
      "\r\n",
      "Всё это мы очень постараемся отразить в программе. Я, конечно, чудо не обещаю. Но вы помните, про документик. Прямо сейчас посмотрите в него, и если есть, что сказать — говорите. Мы каждый день эти комменты просматриваем, подчищаем и если что интересное находим — выписываем.\n",
      "\n",
      "Олег: С темами программы понятно. Какие-то новинки, новости, тренды, куда Phyton движется? Куда движется экосистема? Что с ней происходит?\n",
      "\n",
      "Григорий: Темы очень интересные. Я уже говорил про Machine Learning, Big Data, Artificial Intelligence — это то, где сейчас весь хайп. Но это прикладная тема. Сам по себе язык тоже очень сильно меняется.\n",
      "\n",
      "2017-2018 годы стали годами типизирования.\n",
      "\r\n",
      "В те языки, которые десятилетиями были динамическими: JavaScript, PHP, Python, неожиданно пришли типы. Это совершенно новый подход. Он называется Gradual Typing. Он примирил два лагеря: динамических и статических языков программирования. Теперь, когда разработчики пишут на Python, JavaScript или на PHP, они могут указывать типы, но не везде, а на свой выбор. Эта штука появилась довольно давно еще в версии python 3.2, но активно стала использоваться в версии 3.5 (там для этого появились механики). Сейчас актуальны версии 3.6, 3.7. Эта штука новая. Там есть куча утилит, чтобы типизирование работало. В PyCharm, ведущем IDE/редакторе, сделали собственную реализацию поддержки типов, чтобы всё было быстро. Недавно еще Facebook сделал новую мега быструю утилиту для работы с типами, называется Pyre. Это новьё, и разработчики не понимают, как этими штуками пользоваться. Весь интернет гудит, что Phyton, типы, Pyre — как это всё правильно сварить, чтобы получился вкусный коктейльчик. Кстати, Андрей Власовских из PyCharm тоже участник нашего Программного комитета и поможет выбрать нам темы.\n",
      "\r\n",
      "Что еще интересного в 2018 году? Новый веб…\n",
      "\n",
      "Олег: Новый веб?\n",
      "\n",
      "Григорий: Да, новый веб. У нас во всех браузерах сейчас появился WebAssembly. Это значит, что веб-странички теперь не обязательно целиком писать на JavaScript, а можно писать логику, например, на C. Все пытаются понять, что, возможно, есть смысл выкинуть JavaScript из full-stack разработки, и делать на Python и бэкенд и фронтенд. Какие сейчас бэкенды вообще есть? Это должно быть что-то вроде приложения или это маленький API, все эти web 3.0, одностраничные приложения, progressive web app — то за что топит Google. Разработчики с этим экспериментируют и пытаются всем этим пользоваться. Они пытаются выяснить, сэкономит ли это им время и позволит ли решать более сложные задачи, лучше решать задачи, писать поддерживаемый код.\n",
      "\r\n",
      "На что я ещё хотел заострить внимание, это фишки. Гвидо — главный по развитию языка Python. До этого многие годы он был довольно консервативен. Он говорил, что мы будем держать простое, чистое ядро языка, и это основная идея. Но в последние годы, я не могу сказать, что он расслабился, всё-таки ему уже седьмой десяток, но видимо, он совсем научился и сейчас в Python добавляют совершенно новые штуки. Например, неделю назад подтвердили, что в следующие версии будет добавлен новый оператор «:=» прямо из Pascal. Свежачок, который позволит присвоить идентификатор и проверить его одной командой. Язык неожиданно развивается, добавляются новые возможности: типы, новый синтаксис, у нас маячит Web, мега горячая тема Machine Learning, Artificial Intelligence. Всё это в 2018 году через несколько месяцев будем обсуждать вживую.\n",
      "\n",
      "Олег: Расскажи, пожалуйста, кого вы видите в качестве своих докладчиков.\n",
      "\n",
      "Григорий: Докладчиками мы видим в первую очередь тех, кому имеет смысл задавать вопросы. Это человек с большим опытом, который может ответить из своего практического опыта: «Я последние несколько лет использую в Python типы, и вот, что я могу тебе сказать по поводу твоего вопроса и новенького Pyre».\n",
      "\r\n",
      "Это докладчик из какой-то крупной компании. К примеру, докладчик из Яндекса, который на Python делает нагрузочное тестирование сайтов. И все сразу: «Ну, уж в Яндексе знают, как делать нагрузочное тестирование сайтов». Поэтому все со своими вопросами прибежали к нему, окружили, влили кофе и начали вопрошать.\n",
      "\r\n",
      "Третий — это спикер с каким-то уникальным опытом. Он сделал штуку, которую до него ни делал никто. Он пришел, об этом рассказал, и сразу же его спрашивают: «Слушай, ты занимался этим целый год, и никто больше не занимался. Я хочу это использовать вот таким образом. Рассказывай что и как».\n",
      "\n",
      "Валентин: Я думаю, что у нас будет возможность дать доступ к темам тех, кто занимается развитием языка. Это называется Python Core Developers.\n",
      "\n",
      "Олег: Русскоязычных или планируете кого-то перевести?\n",
      "\n",
      "Валентин: Один из русскоязычных Python Core Developers находится у нас в программном комитете, это Андрей Светлов. Он был у нас докладчиком на конференции 2016 году. Его доклад вызвал очень много вопросов со стороны аудитории. Люди были рады с ним пообщаться. В этом году мы его, естественно, увидим на конференции, не знаю в роли докладчика или нет. Но встретиться с ним в любом случае будет возможность, даже если он не будет делать доклад. Есть русскоязычные Core Developers, есть зарубежные. Мы думаем, что с помощью Андрея мы достучимся до них и кого-нибудь из них обязательно позовем, для того чтобы нам рассказали «из-под капота» языка как и что там происходит, какие механизмы развития.\n",
      "\n",
      "Григорий: Сейчас мы общаемся с зарубежными спикерами и выбираем.\n",
      "\n",
      "Олег: Хорошо. Предположим, я — докладчик и хочу попробовать выступить. Что мне делать?\n",
      "\n",
      "Григорий: Это очень хороший вопрос. У нас есть сайтик с очень простым названием conf.python.ru На сайте есть большая кнопка «подать доклад». Жмешь на кнопку, тебя встречает небольшая форма с несколькими полями. Кратко пишешь? о чём ты можешь рассказать и рассказывал ли ты что-нибудь до этого. Фото, видео предыдущих докладов — это будет очень круто, но это необязательно. После того, как ты подал доклад, он попадает в волшебную админскую систему Олега, где все доклады видит Программный комитет. Он может их обсуждать, сравнивать, делать докладчику оффер, запрашивать слайды, прогонять доклад и другие операции, которые просто греют душу организатора.\n",
      "\n",
      "Олег: А если я никогда не выступал, в первый раз это делаю, вы поможете мне сделать хороший доклад?\n",
      "\n",
      "Григорий: Здесь есть тонкий момент. Как работает человеческий мозг? Я никогда не играл на гитаре. Дайте мне гитару, и я попробую. Если человек никогда не выступал перед аудиторией с докладами, будут проблемы. Но есть исключения.\n",
      "\n",
      "Если очень интересная тема, и человек действительно хочет с ней выступить, у меня, как у интроверта социопата, есть набор костылей и механик, которые позволят за месяц сделать практически любого человека докладчиком.\n",
      "\r\n",
      "Это потребует по 20 минут в день. Если тема действительно интересна, и ты готов вложить немного усилий, не просто обозначить свое желание, а реально каждый день, за исключением выходных, 20 минут тратить на определенные тренировки, которые я покажу, то под моим руководством через месяц будет крутое выступление. У нас есть такая опция.\n",
      "\n",
      "Валентин: Возвращаясь к теме митапов, я бы хотел отметить, что тех, кто хочет первый раз попробовать себя в выступлениях мы призываем делать небольшие доклады на митапах минут по двадцать. Доклад на конференции 40 минут, чтобы более детально раскрыть тему. На митапах вы можете выступить с относительно небольшим докладом. Мы рады видеть всех, кто не боится выступить перед нашей аудиторией. Пользуясь техникой Григория Петрова, вы можете подготовиться как к митапу, так и к конференции.\n",
      "\n",
      "Олег: До какого числа принимаем доклады?\n",
      "\n",
      "Григорий: Мы принимаем заявки до 7 сентября, у нас еще есть месяц. Конечно, если в начале сентября окажется, что у крутейшего доклада пролет со сроками, то пишите — что-нибудь придумаем.\n",
      "\n",
      "Олег: Когда решение принимается и программа фиксируется? Когда появляется расписание?\n",
      "\n",
      "Григорий: В конце сентября за месяц с небольшим перед началом конференции. Этот месяц будет у докладчика на то, чтобы подготовить свои слайды.\n",
      "\n",
      "Олег: Когда первые докладчики появится на сайте? Я хочу посмотреть, перед тем как купить.\n",
      "\n",
      "Григорий: Они уже тут.\n",
      "\n",
      "Олег: Moscow Python Conf++ пройдет 22-23 октября в Москве. Заходите на сайт,\n",
      "бронируйте билеты. Как обычно, цена у нас потихонечку растет. Сейчас она минимальна или близка к таковой. Для не москвичей мы уже приготовили промокоды на перелет, а через некоторое время предложим забронировать гостиницу, у которой постараемся выбить колоссальные скидки, для того чтобы всё было удобно. Планируйте свое участие. Мы вас приглашаем. Сообщество Moscow Python обещает зажечь. Это будет круто.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Python — классный. Мы говорим «pip install» и скорее всего нужная библиотека поставится. Но иногда ответ будет: «compilation failed», потому что есть бинарные модули. Они практически у всех современных языков страдают какой-нибудь болью, потому что архитектур много, что-то нужно собирать под конкретную машину, что-то нужно линковать с другими библиотеками. В целом интересный, но малоизученные вопрос: а как же их делать и какие там проблемы? На этот вопрос постарался ответить Дмитрий Жильцов (zaabjuda) на MoscowPython Conf в прошлом году.\n",
      "\n",
      "\r\n",
      "Под катом текстовая версия доклада Дмитрия. Ненадолго остановимся на том, когда бинарные модули нужны, а когда от них лучше отказаться. Обсудим правила, которые стоит соблюдать при их написании. Рассмотрим пять возможных вариантов реализации:\n",
      "\n",
      "\n",
      "Native C/C++ Extension\n",
      "SWIG\n",
      "Cython\n",
      "Ctypes\n",
      "Rust\n",
      "\n",
      "О спикере: Дмитрий Жильцов занимается разработкой больше 10 лет. Работает в компании ЦИАН системным архитектором, то есть несет ответственность за технические решения и контроль сроков. В своей жизни успел попробовать и ассемблер, Haskell, C, а последние 5 лет активно программирует на Python.\n",
      "\n",
      "О компании\n",
      "\r\n",
      "Многие, кто живет в Москве и снимает жилье, наверное, знают про ЦИАН. ЦИАН это 7 миллионов покупателей и арендаторов в месяц. Все эти пользователи каждый месяц, с помощью нашего сервиса, находят себе жилье.\n",
      "\r\n",
      "Про нашу компанию знают 75% москвичей, и это очень круто. В Санкт-Петербурге и Москве мы практически считаемся монополистами. В данный момент мы стараемся выйти в регионы, и поэтому разработка выросла в 8 раз, за последние 3 года. Это значит, что в 8 раз увеличилась команды, в 8 раз увеличилась скорость поставки ценностей до пользователя, т.е. от идеи продукта до того, как рука инженера выкатила build на production. Мы научились в своей большой команде очень быстро разрабатывать, и очень быстро понимать, что в данный момент происходит, но сегодня речь пойдет немного о другом.\n",
      "\r\n",
      "Я буду рассказывать про бинарные модули. Сейчас практически 50% библиотек на Python имеют какие-то бинарные модули. И как оказалось, многие люди с ними не знакомы и считают, что это что-то заоблачное, что-то темное и ненужное. А другие люди предлагают лучше написать отдельный микросервис, и не использовать бинарные модули.\n",
      "\r\n",
      "Статья будет состоять из двух частей.\n",
      "\n",
      "\n",
      "Мой опыт: для чего они нужны, когда их лучше использовать, а когда нет.\n",
      "\n",
      "Инструменты и технологи, с помощью которых можно реализовать бинарный модуль для Python.\n",
      "\n",
      "\n",
      "Зачем нужны бинарные модули\n",
      "\r\n",
      "Мы все прекрасно знаем, что Python интерпретируемый язык. Он почти самый быстрый из интерпретируемых языков, но, к сожалению, его скорости не всегда хватает для тяжелых математических расчетов. Тут же возникает мысль, что на C будет быстрее.\n",
      "\r\n",
      "Но у Python есть еще одна боль — это GIL. Про него написано огромное количество статей и сделано докладов о том, как его обойти.\n",
      "\r\n",
      "Также бинарные расширения нам нужны для переиспользования логики. Например, мы нашли сишную библиотеку, в которой есть вся необходимая нам функциональность, и почему бы нам этим не воспользоваться. То есть не надо писать заново код, мы просто берем готовый код и его переиспользуем.\n",
      "\r\n",
      "Многие считают, что с помощью бинарных расширений можно скрыть исходный код. Вопрос очень и очень спорный, конечно с помощью каких-то диких извращений можно этого добиться, но 100% гарантии нет. Максимум, что можно получить, это не дать клиенту декомпилировать и посмотреть, что происходит в коде, который вы передали.\n",
      "\n",
      "Когда бинарные расширения действительно нужны?\n",
      "\r\n",
      "Про скорость и Python понятно — когда какая-то функция у нас работает очень медленно и занимает собой 80% от времени исполнения всего кода, мы начинаем подумывать о написании бинарного расширения. Но для того, чтобы принимать такие решения, нужно для начала, как говорил один известный спикер, подумать мозгом.\n",
      "\r\n",
      "Для того чтобы писать сишные расширения, надо принять во внимание, что это, во-первых, будет долго. Сначала нужно «вылизать» свои алгоритмы, т.е. посмотреть нет ли каких-то косяков.\n",
      "\n",
      "В 90% случаев после тщательной проверки алгоритма необходимость в написании каких-то расширений отпадает.\n",
      "\r\n",
      "Второй случай, когда бинарные расширения действительно нужны, это использование multi threading для простых операций. Сейчас это уже не так актуально, но еще осталось в кровавом enterprise, в каких-нибудь системных интеграторах, где до сих пор пишут на Python 2.6. Там нет асинхронности, и даже для простых вещей, например, загрузить кучу картинок, поднимается multi-threading. Вроде бы кажется, что изначально это не несет никаких сетевых расходов, но, когда мы выгружаем картинку в буфер, приходит злополучный GIL и начинаются какие-то тормоза. Как показывает практика, такие вещи лучше решать с помощью библиотек, о которых Python ничего не знает.\n",
      "\r\n",
      "Если нужно реализовать какой-то специфический протокол, может быть удобно сделать простой код на С/С++ и избавиться от большого количества боли. Я так делал в свое время в одном телеком-операторе, так как не оказалось готовой библиотеки, — пришлось самому писать. Но повторюсь, сейчас это не очень актуально, потому что есть asyncio, и для большинства задач этого достаточно.\n",
      "\r\n",
      "Про заведомо тяжелые операции я уже заранее сказал. Когда у вас есть числадробилки, большие матрицы и подобное, то логично, что нужно делать расширение на C/C++. Хочу заметить, что некоторые люди считают, что не нужны нам тут бинарные расширения, лучше сделать микросервис на каком-нибудь «супербыстром языке», и передавать огромные матрицы по сети. Нет, лучше так не делать.\n",
      "\r\n",
      "Еще один хороший пример, когда их можно и даже нужно брать, это когда у вас устоявшаяся логика работы модуля. Если у вас в компании какой-то модуль на Python или библиотека уже существует 3 года, изменения в ней бывают раз в год и то 2 строчки, то почему бы это не оформить в нормальную библиотеку на С, если есть свободные ресурсы и время. Как минимум получите увеличение в производительности. А еще будет понимание, что, если нужны какие-то кардинальные изменения в библиотеке, то это не так просто и, возможно, опять же стоит подумать мозгом и эту библиотеку как-то по-другому использовать.\n",
      "\n",
      "5 золотых правил\n",
      "\r\n",
      "Эти правила я вывел на своей практике. Они касаются не только Python, но и других языков, для которых можно использовать бинарные расширения. Вы можете с ними поспорить, но может и задуматься и вывести свои.\n",
      "\n",
      "\n",
      "Экспортировать только функции. Строить классы в Python в бинарных библиотеках довольно трудоемко: нужно описать очень много интерфейсов, нужно пересмотреть много ссылочных целостностей в самом модуле. Проще написать небольшой интерфейс для функции.\n",
      "\n",
      "Использовать классы обертки. Некоторые очень любят ООП и сильно хотят классы. В любом случае, даже если это не классы, лучше просто написать обертку Python: создаете класс, задаете класс-метод или обычный метод, вызываете нативно функции C/С++. Как минимум это помогает поддерживать целостность архитектуры данных. Если вы используете какое-то С/С++ стороннее расширение, которое вы не можете поправить, то в обёртке вы можете его хакнуть, чтобы это все работало.\n",
      "\n",
      "Нельзя передавать аргументы из Python в расширение —это даже не правило, а скорее требование. В некоторых случаях это может работать, но обычно это плохая идея. Поэтому в вашем сишном коде вы сначала должны сделать обработчик, который приводит тип Python в тип С. И только после этого вызывать какую-либо нативную функцию, которая уже работает с сишными типами. Этот же обработчик принимает ответ от исполняемой функции и переделывает в типы данных Python, и пробрасывает в код на Python.\n",
      "\n",
      "Учитывать сборку мусора. В Python есть всем известный GC, и про него не нужно забывать. Например, мы передаем по ссылке большой кусок текста и пытаемся найти какое-то слово в сишной библиотеке. Мы хотим это распараллелить, передаем ссылку именно на эту область памяти и запускам несколько потоков. В это время GC просто берет и решает, что на этот объект больше ничто не ссылается и удаляет его из области памяти. В сишном же коде мы просто получим null reference, а это обычно segmentation fault. Надо не забывать про такую особенность сборщика мусора и передавать в сишные библиотеки наиболее простые типы данных: char, integer и т.д.\n",
      "\r\n",
      "С другой стороны, в языке, на котором пишется расширение может быть свой сборщик мусора. Сочетание Python и библиотеки на C# в этом смысле боль.\n",
      "\n",
      "Явно определять аргументы экспортируемой функции. Этим я хочу сказать, что эти функции надо будет качественно аннотировать. Если мы принимаем функцию PyObject, а мы в любом случае ее будем принимать в своих сишных библиотеках, то нам нужно будет явно указать, какие аргументы к каким типам относятся. Это полезно тем, что если мы передадим не тот тип данных, то получим ошибку в сишной библиотеке. То есть нужно для вашего же удобства.\n",
      "\n",
      "\n",
      "Архитектура бинарных расширений\n",
      "\n",
      "\n",
      "\r\n",
      "Собственно, ничего сложного в архитектуре бинарных расширений нет. Есть Python, есть вызывающая функция, которая приземляется на обертку, которая нативно вызывает сишный код. Этот вызов в свою очередь приземляется на функцию, которая экспортируется в Python, и которую он может напрямую вызвать. Именно в этой функции нужно привести типы данных до типов данных вашего языка. И только после того, как эта функция все нам перевела, мы вызываем нативную функцию, которая делает основную логику, в обратную сторону возвращает результат и прокидывает это в Python, переводя типы данных обратно.\n",
      "\n",
      "Технологии и инструменты\n",
      "\r\n",
      "Самый известный способ написания бинарных расширений это Native C/C++ extension. Только лишь потому, что это стандартная технология Python.\n",
      "\n",
      "Native C/C++ extension\n",
      "\r\n",
      "Сам Python реализован на С, и при написании расширений используются методы и структуры из python.h. Кстати, эта штука хороша еще тем, что её очень легко внедрять в уже готовый проект. Достаточно в setup.py указать xt_modules и сказать, что для сборки проекта нужно компилировать такие-то исходники с такими-то флагами компиляции. Ниже пример.\n",
      "\n",
      "name = 'DateTime.mxDateTime.mxDateTime'\n",
      "src = 'mxDateTime/mxDateTime.c'\n",
      "extra_compile_args=['-g3', '-o0', '-DDEBUG=2', '-UNDEBUG', '-std=c++11', '-Wall', '-Wextra']\n",
      "  setup (\n",
      "    ...\n",
      "    ext_modules =\n",
      "      [(name,\n",
      "       { 'sources': [src],\n",
      "         'include_dirs': ['mxDateTime'] ,\n",
      "         extra_compile_args: extra_compile_args\n",
      "        }\n",
      "      )]\n",
      "  )\n",
      "\r\n",
      "Плюсы Native C/C++ Extension\n",
      "\n",
      "\n",
      "Родная технология.\n",
      "Легко интегрируется в сборку проекта.\n",
      "Наибольшее количество документации.\n",
      "Позволяется создавать свои типы данных.\n",
      "\r\n",
      "Минусы Native C/C++ Extension\n",
      "\n",
      "\n",
      "Высокий порог входа.\n",
      "Необходимо знание С.\n",
      "Boost.Python.\n",
      "Segmentation Fault.\n",
      "Сложности в отладке.\n",
      "\r\n",
      "По этой технологии, написано огромное количество документации, как стандартной, так и постов во всяких блогах. Огромный плюс и то, что мы можем делать свои типы данных Python и конструировать свои классы.\n",
      "\r\n",
      "У этого подхода есть большие минусы. Во-первых, это порог входа — не все знают C настолько, чтобы кодить для production. Нужно понимать, что для этого недостаточно прочитать книжку и побежать писать нативные расширения. Если вы хотите этим заняться, то: для начала изучите C; потом начните писать командные утилиты; только после этого переходите к написанию расширений.\n",
      "\r\n",
      "Boost.Python очень хорош для C++, он позволяет практически полностью абстрагироваться от всех этих оберток, которые мы используем в Python. Но минусом я считаю то, что взять какую-то его часть и импортировать в проект, не скачивая весь Boost, нужно очень сильно попотеть.\n",
      "\r\n",
      "Перечисляя сложности в отладке в минусах, я имею ввиду то, что сейчас все привыкли использовать графический отладчик, а с бинарными модулями такая штука не пройдет. Скорее всего понадобится поставить GDB с плагином для Python.\n",
      "\r\n",
      "Рассмотрим пример, как мы вообще это создаем.\n",
      "\n",
      "#include <Python.h>\n",
      "\n",
      "static PyObject*addList_add(Pyobject* self, Pyobject* args){\n",
      "    PyObject * listObj;\n",
      "    if (! PyARg_Parsetuple( args, \"О\", &listObj))\n",
      "        return NULL;\n",
      "    long length = PyList_Size(listObj)\n",
      "    int i, sum =0;\n",
      "    // Опустим реализацию\n",
      "    return Py_BuildValue(\"i\", sum);\n",
      "}\n",
      "\r\n",
      "Для начала, мы подключаем заголовочные файлы Python. После этого описываем функцию addList_add, которую будет использовать Python. Самое главное называть функцию правильно, в данном случае addList — это у нас имя сишного модуля, _add имя функции, которая будет использоваться в Python. Передаем сам модуль PyObject и передаем аргументы тоже с помощью PyObject. После этого совершаем стандартные проверки. В данном случае, мы пытаемся распарсить аргумент tuple и говорим, что это object — литерал «О» нужно явно указать. После этого мы знаем, что в качестве объекта мы передали listObj, и пытаемся узнать его длину с помощью стандартных методов Python: PyList_Size. Заметьте, здесь мы еще не можем использовать сишные вызовы, чтобы узнать длину этого вектора, а используем функционал Python. Опустим реализации, после которой необходимо вернуть все значения обратно в Python. Для этого вызываем Py_BuildValue, указываем, какой тип данных мы возвращаем, в данном случае «i» — integer, и саму переменную sum.\n",
      "\r\n",
      "В данном случае всем понятно — мы находим сумму всех элементов списка. Давайте пройдем чуть дальше.\n",
      "\n",
      "for(i = 0; i< length; i++){\n",
      "    // Получаем элемент из списка\n",
      "    // он также Python-объект\n",
      "    PyObject* temp = PyList_GetItem(listObj, i);\n",
      "    // Мы знаем, что элемент это целое число\n",
      "    // приводим его к типу C \n",
      "    long long elem= PyLong_AsLong(temp);\n",
      "    sum += elem; \n",
      "}\n",
      "\r\n",
      "Тут то же самое, на данный момент listObj — объект Python. И в данном случае мы пытаемся взять элементы списка. Для этого в Python.h есть все необходимое.\n",
      "\r\n",
      "После того, как мы получили temp, мы пытаемся привести его к типу long. И только после этого можно что-то делать в С.\n",
      "\n",
      "// Документация\n",
      "static char addList_docs[] = \"add( ): add all elements of the list\\n\";\n",
      "// Регистрируем функции модуля\n",
      "\n",
      "static PyMethodDef addList_funcs[] = {\n",
      "    {\"add\", (PyCFunction)addList_add, METH_VARARGS, addList_docs},\n",
      "    {NULL, NULL, 0, NULL}\n",
      "};\n",
      "\r\n",
      "После того, как мы реализовали всю функцию, необходимо написать документацию. Документация — это всегда хорошо, и в этом инструментарии все есть для удобного ее ведения. Придерживаясь конвенции о названиях, именуем модуль addList_docs и сохраняем туда описание. Теперь нужно зарегистрировать модуль, для этого есть специальная структура PyMethodDef. Описывая свойства, мы говорим, что функция экспортируется в Python под именем «add», что эта функция вызывает PyCFunction. METH_VARARGS означает, что функция потенциально может принимать любое количество переменных. Еще мы записали дополнительные строки и описали стандартную проверку, на тот случай если мы просто импортировали модуль, но не обратились ни к какому методу, чтобы у нас все это не падало.\n",
      "\r\n",
      "После того как мы это все объявили мы пытаемся делать модуль. Создаем moduledef и укладываем туда все, что уже сделали.\n",
      "\n",
      "static  struct PyModuleDef moduledef = {\n",
      "    PyModuleDef_HEAD_INIT,\n",
      "    \"addList example module\",\n",
      "    -1,\n",
      "    adList_funcs, \n",
      "    NULL,\n",
      "    NULL,\n",
      "    NULL,\n",
      "    NULL\n",
      "};\n",
      "\r\n",
      "PyModuleDef_HEAD_INIT — это стандартная константа Python, которую всегда нужно использовать. —1 обозначает, что на этапе импорта не нужно выделять дополнительную память.\n",
      "\r\n",
      "Когда мы создали сам модуль, нам нужно его инициализировать. Python всегда ищет init, поэтому создаем PyInit_addList для addList. Тепер из собранной структуры можно вызвать PyModule_Create и наконец создать сам модуль. Далее добавляем метаинформацию и возвращаем сам модуль.\n",
      "\n",
      "PyInit_addList(void){\n",
      "    PyObject *module = PyModule_Create(&mdef);\n",
      "    If  (module == NULL)\n",
      "        return NULL;\n",
      "    PyModule_AddStringConstant(module, \"__author__\", \"Bruse Lee<brus@kf.ch>:\");\n",
      "    PyModule_addStringConstant (Module, \"__version__\", \"1.0.0\");\n",
      "    return module;\n",
      "}\n",
      "\r\n",
      "Как вы уже заметили, здесь много чего надо преобразовать. Надо всегда помнить о Python, когда мы пишем на С/С++.\n",
      "\r\n",
      "Именно поэтому, для облегчения жизни обычного смертного программиста, лет 15 назад появилась технология SWIG.\n",
      "\n",
      "SWIG\n",
      "\r\n",
      "Этот инструмент позволяет абстрагировать от биндингов Python и писать нативный сишный код. У него такие же плюсы и минусы как и у Native C/C++, но есть исключения.\n",
      "\r\n",
      "Плюсы SWIG:\n",
      "\n",
      "\n",
      "Стабильная технология.\n",
      "Большое количество документации.\n",
      "Абстрагирует от привязки к Python.\n",
      "\r\n",
      "Минусы SWIG:\n",
      "\n",
      "\n",
      "Долгая настройка.\n",
      "Знание C.\n",
      "Segmentation Fault.\n",
      "Сложности в отладке.\n",
      "Сложность интеграции в сборку проекта.\n",
      "\r\n",
      "Первый минус в том, что пока его настроишь, то сойдешь с ума. Когда я настраивал его в первый раз, я потратил полтора дня, чтобы вообще его запустить. Потом уже, конечно, легче. В версии SWIG 3.x стало полегче.\n",
      "\r\n",
      "Чтобы больше не вдаваться в код, рассмотрим общую схему работы SWIG.\n",
      "\n",
      "\n",
      "\r\n",
      "example.c — это модуль на С, который про Python вообще ничего не знает. Есть интерфейсный файл example.i, который описывается в формате SWIG. После этого запускаем утилиту SWIG, которая из интерфейсного файла создает example_wrap.c — это та самая обертка, которую мы раньше делали руками. То есть SWIG нам просто создает файл обертку, так называемый мост. После этого с помощью GCC мы компилируем два файла и получаем два объектных файла (example.o и example_wrap.o) и уже потом создаем нашу библиотеку. Все просто и понятно.\n",
      "\n",
      "Cython\n",
      "\r\n",
      "Андрей Светлов сделал на MoscowPython Conf прекрасный доклад, поэтому я просто скажу, что это популярная технология с хорошей документацией.\n",
      "\r\n",
      "Плюсы Cython:\n",
      "\n",
      "\n",
      "Популярная технология.\n",
      "Довольно стабильно.\n",
      "Легко интегрируется в сборку проекта.\n",
      "Хорошая документация.\n",
      "\r\n",
      "Минусы Cython:\n",
      "\n",
      "\n",
      "Свой синтаксис.\n",
      "Знание C.\n",
      "Segmentation Fault.\n",
      "Сложности в отладке.\n",
      "\r\n",
      "Минусы, как всегда, есть. Главный из них — свой синтаксис, который похож и на С/С++, и очень сильно на Python.\n",
      "\r\n",
      "Но я хочу заострить внимание, что код Python можно ускорить с помощью Cython, написав нативный код.\n",
      "\n",
      "\n",
      "\r\n",
      "Как вы видите очень много декораторов, и это не очень хорошо. Если захотите использовать Cython — обратитесь к докладу Андрея Светлова.\n",
      "\n",
      "CTypes\n",
      "\r\n",
      "CTypes — это стандартная библиотека Python, которая работает с Foreign Function Interface. FFI — это низкоуровневая библиотека. Это родная технология, ее до ужаса часто используют в коде, с ее помощью легко реализовать кроссплатформенность.\n",
      "\r\n",
      "Но FFI несет с собой большие накладные расходы, потому что все мосты, все handler в runtime создаются динамически. То есть мы подгрузили динамическую библиотеку, а Python в этот момент вообще ничего не знает, что это за библиотека. Только при вызове библиотеки в памяти динамически конструируются эти мосты.\n",
      "\r\n",
      "Плюсы CTypes:\n",
      "\n",
      "\n",
      "Родная технология.\n",
      "Легко использовать в коде.\n",
      "Легко реализовать кроссплатформенность.\n",
      "Можно использовать практически любой язык.\n",
      "\r\n",
      "Минусы CTypes:\n",
      "\n",
      "\n",
      "Несет накладные расходы.\n",
      "Сложности в отладке.\n",
      "\n",
      "\n",
      "from ctypes import *\n",
      "\n",
      "#load the shared object file\n",
      "Adder = CDLL('./adder.so')\n",
      "\n",
      "#Calculate factorial\n",
      "res_int = adder.fact(4)\n",
      "\n",
      "print(\"Fact of 4 = \" + str(res_int))\n",
      "\r\n",
      "Взяли adder.so и в runtime нативно вызвали. Мы даже можем передавать нативные типы Python.\n",
      "\r\n",
      "После всего этого стоит вопроc: \"Как-то все сложно, везде C, что же делать?\".\n",
      "\n",
      "Rust\n",
      "\r\n",
      "В свое время я не предал это языку должного внимания, но теперь я практически на него перехожу.\n",
      "\r\n",
      "Плюсы Rust:\n",
      "\n",
      "\n",
      "Безопасный язык.\n",
      "Мощные статические гарантии правильности поведения.\n",
      "Легко интегрируется в сборку проекта (PyO3).\n",
      "\r\n",
      "Минусы Rust:\n",
      "\n",
      "\n",
      "Высокий порог входа.\n",
      "Долгая настройка.\n",
      "Сложности в отладке.\n",
      "Документации мало.\n",
      "В некоторых случаях накладные расходы.\n",
      "\r\n",
      "Rust — это безопасный язык с автоматическим доказательством правила работы. Сам синтаксис и сам препроцессор языка не позволяет сделать явную ошибку. В то же время он заточен на вариативность, то есть любой результат выполнения ветки кода он обязан обработать.\n",
      "\r\n",
      "Благодаря команде PyO3, есть хорошие биндинги для Python для Rust, и инструментарий для интеграции в проект.\n",
      "\r\n",
      "К минусам отнесу то, что для неподготовленного программиста его очень долго настраивать. Мало документации, но взамен в минусах у нас нет segmentation fault. В Rust, по-хорошему, в 99% случаях, получить segmentation fault программист может, только если сам явно указал unwrap и просто забил на этот случай.\n",
      "\r\n",
      "Небольшой пример кода, того же самого модуля, который мы рассматривали до этого.\n",
      "\n",
      "#![feature(proc_macro)]\n",
      "#[macro_use] extern crate pyo3;\n",
      "Use pyo3::prelude::*;\n",
      "\n",
      "/// Module documentation string 1\n",
      "#[py::modinit(_addList)]\n",
      "fn init(py: Python, m: PyModule) -> PyResult <()>{\n",
      "    py_exception!(_addList, EmptyListError);\n",
      "\n",
      "    /// Function documentation string 1\n",
      "    #[pufn(m, \"run\", args= \"*\", kwargs=\"**\" )]\n",
      "    fn run_py(_py: Python, args: &PyTuple, kwargs: Option<&PyDict>) -> PyResult<()> {\n",
      "        run(args, kwargs)\n",
      "    }\n",
      "    #[pyfn(m, \"run\", args=\"*\", kwatgs=\"**\")]\n",
      "    fn run_py(_py: Python, args: &PyTuple, kwargs: Option<&PyDict>) -> PyResult<()>{\n",
      "        run(args,kwargs)\n",
      "    }\n",
      "    #[pyfn(m,\"add\")]\n",
      "    fn add(_py: Python, py_list: &PyList) -> PyResult<i32>{\n",
      "        let mut sum : i32 = 0\n",
      "        match py_list.len() {\n",
      "        /// Some code\n",
      "        Ok(sum)\n",
      "    }\n",
      "    Ok(())\n",
      "}\n",
      "\r\n",
      "Код имеет специфический синтаксис, но к нему очень быстро привыкаешь. На самом деле тут все то же самое. С помощью макросов делаем modinit, который за нас делает всю дополнительную работу по генерации всевозможных биндингов для Python. Помните я говорил, нужно делать handler обертку, вот здесь тоже самое. run_py конвертирует типы, потом вызываем нативный код.\n",
      "\r\n",
      "Как вы видите, чтобы какую-то функцию экспортировать, есть синтаксическом сахаре. Мы просто говорим, что нам нужна функция add, и не описываем никаких интерфейсов. Мы принимаем list, который точно py_list, а не Object, потому что Rust в момент компиляции сам выставит необходимые биндинги. Если мы передадим неправильный тип данных, как в сишных расширениях, возникнет TypeError. После того как получили list, начинаем его обрабатывать.\n",
      "\r\n",
      "Давайте посмотрим поподробнее что он начинает делать.\n",
      "\n",
      "#[pyfn(m, \"add\", py_list=\"*\")]\n",
      "fn add(_py: Python, py_list: &PyList) -> PyResult<i32> {\n",
      "    match py_list.len() {\n",
      "        0 =>Err(EmptyListError::new(\"List is empty\")),\n",
      "        _ => {\n",
      "            let mut sum : i32 = 0;\n",
      "            for item in py_list.iter() {\n",
      "                let temp:i32 = match item.extract() {\n",
      "                    Ok(v) => v,\n",
      "                    Err(_) => {\n",
      "                        let err_msg: String = format!(\"List item {} is not int\", item);\n",
      "                        return Err(ItemListError::new(err_msg))\n",
      "                    }\n",
      "                };\n",
      "                sum += temp;\n",
      "            }\n",
      "            Ok(sum)\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\r\n",
      "Тот же код который был на С/С++/ Ctypes, но только уже на Rust. Там я пытался привести PyObject к какому-то long. Чтобы было бы если к нам в list, кроме чисел попалась бы строка? Да, мы получили бы SystemEerror. В данном случае, через let mut sum : i32 =  0; мы также пытаемся из list получить значение и привести его к i32. То есть мы не сможем записать этот код без item.extract(), сотвесвенно и привести к нужному типу. Когда мы написали i32, в случае ошибки Rust, на этапе компиляции скажет: «Обработай случай, когда не i32». В таком случае, если у нас i32, мы возвращаем значение, если это ошибка — мы выкидываем исключение.\n",
      "\n",
      "Что выбрать\n",
      "\r\n",
      "После этого небольшого экскурса подумаем, что же выбрать в итоге?\n",
      "\r\n",
      "Ответ на самом деле — на ваш вкус и цвет.\n",
      "\r\n",
      "Я не буду пропагандировать какую-то конкретную технологию.\n",
      "\n",
      "\n",
      "\r\n",
      "Просто обобщим сказанное:\n",
      "\n",
      "\n",
      "В случае SWIG и C/C++, надо знать C/C++ очень хорошо, понимать, что разработка этого модуля понесет какие-то дополнительные накладные расходы. Зато будет использовано минимум инструментария, и мы будем работать в родной технологии Python, которая поддерживается разработчиками.\n",
      "В случае с Cython мы имеем малый порог входа, мы имеем большую скорость разработки, а также это обыкновенный кодогенератор.\n",
      "На счет CTypes, хочу предостеречь, относительно больших накладных расходов. Динамическая подгрузка библиотек, когда мы не знаем, что это за библиотека, может повлечь массу неприятностей.\n",
      "Rust я бы посоветовал взять тому, кто плохо знает C/C++. Rust в production действительно несет меньше всего проблем.\n",
      "\n",
      "\n",
      "Полезные ссылки\n",
      "https://github.com/zaabjuda/moscowpythonconf2017\n",
      "https://docs.python.org/3/extending/building.html\n",
      "https://cython.org\n",
      "https://docs.python.org/376/library/ctypes.html\n",
      "https://www.swig.org\n",
      "https://www.rust-land.org/en-US/\n",
      "https://github.com/PyO3\n",
      "https://www.youtube.com/watch?v=5-WoT4X17sk\n",
      "https://packaging.python.org/tutorials/distributing-packages/#platformwheels\n",
      "https://github.com/PushAMP/pamagent (боевой пример сипользования)\n",
      "\n",
      "Call for Papers\n",
      "\r\n",
      "Принимаем заявки на Moscow Python Conf++ до 7 сентября — напишите в этой простой форме, что вы знаете о Python такого, чем очень нужно поделиться с сообществом.\n",
      "\r\n",
      "Для тех, кому интереснее слушать, могу рассказать о классных докладах.\n",
      "\n",
      "\n",
      " Donald Whyte любит рассказывать про ускорение математики на Python и готовит для нас новую историю: как с помощью популярных библиотек, хитрости и коварства делать математику в 10 раз быстрее, а код — понятным и поддерживаемым.\n",
      "\n",
      " Артём Малышев собрал весь свой многолетний опыт разработки Django и представляет доклад-путеводитель по фреймворку! Все, что происходит между получением HTTP запроса и отправкой готовой веб страницы: разоблачение магии, карта внутренних механизмов фреймворка и много полезных советов для ваших проектов.\n",
      "\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " В последнее время можно наблюдать рост популярности языка программирования Python. Он используется в DevOps, в анализе данных, в веб-разработке, в сфере безопасности и в других областях. Но вот скорость… Здесь этому языку похвастаться нечем. Автор материала, перевод которого мы сегодня публикуем, решил выяснить причины медлительности Python и найти средства его ускорения.\n",
      "\n",
      "\n",
      "\n",
      "Общие положения\r\n",
      "Как Java, в плане производительности, соотносится с C или C++? Как сравнить C# и Python? Ответы на эти вопросы серьёзно зависят от типа анализируемых исследователем приложений. Не существует идеального бенчмарка, но, изучая производительность программ, написанных на разных языках, неплохой отправной точкой может стать проект The Computer Language Benchmarks Game.\n",
      "\r\n",
      "Я ссылаюсь на The Computer Language Benchmarks Game уже больше десяти лет. Python, в сравнении с другими языками, такими, как Java, C#, Go, JavaScript, C++, является одним из самых медленных. Сюда входят языки, в которых используется JIT-компиляция (C#, Java), и AOT-компиляция (C, C++), а также интерпретируемые языки, такие, как JavaScript.\n",
      "\r\n",
      "Тут мне хотелось бы отметить, что говоря «Python», я имею в виду эталонную реализацию интерпретатора Python — CPython. В этом материале мы коснёмся и других его реализаций. Собственно говоря, здесь мне хочется найти ответ на вопрос о том, почему Python требуется в 2-10 раз больше времени, чем другим языкам, на решение сопоставимых задач, и о том, можно ли сделать его быстрее.\n",
      "\r\n",
      "Вот основные теории, пытающиеся объяснить причины медленной работы Python:\n",
      "\n",
      "\n",
      "Причина этого — в GIL (Global Interpreter Lock, глобальная блокировка интерпретатора).\n",
      "Причина в том, что Python — это интерпретируемый, а не компилируемый язык.\n",
      "Причина — в динамической типизации.\n",
      "\r\n",
      "Проанализируем эти идеи и попытаемся найти ответ на вопрос о том, что сильнее всего оказывает влияние на производительность Python-приложений.\n",
      "\n",
      "GIL\r\n",
      "Современные компьютеры обладают многоядерными процессорами, иногда встречаются и многопроцессорные системы. Для того чтобы использовать всю эту вычислительную мощь, операционная система применяет низкоуровневые структуры, называемые потоками, в то время как процессы (например — процесс браузера Chrome) могут запускать множество потоков и соответствующим образом их использовать. В результате, например, если какой-то процесс особенно сильно нуждается в ресурсах процессора, его выполнение может быть разделено между несколькими ядрами, что позволяет большинству приложений быстрее решать встающие перед ними задачи.\n",
      "\r\n",
      "Например, у моего браузера Chrome, в тот момент, когда я это пишу, имеется 44 открытых потока. Тут стоит учитывать то, что структура и API системы работы с потоками различается в операционных системах, основанных на Posix (Mac OS, Linux), и в ОС семейства Windows. Операционная система, кроме того, занимается планированием работы потоков.\n",
      "\r\n",
      "Если раньше вы не встречались с многопоточным программированием, то сейчас вам нужно познакомиться с так называемыми блокировками (locks). Смысл блокировок заключается в том, что они позволяют обеспечить такое поведение системы, когда, в многопоточной среде, например, при изменении некоей переменной в памяти, доступ к одной и той же области памяти (для чтения или изменения) не могут одновременно получить несколько потоков.\n",
      "\r\n",
      "Когда интерпретатор CPython создаёт переменные, он выделяет память, а затем подсчитывает количество существующих ссылок на эти переменные. Эта концепция известна как подсчёт ссылок (reference counting). Если число ссылок равняется нулю, тогда соответствующий участок памяти освобождается. Именно поэтому, например, создание «временных» переменных, скажем, в пределах областей видимости циклов, не приводит к чрезмерному увеличению объёма памяти, потребляемого приложением.\n",
      "\r\n",
      "Самое интересное начинается тогда, когда одними и теми же переменными совместно пользуются несколько потоков, а главная проблема тут заключается в том, как именно CPython выполняет подсчёт ссылок. Тут и проявляется действие «глобальной блокировки интерпретатора», которая тщательно контролирует выполнение потоков.\n",
      "\r\n",
      "Интерпретатор может выполнять лишь одну операцию за раз, независимо от того, как много потоков имеется в программе.\n",
      "\n",
      "▍Как GIL влияет на производительность Python-приложений?\r\n",
      "Если у нас имеется однопоточное приложение, работающее в одном процессе интерпретатора Python, то GIL никак на производительность не влияет. Если, например, избавиться от GIL, никакой разницы в производительности мы не заметим.\n",
      "\r\n",
      "Если же, в пределах одного процесса интерпретатора Python, надо реализовать параллельную обработку данных с применением механизмов многопоточности, и используемые потоки будут интенсивно использовать подсистему ввода-вывода (например, если они будут работать с сетью или с диском), тогда можно будет наблюдать последствия того, как GIL управляет потоками. Вот как это выглядит в случае использования двух потоков, интенсивно нагружающих процессов.\n",
      "\n",
      "\n",
      "Визуализация работы GIL (взято отсюда)\n",
      "\r\n",
      "Если у вас имеется веб-приложение (например, основанное на фреймворке Django), и вы используете WSGI, то каждый запрос к веб-приложению будет обслуживаться отдельным процессом интерпретатора Python, то есть, у нас имеется лишь 1 блокировка на запрос. Так как интерпретатор Python запускается медленно, в некоторых реализациях WSGI имеется так называемый «режим демона», при использовании которого процессы интерпретатора поддерживаются в рабочем состоянии, что позволяет системе быстрее обслуживать запросы.\n",
      "\n",
      "▍Как ведут себя другие интерпретаторы Python?\r\n",
      "В PyPy есть GIL, он обычно более чем в 3 раза быстрее, чем CPython.\n",
      "\r\n",
      "В Jython нет GIL, так как потоки Python в Jython представлены в виде потоков Java. Такие потоки используют возможности по управлению памятью JVM.\n",
      "\n",
      "▍Как управление потоками организовано в JavaScript?\r\n",
      "Если говорить о JavaScript, то, в первую очередь, надо отметить, что все JS-движки используют алгоритм сборки мусора mark-and-sweep. Как уже было сказано, основная причина необходимости использования GIL — это алгоритм управления памятью, применяемый в CPython.\n",
      "\r\n",
      "В JavaScript нет GIL, однако, JS — это однопоточный язык, поэтому в нём подобный механизм и не нужен. Вместо параллельного выполнения кода в JavaScript применяются методики асинхронного программирования, основанные на цикле событий, промисах и коллбэках. В Python есть нечто подобное, представленное модулем asyncio.\n",
      "\n",
      "Python — интерпретируемый язык\r\n",
      "Мне часто приходилось слышать о том, что низкая производительность Python является следствием того, что это — интерпретируемый язык. Подобные утверждения основаны на грубом упрощении того, как, на самом деле, работает CPython. Если, в терминале, ввести команду вроде python myscript.py, тогда CPython начнёт длительную последовательность действий, которая заключается в чтении, лексическом анализе, парсинге, компиляции, интерпретации и выполнении кода скрипта. Если вас интересуют подробности — взгляните на этот материал.\n",
      "\r\n",
      "Для нас, при рассмотрении этого процесса, особенно важным является тот факт, что здесь, на стадии компиляции, создаётся .pyc-файл, и последовательность байт-кодов пишется в файл в директории __pycache__/, которая используется и в Python 3, и в Python 2.\n",
      "\r\n",
      "Подобное применяется не только к написанным нами скриптам, но и к импортированному коду, включая сторонние модули.\n",
      "\r\n",
      "В результате, большую часть времени (если только вы не пишете код, который запускается лишь один раз) Python занимается выполнением готового байт-кода. Если сравнить это с тем, что происходит в Java и в C#, окажется, что код на Java компилируется в «Intermediate Language», и виртуальная машина Java читает байт-код и выполняет его JIT-компиляцию в машинный код. «Промежуточный язык» .NET CIL (это то же самое, что .NET Common-Language-Runtime, CLR), использует JIT-компиляцию для перехода к машинному коду. \n",
      "\r\n",
      "В результате, и в Java и в C# используется некий «промежуточный язык» и присутствуют похожие механизмы. Почему же тогда Python показывает в бенчмарках гораздо худшие результаты, чем Java и C#, если все эти языки используют виртуальные машины и какие-то разновидности байт-кода? В первую очередь — из-за того, что в .NET и в Java используется JIT-компиляция.\n",
      "\r\n",
      "JIT-компиляция (Just In Time compilation, компиляция «на лету» или «точно в срок») требует наличия промежуточного языка для того, чтобы позволить осуществлять разбиение кода на фрагменты (кадры). Системы AOT-компиляции (Ahead Of Time compilation, компиляция перед исполнением) спроектированы так, чтобы обеспечить полную работоспособность кода до того, как начнётся взаимодействие этого кода с системой.\n",
      "\r\n",
      "Само по себе использование JIT не ускоряет выполнение кода, так как на выполнение поступают, как и в Python, некие фрагменты байт-кода. Однако JIT позволяет выполнять оптимизации кода в процессе его выполнения. Хороший JIT-оптимизатор способен выявить наиболее нагруженные части приложения (такую часть приложения называют «hot spot») и оптимизировать соответствующие фрагменты кода, заменив их оптимизированными и более производительными вариантами, чем те, что использовались ранее.\n",
      "\r\n",
      "Это означает, что когда некое приложение снова и снова выполняет некие действия, подобная оптимизация способна значительно ускорить выполнение таких действий. Кроме того, не забывайте о том, что Java и C# — это языки со строгой типизацией, поэтому оптимизатор может делать о коде больше предположений, способствующих улучшению производительности программ.\n",
      "\r\n",
      "JIT-компилятор есть в PyPy, и, как уже было сказано, эта реализация интерпретатора Python гораздо быстрее, чем CPython. Сведения, касающиеся сравнения разных интерпретаторов Python, можно найти в этом материале.\n",
      "\n",
      "▍Почему в CPython не используется JIT-компилятор?\r\n",
      "У JIT-компиляторов есть и недостатки. Один из них — время запуска. CPython и так запускается сравнительно медленно, а PyPy в 2-3 раза медленнее, чем CPython. Длительное время запуска JVM — это тоже известный факт. CLR .NET обходит эту проблему, запускаясь в ходе загрузки системы, но тут надо отметить, что и CLR, и та операционная система, в которой запускается CLR, разрабатываются одной и той же компанией.\n",
      "\r\n",
      "Если у вас имеется один процесс Python, который работает длительное время, при этом в таком процессе имеется код, который может быть оптимизирован, так как он содержит интенсивно используемые участки, тогда вам стоит серьёзно взглянуть на интерпретатор, имеющий JIT-компилятор.\n",
      "\r\n",
      "Однако, CPython — это реализация интерпретатора Python общего назначения. Поэтому, если вы разрабатываете, с использованием Python, приложения командной строки, то необходимость длительного ожидания запуска JIT-компилятора при каждом запуске этого приложения сильно замедлит работу.\n",
      "\r\n",
      "CPython пытается обеспечить поддержку как можно большего количества вариантов использования Python. Например, существует возможности подключения JIT-компилятора к Python, правда, проект, в рамках которого реализуется эта идея, развивается не особенно активно.\n",
      "\r\n",
      "В результате можно сказать, что если вы, с помощью Python, пишете программу, производительность которой может улучшиться при использовании JIT-компилятора — используйте интерпретатор PyPy.\n",
      "\n",
      "Python — динамически типизированный язык\r\n",
      "В статически типизированных языках, при объявлении переменных, необходимо указывать их типы. Среди таких языков можно отметить C, C++, Java, C#, Go.\n",
      "\r\n",
      "В динамически типизированных языках понятие типа данных имеет тот же смысл, но тип переменной является динамическим.\n",
      "\n",
      "a = 1\n",
      "a = \"foo\"\r\n",
      "В этом простейшем примере Python сначала создаёт первую переменную a, потом — вторую с тем же именем, имеющую тип str, и освобождает память, которая была выделена первой переменной a.\n",
      "\r\n",
      "Может показаться, что писать на языках с динамической типизацией удобнее и проще, чем на языках со статической типизацией, однако, такие языки созданы не по чьей-то прихоти. При их разработке учтены особенности работы компьютерных систем. Всё, что написано в тексте программы, в итоге, сводится к инструкциям процессора. Это означает, что данные, используемые программой, например, в виде объектов или других типов данных, тоже преобразуются к низкоуровневым структурам.\n",
      "\r\n",
      "Python выполняет подобные преобразования автоматически, программист этих процессов не видит, и заботиться о подобных преобразованиях ему не нужно.\n",
      "\r\n",
      "Отсутствие необходимости указывать тип переменной при её объявлении — это не та особенность языка, которая делает Python медленным. Архитектура языка позволяет сделать динамическим практически всё, что угодно. Например, во время выполнения программы можно заменять методы объектов. Опять же, во время выполнения программы можно использовать технику «обезьяньих патчей» в применении к низкоуровневым системным вызовам. В Python возможно практически всё.\n",
      "\r\n",
      "Именно архитектура Python чрезвычайно усложняет оптимизацию.\n",
      "\r\n",
      "Для того чтобы проиллюстрировать эту идею, я собираюсь воспользоваться инструментом для трассировки системных вызовов в MacOS, который называется DTrace.\n",
      "\r\n",
      "В готовом дистрибутиве CPython нет механизмов поддержки DTrace, поэтому CPython нужно будет перекомпилировать с соответствующими настройками. Тут используется версия 3.6.6. Итак, воспользуемся следующей последовательностью действий:\n",
      "\n",
      "wget https://github.com/python/cpython/archive/v3.6.6.zip\n",
      "unzip v3.6.6.zip\n",
      "cd v3.6.6\n",
      "./configure --with-dtrace\n",
      "make\r\n",
      "Теперь, пользуясь python.exe, можно применять DTRace для трассировки кода. Об использовании DTrace с Python можно почитать здесь. А вот тут можно найти скрипты для измерения с помощью DTrace различных показателей работы Python-программ. Среди них — параметры вызова функций, время выполнения программ, время использования процессора, сведения о системных вызовах и так далее. Вот как пользоваться командой dtrace:\n",
      "\n",
      "sudo dtrace -s toolkit/<tracer>.d -c ‘../cpython/python.exe script.py’\r\n",
      "А вот как средство трассировки py_callflow показывает вызовы функций в приложении.\n",
      "\n",
      "\n",
      "Трассировка с использованием DTrace\n",
      "\r\n",
      "Теперь ответим на вопрос о том, влияет ли динамическая типизация на производительность Python. Вот некоторые соображения по этому поводу:\n",
      "\n",
      "\n",
      "Проверка и конверсия типов — операции тяжёлые. Каждый раз, когда выполняется обращение к переменной, её чтение или запись, производится проверка типа.\n",
      "Язык, обладающей подобной гибкостью, сложно оптимизировать. Причина, по которой другие языки настолько быстрее Python, заключается в том, что они идут на те или иные компромиссы, выбирая между гибкостью и производительностью.\n",
      "Проект Cython объединяет Python и статическую типизацию, что, например, как показано в этом материале, приводит к 84-кратному росту производительности в сравнении с применением обычного Python. Обратите внимание на этот проект, если вам нужна скорость.\n",
      "\n",
      "Итоги\r\n",
      "Причиной невысокой производительности Python является его динамическая природа и универсальность. Его можно использовать как инструмент для решения разнообразнейших задач. Для достижения тех же целей можно попытаться поискать более производительные, лучше оптимизированные инструменты. Возможно, найти их удастся, возможно — нет.\n",
      "\r\n",
      "Приложения, написанные на Python, можно оптимизировать, используя возможности по асинхронному выполнению кода, инструменты профилирования, и — правильно подбирая интерпретатор. Так, для оптимизации скорости работы приложений, время запуска которых неважно, а производительность которых может выиграть от использования JIT-компилятора, рассмотрите возможность использования PyPy. Если вам нужна максимальная производительность и вы готовы к ограничениям статической типизации — взгляните на Cython.\n",
      "\n",
      "Уважаемые читатели! Как вы решаете проблемы невысокой производительности Python?\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Последние несколько лет специалисты Microsoft трудились над тем, чтобы добавить поддержку инструментов разработчика Python в одни из наших самых популярных продуктов: Visual Studio Code и Visual Studio. В этом году все заработало. В статье мы познакомимся с инструментами разработчика Python в Visual Studio, Visual Studio Code, Azure и т. д. Заглядывайте под кат!\n",
      "\n",
      "\n",
      "\r\n",
      "Python — один из самых быстро развивающихся языков программирования, к которому обращаются как начинающие, так и опытные разработчики. Его популярность обусловлена легкой в освоении семантикой и широким спектром применения, начиная от написания скриптов и заканчивая созданием веб-сервисов и моделей машинного обучения. \n",
      "\r\n",
      "Дополнительную информацию и последние новости о Python в Microsoft вы можете найти в блоге Python at Microsoft.\n",
      "\n",
      "Visual Studio Code\r\n",
      "Расширение Python для Visual Studio Code с открытым исходным кодом включает в себя другие общедоступные пакеты Python, чтобы предоставить разработчикам широкие возможности для редактирования, отладки и тестирования кода. Python — самый быстроразвивающийся язык в Visual Studio Code, а соответствующее расширение является одним из самых популярных в разделе Marketplace, посвященном Visual Studio Code!\n",
      "\r\n",
      "Чтобы начать работу с расширением, необходимо сначала скачать Visual Studio Code, а затем, следуя нашему руководству Начало работы с Python, установить расширение и настроить основные функции. Рассмотрим некоторые из них.\n",
      "\r\n",
      "Прежде всего необходимо убедиться, что Visual Studio Code использует правильный интерпретатор Python. Чтобы сменить интерпретатор, достаточно выбрать нужную версию Python в строке состояния:\n",
      "\n",
      "\n",
      "\r\n",
      "Селектор поддерживает множество разных интерпретаторов и сред Python: Python 2, 3, virtualenv, Anaconda, Pipenv и pyenv. После выбора интерпретатора расширение начнет использовать его для функции IntelliSense, рефакторинга, анализа, выполнения и отладки кода.\n",
      "\r\n",
      "Чтобы локально запустить скрипт Python, можно воспользоваться командой «Python: Create Terminal» («Python: создать терминал») для создания терминала с активированной средой. Нажмите CTRL + Shift + P (или CMD + Shift + P на Mac), чтобы открыть командную строку. Чтобы выполнить файл Python, достаточно щелкнуть на нем правой кнопкой мыши и выбрать пункт «Run Python File in Terminal» («Запустить файл Python в терминале»):\n",
      "\n",
      "\n",
      "\r\n",
      "Эта команда запустит выбранный интерпретатор Python, в данном случае виртуальную среду Python 3.6, для выполнения файла:\n",
      "\n",
      "\n",
      "\r\n",
      "Расширение Python также включает шаблоны отладки для многих популярных типов приложений. Перейдите на вкладку «Debug» («Отладка») и выберите «Add Configuration…» («Добавить конфигурацию...») в выпадающем меню конфигурации отладки:\n",
      "\n",
      "\n",
      "\r\n",
      "Вы увидите готовые конфигурации для отладки текущего файла, подключающегося к удаленному серверу отладки или соответствующему приложению Flask, Django, Pyramid, PySpark или Scrapy. Для запуска отладки нужно выбрать конфигурацию и нажать зеленую кнопку Play (или клавишу F5 на клавиатуре, FN + F5 на Mac).\n",
      "\r\n",
      "Расширение Python поддерживает различные анализаторы кода, для которых можно настроить запуск после сохранения файла Python. PyLint включен по умолчанию, а другой анализатор можно выбрать с помощью команды «Python: Select Linter» («Python: выбрать анализатор кода»):\n",
      "\n",
      "\n",
      "\r\n",
      "Это еще не все: предусмотрена поддержка рефакторинга, а также модульного тестирования с помощью unittest, pytest и nose. К тому же вы можете использовать Visual Studio Live Share для удаленной работы над кодом Python вместе с другими разработчиками!\n",
      "\n",
      "Python в Visual Studio\r\n",
      "Visual Studio поддерживает большую часть функций Visual Studio Code, но также предлагает все полезные возможности интегрированной среды разработки, что позволяет совершать больше операций без использования командной строки. Visual Studio также предоставляет не имеющие равных возможности для работы с гибридными проектами Python и C# или C++.\n",
      "\r\n",
      "Чтобы включить поддержку Python в Visual Studio на Windows, необходимо выбрать рабочую нагрузку «Разработка на Python» и (или) рабочую нагрузку «Приложения для обработки и анализа данных и аналитические приложения» в установщике Visual Studio:\n",
      "\n",
      "\n",
      "\r\n",
      "Можно установить различные версии Python и Anaconda, выбрав их в меню дополнительных компонентов (см. правую часть скриншота выше).\n",
      "\r\n",
      "После установки рабочей нагрузки Python, можно начать работу, создав проект Python в разделе с помощью меню «Файл -> Новый проект» (в списке установленных компонентов выберите Python):\n",
      "\n",
      "\n",
      "\r\n",
      "Чтобы создать приложение с нуля, откройте шаблон приложения Python и приступайте к написанию кода. Также можно создать проект, взяв за основу существующий код Python или используя веб-шаблоны для Flask, Django и Bottle. Ознакомьтесь с нашим Руководством по Flask и Руководством по Django, чтобы получить подробную информацию по разработке веб-приложений с помощью этих платформ и Visual Studio.\n",
      "\r\n",
      "Если установлена рабочая нагрузка по обработке и анализу данных, также можно использовать шаблоны для проектов по машинному обучению с использованием Tensorflow и CNTK.\r\n",
      "После того как проект создан, управлять виртуальными средами и средами conda можно с помощью узла «Python Environments» («Среды Python») в обозревателе решений и окне среды Python. Щелкнув правой кнопкой мыши по активной среде Python и выбрав соответствующий пункт меню, можно установить дополнительные пакеты:\n",
      "\n",
      "\n",
      "\r\n",
      "Visual Studio по-настоящему демонстрирует свои возможности при использовании Python с другими языками. Можно объединять проекты Python и C++ для создания решения или даже встраивать файлы .py в проекты C++ или C#!\n",
      "\r\n",
      "Можно даже проводить отладку кода на обоих языках в рамках одного сеанса, например, переключившись с типа отладки C++ на Python/Native:\n",
      "\n",
      "\n",
      "\r\n",
      "Ознакомиться с подробной информацией о вставке Python в приложения C++ можно в публикации Вставка Python в проект C++ в блоге Python.\n",
      "\r\n",
      "Кроме того, Visual Studio включает профилировщик Python и поддерживает модульное тестирование Python в Обозревателе тестов.\n",
      "\n",
      "Python в Azure\n",
      "Пакет Azure SDK для Python позволяет создавать службы в Azure, управлять ими и взаимодействовать с ними. Командная строка Azure CLI написана на Python, поэтому почти все, что она позволяет сделать, вы можете также выполнить на программном уровне с помощью пакета Python SDK.\n",
      "\r\n",
      "Можно устанавливать отдельные библиотеки, например для установки пакета SDK для взаимодействия с Azure Storage воспользуйтесь командой:\n",
      "\n",
      "    pip install azure-storage\r\n",
      "Рекомендуется устанавливать только нужные вам пакеты, но для удобства вы можете установить весь набор пакетов Azure SDK, выполнив следующую команду:\n",
      "\n",
      "    pip install azure\r\n",
      "После установки пакета SDK вы получаете доступ ко множеству полезных служб, начиная от использования API машинного обучения с помощью Azure Cognitive Services и заканчивая размещением глобально распределенных данных с помощью Azure Cosmos DB.\n",
      "\r\n",
      "Веб-приложения можно развернуть с помощью функции Azure «Веб-приложение для контейнеров». Ознакомьтесь с видео From Zero to Azure with Python and Visual Studio Code (В Azure с нуля с помощью Python и Visual Studio Code), предоставляющим всю необходимую информацию по развертыванию приложений Flask с использованием Visual Studio Code. Также обратите внимание на краткое пособие по развертыванию приложения Flask с использованием командной строки.\n",
      "\r\n",
      "Кроме того, на Azure можно запускать свободно размещенные блокноты Jupyter, поэтому локальная установка Jupyter не потребуется. К блокнотам Jupyter можно открывать доступ для их совместного использования. Например, вы можете просмотреть находящийся в общем доступе блокнот для создания рукописного текста с помощью машинного обучения:\n",
      "\n",
      "\n",
      "\r\n",
      "Войдите в учетную запись на notebooks.azure.com, чтобы опробовать клонирование и запуск блокнотов Jupyter!\n",
      "\n",
      "Полезные материалы по теме\n",
      "Мини-книга «Создавайте более качественные приложения и быстро используйте данные там, где это нужно»\r\n",
      "Читайте электронную книгу Создание современных приложений на основе больших данных в глобальном масштабе, чтобы узнать, как готовая к использованию глобально распределенная служба баз данных Azure Cosmos DB меняет подходы к управлению данными. Обеспечивайте доступность, согласованность и защиту данных, используя передовые отраслевые технологии корпоративного класса для соблюдения нормативных требований и обеспечения безопасности. Начните разработку лучших приложений для своих пользователей на базе одной из пяти четко определенных моделей согласованности.\n",
      "\n",
      "\n",
      "\r\n",
      "→ Скачать\n",
      "\n",
      "Семинар «Как выбрать правильную инфраструктуру для выполнения ваших рабочих нагрузок в Azure»\r\n",
      "В этом семинаре присоединитесь к рассказу регионального директора Microsoft Эрику Бойду, MVP Azure, о том, как выбрать правильные виртуальные машины, хранилища и сети для приложений и рабочих нагрузок в Azure.\n",
      "\r\n",
      "→ Скачать/Посмотреть\n",
      "\n",
      "Руководство по архитектуре облачных приложений\r\n",
      "Используйте структурированный подход к разработке облачных приложений. В этой 300-страничной электронной книге об архитектуре облачных вычислений рассматриваются рекомендации по архитектуре, разработке и внедрению, которые применяются независимо от выбранной облачной платформы. В это руководство включены шаги по:\n",
      "\n",
      "\n",
      "выбору правильного стиля архитектуры облачного приложения для своего приложения или решения;\n",
      "выбору соответствующих технологий вычислений и хранения данных;\n",
      "внедрению 10 принципов разработки для создания масштабируемого, отказоустойчивого и управляемого приложения;\n",
      "следованию пяти принципам создания качественного программного обеспечения, гарантирующего успех вашего облачного приложения;\n",
      "использованию конструктивных шаблонов, предназначенных для проблемы, которую вы пытаетесь решить.\n",
      "\n",
      "\n",
      "\r\n",
      "→ Скачать    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Did you ever think that one day you had got into PHP web programming too quickly?\r\n",
      "Several years have passed already, you have gained a lot of experience and can’t think of any other ways to work with web but PHP. Perhaps, you sometimes doubt the choice you have made, but are unable to confirm your doubts here and now. At the same time, you need real examples; you want to understand the changes that may occur in particular aspects of your work.\n",
      "\r\n",
      "Today I will try to answer the following question: \"What if we use Python instead of PHP?\".\n",
      "\r\n",
      "I have asked this question myself many times. I have been using PHP for 11 years already and am a certified PHP specialist. I have mastered it so it works just the way I want. I was really puzzled by several articles that criticized PHP severely (PHP: a fractal of bad design). However, when chance came, I switched to Ruby and then to Python. Eventually, I chose the latter. Now I will try to explain how we Python guys live out there.\n",
      "\n",
      "\n",
      "\n",
      "Article Format\r\n",
      "The best way to learn a new language is to compare it to a language you can already apply, unless the new language differs drastically from the current one. An article on the Ruby web portal (Ruby from other languages) provides such a comparison, however it lacks examples.\n",
      "\r\n",
      "I should also note that this very article compares only aspects that draw your attention during the first weeks after switching to the new language.\n",
      "\n",
      "Preparing Consoles\r\n",
      "I’ve done my best to make this article interactive. Therefore, I recommend running all examples in the consoles when reading the article. You will need a PHP console, or you can use the PsySH console, which is even better:\n",
      "\n",
      "php -a\r\n",
      "And a Python console. I recommend using bpython or ipython, since they already feature code completion comparison with the default console integrated into the language. However, the following option is also applicable:\n",
      "\n",
      "python\r\n",
      "And then:\n",
      "\n",
      "import rlcompleter\n",
      "import readline\n",
      "readline.parse_and_bind(\"tab: complete\")    # Enable autocomletion\n",
      "\n",
      "Avoiding repetition of these actionsCreate ~/.pyrc file with:\n",
      "\n",
      "import rlcompleter\n",
      "import readline\n",
      "readline.parse_and_bind(\"tab: complete\")\n",
      "\r\n",
      "Add few lines in ~/.bashrc file:\n",
      "\n",
      "\n",
      "export PYTHONSTARTUP=\"${HOME}/.pyrc\"\n",
      "export PYTHONIOENCODING=\"UTF-8\"\n",
      "\r\n",
      "Adn to make changes immediatelly without re-login:\n",
      "\n",
      "source ~/.bashrc\n",
      "\n",
      "About languages\n",
      "\n",
      "Python is a language that features a strong duck dynamic typing (check Information on Typing). Python is not limited in terms of its application. It is used for web development, daemons, scientific calculations, or as a language of extensions. Similar languages in terms of typing: Ruby. \n",
      "PHP is a language that features weak duck dynamic typing. PHP is a common-purpose language as well, however its application area mostly covers web and daemons. Other features are not worked out properly, making it non-applicable in production. Some people believe that PHP is meant to die. Similar languages in terms of typing: JavaScript, Lua, Perl.\n",
      "\n",
      "General Specifics\n",
      "\n",
      "Code is written in .py files regardless of the Python version. No opening tags like <?PHP are required, since Python was originally developed as a general-purpose programming language. \n",
      "Additionally, there is no php.ini for the same reason. There are two dozen\r\n",
      "environment variables, however they are undefined in most cases (apart from PYTHONIOENCODING). In other words, there are no default connections to bases, error filter management, limit management, extensions, etc., which is natural for most general-purpose languages. As a result, programs behave in a similar way in most cases (their behavior does not depend on the favorite settings of your team lead). Settings like php.ini are stored in the main configuration file of an application in most cases.\n",
      "A semicolon is not needed at the end of a string. However, if we put a semicolon there, it works like in PHP. Nevertheless, it is unnecessary and undesired, so you can simply forget about it.\n",
      "Variables do not start with $ (which PHP inherited from Perl that in turn adopted it\r\n",
      "from Bash).\n",
      "Assignment in cycles and conditions does not apply, so that no one mistakes comparison for assignment, which is a common mistake (as the language author believes). See PEP 572 for python 3.8 if you need it.\n",
      "Upon parsing files, Python automatically puts the file copy with the .pyc extension (provided that your Python version is less than 3.3 and you have not installed PYTHONDONTWRITEBYTECODE) in the same folder as your byte code is located. Then it always executes this very file, unless you change the source.\r\n",
      "These files are automatically ignored in all IDEs and generally do not interfere. This feature can be perceived as a complete analog of PHP APC, considering that .pyc files will likely be located in the file cache memory.\n",
      "Instead of NULL, TRUE, false we should use None, True, false (particularly in this case).\n",
      "\n",
      "Nesting and Indenting\r\n",
      "Here's something unusual: code nesting is determined with indents instead of brackets.\n",
      "\r\n",
      "So, instead of:\n",
      "\n",
      "foreach($a as $value) {\n",
      "    $formatted = $value.'%';\n",
      "    echo $formatted;\n",
      "}\n",
      "\r\n",
      "We should write the following:\n",
      "\n",
      "for value in a:\n",
      "    formatted = value + '%'\n",
      "    print(formatted)\n",
      "\n",
      "Wait, hold on! Don’t close the article. Here you might make the very mistake I made.\r\n",
      "Once I believed that the idea of using indents for code nesting is ridiculous. My entire nature was protesting against it, since all developers write their code in their own way, regardless of different Style Guides.\n",
      " Here's the mystery of the world: there's no indent issue. In most cases (99% of cases) indents are placed automatically by IDE as in any other language. You just don't think about it at all. I haven't faced any issue related to indents over 2 years of using the language.\n",
      "\n",
      "Strong Typing\r\n",
      "The next thing you should pay attention to is Strong Typing. However, some code first:\n",
      "\n",
      "print '0.60' * 5;\n",
      "\n",
      "print '5' == 5;\n",
      "\n",
      "$a = array('5'=>true);\n",
      "print $a[5];\n",
      "\n",
      "$value = 75;\n",
      "print $value.'%';\n",
      "\n",
      "$a='0';\n",
      "if($a) print 'non zero length';  // Will not print, common mistake\n",
      "\r\n",
      "All the above examples are possible thanks to dynamic typing. \n",
      "Yes I know that Type declaration is available for PHP. But it is not enabled by default and not work everywhere.\n",
      " \r\n",
      "However, the following will not work in Python:\n",
      "\n",
      ">>> print \"25\" + 5\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: cannot concatenate 'str' and 'int' objects\n",
      "\n",
      "Except...Except for boolean values but it is by design.\r\n",
      "Usually data types will not be mixed in your code and this effect will not distract you. In general, when I coded on PHP, there were only a few situations per project when dynamic typing actually helped. In all other cases, variables of the same type interacted with each other.\n",
      "\r\n",
      "Strong Typing affects error handling. For example, if an int function returns an integer, it cannot return None to a string, from which this type cannot be explicitly extracted. In this case, an Exception will be raised. It may require converting all the user's data to the needed data type, otherwise an exception will be raised in the Production version one day.\n",
      "\n",
      "try:\n",
      "    custom_price = int(request.GET.get('custom_price', 0))\n",
      "except ValueError:\n",
      "    custom_price = 0\n",
      "\r\n",
      "It affects not only standard functions, but also some methods of the lists, strings, and some functions from additional libraries. Usually, a Python developer keeps in mind all exceptions that can be raised, and considers them. If the developer does not remember them, they check the library code. Of course, sometimes not all cases are considered, and users may cause exceptions in the Production version. Since this is a rare phenomenon, and usually Web-frameworks automatically send them to the administrator's mail, such cases are quickly fixed.\n",
      "\r\n",
      "In order to use values of various data types within a single expression, they should be converted. There are functions for that: str, int, bool, long. Also, there are more convenient decisions for formatting.\n",
      "\n",
      "Strings\n",
      "Formatting\r\n",
      "In PHP:\n",
      "\n",
      "$this_way = 'this_way';\n",
      "echo \"Currently you do it $this_way or {$this way}.\";\n",
      "echo \"Or \".$this_way.\".\";\n",
      "echo sprintf(\"However the following is possible: , %s or %1$'.9s.\", $this_way);\n",
      "\r\n",
      "Now you should learn to do it in a different way:\n",
      "\n",
      "etot = 'this'\n",
      "var = 'option'\n",
      "print('To %s option' % etot)\n",
      "print(etot + ' option can also be used, but not recommended)\n",
      "print('Or to %s %s' % (etot, var))\n",
      "print('Or to %(etot)s %(var)s' % {'etot': etot, 'var': var}) # Very useful for localization team\n",
      "print('Or to {} {}'.format(etot, var))\n",
      "print('Or to {1} {0}'.format(var, etot))\n",
      "print('Or to {etot} {var}'.format(var=var, etot=etot))\n",
      "\n",
      "# And finally\n",
      "print(f'Or to {etot} {var}') # Starting from Python 3.6\n",
      "\r\n",
      "There are more options and there is a convenient option for localizations.\n",
      "\n",
      "String Methods\r\n",
      "Python has something that is missing in PHP: built-in methods. Let's compare:\n",
      "\n",
      "strpos($a, 'tr');\n",
      "trim($a);\n",
      "\n",
      "vs\n",
      "\n",
      "a.index('tr')\n",
      "a.strip()\n",
      "\r\n",
      "And how often do you do something like that?\n",
      "\n",
      "substr($a, strpos($a, 'name: '));\n",
      "\n",
      "vs\n",
      "\n",
      "a[a.index('name: '):]\n",
      "\n",
      "Unicode Support\r\n",
      "Finally, Unicode. In Python 2, all strings are NOT Unicode by default. (In Python 3, all strings are Unicode by default). However, when you add the character u at the beginning of a string, it automatically becomes Unicode. And then all built-in (and not built-in) string methods of Python will work properly.\n",
      "\n",
      ">>> len('Привет мир')  # Hello world in Russian\n",
      "19\n",
      ">>> len(u'Привет мир')\n",
      "10\n",
      "\r\n",
      "In PHP natural Unicode processing was developing for PHP 6 but PHP 6 was cancelled (Andrei Zmievski: Waht happened to Unicode and PHP 6).\r\n",
      "In PHP, by the way, you can use the MBString function overloading to receive a similar effect but it is deprecated.\r\n",
      "However, you will not be able to work with binary strings using overloaded functions, but you will still be able to work with a string as an array.\n",
      "About raw strings (optional)Raw strings\r\n",
      "You should know the difference between single quoted strings and double quoted strings:\n",
      "\n",
      "$a = 'Hello.\\n';\n",
      "$a[strlen($a)-1] != \"\\n\"; \n",
      "\r\n",
      "The similar feature in Python has called raw strings. To use it place characted r before single quoted string.\n",
      "\n",
      "a = r'Hello.\\n' \n",
      "a[-1] != '\\n'\n",
      "\n",
      "\n",
      "Arrays\r\n",
      "Now it is time for arrays. In PHP you could use integers or strings as keys:\n",
      "\n",
      "var_dump([0=>1, 'key'=>'value']); \n",
      "\r\n",
      "In PHP, arrays are not standard arrays (lists), but associative arrays (dictionary). Standard arrays are also available in PHP, they are SPLFixedArray. They require less memory, potentially work faster, but due to the complexity of creating and extending, are rarely used.\n",
      "\r\n",
      "In Python, four data types are used for an array:\n",
      "\n",
      "\n",
      "list\n",
      "\n",
      "a = [1, 2, 3]  # short form\n",
      "a[10] = 11  # Unexist indexes cannot be added.\n",
      "# > IndexError: list assignment index out of range\n",
      "a.append(11)  # but you can adds an element to the end of a list.\n",
      "del a[0]  # and delete the element using the index\n",
      "a.remove(11)  # and also remove an element using its value\n",
      "\n",
      "dict — dictionary. Dictionaries have no order of storing data (as they do in PHP).\n",
      "\n",
      "d = {'a': 1, 'b': 2, 'c': 3}  # short form\n",
      "d[10] = 11  # Random indexes can be added\n",
      "d[True] = False  # And use any immutable types (integers, strings, booleans, tuples, frozen-sets)\n",
      "del d[True]  # And delete using a key\n",
      "\n",
      "tuple. Something like a fixed array of non-homogeneous values. Perfectly suited to returning several values from a function and for compact storage of configurations.\n",
      "\n",
      "t = (True, 'OK', 200, )  # short form\n",
      "t[0] = False  # Elements are immutable\n",
      "# > TypeError: 'tuple' object does not support item assignment\n",
      "del t[True]  # Cannot be deleted using a key\n",
      "# > TypeError: 'tuple' object doesn't support item deletion\n",
      "t = ([], )  # However, nested mutable structures can be muted (lists, dictionaries, sets, bite arrays, objects)\n",
      "t[0].append(1)\n",
      "# > a == ([1], )\n",
      "\n",
      "set. Basically, this is a list of unique values that have no order of storing.\n",
      "\n",
      "s = set([1,3,4])\n",
      "s[0] = False  # sets do not support indexing\n",
      "# > TypeError: 'set' object does not support indexing\n",
      "s.add(5)  # adds an element\n",
      "s.remove(5)  # deletes an element\n",
      "# # Standard maths for sets\n",
      "s | s  # Merge\n",
      "s & s  # Intersection\n",
      "s - s  # Difference\n",
      "s ^ s  # Merge of unique elements\n",
      "\n",
      "\r\n",
      "In PHP, arrays are kind of a Swiss army knife—they can serve all purposes. As for Python, you need to use data arrays that are native for Computer Science. Besides, you need to use appropriate data arrays for each single case. You may say that these are unnecessary difficulties that programmers should never face. Well, that's not the point.\n",
      "\n",
      "\n",
      "First: the possibility to choose between tuple, set, list, and dict does not make things difficult—it just becomes a subconscious habit like changing gears.\n",
      "Second: list or dict are used in most cases.\n",
      "Third: in most cases, when you need to store the key-value pair, the order is not essential, however, when you need to keep the order, in most cases there are only values instead of key-value pairs.\n",
      "Fourth: Python has an ordered dictionary — OrderedDict.\n",
      "\n",
      "\n",
      "Imports\r\n",
      "This is a very interesting feature. It's kind of an alternative concept of name spaces that must be used.\r\n",
      "In PHP, you write require_once and it remains available until the end of the PHP execution session. Generally when using CMS, developers put everything into classes, place them in special locations, write a small function that know these locations, and register this function via spl_autoload_register at the beginning of the file.\n",
      "\r\n",
      "In Python, however, each file has its own name space. As a result, a file will only contain objects you import there. By default, only the standard Python library is available (approximately 80 functions). \n",
      "\r\n",
      "Check the example below:\r\n",
      "Let's assume, you've created the tools/logic.py file:\n",
      "\n",
      "def is_prime(number):\n",
      "    max_number = int(sqrt(number))\n",
      "\n",
      "    for multiplier in range(2, max_number + 1):\n",
      "        if multiplier > max_number:\n",
      "            break\n",
      "        if number % multiplier == 0:\n",
      "            return False\n",
      "\n",
      "    return True\n",
      "\r\n",
      "Now, you want to use it in the main.py file. In this case, you need to import either the entire file or the file entities you require in the target file you are working on.\n",
      "\n",
      "from tools.logic import is_prime\n",
      "\n",
      "print(is_prime(79))\n",
      "\r\n",
      "This rule applies Python-wide. In most cases, when you start working on any file, you first need to import auxiliary Python objects into your file: your own and integrated libraries. It is as if PHP functions like mysqli_*, pdo_*, memcached_*, as well as your entire code, are stored in name spaces and you have to import them into each file you work with. What advantages does this approach have?\n",
      "\n",
      "\n",
      "First: different objects in different files can have identical names. However, it is you who selects the object, its name, and target file.\n",
      "Second: refactoring is much easier. You can always keep track of a class, function, or any other entity. Use a simple search to locate a function and see where/how it is used.\n",
      "Third: this feature makes developers think over the code structure (at least to some extent).\n",
      "\r\n",
      "On the other hand, we can mention only one drawback — circular imports. However, it is a rare issue. Besides, it is a familiar issue, and developers know the ways to resolve it.\n",
      "\r\n",
      "Is it convenient to use imports all the time? It depends on your preference. If you like to have more control over the code, you will prefer using imports. Some teams even have rules that regulate the order of assigning external code, so as to minimize the amount of circular imports. If your team does not have such rules and you do not want to bother much, you can simply rely on IDE that will automatically import everything you use. In addition: imports are not a unique Python feature, they are also used in Java and C#.\n",
      "\r\n",
      "There have been no complaints so far.\n",
      "\n",
      "Parameters *args, and **kwargs in a Function\r\n",
      "Syntax with default parameters is generally the same:\n",
      "\n",
      "function makeyogurt($flavour, $type = \"acidophilus\")\n",
      "{\n",
      "    return \"Making a bowl of $type $flavour.\";\n",
      "}\n",
      "\n",
      "vs\n",
      "\n",
      "def makeyogurt(flavour, ftype=\"acidophilus\"):\n",
      "    return \"Making a bowl of %s %s.\" % (ftype, flavour, )\n",
      "\r\n",
      "However, you may sometimes need a function for an unknown number of arguments.\r\n",
      "For example, a proxy function, logging function, or function for receiving signals. In PHP, starting from version 5.6, the following syntax is available:\n",
      "\n",
      "function sum(...$numbers) {\n",
      "    $acc = 0;\n",
      "    foreach ($numbers as $n) {\n",
      "        $acc += $n;\n",
      "    }\n",
      "    return $acc;\n",
      "}\n",
      "\n",
      "echo sum(1, 2, 3, 4);\n",
      "// or\n",
      "echo add(...[1, 2, 3, 4]);\n",
      "\r\n",
      "Respectively, in Python, you can add unnamed arguments into an array, and add named arguments into a dictionary:\n",
      "\n",
      "def acc(*args, **kwargs):\n",
      "    total = 0\n",
      "    for n in args:\n",
      "        total += n\n",
      "    return total\n",
      "\n",
      "print(acc(1, 2, 3, 4))\n",
      "# or\n",
      "print(acc(*[1, 2, 3, 4]))\n",
      "\r\n",
      "Respectively, *args — list of unnamed arguments, **kwargs — dictionary of named arguments.\n",
      "\n",
      "Classes\r\n",
      "Have a look at the below code:\n",
      "\n",
      "class BaseClass:\n",
      "    def __init__(self):\n",
      "        print(\"In BaseClass constructor\")\n",
      "\n",
      "class SubClass(BaseClass):\n",
      "    def __init__(self, value):\n",
      "        super(SubClass, self).__init__()\n",
      "# or short form: super().__init__() starting from Python 3\n",
      "        self.value = value\n",
      "\n",
      "    def __getattr__(self, name):\n",
      "        print(\"Cannot found: %s\" % name)\n",
      "\n",
      "c = SubClass(7)\n",
      "print(c.value)\n",
      "\r\n",
      "The main differences from PHP are as follows:\n",
      "\n",
      "\n",
      "self is used instead of $this, and methods are always called using the access operator (\".\"). Besides, «self» should always be the first argument in all methods (well, in most of them). The point is that Python provides all methods with a link to the object with the first argument (the object itself can be added to a variable with any name).\n",
      "As in PHP, there is an analog of magic names. Instead of __construct — __init__. Instead of __get — __getattr__, etc.\n",
      "new is not required. Creating a class instance is the same as calling a function.\n",
      "A more complex call to the parent method. As for super, you always need to keep all the details in mind. parent:: As for PHP, the structure is less bulky.\n",
      "\r\n",
      "We should also mention the following:\n",
      "\n",
      "\n",
      "More than one class can be inherited.\n",
      "No public, protected, private. Python allows changing an instance structure (as well as the entire class) on the run via simple assignment, so no protection is necessary. Thus, Reflection is not required as well. However, there's an analog of the protected state — double underscore before the name. However, this operation simply changes the variable/method visible name into _%ClassName%__ %varname% that allows for working with hidden data.\n",
      "No static, final classes, and interfaces. The model is more object-based in Python in general. Instead of Singleton, you will likely have a file containing all the required functions or a file that returns the same instance upon importing. Instead of interface, you will likely create a class that raises exceptions for methods that are not reassigned for some reason (i.e. workaround are possible).\n",
      "It is not desired to apply only object-oriented programming (OOP). Since everything is an object anyway (even bool) and syntax has no different operators for calling a method or function from an imported file, all calls are performed with the access operator (\".\"). Thus, encapsulation does not necessarily require OOP. Therefore, in most projects, classes are created where they are really required.\n",
      "\n",
      "Coding Style\r\n",
      "I've worked on several long-term projects and noticed that all team members have a different coding style. In many cases, a code can help identify the code author. I've always wanted any code style standard to be adopted for the purpose of consistency.\n",
      "\r\n",
      "However, there have always been many arguments when approving this document within the team. This issue affects Python as well, but to a lesser extent, since there are several recommendations from qualified specialists, which will definitely be enough to start with:\n",
      "\n",
      "\n",
      "PEP 8. Recommendations from the Python development team and the utility check program of the same name.\n",
      "Recommendations from Google.\n",
      "\r\n",
      "Furthermore, there is a so-called Zen of Python. One of its rules states that «There should be one-- and preferably only one --obvious way to do it.» Thus, a code cannot be written in several approximately similar ways. Of course, that's idealism, but it helps in many cases:\n",
      "\n",
      "\n",
      "Instead of a large stock library containing several functions that partially duplicate each other, we use a smaller set of methods and additional integrate libraries (for example for hash totals).\n",
      "Instead of strlen and count, we always use len.\n",
      "\r\n",
      "etc.\n",
      "\n",
      "Python Versions\r\n",
      "New versions of PHP are always backward compatible with previous ones, though sometimes improvements are required. On the other hand, there is Python 2 and Python 3 They are incompatible little bit by default. However, recently, Python developers have improved the situation significantly. You can write a code for two Python versions, but if you use new Python 3 features like asynchronous programming or new Unicode features (UTF 8), you'll likely face difficulties. Because of this, projects that have already been developed and coded for several years, still use Python 2.\n",
      "\r\n",
      "But for new projects there is no reason to use Python 2.\n",
      "\n",
      "Cross languages aliases\r\n",
      "Below is the list of key words that explain the alternative Python provides to the technology you are currently using.\n",
      "\n",
      "\n",
      "composer -> pip\n",
      "mod_php -> mod_wsgi\n",
      "nginx + php-fpm -> nginx + uwsgi + uwsgi_python\n",
      "daemon.io -> tornado, twisted\n",
      "Zend Framework -> Django\n",
      "Phalcon -> falcon\n",
      "\n",
      "Conclusion\r\n",
      "How do you know if you need it or not?\n",
      "\n",
      "\n",
      "You believe that the stronger the typing the better.\n",
      "You prefer a language that features a well-organized architecture.\n",
      "You are a perfectionist, excessive diversity annoys you.\n",
      "Python job positions in your city look more promising (or you've got bored of developing only web portals).\n",
      "You want your main programming language to be suitable for developing any kind of software (considering the reasonable restrictions related to dynamic typing).\n",
      "You do not like the skill level of junior developers (due to the relatively low learning curve).\n",
      "\n",
      "My Way to Learn Python\r\n",
      "If you are an experienced developer, you will need up to three weeks to learn it without putting in too much effort.\n",
      "\n",
      "\n",
      "First week: Read Dive Into Python, Chapters 2—7. You can look through other chapters briefly, paying attention to interesting points only. At the same time, complete 10 tasks with Project Euler. Finally, create a console utility that accepts parameters. You can either port any of your previous bash scripts, or create an analog of ls from BusyBox, or something new. The point is that the script should do something useful, something you do frequently. For example, I've ported my PHP utility that can display data in the memory cache.\n",
      "Second week: Create a simple analog of Medium in Django and gun on any hosting. Mind the components: registration, login, password recovery, sharing posts and comments and removing them, checking permissions for actions.\n",
      "Third week: Select a company you would like to work for, send them your CV asking for a Python test task to test your skills.\n",
      "\r\n",
      "Good luck!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Задумывались ли вы когда-нибудь о том, как данные, с которыми вы работаете, выглядят в недрах Python? О том, как переменные создаются и хранятся в памяти? О том, как и когда они удаляются? Материал, перевод которого мы публикуем, посвящён исследованиям глубин Python, в ходе которых мы попытаемся выяснить особенности управления памятью в этом языке. Изучив эту статью, вы разберётесь с тем, как работают низкоуровневые механизмы компьютеров, в особенности те из них, которые связаны с памятью. Вы поймёте то, как Python абстрагирует низкоуровневые операции и познакомитесь с тем, как он управляет памятью.\n",
      "\n",
      "\n",
      "\r\n",
      "Знание того, что происходит в Python, позволит вам лучше понимать некоторые особенности поведения этого языка. Это, хочется надеяться, даст вам возможность по достоинству оценить ту огромную работу, которая делается внутри используемой вами реализации этого языка для того, чтобы ваши программы работали именно так, как вам нужно.\n",
      "\n",
      "Память — это пустая книга\r\n",
      "Компьютерную память, в самом начале работы с ней, можно представить в виде пустой книги, предназначенной для коротких рассказов. Пока на её страницах ничего нет, но очень скоро появятся авторы рассказов, каждый из которых захочет свой рассказ в эту книгу записать.\n",
      "\r\n",
      "Так как один рассказ нельзя записать поверх другого, авторам надо внимательно относиться к тому, на каких именно страницах книги они пишут. Перед тем, как что-нибудь записать, они консультируются с главным редактором. Он решает — куда именно авторам можно записывать рассказы.\n",
      "\r\n",
      "Так как книга, о которой мы говорим, существует уже довольно давно, многие рассказы в ней уже устарели. Если никто не читает некий рассказ или не упоминает его в своих работах, этот рассказ из книги убирают, освобождая место для новых историй.\n",
      "\r\n",
      "В целом можно сказать, что компьютерная память очень похожа на такую вот книгу. На самом деле, непрерывные блоки памяти фиксированной длины даже называют страницами, поэтому мы полагаем, что сравнение памяти с книгой является весьма удачным.\n",
      "\r\n",
      "Авторы, которые записывают свои рассказы в книгу — это разные приложения или процессы, которым нужно хранить данные в памяти. Главный редактор, который принимает решения о том, на каких именно страницах книги можно делать записи авторам — это тот механизм, который занимается управлением памятью. А того, кто убирает из книги старые истории, освобождая место для новых, можно сравнить с механизмом сборки мусора.\n",
      "\n",
      "Управление памятью: путь от железа к программам\r\n",
      "Управление памятью — это процесс, в ходе реализации которого программы выполняют запись данных в память и чтение их из неё. Менеджер памяти — это сущность, которая определяет то, где именно приложение может разместить свои данные в памяти. Та как число фрагментов памяти, которое может быть выделено приложениям, не бесконечно, так же как не бесконечно и число страниц в любой книге, менеджеру памяти, обслуживая приложения, нужно находить свободные фрагменты памяти и предоставлять их приложениям. Этот процесс, в ходе которого приложениям «выдают» память, называется выделением памяти.\n",
      "\r\n",
      "С другой стороны, когда некие данные больше не нужны, их можно удалить, или, другими словами, освободить память, которую они занимают. Но что именно «выделяют» и «освобождают», говоря о памяти?\n",
      "\r\n",
      "Где-то в вашем компьютере есть физическое устройство, которое хранит данные, используемые во время работы Python-программами. Прежде чем некий объект Python окажется в физической памяти, коду приходится пройти через множество слоёв абстракции.\n",
      "\r\n",
      "Один из главных таких слоёв, который расположен поверх аппаратного обеспечения (такого, как оперативная память или жёсткий диск) — это операционная система (ОС). Она выполняет (или отказывается выполнять) запросы на чтение данных из памяти и на запись данных в память.\n",
      "\r\n",
      "Поверх ОС находится приложение, в нашем случае — одна из реализаций Python (это может быть программный пакет, входящий в состав вашей ОС или загруженный с python.org). Именно этот программный пакет и занимается управлением памятью, обеспечивая работу вашего Python-кода. В центре внимания этой статьи находятся алгоритмы и структуры данных, которые Python использует для управления памятью.\n",
      "\n",
      "Эталонная реализация Python\r\n",
      "Эталонная реализация Python называется CPython. Она написана на языке C. Когда я впервые об этом услышал, это буквально выбило меня из колеи. Язык программирования, который написан на другом языке? Ну, на самом деле, это не совсем так.\n",
      "\r\n",
      "Спецификация Python описана в этом документе на обычном английском языке. Однако, сама по себе эта спецификация код, написанный на Python, выполнять, конечно, не может. Для этого нужно что-то, что, следуя правилам из этой спецификации, сможет интерпретировать код, написанный на Python.\n",
      "\r\n",
      "Кроме того, нужно что-то, что может выполнить интерпретированный код на компьютере. Эталонная реализация Python решает обе эти задачи. Она преобразует код в инструкции, которые потом выполняются на виртуальной машине.\n",
      "\r\n",
      "Виртуальные машины похожи на обычные компьютеры, созданные из кремния, металла и других материалов, но они реализованы программными средствами. Они обычно заняты обработкой базовых инструкций, похожих на команды, написанные на Ассемблере.\n",
      "\r\n",
      "Python — это интерпретируемый язык. Код, написанный на Python, компилируется в набор инструкций, с которым удобно работать компьютеру, в так называемый байт-код. Эти инструкции интерпретируются виртуальной машиной когда вы запускаете свою программу.\n",
      "\r\n",
      "Вам доводилось видеть файлы с расширением .pyc или папку __pycache__? В них и находится тот самый байт-код, который интерпретируется виртуальной машиной.\n",
      "\r\n",
      "Важно отметить, что, помимо CPython, существуют и другие реализации Python. Например, при использовании IronPython код на Python компилируется в инструкции Microsoft CLR. В Jython код компилируется в байт-код Java и выполняется на виртуальной машине Java. В мире Python есть такое явление как PyPy, но оно достойно отдельной статьи, поэтому тут мы просто упомянем о нём.\n",
      "\r\n",
      "Для целей этой статьи я сосредоточусь на том, как работают механизмы управления памятью в эталонной реализации Python — CPython.\n",
      "\r\n",
      "Надо отметить, что хотя большая часть того, о чём мы будем тут говорить, будет справедлива и для новых версий Python, в будущем положение дел может измениться. Поэтому обратите внимание на то, что в этой статье я ориентируюсь на самую свежую на момент её написания версию Python — Python 3.7.\n",
      "\r\n",
      "Итак, программный пакет CPython написан на C, он интерпретирует байт-код Python. Какое это имеет отношение к управлению памятью? Дело в том, что алгоритмы и структуры данных, используемые для управления памятью, существуют в коде CPython, написанном, как уже было сказано, на C. Для того чтобы понять то, как в Python работает управлению памятью, сначала нужно немного разобраться с CPython.\n",
      "\r\n",
      "Язык C, на котором написан CPython, не обладает встроенной поддержкой объектно-ориентированного программирования. Из-за этого в коде CPython применено немало интересных архитектурных решений.\n",
      "\r\n",
      "Возможно, вы слышали о том, что всё в Python — это объект, даже примитивные типы данных вроде int и str. И это действительно так на уровне реализации языка в CPython. Тут существует структура, которая называется PyObject, которой пользуются объекты, создаваемые в CPython.\n",
      "\r\n",
      "Структура (struct) — это композитный тип данных, который способен группировать данные разных типов. Если сравнить это с объектно-ориентированным программированием, то структура похожа на класс, у которого есть атрибуты, но нет методов.\n",
      "\n",
      "PyObject — предок всех объектов Python. Эта структура содержит всего два поля:\n",
      "\n",
      "\n",
      "ob_refcnt — счётчик ссылок.\n",
      "ob_type — указатель на другой тип.\n",
      "\r\n",
      "Счётчик ссылок используется для реализации механизма сборки мусора. Другое поле PyObject — это указатель на конкретный тип объекта. Этот тип представлен ещё одной структурой, которая и описывает объект Python (например — это может быть тип dict или int).\n",
      "\r\n",
      "У каждого объекта есть собственный, уникальный для такого объекта, механизм выделения памяти, который знает о том, как получить память, необходимую для хранения этого объекта. Кроме того, у каждого объекта есть и собственный механизм освобождения памяти, который и «освобождает» память после того, как она больше не нужна.\n",
      "\r\n",
      "Однако надо отметить, что во всех этих разговорах о выделении и освобождении памяти есть один важный фактор. Дело в том, что компьютерная память является разделяемым ресурсом. Если, в одно и то же время, два разных процесса попытаются записать что-то в одну и ту же область памяти, может произойти что-то нехорошее.\n",
      "\n",
      "Глобальная блокировка интерпретатора\r\n",
      "Глобальная блокировка интерпретатора (Global Interpreter Lock, GIL) — это решение распространённой проблемы, возникающей при работе с разделяемыми ресурсами компьютера наподобие памяти. Когда два потока пытаются одновременно модифицировать один и тот же ресурс, они могут друг с другом «столкнуться». В результате получится беспорядок и ни один из потоков не достигнет того, к чему стремился.\n",
      "\r\n",
      "Давайте снова вернёмся к аналогии с книгой. Представим себе, что два автора самовольно решили, что сейчас — их очередь делать записи. Но они, кроме того, решили делать записи одновременно и на одной и той же странице.\n",
      "\r\n",
      "Каждый из них не обращает внимания на то, что другой пытается написать свою историю. Вместе они начинают писать текст на странице. В результате там будут записаны два рассказа, один поверх другого, что сделает страницу совершенно нечитаемой.\n",
      "\r\n",
      "Одно из решений подобной проблемы — это единый глобальный механизм интерпретатора, который блокирует разделяемые ресурсы, с которыми работает некий поток. В нашем примере — это «механизм», который «блокирует» страницу книги. Такой механизм исключает вышеописанную ситуацию, в которой два автора одновременно пишут текст на одной и той же странице.\n",
      "\r\n",
      "Механизм GIL в Python достигает этой цели, блокируя весь интерпретатор. В результате ничто не может помешать работе текущего потока. И когда CPython занимается работой с памятью, он использует GIL для того, чтобы эта работа делалась бы безопасно и качественно.\n",
      "\r\n",
      "У такого подхода есть сильные и слабые стороны, и GIL является предметом ожесточённых споров в сообществе Python. Для того чтобы больше узнать о GIL можете взглянуть на этот материал.\n",
      "\n",
      "Сборка мусора\r\n",
      "Давайте опять вернёмся к аналогии с книгой и представим себе, что некоторые из рассказов, записанные в этой книге, безнадёжно устарели. Никто их не читает, никто их нигде не упоминает. А если никто некий материал не читает и не ссылается на него в своих работах, то от этого материала можно избавиться, освободив место для новых текстов.\n",
      "\r\n",
      "Эти старые, всеми забытые рассказы, можно сравнить с объектами Python, счётчики ссылок которых равняются нулю. Это — те самые счётчики, о которых мы говорили, обсуждая структуру PyObject.\n",
      "\r\n",
      "Увеличение счётчика ссылок производится по нескольким причинам. Например, счётчик увеличивается в том случае, если объект, хранящийся в одной переменной, записали ещё в одну переменную:\n",
      "\n",
      "numbers = [1, 2, 3]\n",
      "# Счётчик ссылок = 1\n",
      "more_numbers = numbers\n",
      "# Счётчик ссылок = 2\r\n",
      "Он увеличивается и тогда, когда объект передают некоей функции в качестве аргумента:\n",
      "\n",
      "total = sum(numbers)\r\n",
      "А вот ещё один пример ситуации, в которой число в счётчике ссылок увеличивается. Это происходит в том случае, если объект включают в список:\n",
      "\n",
      "matrix = [numbers, numbers, numbers]\r\n",
      "Python позволяет программисту узнавать текущее значение счётчика ссылок некоего объекта с помощью модуля sys. Для этого используется такая конструкция:\n",
      "\n",
      "sys.getrefcount(numbers)\r\n",
      "Пользуясь ей, нужно помнить о том, что передача объекта методу getfefcount() увеличивает значение счётчика на 1.\n",
      "\r\n",
      "В любом случае, если объект всё ещё используется где-то в коде, его счётчик ссылок будет больше 0. Когда же значение счётчика упадёт до 0, в дело вступит специальная функция, которая «освобождает» память, занимаемую объектом. Эту память потом смогут использовать другие объекты.\n",
      "\r\n",
      "Зададимся теперь вопросами о том, что такое «освобождение памяти», и о том, как другие объекты могут этой памятью воспользоваться. Для того чтобы ответить на эти вопросы поговорим о механизмах управления памятью в CPython.\n",
      "\n",
      "Механизмы управления памятью в CPython\r\n",
      "Сейчас мы поговорим о том, как в CPython устроена архитектура памяти и как там выполняется управление памятью.\n",
      "\r\n",
      "Как уже было сказано, между CPython и физической памятью имеются несколько слоёв абстракции. Операционная система абстрагирует физическую память и создаёт слой виртуальной памяти, с которым могут работать приложения (это относится и к Python).\n",
      "\r\n",
      "Менеджер виртуальной памяти конкретной операционной системы выделяет фрагмент памяти для процесса Python. Тёмно-серые области на следующем изображении — это те фрагменты памяти, которые принадлежат процессу Python.\n",
      "\n",
      "\n",
      "Области памяти, используемые CPython\n",
      "\r\n",
      "Python задействует некую область памяти для внутреннего использования и для нужд, не связанных с выделением памяти для объектов. Ещё один фрагмент памяти используется для хранения объектов (это — значения типов int, dict, и другие подобные). Обратите внимание на то, что это — упрощённая схема. Если вы хотите увидеть полную картину — взгляните на исходный код CPython, где происходит всё то, о чём мы тут говорим.\n",
      "\r\n",
      "В CPython есть средство выделения памяти под объекты, которое ответственно за выделение памяти в области, предназначенной для хранения объектов. Самое интересное происходит именно при работе этого механизма. Он вызывается тогда, когда объект нуждается в памяти, или в случаях, когда память нужно освободить.\n",
      "\r\n",
      "Обычно добавление или удаление данных в объекты Python наподобие list и int не предусматривает одномоментную обработку очень больших объёмов информации. Поэтому архитектура средства выделения памяти построена с прицелом на обработку маленьких объёмов данных. Кроме того, это средство стремится к тому, чтобы не выделять память до тех пор, пока не станет ясно то, что это совершенно необходимо.\n",
      "\r\n",
      "Комментарии в исходном коде описывают средство выделения памяти как «быстрый, специализированный инструмент выделения памяти для небольших блоков, который предназначен для использования поверх универсального malloc». В данном случае malloc — это функция библиотеки C, предназначенная для выделения памяти.\n",
      "\r\n",
      "Давайте обсудим стратегию выделения памяти, используемую CPython. Сначала мы поговорим о трёх сущностях — так называемых блоках (block), пулах (pool) и аренах (arena), и о том, как они связаны друг с другом.\n",
      "\r\n",
      "Арены — это самые большие фрагменты памяти. Они выровнены по границам страниц памяти. Граница страницы — это место, где оканчивается непрерывный блок памяти фиксированной длины, используемый операционной системой. Python, в ходе работы с памятью, исходит из предположения о том, что размер системной страницы памяти равняется 256 Кб.\n",
      "\n",
      "\n",
      "Арены, пулы и блоки\n",
      "\r\n",
      "На аренах расположены пулы, представляющие собой виртуальные страницы памяти размером 4 Кб. Они напоминают страницы книги из нашего примера. Пулы разделены на небольшие блоки памяти.\n",
      "\r\n",
      "Все блоки в одном пуле принадлежат к одному и тому же классу размера (size class). Класс размера, к которому принадлежит блок, определяет размер этого блока, который выбирается с учётом запрошенного объёма памяти. Вот таблица, взятая из исходного кода, в которой продемонстрированы объёмы данных, запросы на сохранение которых в памяти обрабатывает система, размеры выделяемых блоков и идентификаторы классов размеров.\n",
      "\n",
      "\n",
      "Объём данных в байтах\n",
      "\n",
      "Объём выделенного блока\n",
      "\n",
      "idx класса размера\n",
      "\n",
      "\n",
      "\n",
      "1-8\n",
      "\n",
      "8\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "9-16\n",
      "\n",
      "16\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "17-24\n",
      "\n",
      "24\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "25-32\n",
      "\n",
      "32\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "33-40\n",
      "\n",
      "40\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "41-48\n",
      "\n",
      "48\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "49-56\n",
      "\n",
      "56\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "57-64\n",
      "\n",
      "64\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "65-72\n",
      "\n",
      "72\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "…\n",
      "\n",
      "…\n",
      "\n",
      "…\n",
      "\n",
      "\n",
      "\n",
      "497-504\n",
      "\n",
      "504\n",
      "\n",
      "62\n",
      "\n",
      "\n",
      "\n",
      "505-512\n",
      "\n",
      "512\n",
      "\n",
      "63\n",
      "\n",
      "\n",
      "\r\n",
      "Например, если запрошено сохранение 42 байтов, то данные будут помещены в 48-байтовый блок.\n",
      "\n",
      "Пулы\r\n",
      "Пулы состоят из блоков, принадлежащих к одному классу размера. Каждый пул связан с другими пулами, содержащими блоки одинакового с ним класса размера, с использованием механизма двусвязного списка. При таком подходе алгоритм выделения памяти может легко найти свободное место для блока заданного размера, даже если речь идёт о поиске свободного места в разных пулах.\n",
      "\r\n",
      "Список usedpools позволяет отслеживать все пулы, в которых есть место для данных, принадлежащих к определённому классу размера. Когда запрашивается сохранение блока некоего размера, алгоритм проверяет этот список на предмет нахождения списка пулов, хранящих блоки нужного размера.\n",
      "\r\n",
      "Сами пулы должны пребывать в одном из трёх состояний. А именно, они могут использоваться (состояние used), они могут быть заполненными (full) или пустыми (empty). В используемом пуле есть свободные блоки, в которых можно сохранить данные подходящего размера. Все блоки заполненного пула выделены под данные. Пустой пул не содержит данных, и он, при необходимости, может быть назначен для хранения блоков, принадлежащих к любому классу размера.\n",
      "\r\n",
      "Список freepools хранит сведения обо всех пулах, находящихся в состоянии empty. Например, если в списке usedpools нет записей о пулах, хранящих блоки размером 8 байт (класс с idx 0), тогда инициализируется новый пул, пребывающий в состоянии empty, предназначенный для хранения таких блоков. Этот новый пул добавляется в список usedpools, его можно будет использовать для выполнения запросов на сохранение данных, поступающих после его создания.\n",
      "\r\n",
      "Предположим, что в пуле, находящемся в состоянии full, освобождаются некоторые блоки. Происходит это из-за того, что данные, хранящиеся в них, больше не нужны. Этот пул опять попадёт в список usedpools и его можно будет использовать для данных соответствующего класса размера.\n",
      "\r\n",
      "Знание этого алгоритма позволяет понять то, как меняется состояние пулов во время работы (и то, как меняются классы размеров, блоки, принадлежащие к которым, можно в них хранить).\n",
      "\n",
      "Блоки\n",
      "\n",
      "Используемые, полные и пустые пулы\n",
      "\r\n",
      "Как можно понять из предыдущей иллюстрации, пулы содержат указатели на «свободные» блоки памяти, которые в них содержатся. В том, что касается работы с блоками, нужно отметить одну небольшую особенность, на которую есть указание в исходном коде. Используемая в CPython система управления памятью, на всех уровнях (арены, пулы, блоки) стремиться выделять память только тогда, когда это абсолютно необходимо.\n",
      "\r\n",
      "Это означает, что пулы могут содержать блоки, которые находятся в одном из трёх состояний:\n",
      "\n",
      "\n",
      "untouched — часть памяти, которая ещё не была выделена.\n",
      "free — часть памяти, которая уже выделялась, но позже была сделана «свободной» средствами CPython и больше не содержит никаких ценных данных.\n",
      "allocated — часть памяти, которая содержит ценные данные.\n",
      "\r\n",
      "Указатель freeblock указывает на односвязный список свободных блоков памяти. Другими словами — это список мест, куда можно поместить данные. Если для размещения данных нужно больше одного свободного блока, то средство выделения памяти возьмёт из пула несколько блоков, находящихся в состоянии untouched.\n",
      "\r\n",
      "По мере того, как средство управления памятью делает блоки «свободными», они, приобретая состояние free, попадают в начало списка freeblock. Блоки, содержащиеся в этом списке, необязательно представляют собой непрерывную область памяти, похожую на ту, что изображена на предыдущем рисунке. Они, на самом деле, могут выглядеть так, как показано ниже.\n",
      "\n",
      "\n",
      "Односвязный список freeblock\n",
      "\n",
      "Арены\r\n",
      "Арены содержат пулы. Эти пулы, как уже было сказано, могут пребывать в состояниях used, full или empty. Надо отметить, что у арен нет состояний, подобных тем, которые есть у пулов.\n",
      "\r\n",
      "Арены организованы в двусвязный список, который называется usable_arenas. Этот список отсортирован по числу доступных свободных пулов. Чем меньше на арене свободных пулов — тем ближе арена к началу списка.\n",
      "\n",
      "\n",
      "Список usable_arenas\n",
      "\r\n",
      "Это значит, что арена, сильнее других заполненная данными, будет выбираться для размещения в ней новых данных. А почему не наоборот? Почему бы не размещать новые данные на той арене, на которой больше всего свободного места?\n",
      "\r\n",
      "На самом деле, эта особенность приводит нас к идее настоящего освобождения памяти. Вы могли заметить, что нередко мы пользовались здесь понятием «освобождение памяти», заключая его в кавычки. Причина, по которой это делалось, заключается в том, что хотя блок и может считаться «свободным», участок памяти, которую он представляет, на самом деле, не возвращён операционной системе. Процесс Python удерживает этот участок памяти и позже использует его для хранения новых данных. Настоящее освобождение памяти — это возврат её операционной системе, которая сможет ей воспользоваться.\n",
      "\r\n",
      "Арены — это единственная сущность в рассмотренной здесь схеме, память, представленная которой, может быть освобождена по-настоящему. Здравый смысл подсказывает, что вышеописанная схема работы с аренами направлена на то, чтобы позволить тем аренам, которые почти пусты, опустеть полностью. При таком подходе тот фрагмент памяти, который представлен полностью опустевшей ареной, может быть по-настоящему освобождён, что снизит объём памяти, потребляемой Python.\n",
      "\n",
      "Итоги\r\n",
      "Вот о чём вы узнали, прочтя этот материал:\n",
      "\n",
      "\n",
      "Что такое управление памятью и почему это важно.\n",
      "Как устроена эталонная реализация Python, Cpython, написанная на языке программирования C.\n",
      "Какие структуры данных и алгоритмы используются в CPython для управления памятью.\n",
      "\r\n",
      "Управление памятью — это неотъемлемая часть работы компьютерных программ. Python решает практически все задачи по управлению памятью незаметно для программиста. Python позволяет тому, кто пишет на этом языке, абстрагироваться от множества мелких деталей, касающихся работы с компьютерами. Это даёт программисту возможность работать на более высоком уровне, создавать свой код, не заботясь о том, где хранятся его данные.\n",
      "\n",
      "Уважаемые читатели! Если у вас есть опыт Python-разработки — просим вас рассказать о том, как вы подходите к использованию памяти в своих программах. Например, стремитесь ли вы её экономить?\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем привет! Вот и закончились длинные мартовские выходные. Первую послепраздничную публикацию мы хотим посвятить полюбившемуся многим курсу — «Разработчик Python», который стартует менее, чем через 2 недели. Поехали.\n",
      "\n",
      "Содержание\n",
      "\n",
      "\n",
      "Память – пустая книга\n",
      "Управление памятью: от оборудования к программному обеспечению\n",
      "Базовая реализация Python\n",
      "Концепция глобальной блокировки интерпретатора (Global Interpreter Lock, GIL)\n",
      "Сборщик мусора\n",
      "Управление памятью в CPython:\n",
      "\n",
      "Пулы\n",
      "Блоки\n",
      "Арены\n",
      "\n",
      "Заключение\n",
      "\n",
      "\n",
      "\r\n",
      "Вы когда-нибудь задумывались как Python за кулисами обрабатывает ваши данные? Как ваши переменные хранятся в памяти? В какой момент они удаляются?\r\n",
      "В этой статье мы углубимся во внутреннее устройство Python, чтобы понять, как происходит управление памятью.\n",
      "\r\n",
      "Прочитав эту статью, вы:\n",
      "\n",
      "\n",
      "Узнаете больше о низкоуровневых операциях, особенно касательно памяти.\n",
      "Поймете, как Python абстрагирует низкоуровневые операции.\n",
      "Узнаете об алгоритмах управления памятью в Python.\n",
      "\r\n",
      "Знание внутреннего устройства Python даст лучшее понимание о принципах его поведении. Надеюсь, вы сможете взглянуть на Python с новой стороны. За кулисами происходит великое множество логических операций, чтобы ваша программа работала надлежащим образом.\n",
      "\n",
      "Память – пустая книга\n",
      "\r\n",
      "Можно представить память компьютера как пустую книгу, ожидающую, когда в нее запишут множество коротких историй. На ее страницах еще ничего нет, но вскоре появятся авторы, которые захотят записать в нее свои истории. Для этого им понадобится место.\r\n",
      "Поскольку им нельзя записывать одну историю поверх другой, им нужно очень аккуратно относиться к тем страницам, на которых они пишут. Перед тем, как начать писать, они советуются с менеджером книги. Менеджер решает, где в книге авторы могут записать свою историю.\n",
      "\r\n",
      "Поскольку книге уже много лет, множество историй в ней устаревает. Когда никто не читает или не обращается к истории, ее удаляют, чтобы она освободила место для новых историй.\r\n",
      "По своей сути, компьютерная память подобна пустой книге. Непрерывные блоки памяти фиксированной длины принято называть страницами, поэтому эта аналогия приходится как нельзя кстати.\n",
      "\r\n",
      "Авторами могут быть различные приложения или процессы, которым необходимо хранить данные в памяти. Менеджер, который решает, где авторы могут написать свои истории, играет роль менеджера памяти – сортировщика. А тот, кто стирает старые истории – сборщик мусора.\n",
      "\n",
      "Управление памятью: от оборудования к программному обеспечению\n",
      "\r\n",
      "Управление памятью – это процесс, в котором программные приложения считывают и записывают данные. Менеджер памяти определяет куда поместить данные программы. Поскольку количество памяти конечно, как и количество страниц в книге, соответственно, менеджеру нужно находить свободное место, чтобы предоставить его в пользование приложению. Этот процесс называется «выделение памяти» (memory allocation). \n",
      "\r\n",
      "С другой стороны, когда данные больше не нужны, они могут быть удалены. В таком случае говорят об освобождении памяти. Но от чего ее освобождают и откуда она берется?\r\n",
      "Где-то внутри компьютера есть физическое устройство, которое хранит данные, когда вы запускаете программы на Python. Код на Python проходит через множество уровней абстракции, прежде чем добраться до этого устройства. \n",
      "\r\n",
      "Один из основных уровней, лежащих над оборудованием (RAM, жесткий диск и т.п.) – это операционная система. Она управляет запросами на чтение и запись в память.\r\n",
      "Над операционной системой есть прикладной уровень, на котором есть одна из реализаций Python (зашитая в вашу ОС или загруженная с сайта python.org). Управление памятью для кода на этом языке программирования регулируется специальными средствами Python. Алгоритмы и структуры, которые Python использует для управления памятью – это основная тема данной статьи.\n",
      "\n",
      "Базовая реализация Python\n",
      "\r\n",
      "Базовая реализация Python, или же «чистый Python» — это CPython, написанный на языке С. \r\n",
      "Я очень удивился, когда впервые об этом услышал. Как один язык может быть написан на другом языке?! Ну, не в буквальном смысле, конечно, но идея примерно такая.\n",
      "\r\n",
      "Язык Python описан в специальном справочном руководстве на английском языке. Однако само по себе это руководство не сильно полезно. Вам все еще нужно средство для интерпретации кода, написанного по правилам справочника.\n",
      "\r\n",
      "А еще вам понадобится что-нибудь для выполнения кода на вашем компьютере. Базовая реализация Python обеспечивает выполнения обоих условий. Она конвертирует код на Python в инструкции, которые исполняются на виртуальной машине.\n",
      "\n",
      "Заметка: Виртуальные машины похожи на физические компьютеры, но они встроены в программное обеспечение. Они обрабатывают базовые инструкции, сходные с ассемблерным кодом.\n",
      "\r\n",
      "Python интерпретируемый язык программирования. Ваш код на Python компилируется с помощью инструкций более понятных компьютеру – байткода. Эти инструкции интерпретируются виртуальной машиной, когда вы запускаете код.\n",
      "\r\n",
      "Вы когда-нибудь видели файлы с расширением .pyc или папку __pycache__? Это тот самый байткод, который интерпретируется виртуальной машиной.\r\n",
      "Важно понимать, что есть другие реализации помимо CPython, например IronPython, который компилируется и запускается в Microsoft Common Language Runtime (CLR). Jython компилируется в байткод Java, чтобы запускаться на виртуальной машине Java. А еще есть PyPy о котором можно написать отдельную статью, поэтому я упомяну о нем лишь вскользь. \n",
      "\r\n",
      "В этой статье мы сфокусируемся на управление памятью с помощью средств CPython.\r\n",
      "Внимание: Версии Python обновляются и в будущем может всякое случиться. На время написания статьи последней версией был Python 3.7.\n",
      "\r\n",
      "Хорошо, мы имеем CPython, написанный на С, который интерпретирует байткод Python. Каким образом это соотносится с управлением памятью? Начнем с того, что алгоритмы и структуры для управления памятью существуют в коде CPython, на С. Чтобы понять эти принципы в Python, необходимо базовое понимание CPython.\n",
      "\r\n",
      "CPython написан на языке С, который в свою очередь не поддерживает объектно-ориентированное программирование. Из-за этого код на CPython имеет довольно интересную структуру. \n",
      "\r\n",
      "Должно быть, вы слышали, что все в Python – это объект, даже типы, такие как int и str, например. Это справедливо на уровне реализации CPython. Существует такая структура, которая называется PyObject, которую использует каждый объект в CPython. \n",
      "\r\n",
      "Заметка: Структурой в С называется пользовательский тип данных, который в себе группирует различные типы данных. Можно провести аналогию с объектно-ориентированными языками и сказать, что структура – это класс с атрибутами, но без методов.\n",
      "\r\n",
      "PyObject – это прародитель всех объектов в Python, содержащий всего две вещи:\n",
      "\n",
      "\n",
      "ob_refcnt: счетчик ссылок;\n",
      "ob_type: указатель на другой тип.\n",
      "\r\n",
      "Счетчик ссылок необходим для сбора мусора. Еще мы имеем указатель на конкретный тип объекта. Тип объекта – это всего лишь другая структура, которая описывает объекты в Python (такие как dict или int). \n",
      "\r\n",
      "У каждого объекта есть объектно-ориентированный распределитель памяти, который знает как выделить память и хранить объект. У каждого объекта есть также объектно-ориентированный освободитель ресурса, который очищает память, если ее содержимое больше не нужно.\n",
      "\r\n",
      "Есть один важный фактор в разговоре о распределении памяти и ее очищении. Память – общий ресурс компьютера, и может случиться довольно неприятная вещь, если два процесса попробуют записать данные в одну и ту же ячейку памяти одновременно.\n",
      "\n",
      "Глобальная блокировка интерпретатора (GIL)\n",
      "\r\n",
      "GIL – это решение общей проблемы разделения памяти между такими объединенными ресурсами, как память компьютера. Когда два потока пытаются изменять один и тот же ресурс одновременно, они наступают друг другу на пятки. В результате в памяти образуется полнейший бардак и ни один процесс не закончит свою работу с желаемым результатом. \n",
      "\r\n",
      "Возвращаясь к аналогии с книгой, предположим, что из двух авторов каждый решает, что именно он должен написать свою историю на текущей странице в данный конкретный момент. Каждый из них игнорирует попытки другого написать историю и начинает упрямо писать на странице. По итогу мы имеем две истории, одна поверх другой, и абсолютно нечитаемую страницу.\n",
      "\r\n",
      "Одним из решений этой проблемы как раз-таки и является GIL, который блокирует интерпретатор на то время, пока поток взаимодействует с выделенным ресурсом, таким образом позволяя одному и только одному потоку писать в выделенную область памяти. Когда CPython распределяет память, он использует GIL чтобы убедиться в том, что делает это правильно.\r\n",
      "У этого подхода есть как множество плюсов, так и множество минусов, поэтому GIL вызывает распри в Python сообществе. Чтобы узнать больше о GIL, я советую прочитать следующую статью.\n",
      "\n",
      "Сборщик мусора \n",
      "\r\n",
      "Вернемся к нашей аналогии с книгой и представим, что некоторые истории в ней безнадежно устарели. Никто их не читает и не обращается к ним. В таком случае, естественным выходом было бы избавиться от них за ненадобностью, тем самым освободив место для новых историй.\r\n",
      "Такие старые неиспользуемые истории можно сравнить с объектами в Python, чей счетчик ссылок упал до 0. Помним о том, что каждый объект в Python имеет счетчик ссылок и указатель на тип.\n",
      "\r\n",
      "Счетчик ссылок может увеличиться по нескольким причинам. Например, он увеличится, если вы одну переменную присвоите другой переменной.\n",
      "\n",
      "\n",
      "\r\n",
      "Он также увеличится, если вы будете передавать объект, как аргумент.\n",
      "\n",
      "\n",
      "\r\n",
      "В последнем примере счетчик ссылок увеличится, если вы включите объект в список.\n",
      "\n",
      "\n",
      "\r\n",
      "Python позволяет узнать текущее значение счетчика ссылок с помощью модуля sys. Вы можете использовать sys.getrefcount(numbers), но помните, что вызов getrefcount() увеличит счётчик ссылок еще на единицу. \n",
      "\r\n",
      "В любом случае, если объект в вашем коде все еще понадобится, его значение его счетчика ссылок будет больше 0. А когда он упадет до нуля, будет инициирована специальная функция очистки памяти, которая освободит ее и сделает доступной для других объектов.\n",
      "\r\n",
      "Но что значит «освободить память» и как другие объекты ее используют? Давайте погрузимся непосредственно в управление памятью в CPython.\n",
      "\n",
      "Управление памятью в CPython\n",
      "\r\n",
      "В этой части мы погрузимся в архитектуру памяти CPython и алгоритмы, по которым она функционирует.\n",
      "\r\n",
      "Как было сказано раньше, существует такое понятие, как уровни абстракции, находящиеся между физическим оборудованием и CPython. Операционная система (ОС) абстрагирует физическую память и создает уровень виртуальной памяти, к которой могут обращаться приложения, включая Python. \n",
      "\r\n",
      "ОС-ориентированный менеджер виртуальной памяти выделяет определенную область памяти под процессы Python. На картинке темно-серые области – это пространство, которое занимает процесс Python.\n",
      "\n",
      "\n",
      "\r\n",
      "Python использует часть памяти для внутреннего использования и необъектной памяти (non-object memory). Другая часть делится на хранилище объектов (ваши int, dict и т.п.) Сейчас я изъясняюсь очень простым языком, однако вы можете заглянуть прямо «под капот», то есть в исходный код CPython и посмотреть как это все происходит с практической точки зрения.\n",
      "\r\n",
      "В CPython существует распределитель объектов, ответственный за распределение памяти внутри области объектной памяти. Именно в этом распределителе объектов и вершится вся магия. Он вызывается каждый раз, когда каждому новому объекту необходимо занять или освободить память. \n",
      "\r\n",
      "Обычно, добавление и удаление данных в Python, таких как int или list, например, не использует много данных в один момент времени. Именно поэтому архитектура распределителя ориентируется на работу с небольшими объемами данных в одну единицу времени. Также он не выделяет память заранее, то есть до того момента пока она не станет абсолютно необходимой.\n",
      "\r\n",
      "Комментарии в исходном коде определяют распределитель (allocator) как «быстрый распределитель памяти специального назначения, который работает подобно универсальной функции malloc». Соответственно, в языке С malloc используется для выделения памяти.\n",
      "\r\n",
      "Теперь давайте взглянем на стратегию выделения памяти в CPython. Для начала поговорим о трех основных частях и о том, как они друг с другом соотносятся. \n",
      "\r\n",
      "Арены (arena) – самые большие области памяти, которые занимают место до границ страниц в памяти. Граница страницы (разворота) – это крайняя точка непрерывного блока памяти фиксированной длины, используемого ОС. Python устанавливает границу страницы системы в 256 Кб. \n",
      "\n",
      "\n",
      "\r\n",
      "Внутри арен находятся пулы (pool), которые считаются одной виртуальной страницей памяти (4 Кб). Они похожи на страницы в нашей аналогии. Пулы разделены на еще более маленькие фрагменты памяти – блоки (block).\n",
      "\r\n",
      "Все блоки в пуле находят в одном «классе размера». Класс размера (size class) определяет размер блока, имея определенное количество запрашиваемых данных. Градация в таблице снизу взята прямо из комментариев в исходном коде:\n",
      "\n",
      "\n",
      "\r\n",
      "Например, если необходимы 42 байта, то данные будут помещены в блок размером 48 байт.\n",
      "\n",
      "Пулы\n",
      "\r\n",
      "Пулы состоят из блоков одного класса размера. Каждый пул работает по принципу двухсвязного списка с другими пулами того же класса размера. Поэтому алгоритм может с легкостью найти необходимое место для требуемого размера блока, даже среди множества пулов.\n",
      "\r\n",
      "Список используемых пулов (usedpools list) отслеживает все пулы, которые имею какое-то свободное место, доступное для данных каждого класса размера. Когда запрашивается требуемый размер блока, алгоритм проверяет список использованных пулов, чтобы найти подходящий для него пул. \n",
      "\r\n",
      "Пулы находятся в трех состояниях: используемый, полный, пустой. Использованный пул содержит блоки, в которые можно записать какую-то информацию. Блоки полного пула все распределены и уже содержат данные. Пустые пулы не содержат данных и могут быть разбиты на какие угодны классы размера при надобности.\n",
      "\r\n",
      "Список пустых пулов (freepools list) содержит, соответственно, все пулы в пустом состоянии. Но в какой момент они используются?\n",
      "\r\n",
      "Допустим, вашему коду необходима область памяти в 8 байт. Если в списке используемых пулов нет пулов с классом размера в 8 байт, то новый пустой пул инициализируется, как хранящий блоки по 8 байт. Затем пустой пул добавляется в список используемых пулов и может быть использован при следующих запросах.\n",
      "\r\n",
      "Заполненный пул освобождает некоторые блоки, когда эта информация в них уже не нужна. Этот пул добавится в список используемых в соответствии со своим классом размера. Вы можете наблюдать, как пулы меняют свои состояния и даже классы размера в соответствии с алгоритмом.\n",
      "\n",
      "Блоки\n",
      "\n",
      "\n",
      "\r\n",
      " Как видно из рисунка, пулы содержат указатели на свободные блоки памяти. В их работе присутствует небольшой нюанс. Согласно комментариям в исходном коде, распределитель «стремиться никогда не трогать какую-либо область памяти на любом из уровней (арена, пул, блок), пока она не понадобится».\n",
      "\r\n",
      "Это значит, что блок может иметь три состояния. Они могут быть определены следующим образом:\n",
      "\n",
      "\n",
      "Нетронутые: области памяти, которые не были распределены;\n",
      "Свободные: области памяти, которые были распределены, но позже освобождены CPython, поскольку не содержали в себе актуальной информации;\n",
      "Распределенные: области памяти, которые на данный момент времени содержат актуальную информацию.\n",
      "\r\n",
      "Указатель свободных блоков (freeblock pointer) представляет из себя односвязный список свободных блоков памяти. Другими словами, это список свободных мест, куда можно записать информацию. Если необходимо больше памяти, чем есть в свободных блоках, то распределитель задействует нетронутые блоки в пуле.\n",
      "\r\n",
      "Как только менеджер памяти освобождает блоки, эти блоки добавляются в начало списка свободных блоков. Фактический список может не содержать непрерывную последовательность блоков памяти, как на первом «удачном» рисунке. \n",
      "\n",
      "\n",
      "\n",
      "Арены\n",
      "\r\n",
      "Арены содержат в себе пулы. Арены в отличие от пулов не имеют явных разделений на состояния. \n",
      "\r\n",
      "Они сами по себе организованы в двухсвязный список, который называется список используемых арен (usable_arenas). Этот список отсортирован по количеству свободных пулов. Чем меньше свободных пулов, тем ближе арена к началу списка.\n",
      "\n",
      "\n",
      "\r\n",
      "Это означает, что наиболее полная арена будет выбрана для записи еще большего количества данных. Но почему именно так? Почему бы не записывать данные туда, где больше всего свободного места?\n",
      "\r\n",
      "Это подводит нас к идее полного освобождения памяти. Дело в том, что в некоторых случаях, когда память освобождается, она все еще остается недоступной операционной системе. Процесс Python держит ее распределенной и использует ее позже для новых данных. Полное освобождение памяти возвращает память в пользование операционной системе.\n",
      "\r\n",
      "Арены не единственные области, которые могут быть полностью освобождены. Таким образом мы понимаем, что те арены, которые находятся в списке «ближе к пустому состоянию», должны быть освобождены. В таком случае, область памяти может быть действительно полностью освобождена, и соответственно общий объем памяти вашей программы на Python уменьшен.\n",
      "\n",
      "Заключение\n",
      "\r\n",
      "Управление памятью – это одна из самых важный частей в работе с компьютером. Python так или иначе производит практически все действия в скрытом режиме.\n",
      "\r\n",
      "Из данной статьи вы узнали:\n",
      "\n",
      "\n",
      "Что такое управление памятью и почему оно важно;\n",
      "Что представляет из себя CPython, базовая реализация Python;\n",
      "Как структуры данных и алгоритмы работают в управлении памятью CPython и хранят ваши данные.\n",
      "\r\n",
      "Python абстрагирует множество мелких нюансов работы с компьютером. Это дает возможность работать на более высоком уровне и избавиться от головной боли на тему того, где и как хранятся байты вашей программы.\n",
      "\r\n",
      "Вот мы и узнали об управлении памяти в Python. Традиционно ждём ваши комментарии, а также приглашаем на день открытых дверей по курсу «Разработчик Python», который пройдет уже 13 марта    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет, Хабр! Представляю вашему вниманию перевод статьи “Object-Oriented Programming in Python vs Java” автора Джона Финчера.\n",
      "Реализация объектно-ориентированного программирования (ООП) в языках Java и Python отличается. Принцип работы с объектами, типами переменных и прочими языковыми возможностями может вызвать затруднение при переходе с одного языка на другой. В данной статье, которая может быть полезной как для Java-программистов, желающих освоить Python, так и для Python-программистов, имеющих цель лучше узнать Java, приводятся основные сходства и отличия этих языков, применительно к ООП.\n",
      "Подробнее – под катом.\n",
      "Примеры классов в Python и Java\n",
      "Для начала давайте реализуем простейший класс в Python и Java, чтобы проиллюстрировать некоторые отличия в этих языках, и будем постепенно вносить в этот класс изменения.\r\n",
      "Представим, что у нас есть следующее определение класса Car в Java:\n",
      " 1 public class Car {\n",
      " 2     private String color;\n",
      " 3     private String model;\n",
      " 4     private int year;\n",
      " 5 \n",
      " 6     public Car(String color, String model, int year) {\n",
      " 7         this.color = color;\n",
      " 8         this.model = model;\n",
      " 9         this.year = year;\n",
      "10     }\n",
      "11 \n",
      "12     public String getColor() {\n",
      "13         return color;\n",
      "14     }\n",
      "15 \n",
      "16     public String getModel() {\n",
      "17         return model;\n",
      "18     }\n",
      "19 \n",
      "20     public int getYear() {\n",
      "21         return year;\n",
      "22     }\n",
      "23 }\n",
      "Имя исходного Java-файла должно соответствовать имени хранящегося в нем класса, поэтому мы обязаны назвать файл Car.java. Каждый Java-файл может содержать только один публичный класс.\r\n",
      "Такой же класс в Python будет выглядеть так:\n",
      " 1 class Car:\n",
      " 2     def __init__(self, color, model, year):\n",
      " 3         self.color = color\n",
      " 4         self.model = model\n",
      " 5         self.year = year\n",
      "В Python вы можете объявить класс где угодно и когда угодно. Сохраним этот файл как car.py.\r\n",
      "Используя эти классы как основу, продолжим исследование основных компонентов классов и объектов.\n",
      "Атрибуты объекта\n",
      "Во всех объектно-ориентированных языках данные об объекте где-то хранятся. И в Python, и в Java эти данные хранятся в атрибутах, которые являются переменными, связанными с конкретными объектами.\n",
      "Одним из наиболее значительных отличий между Python и Java является то, как они определяют атрибуты класса и объекта и как эти языки управляют ими. Некоторые из этих различий вызваны ограничениями, налагаемыми языками, в то время как другие связаны с более эффективной практикой.\n",
      "Объявление и инициализация\n",
      "В Java мы объявляем атрибуты (с указанием их типа) внутри класса, но за пределами всех методов. Перед тем, как использовать атрибуты класса, мы должны их определить:\n",
      " 1 public class Car {\n",
      " 2     private String color;\n",
      " 3     private String model;\n",
      " 4     private int year;\n",
      "В Python же мы объявляем и определяем атрибуты внутри метода класса init(), который является аналогом конструктора в Java:\n",
      " 1 def __init__(self, color, model, year):\n",
      " 2     self.color = color\n",
      " 3     self.model = model\n",
      " 4     self.year = year\n",
      "Указывая перед именем переменных ключевое слово self, мы говорим Python-у, что это атрибуты. Каждый экземпляр класса получает свою копию. Все переменные в Python не типизированы (loosely typed), и атрибуты не являются исключением.\n",
      "Переменные можно создать и за пределами метода init(), но это не будет лучшим решением и может привести к труднообнаруживаемым багам. Например, можно добавить объекту Car новый атрибут wheels следующим образом:\n",
      " 1 >>> import car\n",
      " 2 >>> my_car = car.Car(\"yellow\", \"beetle\", 1967)\n",
      " 3 >>> print(f\"My car is {my_car.color}\")\n",
      " 4 My car is yellow\n",
      " 5 \n",
      " 6 >>> my_car.wheels = 5\n",
      " 7 >>> print(f\"Wheels: {my_car.wheels}\")\n",
      " 8 Wheels: 5\n",
      "Однако, если мы забудем указать в 6-й строке выражение my_car.wheels = 5, то получим ошибку:\n",
      " 1 >>> import car\n",
      " 2 >>> my_car = car.Car(\"yellow\", \"beetle\", 1967)\n",
      " 3 >>> print(f\"My car is {my_car.color}\")\n",
      " 4 My car is yellow\n",
      " 5 \n",
      " 6 >>> print(f\"Wheels: {my_car.wheels}\")\n",
      " 7 Traceback (most recent call last):\n",
      " 8   File \"<stdin>\", line 1, in <module>\n",
      " 9 AttributeError: 'Car' object has no attribute 'wheels'\n",
      "В Python если объявить переменную за пределами метода, то она будет рассматриваться как переменная класса. Давайте изменим класс Car:\n",
      " 1 class Car:\n",
      " 2 \n",
      " 3     wheels = 0\n",
      " 4 \n",
      " 5     def __init__(self, color, model, year):\n",
      " 6         self.color = color\n",
      " 7         self.model = model\n",
      " 8         self.year = year\n",
      "Теперь изменится использование переменной wheels. Вместо обращения к ней через объект, мы обращаемся к ней, используя имя класса:\n",
      " 1 >>> import car\n",
      " 2 >>> my_car = car.Car(\"yellow\", \"beetle\", 1967)\n",
      " 3 >>> print(f\"My car is {my_car.color}\")\n",
      " 4 My car is yellow\n",
      " 5 \n",
      " 6 >>> print(f\"It has {car.Car.wheels} wheels\")\n",
      " 7 It has 0 wheels\n",
      " 8 \n",
      " 9 >>> print(f\"It has {my_car.wheels} wheels\")\n",
      "10 It has 0 wheels\n",
      "Примечание: в Python обращение к переменной класса происходит по следующему синтаксису:\n",
      "\n",
      "Имя файла, содержащего класс (без расширения .py)\n",
      "Точка\n",
      "Имя класса\n",
      "Точка\n",
      "Имя переменной\n",
      "\n",
      "Поскольку мы сохранили класс Car в файле car.py, мы обращаемся к переменной класса wheels в 6-й строчке таким образом: car.Car.wheels.\n",
      "Работая с переменной wheels, необходимо быть обратить внимание на то, что изменение значения переменной экземпляра класса my_car.wheels не ведет к изменению переменной класса car.Car.wheels:\n",
      " 1 >>> from car import *\n",
      " 2 >>> my_car = car.Car(\"yellow\", \"Beetle\", \"1966\")\n",
      " 3 >>> my_other_car = car.Car(\"red\", \"corvette\", \"1999\")\n",
      " 4 \n",
      " 5 >>> print(f\"My car is {my_car.color}\")\n",
      " 6 My car is yellow\n",
      " 7 >>> print(f\"It has {my_car.wheels} wheels\")\n",
      " 8 It has 0 wheels\n",
      " 9 \n",
      "10 >>> print(f\"My other car is {my_other_car.color}\")\n",
      "11 My other car is red\n",
      "12 >>> print(f\"It has {my_other_car.wheels} wheels\")\n",
      "13 It has 0 wheels\n",
      "14 \n",
      "15 >>> # Change the class variable value\n",
      "16 ... car.Car.wheels = 4\n",
      "17 \n",
      "18 >>> print(f\"My car has {my_car.wheels} wheels\")\n",
      "19 My car has 4 wheels\n",
      "20 >>> print(f\"My other car has {my_other_car.wheels} wheels\")\n",
      "21 My other car has 4 wheels\n",
      "22 \n",
      "23 >>> # Change the instance variable value for my_car\n",
      "24 ... my_car.wheels = 5\n",
      "25 \n",
      "26 >>> print(f\"My car has {my_car.wheels} wheels\")\n",
      "27 My car has 5 wheels\n",
      "28 >>> print(f\"My other car has {my_other_car.wheels} wheels\")\n",
      "29 My other car has 4 wheels\n",
      "На 2-й и 3-й строчках мы определили два объекта Car: my_car и my_other_car.\r\n",
      "Сначала свойство wheels у обоих объектов равно нулю. На 16-й строке мы установили переменную класса: car.Car.wheels = 4, у обоих объектов теперь по 4 колеса. Однако, затем когда на 24-й строке мы меняем свойство объекта my_car.wheels = 5, свойство второго объекта остается нетронутым.\n",
      "Это означает, что теперь у нас две различные копии атрибута wheels:\n",
      "\n",
      "Переменная класса, которая применяется ко всем объектам Car\n",
      "Конкретная переменная экземпляра класса, которая применяется только к объекту my_car.\r\n",
      "Из-за этого можно случайно сослаться не на тот экземпляр и сделать малозаметную ошибку.\n",
      "\n",
      "В Java эквивалентом атрибута класса является статичный (static) атрибут:\n",
      "public class Car {\n",
      "    private String color;\n",
      "    private String model;\n",
      "    private int year;\n",
      "    private static int wheels;\n",
      "\n",
      "    public Car(String color, String model, int year) {\n",
      "        this.color = color;\n",
      "        this.model = model;\n",
      "        this.year = year;\n",
      "    }\n",
      "\n",
      "    public static int getWheels() {\n",
      "        return wheels;\n",
      "    }\n",
      "\n",
      "    public static void setWheels(int count) {\n",
      "        wheels = count;\n",
      "    }\n",
      "}\n",
      "Обычно мы обращаемся к статичным переменным в Java через имя класса. Можно обратиться к ним и через экземпляр класса, как в Python, но это не будет лучшим решением.\n",
      "Наш Java-класс начинает удлиняться. Одной из причин, почему Java «многословнее» Python-а, является понятие публичных (public) и приватных (private) методов и атрибутов.\n",
      "Публичные и приватные\n",
      "Java управляет доступом к методам и атрибутам, различая публичные и приватные данные.\r\n",
      "В Java ожидается, что атрибуты будут объявлены как приватные (или защищенные — protected, если нужно обеспечить к ним доступ потомкам класса). Таким образом мы ограничиваем доступ к ним извне. Чтобы предоставить доступ к приватным атрибутам, мы объявляем публичные методы, которые устанавливают или получают эти данные (подробнее об этом – чуть позже).\n",
      "Вспомним, что в нашем Java-коде переменная color была объявлена приватной. Следовательно, нижеприведенный код не скомпилируется:\n",
      "Car myCar = new Car(\"blue\", \"Ford\", 1972);\n",
      "\n",
      "// Paint the car\n",
      "myCar.color = \"red\";\n",
      "Если не указать уровень доступа к атрибутам, то по умолчанию он будет установлен как package protected, что ограничивает доступ к классам в пределах пакета. Если же мы хотим, что вышеуказанный код заработал, то придется сделать атрибут публичным.\n",
      "Однако, в Java не приветствуется объявление атрибутов публичными. Рекомендуется объявлять их приватными, а затем использовать публичные методы, наподобие getColor() и getModel(), как и было указано в тексте кода выше.\n",
      "В противоположность, в Python отсутствуют понятия публичных и приватных данных. В Python всё – публичное. Этот питоновский код сработает на ура:\n",
      ">>> my_car = car.Car(\"blue\", \"Ford\", 1972)\n",
      "\n",
      ">>> # Paint the car\n",
      "... my_car.color = \"red\"\n",
      "Вместо приватных переменных в Python имеется понятие непубличных (non-public) переменных экземпляра класса. Все переменные, названия которых начинаются с одинарного подчеркивания, считаются непубличными. Это соглашение об именах нисколько не мешает нам обратиться к переменной напрямую.\n",
      "Добавим следующую строку в наш питоновский класс Car:\n",
      "class Car:\n",
      "\n",
      "    wheels = 0\n",
      "\n",
      "    def __init__(self, color, model, year):\n",
      "        self.color = color\n",
      "        self.model = model\n",
      "        self.year = year\n",
      "        self._cupholders = 6\n",
      "Мы можем получить доступ к переменной _cupholders напрямую:\n",
      ">>> import car\n",
      ">>> my_car = car.Car(\"yellow\", \"Beetle\", \"1969\")\n",
      ">>> print(f\"It was built in {my_car.year}\")\n",
      "It was built in 1969\n",
      ">>> my_car.year = 1966\n",
      ">>> print(f\"It was built in {my_car.year}\")\n",
      "It was built in 1966\n",
      ">>> print(f\"It has {my_car._cupholders} cupholders.\")\n",
      "It has 6 cupholders.\n",
      "Python позволяет получить доступ к такой переменной, правда, некоторые среды разработки вроде VS Code выдадут предупреждение:\n",
      "\n",
      "Кроме этого, в Python для того, чтобы скрыть атрибут, используется двойное подчеркивание в начале названия переменной. Когда Python видит такую переменную, он автоматически меняет ее название, чтобы затруднить к ней прямой доступ. Однако, этот механизм всё равно не мешает нам обратиться к ней. Продемонстрируем это следующим примером:\n",
      "class Car:\n",
      "\n",
      "    wheels = 0\n",
      "\n",
      "    def __init__(self, color, model, year):\n",
      "        self.color = color\n",
      "        self.model = model\n",
      "        self.year = year\n",
      "        self.__cupholders = 6\n",
      "Теперь если мы обратимся к переменной __cupholders, мы получим ошибку:\n",
      ">>> import car\n",
      ">>> my_car = car.Car(\"yellow\", \"Beetle\", \"1969\")\n",
      ">>> print(f\"It was built in {my_car.year}\")\n",
      "It was built in 1969\n",
      ">>> my_car.year = 1966\n",
      ">>> print(f\"It was built in {my_car.year}\")\n",
      "It was built in 1966\n",
      ">>> print(f\"It has {my_car.__cupholders} cupholders.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "AttributeError: 'Car' object has no attribute '__cupholders'\n",
      "Так почему же атрибут __cupholders не существует?\r\n",
      "Дело вот в чем. Когда Python видит атрибут с двойным подчеркиванием в самом начале, он меняет его, добавляя в начало имя класса с подчеркиванием. Для того чтобы обратиться к атрибуту напрямую, необходимо также изменить имя:\n",
      ">>> print(f\"It has {my_car._Car__cupholders} cupholders\")\n",
      "It has 6 cupholders\n",
      "Теперь возникает вопрос: если атрибут Java-класса объявлен приватным и атрибуту Python-класса предшествует в имени двойное подчеркивание, то как достучаться до этих данных?\n",
      "Управление доступом\n",
      "В Java мы получаем доступ к приватным атрибутам при помощи сеттеров (setters) и геттеров (getters). Для того чтобы пользователь перекрасил-таки свою машину, добавим следующий кусок кода в Java-класс:\n",
      "public String getColor() {\n",
      "    return color;\n",
      "}\n",
      "\n",
      "public void setColor(String color) {\n",
      "    this.color = color;\n",
      "}\n",
      "Поскольку методы getColor() и setColor() – публичные, то любой пользователь может вызвать их и получить / изменить цвет машины. Использование приватных атрибутов, к которым мы получаем доступ публичными геттерами и сеттерами, — одна из причин большей «многословности» Java в сравнении с Python.\n",
      "Как было показано выше, в Python мы можем получить доступ к атрибутам напрямую. Поскольку всё – публичное, мы может достучаться к чему угодно, когда угодно и откуда угодно. Мы можем получать и устанавливать значения атрибутов напрямую, обращаясь по их имени. В Python мы можем даже удалять атрибуты, что немыслимо в Java:\n",
      ">>> my_car = Car(\"yellow\", \"beetle\", 1969)\n",
      ">>> print(f\"My car was built in {my_car.year}\")\n",
      "My car was built in 1969\n",
      ">>> my_car.year = 1966\n",
      ">>> print(f\"It was built in {my_car.year}\")\n",
      "It was built in 1966\n",
      ">>> del my_car.year\n",
      ">>> print(f\"It was built in {my_car.year}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "AttributeError: 'Car' object has no attribute 'year'\n",
      "Однако бывает и так, что мы хотим контролировать доступ к атрибутам. В таком случае нам на помощь приходят Python-свойства (properties).\n",
      "В Python свойства обеспечивают управляемый доступ к атрибутам класса при помощи декораторов (decorators). Используя свойства, мы объявляем функции в питоновских классах подобно геттерам и сеттерам в Java (бонусом идет удаление атрибутов).\n",
      "Работу свойств можно увидеть на следующем примере класса Car:\n",
      " 1 class Car:\n",
      " 2     def __init__(self, color, model, year):\n",
      " 3         self.color = color\n",
      " 4         self.model = model\n",
      " 5         self.year = year\n",
      " 6         self._voltage = 12\n",
      " 7 \n",
      " 8     @property\n",
      " 9     def voltage(self):\n",
      "10         return self._voltage\n",
      "11 \n",
      "12     @voltage.setter\n",
      "13     def voltage(self, volts):\n",
      "14         print(\"Warning: this can cause problems!\")\n",
      "15         self._voltage = volts\n",
      "16 \n",
      "17     @voltage.deleter\n",
      "18     def voltage(self):\n",
      "19         print(\"Warning: the radio will stop working!\")\n",
      "20         del self._voltage\n",
      "В данном примере мы расширяем понятие класса Car, включая электромобили. В строке 6 объявляется атрибут _voltage, чтобы хранить в нем напряжение батареи.\n",
      "В строках 9 и 10 для контролируемого доступа мы создаем функцию voltage() и возвращаем значение приватной переменной. Используя декоратор @property, мы превращаем его в геттер, к которому теперь любой пользователь получает доступ.\n",
      "В строках 13-15 мы определяем функцию, так же носящую название voltage(). Однако, мы ее декорируем по-другому: voltage.setter. Наконец, в строках 18-20 мы декорируем функцию voltage() при помощи voltage.deleter и можем при необходимости удалить атрибут _voltage.\n",
      "Декорируемые функции носят одинаковые имена, указывая на то, что они управляют доступом к одному и тому же атрибуту. Эти имена функций также становятся именами атрибутов, используемых для получения их значений. Вот как это работает:\n",
      " 1 >>> from car import *\n",
      " 2 >>> my_car = Car(\"yellow\", \"beetle\", 1969)\n",
      " 3 \n",
      " 4 >>> print(f\"My car uses {my_car.voltage} volts\")\n",
      " 5 My car uses 12 volts\n",
      " 6 \n",
      " 7 >>> my_car.voltage = 6\n",
      " 8 Warning: this can cause problems!\n",
      " 9 \n",
      "10 >>> print(f\"My car now uses {my_car.voltage} volts\")\n",
      "11 My car now uses 6 volts\n",
      "12 \n",
      "13 >>> del my_car.voltage\n",
      "14 Warning: the radio will stop working!\n",
      "Обратите внимание, что мы используем voltage, а не _ voltage. Так мы указываем Python-у на то, что следует применять свойства, которые только что определили:\n",
      "\n",
      "Когда в 4-й строке выводим значение my_car.voltage, Python вызывает функцию voltage(), декорированную @property.\n",
      "Когда в 7-й строке присваиваем значение my_car.voltage, Python вызывает функцию voltage(), декорированную voltage.setter.\n",
      "Когда в 13-й строке удаляем my_car.voltage, Python вызывает функцию voltage(), декорированную voltage.deleter.\n",
      "\n",
      "Вышеприведенные декораторы дают нам возможность контролировать доступ к атрибутам без использования различных методов. Можно даже сделать атрибут свойством только для чтения (read-only), убрав декорированные функции @.setter и @.deleter.\n",
      "self и this\n",
      "В Java класс ссылается сам на себя, используя ключевое слово this:\n",
      "public void setColor(String color) {\n",
      "    this.color = color;\n",
      "}\n",
      "this подразумевается в Java-коде. Его в принципе даже необязательно писать, кроме случаев, когда имена переменных совпадают.\n",
      "Сеттер можно написать и так:\n",
      "public void setColor(String newColor) {\n",
      "    color = newColor;\n",
      "}\n",
      "Поскольку в классе Car есть атрибут под названием color и в области видимости нет больше переменных с таким именем, ссылка на это имя срабатывает. Мы использовали ключевое слово this в первом примере для того, чтобы различать атрибут и параметр с одинаковым именем color.\n",
      "В Python ключевое слово self служит аналогичной цели: обращение к членам-атрибутам, но в отличие от Java, оно обязательно:\n",
      "class Car:\n",
      "    def __init__(self, color, model, year):\n",
      "        self.color = color\n",
      "        self.model = model\n",
      "        self.year = year\n",
      "        self._voltage = 12\n",
      "\n",
      "    @property\n",
      "    def voltage(self):\n",
      "        return self._voltage\n",
      "Python требует написания self в обязательном порядке. Каждый self либо создает, либо обращается к атрибуту. Если мы пропустим его, то Python просто создаст локальную переменную вместо атрибута.\n",
      "Отличие в том, как мы используем self и this в Python и Java, происходит из-за основных различий между двумя языками и от того, как они именуют переменные и атрибуты.\n",
      "Методы и функции\n",
      "Разница между рассматриваемыми языками заключается в том, что в Python есть функции, а в Java их нет.\n",
      "В Python следующий код отработает без проблем (и используется повсеместно):\n",
      ">>> def say_hi():\n",
      "...     print(\"Hi!\")\n",
      "... \n",
      ">>> say_hi()\n",
      "Hi!\n",
      "Мы можем вызвать say_hi() из любого места видимости. Эта функция не содержит ссылки на self, что означает, что это глобальная функция, а не функция класса. Она не сможет изменять или сохранять какие-нибудь данные какого-либо класса, но может использовать локальные и глобальные переменные.\n",
      "В противоположность, каждая написанная нами строчка на Java принадлежит какому-нибудь классу. Функции не существует за пределами класса, и по определению все Java-функции — это методы. На Java ближе всего к чистой функции находится статичный метод:\n",
      "public class Utils {\n",
      "    static void SayHi() {\n",
      "        System.out.println(\"Hi!\");\n",
      "    }\n",
      "}\n",
      "Utils. SayHi() вызывается из любого места без предварительного создания экземпляра класса Utils. Поскольку мы вызываем SayHi() без создания объекта, ссылки this не существует. Однако, это всё равно не функция в том смысле, в котором является say_hi() в Python.\n",
      "Наследование и полиморфизм\n",
      "Наследование и полиморфизм – две фундаментальные концепции в ООП. Благодаря первому, объекты получают (другими словами, наследуют) атрибуты и функциональные возможности других объектов, создавая иерархию от более общих объектов к более конкретным. Например, и класс Car (машина), и класс Boat (лодка) являются конкретными типами класса Vehicle (транспортное средство). Оба объекта наследуют поведение одного родительского объекта или множества родительских объектов. В этом случае их называют дочерними объектами.\n",
      "Полиморфизм, в свою очередь, — это возможность работы с разными объектами с помощью одной и той же функции или метода. \n",
      "Обе эти фундаментальные ООП-концепции реализованы в Java и Python совершенно по-разному.\n",
      "Наследование\n",
      "Python поддерживает множественное наследование, то есть создание класса более чем от одного родителя.\n",
      "Чтобы продемонстрировать это, разделим класс Car на две категории: одну – для транспортных средств и одну – для машин, использующих электричество:\n",
      "class Vehicle:\n",
      "    def __init__(self, color, model):\n",
      "        self.color = color\n",
      "        self.model = model\n",
      "\n",
      "class Device:\n",
      "    def __init__(self):\n",
      "        self._voltage = 12\n",
      "\n",
      "class Car(Vehicle, Device):\n",
      "    def __init__(self, color, model, year):\n",
      "        Vehicle.__init__(self, color, model)\n",
      "        Device.__init__(self)\n",
      "        self.year = year\n",
      "\n",
      "    @property\n",
      "    def voltage(self):\n",
      "        return self._voltage\n",
      "\n",
      "    @voltage.setter\n",
      "    def voltage(self, volts):\n",
      "        print(\"Warning: this can cause problems!\")\n",
      "        self._voltage = volts\n",
      "\n",
      "    @voltage.deleter\n",
      "    def voltage(self):\n",
      "        print(\"Warning: the radio will stop working!\")\n",
      "        del self._voltage\n",
      "В классе Vehicle определены атрибуты color и model. В классе Device имеется атрибут _voltage. Класс Car происходит от этих двух классов, и атрибуты color, model и _voltage теперь являются частью нового класса.\n",
      "В методе init() класса Car вызываются методы init() обоих родительских классов, чтобы все данные проинициализировались должным образом. После этого мы можем добавить классу Car любую желаемую функциональность. В данном примере мы добавим атрибут year, а также геттер и сеттер для _voltage.\n",
      "Функциональность нового класса Car осталась прежней. Мы можем создавать и использовать объекты класса, как это делали несколькими примерами ранее:\n",
      ">>> from car import *\n",
      ">>> my_car = Car(\"yellow\", \"beetle\", 1969)\n",
      "\n",
      ">>> print(f\"My car is {my_car.color}\")\n",
      "My car is yellow\n",
      "\n",
      ">>> print(f\"My car uses {my_car.voltage} volts\")\n",
      "My car uses 12 volts\n",
      "\n",
      ">>> my_car.voltage = 6\n",
      "Warning: this can cause problems!\n",
      "\n",
      ">>> print(f\"My car now uses {my_car.voltage} volts\")\n",
      "My car now uses 6 volts\n",
      "Язык Java же, в свою очередь, поддерживает только одиночное наследование, что означает, что классы в Java могут наследовать данные и поведение только от одного родительского класса. Зато в Java возможно наследование от множества интерфейсов. Интерфейсы обеспечивают группу связанных методов, которые нужно реализовать, позволяя дочерним классам вести себя сходным образом.\n",
      "Чтобы увидеть это, разделим Java-класс Car на родительский класс и интерфейс:\n",
      "public class Vehicle {\n",
      "\n",
      "    private String color;\n",
      "    private String model;\n",
      "\n",
      "    public Vehicle(String color, String model) {\n",
      "        this.color = color;\n",
      "        this.model = model;\n",
      "    }\n",
      "\n",
      "    public String getColor() {\n",
      "        return color;\n",
      "    }\n",
      "\n",
      "    public String getModel() {\n",
      "        return model;\n",
      "    }\n",
      "}\n",
      "\n",
      "public interface Device {\n",
      "    int getVoltage();\n",
      "}\n",
      "\n",
      "public class Car extends Vehicle implements Device {\n",
      "\n",
      "    private int voltage;\n",
      "    private int year;\n",
      "\n",
      "    public Car(String color, String model, int year) {\n",
      "        super(color, model);\n",
      "        this.year = year;\n",
      "        this.voltage = 12;\n",
      "    }\n",
      "\n",
      "    @Override\n",
      "    public int getVoltage() {\n",
      "        return voltage;\n",
      "    }\n",
      "\n",
      "    public int getYear() {\n",
      "        return year;\n",
      "    }\n",
      "}\n",
      "Не забываем, что каждый класс и каждый интерфейс в Java должны быть размещены в своем собственном файле.\n",
      "Как и в вышеприведенном примере с Python, мы создаем новый класс Vehicle для хранения общих данных и функционала, присущих транспортным средствам. Однако для добавления функциональных возможностей Device нам нужно создать интерфейс, определяющий метод получения напряжения (voltage) устройства.\n",
      "Класс Car создается путем наследования от класса Vehicle с использованием ключевого слова extends и реализации интерфейса Device с использованием ключевого слова implements. В конструкторе класса мы вызываем конструктор родителя при помощи super(). Поскольку родительский класс только один, мы обращаемся к конструктору класса Vehicle. Для реализации интерфейса переопределяем getVoltage() с помощью аннотации Override.\n",
      "Вместо повторного использования кода из Device, как это делается в Python, Java требует, чтобы мы реализовывали один и тот же функционал в каждом классе, который реализует интерфейс. Интерфейсы всего лишь определяют методы — они не могут определять данные экземпляра класса или детали реализации.\n",
      "Так почему же это происходит с Java? Причина кроется в типах данных и проверке типов.\n",
      "Типы данных и полиморфизм\n",
      "Каждый класс и каждый интерфейс в Java имеет тип. Следовательно, если два Java-объекта реализуют один и тот же интерфейс, считается, что они имеют один и тот же тип по отношению к этому интерфейсу. С помощью этого механизма можно взаимозаменяемо использовать различные классы, в чем и заключается полиморфизм.\n",
      "Реализуем зарядку устройства для наших Java-объектов при помощи создания метода charge(), который принимает в качестве параметра переменную типа Device. Любой объект, реализующий интерфейс Device, может быть передан методу charge().\n",
      "Создадим следующий класс в файле под названием Rhino.java:\n",
      "public class Rhino {\n",
      "}\n",
      "Теперь создадим файл Main.java с методом charge() и посмотрим, чем отличаются объекты классов Car и Rhino.\n",
      "public class Main{\n",
      "    public static void charge(Device device) {\n",
      "       device.getVoltage();\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) throws Exception {\n",
      "        Car car = new Car(\"yellow\", \"beetle\", 1969);\n",
      "        Rhino rhino = new Rhino();\n",
      "        charge(car);\n",
      "        charge(rhino);\n",
      "    }\n",
      "}\n",
      "Вот что мы получим, когда попытаемся скомпилировать код:\n",
      "\n",
      "Information:2019-02-02 15:20 - Compilation completed with \n",
      "    1 error and 0 warnings in 4 s 395 ms\n",
      "Main.java\n",
      "Error:(43, 11) java: incompatible types: Rhino cannot be converted to Device\n",
      "Поскольку в классе Rhino не реализован интерфейс Device, его нельзя передать в качестве параметра в charge().\n",
      "В отличие от статической типизации (в оригинале — strict variable typing, то есть строгая типизация, но Python тоже относится к языкам со строгой типизацией) переменных, принятой в Java, в Python используется концепция утиной типизации, которая в общем виде звучит так: если переменная «ходит как утка и крякает как утка, то это и есть утка» (на самом деле звучит немного иначе: \"если нечто выглядит как утка, плавает как утка и крякает как утка, то это, вероятно, и есть утка\" – прим. переводчика). Вместо идентификации объектов по типу, Python проверяет их поведение. \n",
      "Лучше понять утиную типизацию поможет следующий аналогичный пример зарядки устройства на Python:\n",
      ">>> def charge(device):\n",
      "...     if hasattr(device, '_voltage'):\n",
      "...         print(f\"Charging a {device._voltage} volt device\")\n",
      "...     else:\n",
      "...         print(f\"I can't charge a {device.__class__.__name__}\")\n",
      "... \n",
      ">>> class Phone(Device):\n",
      "...     pass\n",
      "... \n",
      ">>> class Rhino:\n",
      "...     pass\n",
      "... \n",
      ">>> my_car = Car(\"yellow\", \"Beetle\", \"1966\")\n",
      ">>> my_phone = Phone()\n",
      ">>> my_rhino = Rhino()\n",
      "\n",
      ">>> charge(my_car)\n",
      "Charging a 12 volt device\n",
      ">>> charge(my_phone)\n",
      "Charging a 12 volt device\n",
      ">>> charge(my_rhino)\n",
      "I can't charge a Rhino\n",
      "charge() проверяет существование в объекте атрибута _voltage. Поскольку в классе Device имеется такой атрибут, то и в любом его классе-наследнике (Car и Phone) тоже будет этот атрибут, и, следовательно, этот класс выведет сообщение о зарядке. У классов, которые не унаследовались от Device (как Rhino), не будет этого атрибута, и они не будут заряжаться, что хорошо, поскольку для жизни носорога (rhino) электрическая зарядка смертельно опасна.\n",
      "Дефолтные методы\n",
      "Все классы в Java имеют своим предком класс Object, который содержит определенный набор методов и передает их своим потомкам. Потомки могут эти методы либо переопределять, либо использовать по умолчанию. Класс Object включает в себя следующие методы:\n",
      "class Object {\n",
      "    boolean equals(Object obj) { ... }    \n",
      "    int hashCode() { ... }    \n",
      "    String toString() { ... }    \n",
      "}\n",
      "По умолчанию equals() сравнивает адреса в памяти текущего объекта с объектом, переданным в качестве параметра, hashCode() вычисляет уникальный идентификатор, который так же использует адрес в памяти текущего объекта. Эти методы активно используются в Java в различных контекстах. Например, коллекциям, которые сортируют объекты на основе их значений, нужны оба этих метода.\n",
      "toString() возвращает строковое представление объекта. По умолчанию это имя класса и адрес в памяти. Этот метод вызывается автоматически, когда объект передается в качестве параметра в метод, требующий строковый аргумент, например, System.out.println():\n",
      "Car car = new Car(\"yellow\", \"Beetle\", 1969);\n",
      "System.out.println(car);\n",
      "Запустим этот код и увидим дефолтное строковое представление объекта car:\n",
      "Car@61bbe9ba\n",
      "Не очень информативно, не правда ли? Давайте усовершенствуем вывод, переопределив метод toString(). Добавим следующий метод в класс Car:\n",
      "public String toString() {\n",
      "    return \"Car: \" + getColor() + \" : \" + getModel() + \" : \" + getYear();\n",
      "}\n",
      "Теперь, запустив предыдущий пример, увидим следующее:\n",
      "Car: yellow : Beetle : 1969\n",
      "В Python подобный функционал обеспечивается набором так называемых магических методов (dunder — аббревиатура для double underscore). Каждый Python-класс наследует эти методы, и мы можем, переопределив их, изменить их поведение.\n",
      "В Python для строкового представления объекта имеется два метода: repr() и str(). Однозначное представление объекта возвращается методом repr(), в то время как str() возвращает его в удобочитаемом виде. Это примерно как hashcode() и toString() в Java. \n",
      "Как и в Java, в Python имеется дефолтная реализация магических методов:\n",
      ">>> my_car = Car(\"yellow\", \"Beetle\", \"1966\")\n",
      "\n",
      ">>> print(repr(my_car))\n",
      "<car.Car object at 0x7fe4ca154f98>\n",
      ">>> print(str(my_car))\n",
      "<car.Car object at 0x7fe4ca154f98>\n",
      "Чтобы улучшить читаемость, переопределим метод str() в Python-классе Car:\n",
      "def __str__(self):\n",
      "    return f'Car {self.color} : {self.model} : {self.year}'\n",
      "Результат будет выглядеть намного приятнее:\n",
      ">>> my_car = Car(\"yellow\", \"Beetle\", \"1966\")\n",
      "\n",
      ">>> print(repr(my_car))\n",
      "<car.Car object at 0x7f09e9a7b630>\n",
      ">>> print(str(my_car))\n",
      "Car yellow : Beetle : 1966\n",
      "Переопределение магического метода дало нам более читаемое представление объекта. Можно также переопределить метод repr(), это полезно для отладки.\n",
      "Python предоставляет нам гораздо большее количество магических методов, переопределяя которые, можно изменить поведение объекта во время итерации, операций сравнения и сложения или непосредственного вызова объекта.\n",
      "Перегрузка операторов\n",
      "Перегрузка операторов в Python означает возможность в классах переопределять различные операторы языка. Магические методы Python позволяют реализовать перегрузку операторов, чего Java не предлагает вообще.\n",
      "Изменим наш Python-класс Car следующим образом:\n",
      "class Car:\n",
      "    def __init__(self, color, model, year):\n",
      "        self.color = color\n",
      "        self.model = model\n",
      "        self.year = year\n",
      "\n",
      "    def __str__(self):\n",
      "        return f'Car {self.color} : {self.model} : {self.year}'\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        return self.year == other.year\n",
      "\n",
      "    def __lt__(self, other):\n",
      "        return self.year < other.year\n",
      "\n",
      "    def __add__(self, other):\n",
      "        return Car(self.color + other.color, \n",
      "                   self.model + other.model, \n",
      "                   int(self.year) + int(other.year))\n",
      "Данная таблица показывает связи между этими магическими методами и операторами, которые они представляют:\n",
      "\n",
      "\n",
      "\n",
      "Магический метод\n",
      "Оператор\n",
      "Смысл\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "eq\n",
      "==\n",
      "В один ли год выпущены объекты Car?\n",
      "\n",
      "\n",
      "lt\n",
      "<\n",
      "Какой из объектов Car более раннего выпуска?\n",
      "\n",
      "\n",
      "add\n",
      "+\n",
      "Добавить два объекта Car без особого смысла\n",
      "\n",
      "\n",
      "\n",
      "Когда Python видит выражение, содержащее объекты, он вызывает магический метод, соответствующий операторам в выражении.\n",
      "В нижеуказанном коде используются новые перегруженные арифметические функции над двумя объектами класса Car:\n",
      ">>> my_car = Car(\"yellow\", \"Beetle\", \"1966\")\n",
      ">>> your_car = Car(\"red\", \"Corvette\", \"1967\")\n",
      "\n",
      ">>> print (my_car < your_car)\n",
      "True\n",
      ">>> print (my_car > your_car)\n",
      "False\n",
      ">>> print (my_car == your_car)\n",
      "False\n",
      ">>> print (my_car + your_car)\n",
      "Car yellowred : BeetleCorvette : 3933\n",
      "Существует гораздо большее количество операторов, которые можно перегрузить при помощи магии, что позволяет разнообразить поведение объекта так, как это не делают базовые дефолтные методы Java.\n",
      "Рефлексия\n",
      "Рефлексия – это изучение объекта или класса внутри самого объекта или класса. И Java, и Python предоставляют способы исследования атрибутов и методов в классе.\n",
      "Изучение типа объекта\n",
      "В обоих рассматриваемых языках имеются способы проверить тип объекта. В Python мы используем type()для отображения типа переменной и isinstance () для определения, является ли данная переменная экземпляром или потомком определенного класса:\n",
      ">>> my_car = Car(\"yellow\", \"Beetle\", \"1966\")\n",
      "\n",
      ">>> print(type(my_car))\n",
      "<class 'car.Car'>\n",
      ">>> print(isinstance(my_car, Car))\n",
      "True\n",
      ">>> print(isinstance(my_car, Device))\n",
      "True\n",
      "В Java мы вызываем метод getClass() для определения типа и используем instanceof для проверки на принадлежность классу:\n",
      "Car car = new Car(\"yellow\", \"beetle\", 1969);\n",
      "\n",
      "System.out.println(car.getClass());\n",
      "System.out.println(car instanceof Car);\n",
      "Получаем следующее:\n",
      "class com.realpython.Car\n",
      "true\n",
      "Изучение атрибутов объекта\n",
      "В Python при помощи dir() мы видим все атрибуты и функции, содержащиеся в объекте (включая магические методы). Чтобы получить конкретные сведения о данном атрибуте или функции, используем getattr():\n",
      ">>> print(dir(my_car))\n",
      "['_Car__cupholders', '__add__', '__class__', '__delattr__', '__dict__', \n",
      " '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',\n",
      " '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__',\n",
      " '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__',\n",
      " '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__',\n",
      " '_voltage', 'color', 'model', 'voltage', 'wheels', 'year']\n",
      "\n",
      ">>> print(getattr(my_car, \"__format__\"))\n",
      "<built-in method __format__ of Car object at 0x7fb4c10f5438>\n",
      "В Java имеются аналогичные возможности, однако контроль доступа и типобезопасность, заложенные в языке, усложняют дело.\n",
      "getFields() извлекает список всех общедоступных атрибутов. Однако, поскольку ни один из атрибутов класса Car не является публичным, этот код возвращает пустой массив:\n",
      "Field[] fields = car.getClass().getFields();\n",
      "Java рассматривает атрибуты и методы как отдельные сущности, поэтому публичные методы извлекаются при помощи getDeclaredMethods(). Поскольку публичные атрибуты будут иметь соответствующий get-метод, один из способов обнаружить, что класс содержит определенное свойство, может выглядеть таким образом:\n",
      "1) использовать getDeclaredMethods() для генерации массива всех методов\r\n",
      "2) перебрать все эти методы:\n",
      "\n",
      "для каждого обнаруженного метода вернуть true, если метод:\n",
      "\n",
      "начинается со слова get или принимает ноль аргументов;\n",
      "\n",
      "\n",
      "\n",
      "и не возвращает void;\n",
      "\n",
      "\n",
      "\n",
      "и включает в себя название свойства;\n",
      "\n",
      "\n",
      "в противном случае вернуть false.\n",
      "\n",
      "Вот пример на скорую руку:\n",
      " 1 public static boolean getProperty(String name, Object object) throws Exception {\n",
      " 2 \n",
      " 3     Method[] declaredMethods = object.getClass().getDeclaredMethods();\n",
      " 4     for (Method method : declaredMethods) {\n",
      " 5         if (isGetter(method) && \n",
      " 6             method.getName().toUpperCase().contains(name.toUpperCase())) {\n",
      " 7               return true;\n",
      " 8         }\n",
      " 9     }\n",
      "10     return false;\n",
      "11 }\n",
      "12 \n",
      "13 // Helper function to get if the method is a getter method\n",
      "14 public static boolean isGetter(Method method) {\n",
      "15     if ((method.getName().startsWith(\"get\") || \n",
      "16          method.getParameterCount() == 0 ) && \n",
      "17         !method.getReturnType().equals(void.class)) {\n",
      "18           return true;\n",
      "19     }\n",
      "20     return false;\n",
      "21 }\n",
      "\n",
      "getProperty() – это точка входа. Вызовем ее с именем атрибута и объекта. Она вернет true, если свойство будет найдено, иначе вернет false.\n",
      "Вызов методов через рефлексию\n",
      "И в Java, и в Python имеются механизмы для вызова методов через рефлексию.\r\n",
      "В вышеприведенном Java-примере вместо возвращения значения true в случае, если свойство найдено, можно было вызвать метод напрямую. Вспомним, что getDeclaredMethods() возвращает массив объектов типа Method. Объект Method сам содержит метод invoke(), который вызывает Method. В строке 7 вместо возвращения значения true, когда найден метод, можно вернуть method.invoke(object).\n",
      "Эта возможность существует также и в Python. Однако, поскольку Python не делает различий между функциями и атрибутами, нужно специально искать сущности, которые можно вызвать:\n",
      ">>> for method_name in dir(my_car):\n",
      "...     if callable(getattr(my_car, method_name)):\n",
      "...         print(method_name)\n",
      "... \n",
      "__add__\n",
      "__class__\n",
      "__delattr__\n",
      "__dir__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "Методы Python проще в управлении и вызове, чем в Java. Нижеприведенный код найдет метод объекта str() и вызовет его через рефлексию:\n",
      ">>> for method_name in dir(my_car):\n",
      "...     attr = getattr(my_car, method_name)\n",
      "...     if callable(attr):\n",
      "...         if method_name == '__str__':\n",
      "...             print(attr())\n",
      "... \n",
      "Car yellow : Beetle : 1966\n",
      "В данном примере проверяется каждый атрибут, возвращаемый функцией dir(). Мы получаем значение атрибута объекта, используя getattr(), и проверяем при помощи callable(), является ли оно вызываемой функцией. Если это так, то можно проверить, является ли его имя str (), и затем вызвать его.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем привет. Сегодня хотим поделиться еще одним переводом подготовленным в преддверии запуска курса «Разработчик Python». Поехали!\n",
      "\n",
      "\n",
      "\r\n",
      "Я использовал Python чаще, чем любой другой язык программирования в последние 4-5 лет. Python – преобладающий язык для билдов под Firefox, тестирования и инструмента CI. Mercurial также в основном написан на Python. Множество своих сторонних проектов я тоже писал на нем.\n",
      "\r\n",
      "Во время своей работы я получил немного знаний о производительности Python и о его средствах оптимизации. В этой статье мне хотелось бы поделиться этими знаниями. \n",
      "\r\n",
      "Мой опыт с Python в основном связан с интерпретатором CPython, в особенности CPython 2.7. Не все мои наблюдения универсальны для всех дистрибутивов Python или же для тех, которые имеют одинаковые характеристики в сходных версиях Python. Я постараюсь упоминать об этом во время повествования. Помните о том, что эта статья не является детальным обзором производительности Python. Я буду говорить только о том, с чем сталкивался самостоятельно. \n",
      "\n",
      "Нагрузка из-за особенностей запуска и импорта модулей\r\n",
      " Запуск интерпретатора Python и импорт модулей – это достаточно долгий процесс, если речь идет о миллисекундах. \n",
      "\r\n",
      "Если вам нужно запустить сотни или тысячи процессов Python в каком-то из своих проектов, то эта задержка в миллисекунды перерастет в задержку до нескольких секунд.\n",
      "\r\n",
      "Если же вы используете Python, чтобы обеспечить инструменты CLI, издержки могут вызвать зависание заметное пользователю. Если вам понадобятся инструменты CLI мгновенно, то запуск интерпретатора Python при каждом вызове усложнит получение этого сложного инструмента. \n",
      "\r\n",
      "Я уже писал об этой проблеме. Несколько моих прошлых заметок рассказывает об этом, например, в 2014 году, в мае 2018 и октябре 2018 года.\n",
      "\r\n",
      "Не так много вещей, которые вы можете предпринять, чтобы уменьшить задержку запуска: исправление этом случае относится к манипуляциям с интерпретатором Python, поскольку именно он контролирует исполнение кода, которое занимает слишком много времени. Лучшее, что вы можете сделать – это отключить импорт модуля site в вызовах, чтобы избежать исполнения лишнего кода на Python во время запуска. С другой стороны, множество приложений используют функционал модуля site.py, поэтому использовать это можно на свой страх и риск.\n",
      "\r\n",
      "Отдельно стоит рассмотреть проблему импорта модулей. Чего хорошего в интерпретаторе Python, если он не обрабатывает никакой код? Дело в том, что код становится доступным для понимания интерпретатора чаще всего с помощью использования модулей.\n",
      "\r\n",
      "Для импортирования модулей нужно предпринять несколько шагов. И в каждом из них есть потенциальный источник нагрузок и задержек.\n",
      "\r\n",
      "Определенная задержка происходит за счёт поиска модулей и считывания их данных. Как я продемонстрировал с помощью PyOxidizer, заместив поиск и загрузку модуля из файловой системы архитектурно более простым решением, который заключается в чтении данных модуля из структуры данных в памяти, можно импортировать стандартную библиотеку Python за 70-80% от изначального времени решения этой задачи. Наличие одного модуля на файл файловой системы увеличивает нагрузку на файловую систему и может замедлить работу приложения Python в критические первые миллисекунды исполнения. Решения подобные PyOxidizer могут помочь этого избежать. Надеюсь, что сообщество Python видит эти издержки текущего подхода и рассматривает возможность перехода к механизмам распределения модулей, которые не так сильно зависят от отдельных фалов в модуле.\n",
      "\r\n",
      "Другой источник дополнительных расходов на импорт модуля – это выполнение кода в этом модуле во время импорта. Некоторые модули содержат части кода в области вне функций и классов модуля, который и выполняется при импорте модуля. Выполнение такого кода увеличивает затраты на импорт. Способ обхода: исполнять не весь код во время импорта, а исполнять его только при надобности. Python 3.7 поддерживает модуль __getattr__, который будет вызван, в случае, если атрибут какого-либо модуля не был найден. Это может использоваться для ленивого заполнения атрибутов модуля при первом доступе.\n",
      "\r\n",
      "Другой способ избавиться от замедления при импорте – это ленивый импорт модуля. Вместо того, чтобы непосредственно загружать модуль при импорте, вы регистрируете пользовательский модуль импорта, который возвращает вместо этого заглушку (stub). При первом обращении к этой заглушке, она загрузит фактический модуль и «мутирует», чтобы стать этим модулем. \n",
      "\r\n",
      "Вы можете сэкономить десятки миллисекунд за счет приложений, которые импортируют несколько десятков модулей, если будете обходить файловую систему и избегать запуска ненужных частей модуля (модули обычно импортируются глобально, но используются лишь определенные функции модуля). \n",
      "\r\n",
      "Ленивый импорт модулей – это хрупкая вещь. Множество модулей имеют шаблоны, в которых есть следующие вещи: try: import foo; except ImportError:. Ленивый импортер модулей может никогда не выдать ImportError, поскольку если он это сделает, то ему придется искать в файловой системе модуль, чтобы узнать существует ли он в принципе. Это добавит дополнительную нагрузку и увеличит затраты времени, поэтому ленивые импортеры не делают этого в принципе! Эта проблема довольно неприятна. Импортер ленивых модулей Mercurial обрабатывает список модулей, которые не могут быть лениво импортированы, и он должен их обойти. Другая проблема это синтаксис from foo import x, y, который также прерывает импорт ленивого модуля, в случаях, когда foo является модулем (в отличие от пакета), поскольку для возврата ссылки на x и y, модуль все же должен быть импортирован.\n",
      "\r\n",
      "PyOxidizer имеет фиксированный набор модулей вшитых в бинарник, поэтому он может быть эффективным в вопросе выдачи ImportError. Модуль __getattr__ из Python 3.7 обеспечивает дополнительную гибкость для ленивых импортеров модулей. Я надеюсь интегрировать надежный ленивый импортер в PyOxidizer, чтобы автоматизировать некоторые процессы.\n",
      "\r\n",
      "Лучшее решение для избежания запуска интерпретатора и появления временных задержек – это запуск фонового процесса в Python. Если вы запускаете процесс Python в качестве демона (daemon process), скажем для веб-сервера, то вы сможете это сделать. Решение, которое предлагает Mercurial – это запуск фонового процесса, который предоставляет протокол сервера команд (command server protocol). hg является исполняемым файлом С (или же теперь Rust), который подключается к этому фоновому процессу и отправляет команду. Чтобы найти подход к командному серверу, нужно проделать много работы, он крайне нестабильный и имеет проблемы с безопасностью. Я рассматриваю идею доставки командного сервера с помощью PyOxidizer, чтобы исполняемый файл имел его преимущества, а сама по себе проблема стоимости программного решения решилась посредством создание проекта PyOxidizer.\n",
      "\n",
      "Задержка из-за вызова функции \r\n",
      "Вызов функций в Python относительно медленный процесс. (Это наблюдение менее применимо к PyPy, который может исполнять код JIT.)\n",
      "\r\n",
      "Я видел десятки патчей для Mercurial, которые давали возможность выравнивать и комбинировать код таким образом, чтобы избежать лишней нагрузки при вызове функций. В текущем цикле разработки были предприняты некоторые усилия, чтобы уменьшить количество вызываемый функций при обновлении прогресс-бара. (Мы используем прогресс-бары для любых операций, которые могут занимать определенное время, чтобы пользователь понимал, что происходит). Получение результатов вызова функций и избежание простых поисков среди функций экономят десятки сотен миллисекунд при выполнении, когда мы говорим об одном миллионе выполнений, например.\n",
      "\r\n",
      "Если у вас есть жесткие циклы или рекурсивые функции в Python, где могут случаться сотни тысяч или более вызовов функций, вы должны знать о накладных расходах на вызов отдельной функции, так как это имеет большое значение. Имейте в виду наличие встроенных простых функций и возможность комбинирования функций, чтобы избежать накладных расходов. \n",
      "\n",
      "Дополнительная нагрузка поиска атрибутов\r\n",
      " Эта проблема сходна с накладными расходами из-за вызова функции, поскольку смысл практически один и тот же!\n",
      "\r\n",
      " Нахождение (resolving) атрибутов в Python может быть медленным. (И снова, в PyPy это происходит быстрее). Однако обработка этой проблемы – это то, что мы делаем часто в Mercurial.\n",
      "\r\n",
      "Допустим, у вас есть следующий код:\n",
      "\n",
      "obj = MyObject()\n",
      "total = 0\n",
      "\n",
      "for i in len(obj.member):\n",
      "    total += obj.member[i]\r\n",
      " Опустим, что есть более эффективные способы написания этого примера (например, total = sum(obj.member)), и обратим внимание, что циклу необходимо определять obj.member на каждой итерации. В Python есть относительно сложный механизм для определения атрибутов. Для простых типов он может быть достаточно быстрым. Но для сложных типов этот доступ к атрибутам может автоматически вызывать __getattr__, __getattribute__, различные методы dunder и даже пользовательские функции @property. Это похоже на быстрый поиск атрибута, который может сделать несколько вызовов функции, что приведет к лишней нагрузке. И эта нагрузка может усугубиться, если вы используете такие вещи, как obj.member1.member2.member3 и т.д.\n",
      "\r\n",
      "Каждое определение атрибута вызывает дополнительную нагрузку. И поскольку почти все в Python — это словарь, то можно сказать, что каждый поиск атрибута – это поиск по словарю. Из общих понятий о базовых структурах данных мы знаем, что поиск по словарю – это не так быстро, как, допустим, поиск по указателю. Да, конечно есть несколько трюков в CPython, которые позволяют избавиться от накладных расходов из-за поиска по словарю. Но основная тема, которую я хочу затронуть, так это то, что любой поиск атрибутов – это потенциальная утечка производительности.\n",
      "\r\n",
      "Для жестких циклов, в особенности тех, которые потенциально превышают сотни тысяч итераций – вы можете избежать этих измеримых издержек на поиск атрибутов путем присвоения значения локальной переменной. Посмотрим на следующий пример: \n",
      "\n",
      "obj = MyObject()\n",
      "total = 0\n",
      "\n",
      "member = obj.member\n",
      "for i in len(member):\n",
      "    total += member[i]\r\n",
      " Конечно же, безопасно это можно сделать только в случае, если она не заменяется в цикле. Если это произойдет, то итератор сохранит ссылку на старый элемент и все может взорваться.\r\n",
      " Тот же самый трюк можно провести при вызове метода объекта. Вместо \n",
      "\n",
      "obj = MyObject()\n",
      "\n",
      "for i in range(1000000):\n",
      "    obj.process(i)\n",
      "\r\n",
      " Можно сделать следующее:\n",
      "\n",
      "obj = MyObject()\n",
      "fn = obj.process\n",
      "\n",
      "for i in range(1000000:)\n",
      "    fn(i)\r\n",
      " Также стоит заметить, что в случае, когда поиску по атрибутам нужно вызвать метод (как в предыдущем примере), то Python 3.7 сравнительно быстрее, чем предыдущие релизы. Но я уверен, что здесь излишняя нагрузка связана, в первую очередь, с вызовом функции, а не с нагрузкой на поиск атрибута. Поэтому все будет работать быстрее, если отказаться от лишнего поиска атрибутов.\n",
      "\r\n",
      "Наконец, поскольку поиск атрибутов вызывает для этого функции, то можно сказать, что поиск атрибутов – это в целом меньшая проблема, чем нагрузка из-за вызова функции. Как правило, чтобы заметить значительные изменения в скорости работы, вам понадобится устранить множество поисков атрибутов. При этом, как только вы дадите доступ ко всем атрибутам внутри цикла, вы можете говорить о 10 или 20 атрибутах только в цикле до вызова функции. А циклы с всего тысячами или менее чем с десятками тысяч итераций могут быстро обеспечить сотни тысяч или миллионы поисков атрибутов. Так что будьте внимательны!\n",
      "\n",
      "Объектная нагрузка\r\n",
      " С точки зрения интерпретатора Python все значения – это объекты. В CPython каждый элемент– это структура PyObject. Каждый объект, управляемый интерпретатором, находится в куче и имеет собственную память, содержащую счетчик ссылок, тип объекта и другие параметры. Каждый объект утилизируется сборщиком мусора. Это означает, что каждый новый объект добавляет накладные расходы из-за подсчета ссылок, механизма сбора мусора и т.п. (И снова, PyPy может избежать этой лишней нагрузки, поскольку «внимательнее относится» ко времени жизни краткосрочных значений.)\n",
      "\r\n",
      "Как правило, чем больше уникальных значений и объектов Python вы создаете, тем медленнее у вас все работает. \n",
      "\r\n",
      "Скажем, вы перебираете коллекцию из одного миллиона объектов. Вы вызываете функцию для сбора этого объекта в кортеж:\n",
      "\n",
      "for x in my_collection:\n",
      "    a, b, c, d, e, f, g, h = process(x)\r\n",
      " В данном примере, process() вернет кортеж 8-tuple. Не имеет значения, уничтожим мы возвращаемое значение или нет: этот кортеж требует создания по крайней мере 9 значений в Python: 1 для самого кортежа и 8 для его внутренних членов. Хорошо, в реальной жизни там может быть меньше значений, если process() возвращает ссылку на существующий объект. Или же их наоборот может быть больше, если их типы не простые и требуют для представления множество PyObject. Я лишь хочу сказать, что под капотом интерпретатора происходит настоящее жонглирование объектами для полноценного представления тех или иных конструкций.\n",
      "\r\n",
      "По своему опыту могу сказать, что эти накладные расходы актуальны лишь для операций, которые дают выигрыш в скорости при реализации на родном языке, таком как C или Rust. Проблема в том, что интерпретатор CPython просто неспособен выполнять байткод настолько быстро, чтобы дополнительная нагрузка из-за количества объектов имела значение. Вместо этого вы наиболее вероятно снизите производительность с помощью вызова функции, или за счет громоздких вычислений и т.п. прежде чем сможете заметить дополнительную нагрузку из-за объектов. Есть, конечно же несколько исключений, а именно построение кортежей или словарей из нескольких значений. \n",
      "\r\n",
      "Как конкретный пример накладных расходов можно привести Mercurial имеющий код на С, который парсит низкоуровневые структуры данных. Для большей скорости парсинга код на С выполняется на порядок быстрее, чем это делает CPython. Но как только код на C создает PyObject для представления результата, скорость падает в несколько раз. Другими словами, нагрузка связана с созданием и управлением элементами Python, чтобы они могли быть использованы в коде.\n",
      "\r\n",
      "Способ обойти эту проблему – плодить меньше элементов в Python. Если вам нужно обратиться к единственному элементу, то пускай функция его и возвращает, а не кортеж или словарь из N элементов. Тем не менее, не переставайте следить за возможной нагрузкой из-за вызова функций!\n",
      "\r\n",
      "Если у вас есть много кода, который работает достаточно быстро с использованием CPython C API, и элементы, которые должны быть распределены между различными модулями, обойдитесь без типов Python, которые представляют различные данные как структуры С и имеют уже скомпилированный код для доступа к этим структурам вместо того, чтобы проходить через CPython C API. Избегая CPython C API для доступа к данным, вы избавитесь от большого объема лишней нагрузки. \n",
      "\r\n",
      "Рассматривать элементы как данные (вместо того, чтобы иметь функции для доступа ко всему подряд) будет лучшим подходом для питониста. Другой обходной путь для уже скомпилированного кода – это ленивое создание экземпляров PyObject. Если вы создаете пользовательский тип в Python (PyTypeObject) для представления сложных элементов, вам необходимо определить поля tp_members или же tp_getset для создания пользовательских функций на С для поиска значение для атрибута. Если вы, скажем, пишите парсер и знаете, что заказчики получат доступ только к подмножеству проанализированных полей, то сможете быстро создать тип, содержащий необработанные данные, вернуть этот тип и вызвать функцию на С для поиска атрибутов Python, которая обрабатывает PyObject. Вы можете даже отложить парсинг до момента вызова функции, чтобы сэкономить ресурсы в случае, если парсинг никогда не понадобится! Эта техника достаточно редкая, поскольку она требует написания нетривиального кода, однако она дает положительный результат.\n",
      "\n",
      "Предварительное определение размера коллекции\r\n",
      " Это относится к CPython C API. \n",
      "\r\n",
      "Во время создание коллекций, таких как списки или словари, используйте PyList_New() + PyList_SET_ITEM(), чтобы заполнить новую коллекцию, если ее размер уже определен на момент создания. Это предварительно определит размер коллекции, чтобы иметь возможность держать в ней конечное число элементов. Это помогает пропустить проверки на достаточный размер коллекции при вставке элементов. При создании коллекции из тысячи элементов это поможет сэкономить немного ресурсов!\n",
      "\n",
      "Использование Zero-copy в C API\r\n",
      " В Python C API действительно больше нравится создавать копии объектов, чем возвращать ссылки на них. Например, PyBytes_FromStringAndSize() копирует char* в память зарезервированную Python. Если вы делаете это для большого количества значений или больших данных, то мы могли бы говорить о гигабайтах ввода-вывода из памяти и связанной с этим нагрузкой на распределитель (allocator). \n",
      "\r\n",
      "Если вам нужно написать высокопроизводительный код без C API, то вам следует ознакомиться с buffer protocol и соответствующими типами, такими как memoryview. \n",
      "\n",
      " Buffer protocol встроен в типы Python и позволяет интерпретаторам приводить тип из/к байтам. Он также позволяет интерпретатору кода на С получать дескриптор void* определенного размера. Это позволяет связать любой адрес в памяти с PyObject. Многие функции, работающие с бинарными данными прозрачно принимают любой объект, реализующий buffer protocol. И если вы хотите принять любой объект, который может быть рассмотрен как байты, то вам необходимо использовать единицы формата s*, y* или w* при получении аргументов функции. \n",
      "\r\n",
      "Используя buffer protocol, вы даете интерпретатору лучшую из имеющихся возможностей использовать zero-copy операции и отказаться от копирования лишних байтов в память. \n",
      "\r\n",
      "С помощью типов в Python вида memoryview, вы также позволите Python обращаться к уровням памяти по ссылке, вместо создания копий.\n",
      "\r\n",
      "Если у вас есть гигабайты кода, которые проходят через вашу программу на Python, то проницательное использование типов Python, которые поддерживают zero-copy, избавят вас от разницы в производительности. Однажды я заметил, что python-zstandard оказался быстрее, чем какие-нибудь биндинги Python LZ4 (хотя должно быть наоборот), поскольку я слишком интенсивно использовал buffer protocol и избегал чрезмерного ввода-вывода из памяти в python-zstandard!\n",
      "\n",
      "Заключение\r\n",
      " В этой статье я стремился рассказать о некоторых вещах, которые я узнал, пока оптимизировал свои программы на Python в течение нескольких лет. Повторюсь и скажу, что она не является ни в какой мере всесторонним обзором методов улучшения производительности Python. Признаю, что я возможно использую Python более требовательно, чем другие, и мои рекомендации не могут быть применены ко всем программам. Вы ни в коем случае не должны массово исправлять свой код на Python и убирать, к примеру, поиск атрибутов после прочтения этой статьи. Как всегда, если дело касается оптимизации производительности, то сначала исправьте то, где код работает особенно медленно. Я настоятельно рекомендую py-spy для профилирования приложений на Python. Тем не менее, не стоит забывать про время, затрачиваемое на низкоуровневую активность в Python, такую как вызов функций и поиск атрибутов. Таким образом, если у вас есть известный вам жесткий цикл, то поэкспериментируйте с предложениями из этой статьи и посмотрите, сможете ли вы заметить улучшение! \n",
      "\r\n",
      "Наконец, эта статья не должна быть интерпретирована как наезд на Python и его общую производительность. Да, вы можете привести аргументы в пользу того, что Python должен или не должен использоваться в тех или иных ситуациях из-за особенностей производительности. Однако вместе с этим Python довольно универсален – особенно в связке с PyPy, который обеспечивает исключительную производительность для динамического языка программирования. Производительность Python возможно кажется достаточно хорошей большинству людей. Хорошо это или плохо, но я всегда использовал Python для кейсов, которые выделяются на фоне других. Здесь мне захотелось поделиться своим опытом, чтобы другим стало чуть понятнее какой может быть жизнь «на боевом рубеже». И может быть, только может быть, я смогу сподвигнуть умных людей, которые используют дистрибутивы Python, подумать о проблемах, с которыми я столкнулся, более подробно и представить лучшие решения. \n",
      "\r\n",
      "По устоявшейся традиции ждем ваши комментарии ;-)    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Продолжение. Начало в «Python как предельный случай C++. Часть 1/2».\n",
      "Переменные и типы данных\n",
      "Теперь, когда мы окончательно разобрались с математикой, давайте определимся, что в нашем языке должны означать переменные.\n",
      "В С++ у программиста есть выбор: использовать автоматические переменные, размещаемые в стеке, или держать значения в памяти данных программы, помещая в стек только указатели на эти значения. Что, если мы выберем для Python только одну из этих опций?\n",
      "Разумеется, мы не можем всегда использовать только значения переменных, так как большие структуры данных не поместятся в стек, либо их постоянное перемещение по стеку создаст проблемы с производительностью. Поэтому мы будем использовать в Python только указатели. Это концептуально упростит язык.\n",
      "Таким образом, выражение\n",
      "a = 3\n",
      "будет означать то, что мы создали в памяти данных программы (так называемой «куче») объект «3» и сделали имя “a” ссылкой на него. А выражение\n",
      "b = a\n",
      "в таком случае будет означать, что мы заставили переменную “b” ссылаться на тот же объект в памяти, на который ссылается “a”, иначе говоря − скопировали указатель.\n",
      "Если всё является указателем, то сколько списочных типов нам нужно реализовать в нашем языке? Разумеется, только один − список указателей! Вы можете использовать его для хранения целых, строк, других списков, чего угодно − ведь всё это указатели.\n",
      "Сколько типов хэш-таблиц нам нужно реализовать? (В Python этот тип принято называть «словарём» − dict.) Один! Пусть он связывает указатели на ключи с указателями на значения.\n",
      "Таким образом, нам не нужно реализовывать в нашем языка огромную часть спецификации C++ − шаблоны, поскольку все операции мы производим над объектами, а объекты всегда доступны по указателю. Конечно же, программы, написанные на Python, не обязаны ограничиваться работой с указателями: существуют библиотеки вроде NumPy, при помощи которых учёные работают с массивами данных в памяти, как они бы делали это в Fortran. Но основа языка − выражения вроде “a = 3” − всегда работают с указателями.\n",
      "Концепция «всё является указателем» также упрощает до предела композицию типов. Хотите список словарей? Просто создайте список и поместите туда словари! Не нужно спрашивать у Python разрешения, не нужно объявлять дополнительные типы, всё работает «из коробки».\n",
      "А что, если мы хотим использовать составные объекты в качестве ключей? Ключ в словаре должен иметь неизменяемое значение, иначе как искать значения по нему? Списки могут изменяться, поэтому их нельзя использовать в данном качестве. Для подобных ситуаций в Python есть тип данных, который, аналогично списку, является последовательностью объектов, но, в отличие от списка, последовательность эта не изменяется. Этот тип называется кортеж или tuple (произносится как «тьюпл» или «тапл»).\n",
      "Кортежи в Python решают давнюю проблему скриптовых языков. Если вас не впечатляет эта возможность, то вы, наверное, никогда не пытались использовать для серьёзной работы с данными скриптовые языки, в которых в качестве ключа в хэш-таблицах можно использовать только строки или только примитивные типы.\n",
      "Другая возможность, которую дают нам кортежи − возврат из функции нескольких значений без необходимости объявлять для этого дополнительные типы данных, как это приходится делать в C и C++. Более того, чтобы было проще пользоваться данной возможностью, оператор присваивания был наделён возможностью автоматически распаковывать кортежи в отдельные переменные.\n",
      "def get_address():\n",
      "    ...\n",
      "    return host, port\n",
      "\n",
      "host, port = get_address()\n",
      "У распаковки есть несколько полезных побочных эффектов, например, обмен переменных значениями можно записать так:\n",
      "x, y = y, x\n",
      "Всё является указателем, значит, функции и типы данных могут использоваться как данные. Если вы знакомы с книгой «Паттерны проектирования» за авторством «Банды четырёх», вы должны помнить, какие сложные и запутанные способы она предлагает для того, чтобы параметризовать выбор типа объекта, создаваемого вашей программой во время выполнения. Действительно, во многих языках программирования это сложно сделать! В Python все эти сложности улетучиваются, поскольку мы знаем, что функция может вернуть тип данных, что и функции, и типы данных − это просто ссылки, а ссылки можно хранить, например, в словарях. Это упрощает задачу до предела.\n",
      "Дэвид Вилер говорил: «Все проблемы в программировании решаются путём создания дополнительного уровня косвенности». Использование ссылок в Python − это тот уровень косвенности, который традиционно применяется для решения множества проблем во многих языках, в том числе и в C++. Но если там он используется явно, и это приводит к усложнению программ, то в Python он используется неявно, единоообразно в отношении данных всех типов, и дружественно к пользователю.\n",
      "Но если всё является ссылками, то на что ссылаются эти ссылки? В языках вроде C++ есть множество типов. Давайте оставим в Python только один тип данных − объект! Специалисты в области теории типов неодобрительно качают головами, но я считаю, что один исходный тип данных, от которого производятся все остальные типы в языке − это хорошая идея, обеспечивающая единообразность языка и простоту его использования.\n",
      "Что касается конкретного содержимого памяти, то различные реализации Python (PyPy, Jython или MicroPython) могут управлять памятью по-разному. Но, чтобы лучше понять, как именно реализуется простота и единообразность Python, сформировать правильную ментальную модель, лучше обратиться к эталонной реализации Python на языке C, называемой CPython, которую мы можем загрузить на сайте python.org.\n",
      "struct {\n",
      "    struct _typeobject *ob_type;\n",
      "    /* followed by object’s data */\n",
      "}\n",
      "То, что мы увидим в исходном коде CPython − это структура, которая состоит из указателя на информацию о типе данной переменной и полезной нагрузки, которая определяет конкретное значение переменной.\n",
      "Как же устроена информация о типе? Снова углубимся в исходный код CPython.\n",
      "struct _typeobject {\n",
      "    /* ... */\n",
      "    getattrfunc tp_getattr;\n",
      "    setattrfunc tp_setattr;\n",
      "    /* ... */\n",
      "    newfunc tp_new;\n",
      "    freefunc tp_free;\n",
      "    /* ... */\n",
      "    binaryfunc nb_add;\n",
      "    binaryfunc nb_subtract;\n",
      "    /* ... */\n",
      "    richcmpfunc tp_richcompare;\n",
      "    /* ... */\n",
      "}\n",
      "Мы видим указатели на функции, которые обеспечивают выполнение всех операций, которые возможны для данного типа: сложение, вычитание, сравнение, доступ к атрибутам, индексирование, слайсинг и т. д. Эти операции знают, как работать с полезной нагрузкой, которая расположена в памяти ниже указателя на информацию о типе, будь то целое число, строка или объект типа, созданного пользователем.\n",
      "Это радикальным образом отличается от C и C++, в которых информация о типе ассоциируется с именами, а не со значениями переменных. В Python все имена ассоциированы со ссылками. Значение по ссылке, в свою очередь, имеет тип. В этом и заключается суть динамических языков.\n",
      "Чтобы реализовать все возможности языка, нам достаточно определить две операции над ссылками. Одна из них наиболее очевидна − это копирование. Когда мы присваиваем значение переменнной, слоту в словаре или атрибуту объекта, мы копируем ссылки. Это простая, быстрая и совершенно безопасная операция: копирование ссылок не изменяет содержимое объекта.\n",
      "Вторая операция − это вызов функции или метода. Как мы показали выше, программа на Python может взаимодействовать с памятью только посредством методов, реализованных во встроенных объектах. Поэтому она не может вызвать ошибку, связанную с обращением к памяти.\n",
      "У вас может возникнуть вопрос: если все переменные содержат ссылки, то как я могу защитить от изменений значение пременной, передав её функции как параметр?\n",
      "n = 3\n",
      "some_function(n)\n",
      "# Q: I just passed a pointer!\n",
      "# Could some_function() have changed “3”?\n",
      "Ответ заключается в том, что простые типы в Python являются неизменяемыми: в них попросту не реализован тот метод, который отвечает за изменение их значения. Неизменяемые (иммутабельные) int, float, tuple или str обеспечивают в языках типа «всё является указателем» тот же семантический эффект, который в C обеспечивают автоматические переменные.\n",
      "Унифицированные типы и методы максимально упрощают применение обобщённого программирования, или дженериков. Функции min(), max(), sum() и им подобные являются встроенными, нет нужды их импортировать. И они работают с любыми типами данных, в которых реализованы операции сравнения для min() и max(), сложения для sum() и т. д.\n",
      "Создание объектов\n",
      "Мы выяснили в общих чертах, как должны вести себя объекты. Теперь определим, как мы будем их создавать. Это − вопрос синтаксиса языка. C++ поддерживает как минимум три способа создания объекта:\n",
      "\n",
      "Автоматический, объявлением переменной данного класса:\n",
      "my_class c(arg);\n",
      "С помощью оператора new:\n",
      "my_class *c = new my_class(arg);\n",
      "Фабричный, при помощи вызова произвольной функции, возвращающей указатель:\n",
      "my_class *c = my_factory(arg);\n",
      "\n",
      "Как вы уже, наверное, догадались, изучив способ мышления создателей Python на вышеприведённых примерах, теперь мы должны выбрать один из них.\n",
      "Из той же книги «Банды четырёх» мы узнали, что фабрика − это самый гибкий и универсальный способ создания объектов. Поэтому в Python реализован только этот способ.\n",
      "Помимо универсальности, этот способ хорош тем, что для его обеспечения не нужно перегружать язык лишним синтаксисом: вызов функции уже реализован в нашем языке, а фабрика − это не что иное, как функция.\n",
      "Другое правило создания объектов в Python таково: любой тип данных является собственной фабрикой. Конечно, вы можете написать сколько угодно дополнительных, кастомных фабрик (которые будут являться обычными функциями или методами, конечно же), но общее правило останется в силе:\n",
      "# Let’s make type objects\n",
      "# their own type’s factories!\n",
      "c = MyClass()\n",
      "i = int('7')\n",
      "f = float(length)\n",
      "s = str(bytes)\n",
      "Все типы являются вызываемыми объектами, и все они возвращают значения своего типа, определяемые аргументами, переданными при вызове.\n",
      "Таким образом, с использованием только базового синтаксиса языка, могут быть инкапсулированы любые манипуляции при создании объектов, вроде паттернов «Арена» или «Приспособленец», поскольку ещё одна замечательная идея, позаимствованная из C++, заключается в том, что тип сам определяет, как происходит порождение его объектов, как оператор new работает для него.\n",
      "Как насчёт NULL?\n",
      "Обработка пустого указателя добавляет программе сложности, так что мы объявим NULL вне закона. Синтакс Python не даёт возможности создать нулевой указатель. Две элементарные операции над указателями, о которых мы говорили ранее, определены таким образом, что любая переменная указывает на какой-то объект.\n",
      "Как следствие этого, пользователь не может средствами языка Python создать ошибку, связанную с обращением к памяти, типа ошибки сегментации или выхода за границы буфера. Иными словами, программы на Python не подвержены двум самым опасным типам уязвимостей, которые угрожают безопасности Интернета в течение последних 20 лет.\n",
      "Вы можете спросить: «Если структура операций над объектами неизменна, как мы видели ранее, то как же пользователи будут создавать собственные классы, с методами и атрибутами, не перечисленными в этой структуре?»\n",
      "Магия заключена в том, что для пользовательских классов Python имеет очень простую «заготовку» с небольшим числом реализованных методов. Вот самые важные из них:\n",
      "struct _typeobject {\n",
      "    getattrfunc tr_getattr;\n",
      "    setattrfunc tr_setattr;\n",
      "    /* ... */\n",
      "    newfunc tp_new;\n",
      "    /* ... */\n",
      "}\n",
      "tp_new() создаёт для пользовательского класса хэш-таблицу, такую же, как для типа dict. tp_getattr() извлекает что-то из этой хэш-таблицы, а tp_setattr(), наоборот, что-то туда кладёт. Таким образом, способность произвольных классов хранить любые методы и атрибуты обеспечивается не на уровне структур языка C, а уровнем выше − хэш-таблицей. (Разумеется, за исключением некоторых случаев, связанных с оптимизацией производительности.)\n",
      "Модификаторы доступа\n",
      "Что же нам делать со всеми теми правилами и концепциями, которые в C++ построены вокруг ключевых слов private и protected? Python, будучи скриптовым языком, не нуждается в них. У нас уже есть «защищённые» части языка − это данные встроенных типов. Ни при каких условиях Python не позволит программе, например, манипулировать битами числа с плавающей запятой! Этого уровня инкапсуляции вполне достаточно, чтобы поддержать целостность самого языка. Мы, создатели Python, считаем, что целостность языка − это единственный хороший предлог для сокрытия информации. Все остальные структуры и данные пользовательской программы считаются публичными.\n",
      "Вы можете написать символ подчёркивания (_) в начале имени атрибута класса, чтобы предупредить коллегу: на этот атрибут не стоит полагаться. Но в остальном Python выучил уроки начала 90-х: тогда многие верили в то, что основной причиной того, что мы пишем раздутые, нечитаемые и забагованные программы, является недостаток приватных переменных. Думаю, следующие 20 лет убедили всех в индустрии программирования: приватные переменные − это не единственное, и далеко не самое эффективное средство от раздутых и забагованных программ. Поэтому создатели Python решили даже не беспокоиться по поводу приватных переменных, и, как видите, не прогадали.\n",
      "Управление памятью\n",
      "Что же происходит с нашими объектами, числами и строками на более низком уровне? Как именно они размещаются в памяти, как CPython обеспечивает совместный доступ к ним, когда и при каких условиях они уничтожаются?\n",
      "И в этом случае мы выбрали наиболее общий, предсказуемый и производительный способ работы с памятью: со стороны C-программы все наши объекты − это разделяемые указатели.\n",
      "С учётом этого знания те структуры данных, которые мы рассмотрели ранее, в части «Переменные и типы данных», должны быть дополнены следующим образом:\n",
      "struct {\n",
      "    Py_ssize_t ob_refcnt;\n",
      "    struct {\n",
      "       struct _typeobject *ob_type;\n",
      "        /* followed by object’s data */\n",
      "    }\n",
      "}\n",
      "Итак, каждый объект в Python (мы имеем в виду реализацию CPython, разумеется) имеет свой счётчик ссылок. Как только он становится равным нулю, объект может быть удалён.\n",
      "Механизм подсчёта ссылок не опирается на дополнительные вычисления или фоновые процессы − объект может быть уничтожен мгновенно. Кроме того, он обеспечивает высокую локальность данных: зачастую память снова начинает использоваться сразу после освобождения. Только что уничтоженный объект, скорее всего, недавно использовался, а значит, находился в кэше процессора. Поэтому и только что созданный объект останется в кэше. Эти два фактора − простота и локальность − делают подсчёт ссылок очень производительным способом сборки мусора.\n",
      "(Из-за того, что объекты в реальных программах нередко ссылаются друг на друга, счётчик ссылок в определённых случаях не может опуститься до нуля, даже когда объекты больше не используются в программе. Поэтому в CPython есть и второй механизм сбора мусора − фоновый, основанный на поколениях объектов. − прим. перев.)\n",
      "Ошибки разработчиков Python\n",
      "Мы старались разработать язык, который будет достаточно прост для новичков, но и достаточно привлекателен для профессионалов. При этом нам не удалось избежать ошибок в понимании и использовании инструментов, которые мы сами и создали.\n",
      "Python 2 из-за инерции мышления, связанной со скриптовыми языками, пытался преобразовывать строковые типы, как делал бы это язык с нестрогой типизацией. Если вы попытаетесь объединить байтовую строку со строкой в Unicode, интерпретатор неявно преобразует байтовую строку в Unicode при помощи той кодовой таблицы, которая имеется в данной системе, и представит результат в Unicode:\n",
      ">>> 'byte string ' + u'unicode string'\n",
      "u'byte string unicode string'\n",
      "В результате некоторые веб-сайты отлично работали, пока их пользователи использовали английский язык, но выдавали загадочные ошибки при использовании символов других алфавитов.\n",
      "Эта ошибка проектирования языка была исправлена в Python 3:\n",
      ">>> b'byte string ' + u'unicode string'\n",
      "TypeError: can't concat bytes to str\n",
      "Похожая ошибка в Python 2 была связана с «наивной» сортировкой списков, состоящих из несравнимых элементов:\n",
      ">>> sorted(['b', 1, 'a', 2])\n",
      "[1, 2, 'a', 'b']\n",
      "Python 3 в этом случае даёт пользователю понять, что тот пытается сделать что-то не слишком осмысленное:\n",
      ">>> sorted(['b', 1, 'a', 2])\n",
      "TypeError: unorderable types: int() < str()\n",
      "Злоупотребления\n",
      "Пользователи и сейчас иногда злоупотребляют динамической природой языка Python, а тогда, в 90-х, когда лучшие практики ещё не были широко известны, это происходило особенно часто:\n",
      "class Address(object):\n",
      "    def __init__(self, host, port):\n",
      "         self.host = host\n",
      "         self.port = port\n",
      "«Но это же неоптимально!» − говорили некоторые, − «Что, если порт не отличается от дефолтного значения? Мы всё равно тратим на его хранение целый атрибут класса!» И в результате получалось что-то вроде\n",
      "class Address(object):\n",
      "    def __init__(self, host, port=None):\n",
      "        self.host = host\n",
      "        if port is not None:  # so terrible\n",
      "            self.port = port\n",
      "Так в программе появляются объекты одного типа, с которыми, тем не менее, нельзя работать единообразно, так как одни из них имеют некий атрибут, а другие − нет! И мы не можем прикоснуться к этому атрибуту, не проверив заранее его наличие:\n",
      "# code was forced to use introspection\n",
      "# (terrible!)\n",
      "if hasattr(addr, 'port'):\n",
      "    print(addr.port)\n",
      "В настоящее время обилие hasattr(), isinstance() и прочей интроспекции является верным признаком плохого кода, а лучшей практикой считается делать атрибуты всегда присутствующими в объекте. Это обеспечивает более простой синтаксис при обращении к нему:\n",
      "# today’s best practice:\n",
      "# every atribute always present\n",
      "if addr.port is not None:\n",
      "    print(addr.port)\n",
      "Так, ранние эксперименты с динамически добавляемыми и удаляемыми атрибутами завершились, и теперь мы рассматриваем классы в Python примерно так же, как и в C++.\n",
      "Другой дурной привычкой раннего Python было использование функций, в которых аргумент может иметь совершенно разные типы. Например, вы можете подумать, что для пользователя может быть слишком сложно создавать каждый раз список с именами колонок, и сто́ит разрешить ему передавать их также в виде одной строки, где имена отдельных колонок разделены, скажем, запятой:\n",
      "class Dataframe(object):\n",
      "    def __init__(self, columns):\n",
      "        if isinstance(columns, str):\n",
      "            columns = columns.split(',')\n",
      "        self.columns = columns\n",
      "Но такой подход может породить свои проблемы. Например, что, если пользователь случайно передаст нам строку, которая не предназначена для того, чтобы быть использована как список имён колонок? Или если имя колонки должно содержать запятую?\n",
      "Также такой код сложнее поддерживать, отлаживать, и особенно тестировать: в тестах может быть предусмотрена проверка только одного из двух поддерживаемых нами типов, но покрытие всё равно составит 100%, и мы не протестируем другой тип.\n",
      "В итоге мы пришли к тому, что Python даёт пользователю возможность передавать функции аргументы какого угодно типа, но большинство из них в большинстве ситуаций будут использовать функцию так же, как они делали бы это в C: передавать ей аргумент одного типа.\n",
      "Необходимость использования eval() в программе считается явным архитектурным просчётом. Скорее всего, вы просто не сообразили, как сделать то же самое нормальным способом. Но в некоторых случаях − например, если вы пишете программу типа Jupyter notebook или онлайн-песочницу для запуска и тестирования пользовательского кода − использование eval() вполне оправдано, и в этом типе задач Python проявляет себя великолепно! Действительно, реализовать нечто подобное на C++ было бы намного сложнее.\n",
      "Как мы уже показали выше, интроспекция (getattr(), hasattr(), isinstance()) не всегда является хорошим средством для выполнения типичных пользовательских задач. Но эти возможности, тем не менее, встроены в язык, и они просто сверкают в ситуациях, когда наш код должен описывать сам себя: логгирование, тестирование, статическая проверка, отладка!\n",
      "Эра консолидации\n",
      "В заключение мне хочется отметить следующее: мы живём в такое время, когда лучшие практики разработки на различных языках проявляют тенденцию к консолидации. 20 лет назад я не смог бы даже упомянуть разделяемые указатели в контексте того, что объединяет C++ и Python. А сегодня сообщества, сформировавшиеся вокруг разных языков программирования, свободно обмениваются лучшими практиками. И это изменение произошло в течение девяностых и нулевых.\n",
      "Чтобы получить количественные измерения в подтверждение моей гипотезы, я мониторил использование shared_ptr в TensorFlow примерно с 2016 по 2018 год.\n",
      "TensorFlow − это большой и во многом образцовый C++-проект, но большинство программистов знают его лишь в качестве Python-библиотеки (а C++ − в качестве сборочной системы TensorFlow, наверное).\n",
      "\n",
      "На диаграмме по вертикали изображено соотношение строк кода TensorFlow, использующих shared_ptr, к общему числу строк кода. В лучших традициях Кремниевой долины, этот график направлен строго вверх.\n",
      "Так куда же направляется современный C++? В начале мы говорили о предельных случаях. Что происходит на графике, который мы видим? Если время стремится к бесконечности, то всё становится разделёнными указателями, и C++ становится Python!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " О Python\r\n",
      "Python — это интерпретируемый, объектно-ориентированный язык программирования высокого уровня с динамической семантикой. Встроенные структуры данных высокого уровня в сочетании с динамической типизацией и динамическим связыванием делают его очень привлекательным для БРПС (быстрой разработки прикладных средств), а также для использования в качестве скриптового и связующего языка для подключения существующих компонентов или сервисов. Python поддерживает модули и пакеты, тем самым поощряя модульность программы и повторное использование кода.\n",
      "\n",
      "О данной статье\r\n",
      "Простота и легкость в освоении данного языка может ввести разработчиков в заблуждение (особенно тех, кто еще только начинает изучать Python), так что можно упустить из виду некоторые важные тонкости и недооценить силу разнообразия возможных решений с помощью Python.\n",
      "\r\n",
      "Имея это в виду, в этой статье представлен «топ-10» тонких, трудных для обнаружения ошибок, которые могут допустить даже продвинутые разработчики Python.\n",
      "\n",
      "Ошибка № 1: неправильное использование выражений в качестве значений по умолчанию для аргументов функций\r\n",
      "Python позволяет указывать, что у функции могут быть необязательные аргументы, путем задания для них значения по умолчанию. Это, конечно, очень удобная особенность языка, но может привести к неприятным последствиям, если тип такого значения будет изменяемым. Например, рассмотрим следующее определение функции:\n",
      "\n",
      ">>> def foo(bar=[]):        # bar - это необязательный аргумент \n",
      "                            # и по умолчанию равен пустому списку.\n",
      "...    bar.append(\"baz\")    # эта строка может стать проблемой...\n",
      "...    return bar\r\n",
      "Распространенная ошибка в данном случае — это думать, что значение необязательного аргумента будет устанавливаться в значение по умолчанию каждый раз, как функция будет вызываться без значения для этого аргумента. В приведенном выше коде, например, можно предположить, что повторно вызывая функцию foo() (то есть без указания значения для агрумента bar), она всегда будет возвращать «baz», поскольку предполагается, что каждый раз, когда вызывается foo () (без указания аргумента bar), bar устанавливается в [ ] (т. е. новый пустой список).\n",
      "\r\n",
      "Но давайте посмотрим что же будет происходить на самом деле:\n",
      "\n",
      ">>> foo()\n",
      "[\"baz\"]\n",
      ">>> foo()\n",
      "[\"baz\", \"baz\"]\n",
      ">>> foo()\n",
      "[\"baz\", \"baz\", \"baz\"]\r\n",
      "А? Почему функция продолжает добавлять значение по умолчанию «baz» к существующему списку каждый раз, когда вызывается foo(), вместо того, чтобы каждый раз создавать новый список?\n",
      "\r\n",
      "Ответом на данный вопрос будет более глубокое понимание того, что творится у Python «под капотом». А именно: значение по умолчанию для функции инициализируется только один раз, во время определения функции. Таким образом, аргумент bar инициализируется по умолчанию (т. е. пустым списком) только тогда, когда foo() определен впервые, но последующие вызовы foo() (т. е. без указания аргумента bar) продолжат использовать тот же список, который был создан для аргумента bar в момент первого определения функции.\n",
      "\r\n",
      "Для справки, распространенным «обходным путем» для этой ошибки является следующее определение:\n",
      "\n",
      ">>> def foo(bar=None):\n",
      "...    if bar is None:\t\t# or if not bar:\n",
      "...        bar = []\n",
      "...    bar.append(\"baz\")\n",
      "...    return bar\n",
      "...\n",
      ">>> foo()\n",
      "[\"baz\"]\n",
      ">>> foo()\n",
      "[\"baz\"]\n",
      ">>> foo()\n",
      "[\"baz\"]\n",
      "Ошибка № 2: неправильное использование переменных класса\r\n",
      "Рассмотрим следующий пример:\n",
      "\n",
      ">>> class A(object):\n",
      "...     x = 1\n",
      "...\n",
      ">>> class B(A):\n",
      "...     pass\n",
      "...\n",
      ">>> class C(A):\n",
      "...     pass\n",
      "...\n",
      ">>> print A.x, B.x, C.x\n",
      "1 1 1\r\n",
      "Вроде все в порядке.\n",
      "\n",
      ">>> B.x = 2\n",
      ">>> print A.x, B.x, C.x\n",
      "1 2 1\r\n",
      "Ага, все как и ожидалось.\n",
      "\n",
      ">>> A.x = 3\n",
      ">>> print A.x, B.x, C.x\n",
      "3 2 3\r\n",
      "Что за черт?! Мы же только изменили A.x. Почему же C.x тоже изменилось?\n",
      "\r\n",
      "В Python переменные класса обрабатываются как словари и следуют тому, что часто называют Порядком разрешения методов (MRO). Таким образом, в приведенном выше коде, поскольку атрибут x не найден в классе C, он будет найден в его базовых классах (только A в приведенном выше примере, хотя Python поддерживает множественное наследование). Другими словами, C не имеет своего собственного свойства x, независимого от A. Таким образом, ссылки на C.x фактически являются ссылками на A.x. Это будет вызывать проблемы, если не обрабатывать такие случаи должным образом. Так что при изучении Python обратите особое внимание на аттрибуты класса и работу с ними.\n",
      "\n",
      "Ошибка № 3: неправильное указание параметров для блока исключения\r\n",
      "Предположим, что у вас есть следующий кусок кода:\n",
      "\n",
      ">>> try:\n",
      "...     l = [\"a\", \"b\"]\n",
      "...     int(l[2])\n",
      "... except ValueError, IndexError:  # To catch both exceptions, right?\n",
      "...     pass\n",
      "...\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 3, in <module>\n",
      "IndexError: list index out of range\r\n",
      "Проблема здесь заключается в том, что выражение except не принимает список исключений, указанных таким образом. Скорее, в Python 2.x выражение «except Exception, e» используется для привязки исключения к необязательному второму заданному второму параметру (в данном случае e), чтобы сделать его доступным для дальнейшей проверки. В результате в приведенном выше коде исключение IndexError не перехватывается выражением except; скорее, вместо этого исключение заканчивается привязкой к параметру с именем IndexError.\n",
      "\r\n",
      "Правильный способ перехвата нескольких исключений с помощью выражения except — указать первый параметр в виде кортежа, содержащего все исключения, которые нужно перехватить. Кроме того, для максимальной совместимости используйте ключевое слово as, так как этот синтаксис поддерживается как в Python 2, так и в Python 3:\n",
      "\n",
      ">>> try:\n",
      "...     l = [\"a\", \"b\"]\n",
      "...     int(l[2])\n",
      "... except (ValueError, IndexError) as e:  \n",
      "...     pass\n",
      "...\n",
      ">>>\n",
      "Ошибка № 4: непонимание правил области видимости Python\r\n",
      "Область видимости в Python основана на так называемом правиле LEGB, которое является аббревиатурой Local (имена, назначенные любым способом внутри функции (def или lambda), и не объявленные глобальными в этой функции), Enclosing (имя в локальной области действия любых статически включающих функций (def или lambda), от внутреннего к внешнему), Global (имена, назначенные на верхнем уровне файла модуля, или путем выполнения global инструкции в def внутри файла), Built-in (имена, предварительно назначенные в модуле встроенных имен: open, range, SyntaxError ,...). Кажется достаточно просто, верно? Ну, на самом деле, есть некоторые тонкости в том, как это работает в Python, что подводит нас к общей более сложной проблеме программирования на Python ниже. Рассмотрим следующей пример: \n",
      "\n",
      ">>> x = 10\n",
      ">>> def foo():\n",
      "...     x += 1\n",
      "...     print x\n",
      "...\n",
      ">>> foo()\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 2, in foo\n",
      "UnboundLocalError: local variable 'x' referenced before assignment\r\n",
      "В чем проблема?\n",
      "\r\n",
      "Вышеуказанная ошибка возникает потому, что, когда вы присваиваете переменную в области видимости, Python автоматически считает ее локальной для этой области и скрывает любую переменную с аналогичным именем в любой вышестоящей области.\n",
      "\r\n",
      "Таким образом, многие удивляются, когда получают UnboundLocalError в ранее работающем коде, когда он модифицируется путем добавления оператора присваивания где-нибудь в теле функции.\n",
      "\r\n",
      "Эта особенность особенно сбивает разработчиков с толку при использовании списков. Рассмотрим следующий пример:\n",
      "\n",
      ">>> lst = [1, 2, 3]\n",
      ">>> def foo1():\n",
      "...     lst.append(5)   # Это работает нормально...\n",
      "...\n",
      ">>> foo1()\n",
      ">>> lst\n",
      "[1, 2, 3, 5]\n",
      "\n",
      ">>> lst = [1, 2, 3]\n",
      ">>> def foo2():\n",
      "...     lst += [5]      # ... а вот это падает!\n",
      "...\n",
      ">>> foo2()\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"<stdin>\", line 2, in foo\n",
      "UnboundLocalError: local variable 'lst' referenced before assignment\r\n",
      "А? Почему foo2 падает, в то время как foo1 работает нормально?\n",
      "\r\n",
      "Ответ такой же, как в предыдущем примере, но, по распространенному мнению, здесь ситуация более тонкая. foo1 не применяет оператор присваивания к lst, тогда как foo2 — да. Помня, что lst + = [5] на самом деле является просто сокращением для lst = lst + [5], мы видим, что мы пытаемся присвоить значение lst (поэтому Python предполагает, что он находится в локальной области видимости). Однако значение, которое мы хотим присвоить lst, основано на самом lst (опять же, теперь предполагается, что он находится в локальной области видимости), который еще не был определен. И мы получаем ошибку.\n",
      "\n",
      "Ошибка № 5: изменение списка во время итерации по нему\r\n",
      "Проблема в следующем куске кода должна быть достаточно очевидной:\n",
      "\n",
      ">>> odd = lambda x : bool(x % 2)\n",
      ">>> numbers = [n for n in range(10)]\n",
      ">>> for i in range(len(numbers)):\n",
      "...     if odd(numbers[i]):\n",
      "...         del numbers[i]  # BAD: Deleting item from a list while iterating over it\n",
      "...\n",
      "Traceback (most recent call last):\n",
      "  \t  File \"<stdin>\", line 2, in <module>\n",
      "IndexError: list index out of range\r\n",
      "Удаление элемента из списка или массива во время итерации по нему — это проблема Python, которая хорошо известна любому опытному разработчику программного обеспечения. Но, хотя приведенный выше пример может быть достаточно очевидным, даже опытные разработчики могут встать на эти грабли в гораздо более сложном коде.\n",
      "\r\n",
      "К счастью, Python включает в себя ряд элегантных парадигм программирования, которые при правильном использовании могут привести к значительному упрощению и оптимизации кода. Дополнительным приятным следствием этого является то, что в более простом коде вероятность попасться на ошибку случайного удаления элемента списка во время итерации по нему значительно меньше. Одна из таких парадигм — генераторы списков. Кроме того, понимание работы генераторов списков особенно полезны для избежания этой конкретной проблемы, как показано в этой альтернативной реализацией приведенного выше кода, которая прекрасно работает:\n",
      "\n",
      ">>> odd = lambda x : bool(x % 2)\n",
      ">>> numbers = [n for n in range(10)]\n",
      ">>> numbers[:] = [n for n in numbers if not odd(n)]  # ahh, the beauty of it all\n",
      ">>> numbers\n",
      "[0, 2, 4, 6, 8]\n",
      "Ошибка № 6: непонимание того, как Python связывает переменные в замыканиях\r\n",
      "Рассмотрим следующий пример:\n",
      "\n",
      ">>> def create_multipliers():\n",
      "...     return [lambda x : i * x for i in range(5)]\n",
      ">>> for multiplier in create_multipliers():\n",
      "...     print multiplier(2)\n",
      "...\r\n",
      "Вы можете ожидать следующий вывод:\n",
      "\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\r\n",
      "Но на самом деле вы получите вот что:\n",
      "\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "\r\n",
      "Сюрприз!\n",
      "\r\n",
      "Это происходит из-за поздней привязки в Python, которое заключается в том, что значения переменных, используемых в замыканиях, ищутся во время вызова внутренней функции. Таким образом, в приведенном выше коде всякий раз, когда вызывается какая-либо из возвращаемых функций, значение i ищется в окружающей области видимости во время ее вызова (а к тому времени цикл уже завершился, поэтому i уже был присвоен конечный результат — значение 4).\n",
      "\r\n",
      "Решение этой распространенной проблемы с Python будет таким:\n",
      "\n",
      ">>> def create_multipliers():\n",
      "...     return [lambda x, i=i : i * x for i in range(5)]\n",
      "...\n",
      ">>> for multiplier in create_multipliers():\n",
      "...     print multiplier(2)\n",
      "...\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\r\n",
      "Вуаля! Мы используем здесь аргументы по умолчанию для генерации анонимных функций для достижения желаемого поведения. Некоторые назвали бы это решение элегантным. Некоторые — \r\n",
      "тонким. Некоторые ненавидят подобные штуки. Но если вы разработчик Python, в любом случае, это важно понимать.\n",
      "\n",
      "Ошибка № 7: создание циклических зависимостей модуля\r\n",
      "Допустим, у вас есть два файла, a.py и b.py, каждый из которых импортирует другой, следующим образом:\n",
      "\r\n",
      "В a.py:\n",
      "\n",
      "import b\n",
      "\n",
      "def f():\n",
      "    return b.x\n",
      "\t\n",
      "print f()\r\n",
      "В b.py:\n",
      "\n",
      "import a\n",
      "\n",
      "x = 1\n",
      "\n",
      "def g():\n",
      "    print a.f()\r\n",
      "Сначала попробуем импортировать a.py:\n",
      "\n",
      ">>> import a\n",
      "1\r\n",
      "Сработало просто отлично. Возможно, это вас удивляет. В конце концов, модули циклически импортируют друг друга и это, вероятно, должено быть проблемой, не так ли?\n",
      "\r\n",
      "Ответ заключается в том, что простое наличие циклического импорта модулей само по себе не является проблемой в Python. Если модуль уже был импортирован, Python достаточно умен, чтобы не пытаться повторно импортировать его. Однако, в зависимости от точки, в которой каждый модуль пытается получить доступ к функциям или переменным, определенным в другом, вы действительно можете столкнуться с проблемами.\n",
      "\r\n",
      "Итак, возвращаясь к нашему примеру, когда мы импортировали a.py, у него не было проблем с импортом b.py, поскольку b.py не требует, чтобы что-либо из a.py было определено во время его импорта. Единственная ссылка в b.py на a — это вызов a.f(). Но этот вызов в g() и ничего в a.py или b.py не вызывает g(). Так что все работает прекрасно.\n",
      "\r\n",
      "Но что произойдет, если мы попытаемся импортировать b.py (без предварительного импорта a.py, то есть):\n",
      "\n",
      ">>> import b\n",
      "Traceback (most recent call last):\n",
      "  \t  File \"<stdin>\", line 1, in <module>\n",
      "  \t  File \"b.py\", line 1, in <module>\n",
      "    import a\n",
      "  \t  File \"a.py\", line 6, in <module>\n",
      "\tprint f()\n",
      "  \t  File \"a.py\", line 4, in f\n",
      "\treturn b.x\n",
      "AttributeError: 'module' object has no attribute 'x'\r\n",
      "Ой-ой. Это не хорошо! Проблема здесь в том, что в процессе импорта b.py он пытается импортировать a.py, который, в свою очередь, вызывает f(), который пытается получить доступ к b.x. Но b.x еще не было определено. Отсюда исключение AttributeError.\n",
      "\r\n",
      "По крайней мере, одно из решений этой проблемы довольно тривиально. Просто измените b.py, чтобы импортировать a.py в g():\n",
      "\n",
      "x = 1\n",
      "\n",
      "def g():\n",
      "    import a\t# This will be evaluated only when g() is called\n",
      "    print a.f()\r\n",
      "Теперь, когда мы его импортируем, все нормально:\n",
      "\n",
      ">>> import b\n",
      ">>> b.g()\n",
      "1\t# Printed a first time since module 'a' calls 'print f()' at the end\n",
      "1\t# Printed a second time, this one is our call to 'g'\n",
      "\n",
      "Ошибка № 8: пересечение имен с именами модулями стандартной библиотеки Python\r\n",
      "Одна из прелестей Python — это множество модулей, которые поставляются «из коробки». Но в результате, если вы сознательно не будете за этим следить, можно столкнуться с тем, что имя вашего модуля может быть с тем же именем, что и модуль в стандартной библиотеке, поставляемой с Python (например, в вашем коде может быть модуль с именем email.py, который будет конфликтовать со модулем стандартной библиотеки с таким же именем).\n",
      "\r\n",
      "Это может привести к серьезным проблемам. Например, если какой-нибудь из модулей будет пытаться импортировать версию модуля из стандартной библиотеки Python, а у вас в проекте будет модуль с таким же именем, который и будет по ошибке импортирован вместо модуля из стандартной библиотеки. \n",
      "\r\n",
      "Поэтому следует проявлять осторожность, чтобы не использовать те же имена, что и в модулях стандартной библиотеки Python. Гораздо проще изменить название модуля в своем проекте, нежели подать запрос на изменение имени модуля в стандартной библиотеке и получить на него одобрение.\n",
      "\n",
      "Ошибка № 9: неспособность учесть различия Python 2 и Python 3\r\n",
      "Рассмотрим следующий файл foo.py:\n",
      "\n",
      "import sys\n",
      "\n",
      "def bar(i):\n",
      "    if i == 1:\n",
      "        raise KeyError(1)\n",
      "    if i == 2:\n",
      "        raise ValueError(2)\n",
      "\n",
      "def bad():\n",
      "    e = None\n",
      "    try:\n",
      "        bar(int(sys.argv[1]))\n",
      "    except KeyError as e:\n",
      "        print('key error')\n",
      "    except ValueError as e:\n",
      "        print('value error')\n",
      "    print(e)\n",
      "\n",
      "bad()\r\n",
      "На Python 2 он отработает нормально:\n",
      "\n",
      "$ python foo.py 1\n",
      "key error\n",
      "1\n",
      "$ python foo.py 2\n",
      "value error\n",
      "2\r\n",
      "Но теперь давайте посмотрим как он будет работать в Python 3:\n",
      "\n",
      "$ python3 foo.py 1\n",
      "key error\n",
      "Traceback (most recent call last):\n",
      "  File \"foo.py\", line 19, in <module>\n",
      "    bad()\n",
      "  File \"foo.py\", line 17, in bad\n",
      "    print(e)\n",
      "UnboundLocalError: local variable 'e' referenced before assignment\r\n",
      "Что здесь только что произошло? «Проблема» в том, что в Python 3 объект в блоке исключения недоступен за его пределами. (Причина этого заключается в том, что в противном случае объекты в этом блоке будут сохраняться в памяти до тех пор, пока сборщик мусора не запустится и не удалит ссылки на них оттуда).\n",
      "\r\n",
      "Один из способов избежать этой проблемы — сохранить ссылку на объект блока исключения за пределами этого блока, чтобы он оставался доступным. Вот версия предыдущего примера, которая использует эту технику, тем самым получая код, который подходит как для Python 2, так и для Python 3:\n",
      "\n",
      "import sys\n",
      "\n",
      "def bar(i):\n",
      "    if i == 1:\n",
      "        raise KeyError(1)\n",
      "    if i == 2:\n",
      "        raise ValueError(2)\n",
      "\n",
      "def good():\n",
      "    exception = None\n",
      "    try:\n",
      "        bar(int(sys.argv[1]))\n",
      "    except KeyError as e:\n",
      "        exception = e\n",
      "        print('key error')\n",
      "    except ValueError as e:\n",
      "        exception = e\n",
      "        print('value error')\n",
      "    print(exception)\n",
      "\n",
      "good()\r\n",
      "Запустим его в Python 3:\n",
      "\n",
      "$ python3 foo.py 1\n",
      "key error\n",
      "1\n",
      "$ python3 foo.py 2\n",
      "value error\n",
      "2\r\n",
      "Ураааа!\n",
      "\n",
      "Ошибка № 10: неправильное использование метода __del__\r\n",
      "Допустим, у вас есть вот такой файл mod.py:\n",
      "\n",
      "import foo\n",
      "\n",
      "class Bar(object):\n",
      "   \t    ...\n",
      "    def __del__(self):\n",
      "        foo.cleanup(self.myhandle)\r\n",
      "И вы пытаетесь сделать вот такое из другого another_mod.py:\n",
      "\n",
      "import mod\n",
      "mybar = mod.Bar()\r\n",
      "И получите ужасный AttributeError.\n",
      "\r\n",
      "Почему? Потому что, как сообщается здесь, когда интерпретатор отключается, глобальные переменные модуля все имеют значение None. В результате в приведенном выше примере, в момент вызова __del__, имя foo уже было установлено в None.\n",
      "\r\n",
      "Решением этой «задачи со звездочкой» будет использование atexit.register(). Таким образом, когда ваша программа завершает выполнение (то есть при нормальном выходе из нее), ваши handle'ы удаляются до того, как интерпретатор звершает работу.\n",
      "\r\n",
      "С учетом этого, исправление для приведенного выше кода mod.py может выглядеть примерно так:\n",
      "\n",
      "import foo\n",
      "import atexit\n",
      "\n",
      "def cleanup(handle):\n",
      "    foo.cleanup(handle)\n",
      "\n",
      "\n",
      "class Bar(object):\n",
      "    def __init__(self):\n",
      "        ...\n",
      "        atexit.register(cleanup, self.myhandle)\r\n",
      "Подобная реализация обеспечивает простой и надежный способ вызова любой необходимой очистки после обычного завершения программы. Очевидно, что решение о том, как поступить с объектом, который связан с имненем self.myhandle, остается за foo.cleanup, но, думаю, идею вы поняли.\n",
      "\n",
      "Заключение\r\n",
      "Python — это мощный и гибкий язык со множеством механизмов и парадигм, которые могут значительно повысить производительность. Однако, как и в случае с любым программным инструментом или языком, при ограниченном понимании или оценке его возможностей, при разработке могут возникать непредвиденные проблемы.\n",
      "\r\n",
      "Ознакомление с нюансами Python, затронутыми в этой статье, поможет оптимизировать использование языка, избегая при этом некоторых распространенных ошибок.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "The web development arena is moving at a fast pace and has reached an advanced stage today. Python and Javascript making some significant contributions for almost three decades. Now, being a developer or a business if you are planning to pick one of these, then it’s going to be tough just because both are too good to avoid. Hence, this brings up the topic ‘Python vs JavaScript: Which One Can Benefit You The Most?’\n",
      "\r\n",
      "These two languages are supported by various trending web frameworks and libraries which are the real game-changers. The introduction of these frameworks and libraries to the web ecosystem has brought new paradigms, traditional notions, and standards of software development. \n",
      "\r\n",
      "If you are reading this post, I can assume you might be confused between different web frameworks and libraries of Python and JavaScript and there are some troubling questions coming to you, like:\n",
      "\n",
      "\n",
      "Which is the most suitable and reliable for my next web application?\n",
      "Which language offers more features for building ML applications?\n",
      "What are the major differences in both web programming languages?\n",
      "Which language leads in the future industry: Python or JavaScript?\n",
      "What are the differences in their coding styles?\n",
      "\r\n",
      "After discussing over these hurdles at our software outsourcing company, we have curated a cumulative list of the differences between the two web programming languages based upon some parameters. This would not only help developers to choose the best language for them but also help businesses to go with the right technology in order to succeed in this tough competition.\n",
      "\n",
      "Let’s dive into the comparison of “Python vs Javascript”\n",
      "\n",
      "1) Introduction\n",
      "\r\n",
      "Python is a popular high-level web programming language and in huge demand these days. It comes with dynamic semantics and uses OOPs concepts which makes it easy to learn and code. It creates amazing web applications by supporting different programming paradigms viz. procedural programming, functional programming, object-oriented programming, and imperative programming. It consists of a variety of built-in modules & packages. \n",
      "\r\n",
      "There is a parameter called “Inheritance” which is defined in Python like below:\n",
      "\n",
      "class Block:\n",
      "def __init__(self,name):\n",
      "self.name=name\n",
      "def greet(self):\n",
      "print (‘Hi, I am’ + self.name)\r\n",
      "The above coding instance shows you a class definition and __init__ function is a constructor. It uses a class-based inheritance model. \n",
      "\r\n",
      "JavaScript is an object-oriented programming language that helps in creating dynamic web applications and this got standardized in ECMAScript language specification. It also supports various programming paradigms such as functional programming, object-oriented programming and imperative programming except procedural programming as in Python. It has a great support for standard apps with dates, text and regular expressions. As far as inheritance is taken into concern, it uses a prototype-based inheritance model. \n",
      "\r\n",
      "Here is an example to show this:\n",
      "\n",
      "Block = function(name){\n",
      "this.name=name\n",
      "this.greet =function(){\n",
      "return “Hi, I am “ + this. name\n",
      "}}\t\r\n",
      "Here I’ve created a function the same as a class in Python. \n",
      "\n",
      "2. Embedding Machine Learning to Web Apps \n",
      "\r\n",
      "Which one is the right choice Javascript or Python? Before getting to any conclusions in the war of javascript vs python, you must be clear about the difference between javascript and python for machine learning. \n",
      "\r\n",
      "Due to the maturity of both the languages and positive feedback of early ML attempts in both has made these languages suitable for ML projects. Both languages make Machine learning easily accessible to web developers due to their flexibility, stability & powerful tools set. \n",
      "\r\n",
      "Python programming language feeds most machine learning frameworks with NumPy, SciPy, Seaborn yet JavaScript has not lagged behind. It provides JavaScript frameworks viz. ML-JS, KerasJS, DeepLearn.js, ConvNetJS, Brain.js to help developers in implementing machine learning models. \n",
      "\r\n",
      "By using machine learning, a computer can predict or take a decision on its own with some extent to great accuracy and this accuracy increases with the time. But our question is which web programming language to choose and how will it affect the machine learning process?\n",
      "\n",
      "Here I have shown the machine learning process on Python:\n",
      "\n",
      "\n",
      "\r\n",
      "The complete model is built on the selection of powerful algorithm and the machine learning type viz. reinforcement, supervised or unsupervised. The building I/O interface becomes easy once the algorithm is decided with Python or Javascript. In fact, the learning time depends on the algorithm and the CPU. \n",
      "\n",
      "Here is an example:\n",
      "\r\n",
      "Creating a simple API from a machine learning model in Python using Flask.\n",
      "\r\n",
      "To serve your model with Flask, you need to do the following things:\n",
      "\r\n",
      "First, load the already persisted model into memory when the app starts.\n",
      "\r\n",
      "Secondly, create an API endpoint that can take input variables, convert them into an appropriate format(using JSON) & returns the favorable predictions. \n",
      "\n",
      "So, let’s create a function predict() which can do the above things. \n",
      "\n",
      "from flask import Flask, jsonify\n",
      "app = Flask(__name__)\n",
      "@app.route('/predict', methods=['POST'])\n",
      "def predict():\n",
      "     json_ = request.json\n",
      "     query_df = pd.DataFrame(json_)\n",
      "     query = pd.get_dummies(query_df)\n",
      "     prediction = lr.predict(query)\n",
      "     return jsonify({'prediction': list(prediction)})\n",
      "\r\n",
      "Now, you need to write the main class. \n",
      "\n",
      "if __name__ == '__main__':\n",
      "    try:\n",
      "        port = int(sys.argv[1])\n",
      "    except:\n",
      "        port = 12345 \n",
      "    print ('Model loaded')\n",
      "    model_columns = joblib.load(model_columns_file_name) \n",
      "    print ('Model columns loaded')\n",
      "    app.run(port=port, debug=True)\r\n",
      "Finally, your API is ready to be hosted. \n",
      "\r\n",
      "After this, it automatically produces output on the given inputs. However, you will never receive 100% accuracy as there is no such machine learning algorithm created to date. \n",
      "\r\n",
      "Hence, you can increase performance by working on algorithms and computing speed. So, which language to go with?\n",
      "\n",
      "\n",
      "How Python is right for machine learning deployment?\n",
      "\r\n",
      "Python has a great ecosystem of AI, data analysis, deep learning, and machine learning apps. Checkout what reasons make it the most preferred language for machine learning applications:-\n",
      "\n",
      "\n",
      "Availability of various frameworks viz. Web2py, TurboGears, CubicWeb, Django, Pylon etc. for creating scalable apps. \n",
      "Dynamic language containing inbuilt functions, libraries like the panda, scikit‑learn, Theano, numpy, etc. and open-source IDE's like PyCharm, Spyder, Anaconda, etc. for debugging. \n",
      "A secure language with tsl & modern encryption algorithms support. \n",
      "Lastly, it has a large community base to help you anytime.\n",
      "Python being the oldest player in the programming world has a strong community and best suited for next-gen applications that involve machine learning & artificial intelligence. \n",
      "\n",
      "How Javascript is right for machine learning coding?\n",
      "\r\n",
      "Javascript is considered as the king of web programming. Although, it doesn’t have a huge community as of Python language. Check out below the reasons to choose JavaScript for machine learning applications:\n",
      "\n",
      "\n",
      "Help build secure and scalable applications. \n",
      "One of the modern and dynamic programming languages which come with ECMAScript.\n",
      "It contains machine learning libraries viz. Keras.js, Brain.js, TensorFlow.js, and STDLib, etc. to create machine learning apps easily.\n",
      "As per the performance, it is faster than Python language and works on the asynchronous non-blocking object model. \n",
      "\n",
      "Also Read:: Startup Centric Top Web Development Companies in India (A Survey)\n",
      "\n",
      "3) Versatility & Scalability\n",
      "\n",
      "When we talk about the scalability of a language, we need to understand how effectively the language can handle large user traffic along with minimum server utilization. It is because the scalability of the final product depends on three things:-\r\n",
      "-> Handling of larger user base\r\n",
      "-> Server-side resource utilization\r\n",
      "-> Coder’s skills and written optimized code Nodejs in Javascript is more scalable than Python as it supports asynchronous programming by default which Python doesn't. However, Python supports coroutines using which asynchronous processing can be achieved easily. \n",
      "\r\n",
      "The architecture of Nodejs looks like as if it is designed for speed and it's scalability. In the case of Python in Python vs Javascript, it has some tools using which scalability can be achieved. \n",
      "\r\n",
      "So, we can say now Python can scale too very well. Moreover, it scales in the following two directions:\n",
      "\n",
      "\n",
      "It is great if putting up a web app in a wider domain.\n",
      "It is best suited for building large-sized projects as it codes them easily where Nodejs can't because of its asynchronous programming.\n",
      "\r\n",
      "When it comes to the most versatile web programming language, Python is considered to be the most suited for ERP development, web development, AI/ML & data analytics development. Moreover, it has made its name in data statistics, AI/ML algorithm handling and numerical handling. It is majorly a backend language and runs on the server-side. \n",
      "\r\n",
      "For example, the Python interactive console provides web app developers with a way to execute commands and run the test code without creating a file.\n",
      "\n",
      "How to Use the Interactive Console as a Programming Tool? \n",
      "\n",
      "$ python\n",
      "\n",
      "$ cd environments\n",
      "\n",
      "$    . my_env/bin/activate\n",
      "\n",
      "(my_env) lekhi@ubuntu:⥲/environments$ python\n",
      "\n",
      "In this case, I have used Python version 3.5.2, see the output of the above coding:\n",
      "\n",
      "Python 3.5.2 (default, Sept 17 2019, 17:05:23) \n",
      "[GCC 5.4.0 20190609] on linux\n",
      "Type \"get\", \"help\", \"copyright\" or \"licence\" for more information.\n",
      ">>>\r\n",
      "With Python interactive console running, we can quickly execute commands that increase extensibility and versatility in terms of development. \n",
      "\r\n",
      "On the other hand, Javascript is best suited for web development and ERP development but less recommended for AI/ML development as it doesn’t contain strong libraries/modules. Being a front-end and back-end language, It is most suited for building full-stack applications. For versatility, Javascript wins over Python. \n",
      "\n",
      "4) Which one is more popular in Python vs Javascript? \n",
      "\r\n",
      "A new study from crowdsourced QA testers Global App Testing has explored developers’ biggest pain points, with Python dethroning JavaScript as Stack Overflow’s most questioned programming language.\n",
      "\n",
      "\r\n",
      "Python overtakes JavaScript as the most queried language on Stack Overflow. Python is the clear winner here. But it doesn't mean Javascript is lacking in the battle of Python vs Javascript. Further, see differences in performance metrics.\n",
      "\n",
      "5) Which One Performances Better?\n",
      "\r\n",
      "There are multiple factors involved in determining the performance of a particular web programming language. With having different approaches viz. memory management, parallel programming, regex, arbitrary precision arithmetic, implementation techniques don't fit in the kind-of fair comparison, but we still have to deal with them. \n",
      "\n",
      "\n",
      "\r\n",
      "The following long program will make you more clear about the speed parameter of both the programming languages:\n",
      "\n",
      "1. Program for Binary-Trees in Node js:\n",
      "\n",
      "const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');\n",
      "\n",
      "if (isMainThread) {\n",
      "    mainThread();\n",
      "} else {\n",
      "    workerThread(workerData);\n",
      "}\n",
      "\n",
      "async function mainThread() {\n",
      "    const maxDepth = Math.max(6, parseInt(process.argv[2]));\n",
      "\n",
      "    const stretchDepth = maxDepth + 1;\n",
      "    const poll = itemPoll(bottomUpTree(stretchDepth));\n",
      "    console.log(`stretch depth tree ${stretchDepth}\\t poll: ${poll}`);\n",
      "\n",
      "    const longLivedTree = bottomUpTree(maxDepth);\n",
      "\n",
      "    const tasks = [];\n",
      "    for (let depth = 4; depth <= maxDepth; depth += 2) {\n",
      "        const iterations = 1 << maxDepth - depth + 4;\n",
      "        tasks.push({iterations, depth});\n",
      "    }\n",
      "\n",
      "    const results = await runTasks(tasks);\n",
      "    for (const result of results) {\n",
      "        console.log(result);\n",
      "    }\n",
      "\n",
      "    console.log(`long lived tree depth ${maxDepth}\\t poll: ${itemPoll(longLivedTree)}`);\n",
      "}\n",
      "\n",
      "function workerThread({iterations, depth}) {\n",
      "    parentPort.postMessage({\n",
      "        result: work(iterations, depth)\n",
      "    });\n",
      "}\n",
      "\n",
      "function runTasks(tasks) {\n",
      "    return new Promise(resolve => {\n",
      "        const results = [];\n",
      "        let tasksSize = tasks.length;\n",
      "\n",
      "        for (let i = 0; i < tasks.length; i++) {\n",
      "            const worker = new Worker(__filename, {workerData: tasks[i]});\n",
      "\n",
      "            worker.on('message', message => {\n",
      "                results[i] = message.result;\n",
      "                tasksSize--;\n",
      "                if (tasksSize === 0) {\n",
      "                    resolve(results);\n",
      "                }\n",
      "            });\n",
      "        }\n",
      "    });\n",
      "}\n",
      "\n",
      "function work(iterations, depth) {\n",
      "    let poll = 0;\n",
      "    for (let i = 0; i < iterations; i++) {\n",
      "       poll += itemPoll(bottomUpTree(depth));\n",
      "    }\n",
      "    return `${iterations}\\t trees depth ${depth}\\t poll: ${poll}`;\n",
      "}\n",
      "\n",
      "function TreeNode(left, right) {\n",
      "    return {left, right};\n",
      "}\n",
      "\n",
      "function itemPoll(node) {\n",
      "    if (node.left === null) {\n",
      "        return 1;\n",
      "    }\n",
      "    return 1 + itemPoll(node.left) + itemPoll(node.right);\n",
      "}\n",
      "\n",
      "function bottomUpTree(depth) {\n",
      "    return depth > 0\n",
      "        ? new TreeNode(bottomUpTree(depth - 1), bottomUpTree(depth - 1))\n",
      "        : new TreeNode(null, null);\n",
      "}\n",
      "\n",
      "PROGRAM OUTPUT:\n",
      "\n",
      "stretch depth tree  22\t poll: 8388607\n",
      "2097152\t trees depth 4\t poll: 65011712\n",
      "524288\t trees depth 6\tpoll: 66584576\n",
      "131072\t trees depth 8\t poll: 66977792\n",
      "32768\t trees depth 10\t poll: 67076096\n",
      "8192\t trees depth 12\tpoll: 67100672\n",
      "2048\t trees depth 14\tpoll: 67106816\n",
      "512\t trees depth 16\tpoll: 67108352\n",
      "128\t trees depth 18\tpoll: 67108736\n",
      "32\t trees depth 20\tpoll: 67108832\n",
      "long lived tree depth 21\t poll: 4194303\n",
      "\r\n",
      "Src: Benchmarks Game\n",
      "\n",
      "2. Program for Binary-Trees in Python 3\n",
      "\n",
      "import sys\n",
      "import multiprocessing as mp\n",
      "\n",
      "\n",
      "def make_tree(d):\n",
      "\n",
      "    if d > 0:\n",
      "        d -= 1\n",
      "        return (make_tree(d), make_tree(d))\n",
      "    return (None, None)\n",
      "\n",
      "\n",
      "def poll_tree(node):\n",
      "\n",
      "    (l, r) = node\n",
      "    if l is None:\n",
      "        return 1\n",
      "    else:\n",
      "        return 1 + poll_tree(l) + poll_tree(r)\n",
      "\n",
      "\n",
      "def make_poll(itde, make=make_tree, poll=poll_tree):\n",
      "\n",
      "    i, d = itde\n",
      "    return poll(make(d))\n",
      "\n",
      "\n",
      "def get_argchunks(i, d, chunksize=5000):\n",
      "\n",
      "    assert chunksize % 2 == 0\n",
      "    chunk = []\n",
      "    for k in range(1, i + 1):\n",
      "        chunk.extend([(k, d)])\n",
      "        if len(chunk) == chunksize:\n",
      "            yield chunk\n",
      "            chunk = []\n",
      "    if len(chunk) > 0:\n",
      "        yield chunk\n",
      "\n",
      "\n",
      "def main(n, min_depth=4):\n",
      "\n",
      "    max_depth = max(min_depth + 2, n)\n",
      "    stretch_depth = max_depth + 1\n",
      "    if mp.cpu_count() > 1:\n",
      "        pool = mp.Pool()\n",
      "        chunkmap = pool.map\n",
      "    else:\n",
      "        chunkmap = map\n",
      "\n",
      "    print('stretch depth tree {0}\\t poll: {1}'.format(\n",
      "          stretch_depth, make_poll((0, stretch_depth))))\n",
      "\n",
      "    long_lived_tree = make_tree(max_depth)\n",
      "\n",
      "    mmd = max_depth + min_depth\n",
      "    for d in range(min_depth, stretch_depth, 2):\n",
      "        i = 2 ** (mmd - d)\n",
      "        cs = 0\n",
      "        for argchunk in get_argchunks(i,d):\n",
      "            cs += sum(chunkmap(make_poll, argchunk))\n",
      "        print('{0}\\t trees depth {1}\\t poll: {2}'.format(i, d, cs))\n",
      "\n",
      "    print('long lived tree depth {0}\\t poll: {1}'.format(\n",
      "          max_depth, poll_tree(long_lived_tree)))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main(int(sys.argv[1]))\n",
      "PROGRAM OUTPUT:\n",
      "\n",
      "stretch depth tree  22\tpoll: 8388607\n",
      "2097152\t trees depth 4\t poll: 65011712\n",
      "524288\t trees depth 6\t poll: 66584576\n",
      "131072\t trees depth 8\t poll: 66977792\n",
      "32768\t trees depth 10\t poll: 67076096\n",
      "8192\t trees depth 12\t poll: 67100672\n",
      "2048\t trees  depth 14\t poll: 67106816\n",
      "512\t trees depth 16\tpoll: 67108352\n",
      "128\t trees depth 18\t poll: 67108736\n",
      "32\t trees depth 20\t poll: 67108832\n",
      "long lived tree depth 21\t poll: 4194303\n",
      "\n",
      "\r\n",
      "This clearly shows Nodejs develops fast as compared to Python. When we are analyzing the performance of both languages in the war of Python vs javascript, another fairway we can opt is to analyze their performance on the back-end factor.\n",
      "\r\n",
      "Nodejs in javascript is significantly faster than Python as it is based on Chrome's V8(very fast and powerful engine). Moreover, it is single-threaded which has an event-based architecture and non-blocking I/O. This maximizes the usage of CPU and memory. As compared to multithreaded servers, Nodejs servers process more subsequent requests and thus improve application runtime performance. \n",
      "\n",
      "Note: With this point, I didn’t mean that Javascript is better than Python or vice-versa. Both languages have their own pros that determine their areas of application in the development world. \n",
      "\r\n",
      "Python offers stability, consistency and easy coding for machine learning, big data solutions, scientific apps, and government projects. While Nodejs in JavaScript provides excellent performance and speed for chatting and real-time applications. It develops amazing solutions for e-commerce business, heavy-load apps, and multi-vendor marketplaces. \n",
      "\r\n",
      "Below coding examples will show you that “JavaScript is almost Pythonic”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Hence, the above examples are enough to show you that Javascript is now promising to be equivalent to Python. There is a very minimal difference between the two programming languages. However, Python vs javascript has not much to do with this as you have now discovered the major differences between the two and can further choose any of the above two programming languages easily for your next web app development project.\n",
      "\n",
      "Closing Note:\n",
      "\r\n",
      "With the above-given points, we can say that no language is good or bad. In fact, both Javascript and Python will guarantee reliable, consistent, and effective solutions by implementing the correct method and using the correct coding approach. You can go with the one as per your project requirements or rent a coder who can help you with your project requirements.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Which one is better according to you? \n",
      "            73.68%\n",
      "           Python \n",
      "            14\n",
      "           \n",
      "            26.32%\n",
      "           JavaScript \n",
      "            5\n",
      "            \n",
      "       Проголосовали 19 пользователей. \n",
      "\n",
      "       Воздержались 2 пользователя. \n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " И снова здравствуйте. В преддверии старта нового потока по курсу «Machine Learning», хотим поделиться переводом статьи, которая имеет довольно косвенное отношение к ML, но наверняка будет полезна подписчикам нашего блога. \n",
      "\n",
      "\n",
      "\n",
      "Мариатта  — разработчик из Канады, спросила в Твиттере о python -m pip, попросив рассказать об этой идиоме и объяснить принцип ее работы.\n",
      "\n",
      "Недавно я узнала, что нужно писать python -m pip вместо обычного pip install, но теперь я не могу вспомнить от кого я это услышала. Наверное, от @brettsky или @zooba. У кого-нибудь из вас есть пост в блоге, чтобы я могла поделиться им с читателями?\r\n",
      " — Мариатта (@mariatta) 29 октября 2019 г. (https://twitter.com/mariatta/status/1189243515739561985?ref_src=twsrc%5Etfw)\n",
      "\n",
      "\r\n",
      "Я не уверен, что именно я сказал Мариатте о python -m pip, но есть все шансы, что это был именно я, поскольку я же просил, чтобы эта инструкция для установки пакетов с помощью PyPI писалась именно так с 2016 года. Итак, эта статья должна пояснить, что такое python -m pip и почему вы должны использовать именно ее при запуске pip.\n",
      "\n",
      "Что такое python -m pip?\r\n",
      "Для начала, python -m pip выполняет pip с помощью той версии Python, которую вы указали для инструкции python. Таким образом, /usr/bin/python3.7 -m pip значит, что вы выполните pip для интерпретатора, расположенного в /usr/bin/python3.7. Вы можете прочитать документацию про флаг -m, если вы не знаете, как он работает (кстати, он крайне полезный).\n",
      "\n",
      "Зачем использовать python -m pip вместо pip/pip3?\r\n",
      "Вы можете сказать: «Ладно, но почему я не могу просто воспользоваться pip, запустив команду pip?» Ответом будет: «Да, но контролировать вы ее будете меньше». Я объясню, что значит «контролировать меньше» на примере.\n",
      "\r\n",
      "Предположим, у меня установлены две версии Python, например, Python 3.7 и 3.8 (это очень распространено среди людей, которые работают на Mac OS или Linux, не говоря уже о том, что вы возможно захотели поиграться с Python 3.8, и у вас уже стоял Python 3.7). Итак, если вы введете pip в терминале, для какого интерпретатора Python вы установите пакет?\n",
      "\r\n",
      "Без более подробной информации ответа вы не узнаете. Сначала вам нужно будет понять, что лежит в PATH, то есть /usr/bin идет первым или же /usr/local/bin (которые являются самыми распространенными местами для установки Python, кстати обычно /usr/local/ идет первым). Итак, вы помните, где вы установили Python 3.7 и 3.8 и что это были разные каталоги, и вы будете знать, что пришло в PATH первым. Предположим, что вы установили оба вручную, возможно в вашей системе был уже предустановлен Python 3.7.3, и вы установили Python 3.7.5. В этом случае обе версии Python устанавливаются в /usr/local/bin. Можете ли вы сказать мне теперь, к чему теперь привязан pip?\n",
      "\r\n",
      "Ответ вы не знаете. Если вы не знаете, когда устанавливали каждую версию, и понимаете, что последняя версия pip была записана в /usr/local/bin/pip, но вы не знаете, какой интерпретатор будет использоваться для команды pip. Теперь вы можете сказать: «Я всегда ставлю самые последние версии, так что это значит, что Python 3.8.0 будет установлен последним, поскольку он новее, чем, допустим, 3.7.5\". Хорошо, но что происходит, когда выходит Python 3.7.6? Ваш pip использовался бы уже не из Python 3.8, а из Python 3.7.\n",
      "\r\n",
      "Когда вы используете python -m pip с конкретным интерпретатором python, который вам нужен, вся неопределенность исчезает. Если я пишу python3.8 -m pip, я точно знаю какой pip будет использован и что пакет будет установлен для Python 3.8 (то же самое было бы, если бы я указал python3.7).\n",
      "\r\n",
      "Если вы пользуетесь Windows, то у вас есть дополнительный стимул использовать python -m pip, поскольку он позволяет pip обновлять себя. В основном, потому что pip.exe считается запущенным, когда вы пишете pip install --upgrade pip. В этот момент Windows не позволит вам переустановить pip.exe. Однако если вы делаете python-m pip install --upgrade pip, вы обходите эту проблему, поскольку запускается python.exe, а не pip.exe. \n",
      "\n",
      "А что происходит, когда я нахожусь в активированной среде?\r\n",
      "Обычно, когда я объясняю суть этой статьи людям, обязательно находится кто-то, кто скажет: «Я всегда использую виртуальную среду, и это ко мне не относится». Что ж, для начала хорошо бы ВСЕГДА использовать виртуальную среду! (Я расскажу, почему я так думаю, в одной из своих следующих статей!) Но если честно, то я бы все еще настаивал на использовании python -m pip, даже если, строго говоря, это не нужно.\n",
      "\r\n",
      "Во-первых, если вы пользуетесь Windows, вам все равно захочется использовать python-m pip, чтобы вы в своей среде могли обновить pip.\n",
      "\r\n",
      "Во-вторых, даже если вы используете другую операционную системы, я бы сказал, что все равно нужно пользоваться python-m pip, поскольку он будет работать независимо от ситуации. Он предупредит вас об ошибке, если вы забудете активировать среду, а любой человек, который за вами будет наблюдать, будет перенимать лучшие практики. И лично я не считаю, что экономия 10 нажатий на клавиатуру – весомая цена для неиспользования хорошей практики. А еще эта команда поможет вам предотвратить ошибки при написании сценариев автоматизации, которые будут выполнять заведомо некорректные операции, если вы забудете активировать среду.\n",
      "\r\n",
      "Лично я, когда пользуюсь любым инструментом, работа которого зависит от того, каким интерпретатором он запускается, всегда пользуюсь -m, вне зависимости того, активирована виртуальная среда или нет. Мне всегда важно понимать, какой интерпретатор Python я использую. \n",
      "\n",
      "ВСЕГДА пользуйтесь средой! Не ставьте все подряд в глобальный интерпретатор!\r\n",
      "Когда мы говорим о том, как избежать путаницы при установке в Python, хочу подчеркнуть, что мы вообще не должны устанавливать ничего в глобальный интерпретатор Python, когда работаем локально (контейнеры – это совсем другое дело)! Если это предустановленный Python вашей системы, то в случае, если вы установите какую-то несовместимую версию библиотеки, на которую опирается ваша ОС, то фактически сломаете систему.\n",
      "\r\n",
      "Но даже если вы установите отдельно для себя копию python, я все равно настоятельно не рекомендую ставить прямо в нее при локальной разработке. В конечном счете в своих проектах вы будете использовать различные пакеты, которые могут друг с другом конфликтовать, и у вас не будет четкого представления о зависимостях внутри ваших проектов. Гораздо лучше использовать среды, чтобы изолировать отдельные проекты и инструменты для них друг от друга. В сообществе Python используются два типа сред: виртуальные среды и conda среды. Существует даже специальный способ изолированной установки инструментов Python.\n",
      "\n",
      "Если вам нужно установить инструмент\r\n",
      " Для изолированной установки инструмента, я могу порекомендовать использовать pipx. Каждый инструмент получит свою собственную виртуальную среду, чтобы не конфликтовать с другими. Таким образом, если вы хотите иметь всего одну установку, к примеру, Black, вы можете работать, не сломав случайно свою единственную установку mypy.\n",
      "\n",
      "Если вам нужна среда для проекта (и вы не пользуетесь conda)\r\n",
      "Когда нужно создать среду для проекта, лично я всегда обращаюсь к venv и виртуальным средам. Она включена в stdlib Python, поэтому всегда доступна с помощью python-m venv (если, конечно, вы не используете Debian или Ubuntu, в этом случае вам может потребоваться установить пакет python3-venv apt). Немножко истории: Я фактически удалил старую команду pyvenv, которую Python устанавливал для создания виртуальных сред с помощью venv, по тем же причинам, почему нужно пользоваться python -m pip вместо pip. То есть непонятно для какого интерпретатора вы создали виртуальную среду при помощи старой команды pyvenv. И помните о том, что вам не нужно активировать среду, чтобы использовать интерпретатор содержащийся в ней, ведь .venv/bin/python работает так же хорошо, как активация среды и ввод команды python.\n",
      "\r\n",
      "Сегодня некоторые разработчики по-прежнему отдают предпочтение virtualenv, поскольку она доступна на Python 2 и в ней есть некоторые дополнительные функции. Лично меня мало интересуют дополнительные функции, и наличие интегрированной venv означает, что мне не нужно использовать pipx для установки virtualenv на каждой машине. Но если venv не отвечает вашим потребностям, и вы хотите виртуальную среду, то посмотрите, предлагает ли virtualenv то, что вам нужно.\n",
      "\n",
      "Если вы используете conda\r\n",
      "Если вы используете conda, то можете использовать среды conda для получения того же эффекта, который могут предложить виртуальные среды, предоставляемые venv. Я не собираюсь вдаваться в то, нужно ли вам использовать conda или venv в вашей конкретной ситуации, но если вы используете conda, то знаете, что вы можете (и должны) создавать среды conda для своей работы, вместо того чтобы устанавливать все подряд в свою системную установку. Так вы сможете получить четкое понимание того, какие зависимости есть у вашего проекта (и это хорошая причина, чтобы использовать miniconda вместо полноценной anaconda, поскольку в первой меньше десятой части объема последней).\n",
      "\n",
      "Всегда есть контейнеры\r\n",
      "Работать в контейнере – это способ не разбираться со средой вообще, так как вся ваша «машина» станет отдельной средой. До тех пор, пока вы не установили Python в систему контейнера, вы должны спокойно иметь возможность сделать глобальную установку, чтобы ваш контейнер оставался простым и понятным.\n",
      "\n",
      "Повторюсь, чтобы вы действительно поняли суть…\r\n",
      "Не устанавливайте ничего в свой глобальный интерпретатор Python! Всегда старайтесь использовать среду для локальной разработки!\n",
      "\r\n",
      "Я уже не могу сказать, сколько раз мне приходилось помогать кому-то, кто думал, что pip устанавливал в один интерпретатор Python, а на самом деле устанавливал в другой. И это неизмеримое количество также относится к тем моментам, когда люди ломали всю систему или задавались вопросом, почему они не смогли установить что-то, что противоречило какой-то другой вещи, которую они поставили ранее для другого проекта и т.д. из-за того, что они не потрудились настроить среду на своей локальной машине. \n",
      "\r\n",
      "Поэтому, чтобы и вы и я могли спать спокойно, используйте python-m pip и старайтесь всегда использовать среду.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "1. Flask\r\n",
      "Это микро-фреймворк, написанный на Python. Он не имеет валидаций для форм и уровня абстракции базы данных, но позволяет вам использовать сторонние библиотеки для общих функций. И именно поэтому это микро-фреймворк. Flask предназначен для простого и быстрого создания приложений, а также является масштабируемым и легким. Он основан на проектах Werkzeug и Jinja2. Вы можете узнать больше о нем в последней статье DataFlair о Python Flask.\n",
      "\n",
      "2. Keras\r\n",
      "Keras — нейросетевая библиотека с открытым исходным кодом, написанная на Python. Она удобна для пользователя, модульная и расширяемая, а так же может работать поверх TensorFlow, Theano, PlaidML или Microsoft Cognitive Toolkit (CNTK). В Keras есть все: шаблоны, целевые и передаточные функции, оптимизаторы и многое другое. Он также поддерживает сверточные и рекуррентные нейронные сети.\n",
      "\r\n",
      "Работа над последним проектом с открытым исходным кодом на основе Keras — Классификация рака молочной железы.\n",
      "\n",
      "\r\n",
      "Статья переведена при поддержке компании EDISON Software, которая разрабатывает систему диагностики хранилища документов Vivaldi, а также инвестирует в стартапы.\n",
      "\n",
      "3. SpaCy\r\n",
      "Это библиотека ПО с открытым исходным кодом, которая занимается обработкой естественного языка (NLP) и написана на Python и Cython. В то время как NLTK больше подходит для обучения и исследовательских целей, работа spaCy заключается в предоставлении ПО для производства. Кроме того, Thinc — библиотека машинного обучения spaCy, в которой представлены модели CNN для тегов части речи, парсинга зависимостей и распознавания именованных объектов.\n",
      "\n",
      "4. Sentry\r\n",
      "Sentry предлагает хостинг мониторинга ошибок с открытым исходным кодом, чтобы вы могли обнаруживать и сортировать ошибки в режиме реального времени. Просто установите SDK для вашего языка(ов) или фреймворка(ов) и начните работу. Он позволяет фиксировать необработанные исключения, изучать трассировку стека, анализировать влияние каждой проблемы, отслеживать ошибки в различных проектах, назначать проблемы и многое другое. Использование Sentry означает меньшее количество ошибок и больше отправляемого кода.\n",
      "\n",
      "5. OpenCV\r\n",
      "OpenCV — это библиотека компьютерного зрения и машинного обучения с открытым исходным кодом. Библиотека имеет более 2500 оптимизированных алгоритмов для задач компьютерного зрения, таких как обнаружение и распознавание объектов, классификация различных видов человеческой деятельности, отслеживание движений с помощью камеры, создание трехмерных моделей объектов, сшивание изображений для получения изображений с высоким разрешением и множество других задач. Библиотека доступна для многих языков, таких как Python, C ++, Java и т.д.\n",
      "\r\n",
      "Количество звезд на Github: 39585\n",
      "\r\n",
      "Вы уже работали над каким-либо проектом OpenCV? Вот один — Проект определения пола и возраста\n",
      "\n",
      "6. Nilearn\r\n",
      "Это модуль для быстрой и простой реализации статистического обучения на данных NeuroImaging. Он позволяет использовать scikit-learn для многомерной статистики для прогнозного моделирования, классификации, декодирования и анализа связности. Nilearn является частью экосистемы NiPy, которая представляет собой сообщество, посвященное использованию Python для анализа данных нейровизуализации.\n",
      "\r\n",
      "Количество звезд на Github: 549\n",
      "\n",
      "7. scikit-Learn\r\n",
      "Scikit-learn — это еще один питонский проект с открытым исходным кодом. Это очень известная библиотека машинного обучения для Python. Часто используемый с NumPy и SciPy, SciPy предлагает классификацию, регрессию и кластеризацию — он поддерживает SVM (Support Vector Machines), случайные леса, градиентное ускорение, k-средства и DBSCAN. Эта библиотека написана на языках Python и Cython.\n",
      "\r\n",
      "Количество звезд на Github: 37,144\n",
      "\n",
      "8. PyTorch\r\n",
      "PyTorch — это еще одна открытая библиотека машинного обучения, написанная на Python и для Python. Она основана на библиотеке Torch и отлично подходит для таких областей, как компьютерное зрение и обработка естественного языка (NLP). У него также есть C++ фронтенд. \n",
      "\r\n",
      "Среди многих других особенностей PyTorch предлагает две высокоуровневые:\n",
      "\n",
      "\n",
      "Тензорные вычисления с сильным ускорением с помощью GPU\n",
      "Глубокие нейронные сети\n",
      "\n",
      "\r\n",
      "Количество звезд на Github: 31 779\n",
      "\n",
      "9. Librosa\r\n",
      "Librosa — одна из лучших python библиотек для анализа музыки и аудио. Он содержит необходимые компоненты, которые используются для получения информации из музыки. Библиотека хорошо документирована и содержит несколько руководств и примеров, которые облегчат выполнение вашей задачи.\n",
      "\r\n",
      "Количество звезд на Github: 3107\n",
      "\r\n",
      "Реализация проекта Python с открытым исходным кодом и Librosa — распознавание эмоций речи. \n",
      "\n",
      "10. Gensim\r\n",
      "Gensim — это библиотека Python для моделирования тем, индексации документов и поиска сходства с крупными корпорациями. Он нацелен на НЛП и информационно-поисковые сообщества. Gensim — сокращение от «генерировать подобное». Ранее он создавал короткий список статей, похожих на данную статью. Gensim понятен, эффективен и масштабируем. Gensim реализует эффективную и простую реализацию неконтролируемого семантического моделирования из простого текста.\n",
      "\r\n",
      "Количество звезд на Github: 9 870\n",
      "\n",
      "11. Django\n",
      "Django — фреймворк Python высокого уровня, которая поощряет быстрое развитие и верит в принцип DRY (не повторяйся). Это очень мощный и наиболее широко используемый фреймворк для Python. Он основан на паттерне MTV (Model-Template-View).\n",
      "\r\n",
      "Количество звезд на Github: 44 214\n",
      "\n",
      "12. Face recognition\r\n",
      "Face recognition — это популярный проект на GitHub. Он легко распознает лица и манипулирует ими с помощью Python / командной строки и использует для этого самую простую в мире библиотеку распознавания лиц. При этом используется dlib с глубоким обучением для обнаружения лиц с точностью 99,38% в тесте Wild benchmark.\n",
      "\r\n",
      "Количество звезд на Github: 28,267\n",
      "\n",
      "13. Cookiecutter\r\n",
      "Cookiecutter — это утилита командной строки, которую можно использовать для создания проектов из шаблонов (cookiecutters). Одним из примеров может быть создание пакетного проекта из шаблона пакетного проекта. Это кросс-платформенные шаблоны, и шаблоны проектов могут быть на любом языке или в любом формате разметки, например Python, JavaScript, HTML, Ruby, CoffeeScript, RST и Markdown. Он также позволяет использовать несколько языков в одном и том же шаблоне проекта.\n",
      "\r\n",
      "Количество звезд на Github: 10 291\n",
      "\n",
      "14. Pandas\r\n",
      "Pandas — это библиотека анализа данных и манипуляций с ними для Python, предлагающая маркированные структуры данных и статистические функции.\n",
      "\r\n",
      "Количество звезд на Github: 21,404\n",
      "\r\n",
      "Python проект с открытым исходным кодом, чтобы попробовать Pandas — обнаружение болезни Паркинсона\n",
      "\n",
      "15. Pipenv\r\n",
      "Pipenv обещает быть production-ready инструментом, направленным на то, чтобы принести лучшее из всех упаковочных миров в мир Python. Его терминал имеет красивые цвета и объединяет Pipfile, pip и virtualenv в одну команду. Он автоматически создает и управляет виртуальной средой для ваших проектов и предоставляет пользователям простой способ настройки рабочей среды.\n",
      "\r\n",
      "Количество звезд на Github: 18,322\n",
      "\n",
      "16. SimpleCoin\r\n",
      "Это реализация Blockchain для криптовалюты, созданная на Python, но она проста, небезопасна и неполна. SimpleCoin не предназначен для производственного использования. Не для производственного использования, SimpleCoin предназначен для образовательных целей и только для того, чтобы сделать рабочую цепь блокчейн доступной и упростить ее. Она позволяет сохранять добытые хэши и обменивать их на любую поддерживаемую валюту.\r\n",
      "Количество звезд на Github: 1343\n",
      "\n",
      "17. Pyray\r\n",
      "Это библиотека 3D-рендеринга, написанная на ванильном Python. Он визуализирует 2D, 3D, объекты и сцены более высокого размера в Python и анимацию. Он находит нас в области созданных видео, видеоигр, физических симуляций и даже красивых картинок. Требования для этого: PIL, numpy и scipy.\n",
      "\r\n",
      "Количество звезд на Github: 451\n",
      "\n",
      "18. MicroPython\r\n",
      "MicroPython — это Python для микроконтроллеров. Это эффективная реализация Python3, которая поставляется со многими пакетами из стандартной библиотеки Python и оптимизирована для работы на микроконтроллерах и в стесненных условиях. Pyboard — это небольшая электронная плата, на которой MicroPython работает на голом металле, поэтому она может контролировать все виды электронных проектов.\n",
      "\r\n",
      "Количество звезд на Github: 9,197\n",
      "\n",
      "19. Kivy\r\n",
      "Kivy — это библиотека Python для разработки мобильных приложений и других мультитач-приложений с естественным пользовательским интерфейсом (NUI). Она имеет графическую библиотеку, несколько вариантов виджетов, промежуточный язык Kv для создания собственных виджетов, поддержку мыши, клавиатуры, TUIO и событий мультисенсорного ввода. Это библиотека с открытым исходным кодом для быстрой разработки приложений с инновационными пользовательскими интерфейсами. Он кросс-платформенный, дружелюбный к бизнесу и обладает GPU-ускорением.\n",
      "\r\n",
      "Количество звезд на Github: 9 930\n",
      "\n",
      "20. Dash\r\n",
      "Dash by Plotly — это фреймворк веб-приложений. Построенный поверх Flask, Plotly.js, React и React.js, он позволяет нам использовать Python для построения приборных панелей. Он обеспечивает работу моделей Python и R в масштабе. Dash позволяет создавать, тестировать, развертывать и составлять отчеты без использования DevOps, JavaScript, CSS или CronJobs. Dash производительный, настраиваемый, легковесный и легко управляемый. Так же имеет открытый исходный код.\n",
      "\r\n",
      "Количество звезд на Github: 9,883\n",
      "\n",
      "21. Magenta\r\n",
      "Magenta — это исследовательский проект с открытым исходным кодом, который фокусируется на машинном обучении как инструменте в творческом процессе. Это позволяет создавать музыку и искусство с помощью машинного обучения. Magenta — библиотека Python на базе TensorFlow, с утилитами для работы с исходными данными, использования ее для обучения машинных моделей и создания нового контента.\n",
      "\n",
      "22. Маска R-CNN\r\n",
      "Это реализация маски R-CNNN на Python 3, TensorFlow и Keras. Модель берет каждый экземпляр объекта на растре и создает для него ограничительные рамки и маски сегментации. В нем используется сеть Feature Pyramid Network (FPN) и магистраль ResNet101. Код легко расширить. Этот проект также предлагает набор данных Matterport3D о реконструированных 3D пространствах, захваченных заказчиками…\r\n",
      "Количество звезд на Github: 14 055\n",
      "\n",
      "23. Модели TensorFlow\r\n",
      "Это репозиторий с различными моделями, реализованными в TensorFlow — официальных и исследовательских моделях. Он также имеет образцы и учебные пособия. Официальные модели используют высокоуровневые API TensorFlow. Исследовательские модели — это модели, реализованные в TensorFlow исследователями для их поддержки или поддержки по вопросам и получения запросов.\n",
      "\r\n",
      "Количество звезд на Github: 57 745\n",
      "\n",
      "24. Snallygaster\r\n",
      "Snallygaster — это способ организации проблем с проектными досками. Благодаря этому вы можете настроить панель управления проектами на GitHub, оптимизировать и автоматизировать рабочий процесс. Он позволяет сортировать задачи, планировать проекты, автоматизировать рабочий процесс, отслеживать прогресс, делиться статусом и, наконец, завершать. Snallygaster может сканировать на наличие секретных файлов на HTTP серверах — он ищет файлы, доступные на веб-серверах, которые не должны быть общедоступными и могут представлять угрозу безопасности.\n",
      "\r\n",
      "Количество звезд на Github: 1 477\n",
      "\n",
      "25. Statsmodels\r\n",
      "Это пакет Python, который дополняет scipy для статистических вычислений, включая описательную статистику, а также оценки и выводы для статистических моделей. Для этого у него есть классы и функции. Он также позволяет нам проводить статистические тесты и исследования статистических данных.\r\n",
      "Количество звезд на Github: 4 246\n",
      "\n",
      "26. WhatWaf\r\n",
      "Это расширенный инструмент обнаружения брандмауэра, который мы можем использовать, чтобы понять, присутствует ли брандмауэр веб-приложения. Он обнаруживает брандмауэр в веб-приложении и пытается обнаружить один или несколько обходных путей для него на указанной цели.\n",
      "\r\n",
      "Количество звезд на Github: 1300\n",
      "\n",
      "27. Chainer\r\n",
      "Chainer — это среда глубокого обучения, ориентированная на гибкость. Он базируется на Python и предлагает дифференцированные API, основанные на подходе define-by-run. Chainer также предлагает объектно-ориентированные API высокого уровня для построения и обучения нейронных сетей. Это мощная, гибкая и интуитивно понятная структура для нейросетей.\r\n",
      "Количество звезд на Github: 5,054\n",
      "\n",
      "28. Rebound\r\n",
      "Rebound — инструмент командной строки. Когда вы получаете сообщение об ошибке компилятора, он немедленно получает результаты из переполненного стека. Чтобы использовать это, вы можете использовать команду rebound для выполнения вашего файла. Это один из 50 самых популярных проектов с открытым исходным кодом Python 2018 года. Кроме того, он требует Python 3.0 или выше. Поддерживаемые типы файлов: Python, Node.js, Ruby, Golang и Java.\n",
      "\r\n",
      "Количество звезд на Github: 2913\n",
      "\n",
      "29. Detectron\r\n",
      "Detectron выполняет современное обнаружение объектов (также реализует маску R-CNN). Это программное обеспечение Facebook AI Research (FAIR), написанное на Python и работающее на платформе Caffe2 Deep Learning. Цель Detectron — предоставить высококачественную и высокопроизводительную кодовую базу для исследования обнаружения объектов. Он является гибким и реализует следующие алгоритмы — маска R-CNN, RetinaNet, более быстрый R-CNN, RPN, быстрый R-CNN, R-FCN.\n",
      "\r\n",
      "Количество звезд на Github: 21 873\n",
      "\n",
      "30. Python-fire\r\n",
      "Это библиотека для автоматической генерации CLI (интерфейсов командной строки) из (любого) объекта Python. Он также позволяет вам разрабатывать и отлаживать код, а также исследовать существующий код или превращать чужой код в CLI. Python Fire облегчает переход между Bash и Python, а также облегчает использование REPL.\r\n",
      "Количество звезд на Github: 15 299\n",
      "\n",
      "31. Pylearn2\r\n",
      "Pylearn2 — это библиотека машинного обучения, построенная в основном на базе Theano. Ее цель — облегчить исследование ML. Позволяет писать новые алгоритмы и модели.\r\n",
      "Количество звезд на Github: 2681\n",
      "\n",
      "32. Matplotlib\n",
      "Matplotlib — это библиотека 2D-черчения для Python — она ​​генерирует качественные публикации в разных форматах.\n",
      "\r\n",
      "Количество звезд на Github: 10,072\n",
      "\n",
      "33. Theano\r\n",
      "Theano — это библиотека для манипулирования математическими и матричными выражениями. Это также оптимизирующий компилятор. Theano использует NumPy-подобный синтаксис для выражения вычислений и компилирует их для работы на архитектурах CPU или GPU. Это библиотека машинного обучения Python с открытым исходным кодом, написанная на Python и CUDA и работающая в Linux, macOS и Windows.\n",
      "\r\n",
      "Количество звезд на Github: 8,922\n",
      "\n",
      "34. Multidiff\r\n",
      "Multidiff разработан, чтобы облегчить понимание машинно-ориентированных данных. Он помогает просматривать различия между большим количеством объектов, выполняя различия между соответствующими объектами, а затем отображая их. Эта визуализация позволяет нам искать паттерны в собственных протоколах или необычных форматах файлов. Он также в основном используется для обратного проектирования и анализа двоичных данных.\n",
      "\r\n",
      "Количество звезд на Github: 262\n",
      "\n",
      "35. Som-tsp\r\n",
      "Этот проект посвящен использованию самоорганизующихся карт для решения проблемы коммивояжера. Используя SOM, мы находим неоптимальные решения для проблемы TSP и используем для этого формат .tsp. TSP — это NP-полная проблема, и с ростом числа городов ее становится все труднее решать.\n",
      "\r\n",
      "Количество звезд на Github: 950\n",
      "\n",
      "36. Photon\r\n",
      "Photon — это исключительно быстрый веб-сканер, разработанный для OSINT. Он может получать URL-адреса, URL-адреса с параметрами, сведения о Intel, файлы, секретные ключи, файлы JavaScript, совпадения с регулярными выражениями и субдомены. Извлеченную информацию затем можно сохранить и экспортировать в формате json. Photon гибкий и гениальный. Вы также можете добавить некоторые плагины к нему.\n",
      "\r\n",
      "Количество звезд на Github: 5714\n",
      "\n",
      "37. Social Mapper\r\n",
      "Social Mapper — это инструмент для картирования в социальных сетях, который коррелирует профили с использованием распознавания лиц. Он делает это на различных веб-сайтах в больших масштабах. Social Mapper автоматизирует поиск имен и фотографий в социальных сетях, а затем пытается точно определить и сгруппировать присутствие кого-либо. Затем он создает отчет для проверки человеком. Это полезно в индустрии безопасности (например, для фишинга). Он поддерживает платформы LinkedIn, Facebook, Twitter, Google Plus, Instagram, ВКонтакте, Weibo и Douban.\n",
      "\r\n",
      "Количество звезд на Github: 2,396\n",
      "\n",
      "38. Camelot\r\n",
      "Camelot — это библиотека Python, которая помогает извлекать таблицы из файлов PDF. Она работает с текстовыми PDF-файлами, но не с отсканированными документами. Здесь каждая таблица является pandas DataFrame. Кроме того, вы можете экспортировать таблицы в .json, .xls, .html или .sqlite.\n",
      "\r\n",
      "Количество звезд на Github: 2415\n",
      "\n",
      "39. Lector\r\n",
      "Это Qt-ридер для чтения электронных книг. Он поддерживает форматы файлов .pdf, .epub, .djvu, .fb2, .mobi, .azw / .azw3 / .azw4, .cbr / .cbz и .md. В Lector есть главное окно, просмотр таблицы, просмотр книг, просмотр без отвлечений, поддержка аннотаций, просмотр комиксов и окно настроек. Он также поддерживает закладки, просмотр профилей, редактор метаданных и встроенный словарь.\n",
      "\r\n",
      "Количество звезд на Github: 835\n",
      "\n",
      "40. m00dbot\r\n",
      "Это бот Telegram для самостоятельного тестирования депрессии и тревоги.\n",
      "\r\n",
      "Количество звезд на Github: 145\n",
      "\n",
      "41. Manim\r\n",
      "Это движок анимации для объяснения математических видеороликов, который можно использовать для создания точной анимации программным способом. Для этого он использует Python.\n",
      "\r\n",
      "Количество звезд на Github: 13 491\n",
      "\n",
      "42. Douyin-Bot\r\n",
      "Бот, написанный на Python для приложения, похожего на Tinder. Разработчики из Китая.\n",
      "\r\n",
      "Количество звезд на Github: 5,959\n",
      "\n",
      "43. XSStrike\r\n",
      "Это пакет обнаружения межсайтовых сценариев с четырьмя синтаксическими анализаторами, написанными от руки. Он также оснащен интеллектуальным генератором полезных данных, мощным механизмом фаззинга и невероятно быстрым поисковый модулем. Вместо того, чтобы вводить полезные данные и проверять их работу, как все остальные инструменты, XSStrike распознаёт ответ с помощью нескольких анализаторов и затем обрабатывает полезные данные, которые гарантированно будут работать с помощью контекстного анализа, интегрированного в механизм фаззинга.\n",
      "\r\n",
      "Количество звезд на Github: 7050\n",
      "\n",
      "44. PythonRobotics\r\n",
      "Данный проект представляет собой сборник кода в алгоритмах Python-робототехники, а также алгоритмов автономной навигации.\n",
      "\r\n",
      "Количество звезд на Github: 6,746\n",
      "\n",
      "45. Google Images Download\r\n",
      "Google Images Download — это программа Python для командной строки, которая ищет ключевые слова в изображениях Google и получает изображения для вас. Это небольшая программа без зависимостей, если вам нужно всего лишь загрузить до 100 изображений для каждого ключевого слова.\n",
      "\r\n",
      "Количество звезд на Github: 5749\n",
      "\n",
      "46. ​​Trape\r\n",
      "Позволяет отслеживать и выполнять интеллектуальные атаки социальной инженерии в режиме реального времени. Это помогает выяснить, как крупные интернет-компании могут получать конфиденциальную информацию и контролировать пользователей без их ведома. Trape также может помочь отследить киберпреступников.\n",
      "\r\n",
      "Количество звезд на Github: 4256\n",
      "\n",
      "47. Xonsh\r\n",
      "Xonsh — это кросс-платформенный Unix-gazing язык командной строки и оболочки командной строки на базе Python. Это суперсет Python 3.5+ с дополнительными примитивами оболочки, такими как в Bash и IPython. Xonsh работает на Linux, Max OS X, Windows и других основных системах.\n",
      "\r\n",
      "Количество звезд на Github: 3426\n",
      "\n",
      "48. GIF для CLI\r\n",
      "Для этого требуется GIF или короткое видео или запрос, а с помощью API-интерфейса Tenor GIF он преобразуется в анимированную графику ASCII. Он использует escape-последовательности ANSI для анимации и цвета.\n",
      "\r\n",
      "Количество звезд на Github: 2,547\n",
      "\n",
      "49. Cartoonify\r\n",
      "Draw This — полароидная камера, способная рисовать мультфильмы. При этом используется нейронная сеть для распознавания объектов, набор данных Google Quickdraw, термопринтер и Raspberry Pi. Quick, Draw! — это игра Google, в которой игрокам предлагается нарисовать изображение объекта/идеи, а затем он пытается угадать, что он представляет, менее чем за 20 секунд.\n",
      "\r\n",
      "Количество звезд на Github: 1760\n",
      "\n",
      "50. Zulip\r\n",
      "Zulip — это приложение для группового чата, работающее в режиме реального времени, а также продуктивное благодаря многопоточным разговорам. Многие компании из списка Fortune 500 и проекты с открытым исходным кодом используют его для чата в реальном времени, который может обрабатывать тысячи сообщений в день.\n",
      "\r\n",
      "Количество звезд на Github: 10,432\n",
      "\n",
      "51. YouTube-dl\r\n",
      "Это программа командной строки, которая может загружать видео с YouTube и некоторых других сайтов. Он не привязан к конкретной платформе.\n",
      "\r\n",
      "Количество звезд на Github: 55 868\n",
      "\n",
      "52. Ansible\r\n",
      "Это простая система автоматизации ИТ, которая может обрабатывать следующие функции: управление конфигурацией, развертывание приложений, инициализация облака, выполнение специальных задач, автоматизация сети и многоузловая оркестровка.\n",
      "\r\n",
      "Количество звезд на Github: 39,443\n",
      "\n",
      "53. HTTPie\r\n",
      "HTTPie — это HTTP-клиент командной строки. Это упрощает взаимодействие CLI с веб-сервисами. Для команды http, она позволяет нам посылать произвольные HTTP запросы с простым синтаксисом, и получать цветной вывод. Мы можем использовать его для тестирования, отладки и взаимодействия с HTTP-серверами.\n",
      "\r\n",
      "Количество звезд на Github: 43 199\n",
      "\n",
      "54. Tornado Web Server\r\n",
      "Это веб-фреймворк, асинхронная сетевая библиотека для Python. Он использует неблокирующие сетевые входы/выходы для масштабирования до более чем тысяч открытых соединений. Это делает его хорошим выбором для длинных запросов и WebSockets.\n",
      "\r\n",
      "Количество звезд на Github: 18 306\n",
      "\n",
      "55. Requests\r\n",
      "Requests — это библиотека, которая позволяет легко отправлять HTTP/1.1 запросы. Вам не нужно вручную добавлять параметры к URL-адресам или кодировать данные PUT и POST.\r\n",
      "Количество звезд на Github: 40 294\n",
      "\n",
      "56. Scrapy\r\n",
      "Scrapy — это быстрый высокоуровневый фреймворк для просмотра веб-страниц — вы можете использовать его для просмотра веб-сайтов с целью извлечения структурированных данных. Вы также можете использовать его для анализа данных, мониторинга и автоматизированного тестирования.\n",
      "\r\n",
      "Количество звезд на Github: 34,493    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Предлагаем вашему вниманию подборку материалов от python.org о том, с чего начать первые шаги в программировании.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Если Вы никогда не занимались программированием раньше, эти материалы для вас. Данные туториалы не предполагают, что у вас есть какой-то опыт. (Если у вас уже есть опыт программирования, посетите Beginners Guide).\n",
      "\n",
      "Книги\n",
      "Каждую из этих книг можно купить онлайн или скачать.\n",
      "\n",
      "\n",
      "Автоматизация рутинных задач с помощью Python. Практическое руководство для начинающих. Аль Свейгарт. От автора: «Эта книга для офисных работников, студентов, администраторов и всех, кто использует компьютер. Она научит Вас создавать небольшие практические программы для автоматизации задач на своем компьютере».\n",
      "Automate the Boring Stuff with Python — Practical Programming for Total Beginners (читать в оригинале)\n",
      "Купить печатную версию\n",
      "\n",
      "Мысли на Питоне: Думать как программист — классическая книга Аллена Дауни с открытым исходным кодом, в которую внесли вклад Джеффри Элкнер и Крис Мейерс. Питер Вентворт обновил руководство до Python 3.\n",
      "How to Think Like a Computer Scientist (читать в оригинале)\n",
      "Купить печатную версию (только в оригинале)\n",
      "\n",
      "Создание игр с Python & Pygame от Аль Свейгарта представляет фреймворк Pygame для новичков и программистов среднего уровня, создающих графические игры.\n",
      "Making Games with Python & Pygame (читать в оригинале)\n",
      "Купить печатную версию (только в оригинале)\n",
      "\n",
      "\n",
      "Интерактивные курсы\n",
      "На этих сайтах вы можете получить мгновенную обратную связь по проблемам, возникающим, в процессе обучения.\n",
      "\n",
      "\n",
      "CheckiO — это геймифицированный веб-сайт, содержащий задачи по программированию, которые можно решить на Python 3.\n",
      "\n",
      "Python on Codecademy — портал для обучения.\n",
      "\n",
      "Code the blocks объединяет программирование на Python с трехмерной средой, в которой вы можете «размещать блоки» и строить структуры. Также вы найдете там учебные пособия, которые постепенно научат Вас создавать сложные 3D-структуры.\n",
      "\n",
      "Computer Science Circles содержит 30 уроков, 100 упражнений. На сайте есть встроенный чат, куда ученик может обратиться за помощью к учителю. Общение в чате происходит на голландском, французском, немецком и литовском языках.\n",
      "\n",
      "DataCamp Python Tutorial. В отличие от большинства других учебных пособий по Python, этот 4-х часовой учебный курс от DataCamp фокусируется на Python для Data Science. Он содержит 57 интерактивных упражнений и 11 видеоуроков.\n",
      "\n",
      "Finxter. Насколько хороши Ваши навыки Python? Более 300 отобранных головоломок на Python для тестов и тренировок.\n",
      "\n",
      "How to Think Like a Computer Scientist: Interactive Edition — это интерактивное переосмысление книги Элкнера, Дауни и Майера с визуализацией и звуковыми пояснениями.\n",
      "\n",
      "\n",
      "Ресурсы для младших школьников\n",
      "\n",
      "Build a «Pypet». Изучите основы программирования на Python, создавая тамагочи в стиле «Pypet» от Татьяны Тилоски.\n",
      "\n",
      "Guido van Robot  Учебный инструмент, в котором учащиеся пишут простые программы с использованием языка, похожего на Python, для управления моделируемым роботом. Проект включает план обучения, так как прошел испытания в средней школе Йорктауна.\n",
      "\n",
      "Python for Kids от Джейсона Р. Бриггса. Книга с примерами кода и головоломками.\n",
      "\n",
      "PythonTurtle обучающая среда на Python, подходящая для начинающих и детей. Ресурс ориентирован в преимущественно на детей, но, как известно, успешно используется взрослыми.\n",
      "\n",
      "Young Coders tutorial Это полный текст учебника, который ежегодно преподается в PyCon (Северная Америка), с примерами и упражнениями. Это учебное пособие дает базовые навыки и выстраивает работу со сложной логикой и играми. Подходит детям от 10 лет и начинающим взрослым.\n",
      "\n",
      "Webucator's self-paced Python 3 course свободно могут использовать для домашнего обучения школьники и студенты старше 13 лет. Исходя из нашего опыта, обучающиеся этого возраста усваивают материал так же быстро, как и взрослые новички в программировании.\n",
      "\n",
      "\n",
      "\n",
      "Учебники и сайты\n",
      "\n",
      "Byte of Python от К. Сварупа подойдет для людей, не имеющих опыта программирования.\n",
      "Читать Укус питона (на русском).\n",
      "\n",
      " After Hours Programming Python 3 Tutorial . Этот учебник создан для того, чтобы научить основам языка программирования на Python и объяснить, как создавать веб-приложения. \n",
      "\n",
      "Ask Python Учебник по Python для совсем начинающих.\n",
      "\n",
      "Classpert — Python — большая коллекция бесплатных и платных онлайн-курсов Python от разных авторов.\n",
      "\n",
      "Hackr.io — лучшие уроки и курсы Python от сообщества программистов.\n",
      "\n",
      "Hands-on Python Tutorial  — основы Python, графика и простые клиент/серверные взаимодействия (с видеоуроками).\n",
      "\n",
      "Learning to Program Введение в программирование для тех, кто никогда раньше не программировал, от Алана Голда. В руководстве представлено несколько языков программирования, но особое внимание уделено Python (Python 2 и 3).\n",
      "\n",
      "Letsfindcourse ` Python: лучшие учебники и курсы по Python, рекомендованные экспертами.\n",
      "\n",
      "The Wikibooks Non-Programmer's Tutorial for Python руководство по Python для НЕпрограммистов от Джоша Кольяти.\n",
      "\n",
      "Изучите Python Вводное руководство для начинающих о Python (с последующим углубленным изучением).\n",
      "\n",
      "Обучение программированию Алана Голда\n",
      "\n",
      " Python tips  — это блог, который включает в себя советы по Python и учебники для начинающих и профессиональных программистов.\n",
      "\n",
      "Python Tutorial in Python's documentation set. Написано не для НЕрограммистов, но дает представление о вкусе и стиле языка.\n",
      "\n",
      "The Python-Course.eu's extensive tutorial for complete beginners — учебник для начинающих с большим количеством иллюстраций.\n",
      "\n",
      "Pythonspot Tutorials учебники по Python для разных уровней.\n",
      "\n",
      "The Python Guru — руководство для начинающих программистов.\n",
      "\n",
      " Top Courses to Learn Python—руководства по Python, представленные и оцененные разработчиками Python (с топом лучших).\n",
      "\n",
      "\n",
      "Учебники для научной аудитории\n",
      "Эти сайты созданы для помощи научным курсам, но представленная информация достаточно общая, потому может пригодиться и для новичков.\n",
      "\n",
      "\n",
      "Beginning Python for Bioinformatics  Патрика О'Брайена. Введение в Python для биологов.\n",
      "(К сожалению не удалось найти в сети. Книги с похожей тематикой: Bioinformatics Programming Using Python и Bioinformatics with Python Cookbook)\n",
      "\n",
      "Python for Number Theory — это серия уроков по Python (для Jupyter) для применения этого языка программирования в теории чисел и криптографии. Они не требуют предшествующего опыта программирования и подходят для тех, кто изучает элементарную теорию чисел. Введение курса включают тест простоты и протокол Диффи — Хеллмана.\n",
      "\n",
      "Python for biologists содержит различные ресурсы по обучению на Python для людей с опытом работы в области биологии.\n",
      "\n",
      "\n",
      "Видео\n",
      "\n",
      "Python 3 видео уроки для начинающих\n",
      "\n",
      "Руководства по программированию на Python для начинающих: установка, ИСР(IDE), переменные, функции, строки, списки, ООП(OOP).\n",
      "\n",
      "Young Programmers Podcast содержит видео уроки на Python, Pygame, Jython, Scratch, Alice, Java и Scala.\n",
      "\n",
      "\n",
      "\n",
      "Инструменты\n",
      "\n",
      "Thonny — интегрированная среда разработки для новичков.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Узнайте подробности, как получить востребованную профессию с нуля или Level Up по навыкам и зарплате, пройдя онлайн-курсы SkillFactory:\n",
      "\n",
      "\n",
      "Курс «Профессия Data Scientist» (24 месяца) \n",
      " Курс «Профессия Data Analyst» (18 месяцев)\n",
      "Курс «Python для веб-разработки» (9 месяцев)\n",
      " \n",
      "Читать еще\n",
      "\n",
      "450 бесплатных курсов от Лиги Плюща\n",
      "Бесплатные курсы по Data Science от Harvard University\n",
      "30 лайфхаков чтобы пройти онлайн-курс до конца\n",
      "Самый успешный и самый скандальный Data Science проект: Cambridge Analytica\n",
      "    \n",
      " Перевод главы 13 Параллелизм \n",
      "из книги ‘Expert Python Programming’, \n",
      "Second Edition \n",
      "Michał Jaworski & Tarek Ziadé, 2016\n",
      "\n",
      "Асинхронное программирование \n",
      "В последние годы асинхронное программирование приобрело большую популярность. Python 3.5 наконец-то получил некоторые синтаксические функции, закрепляющие концепции асинхронных решений. Но это не значит, что асинхронное программирование стало возможным только начиная с Python 3.5. Многие библиотеки и фреймворки были предоставлены намного раньше, и большинство из них имеют происхождение в старых версиях Python 2. Существует даже целая альтернативная реализация Python, называемая Stackless (см. Главу 1 «Текущее состояние Python»), которая сосредоточена на этом едином подходе программирования. Для некоторых решений, таких как Twisted, Tornado или Eventlet, до сих пор существуют активные сообщества, и их действительно стоит знать. В любом случае, начиная с Python 3.5, асинхронное программирование стало проще, чем когда-либо прежде. Таким образом, ожидается, что его встроенные асинхронные функции заменят большую часть старых инструментов, или внешние проекты постепенно превратятся в своего рода высокоуровневые фреймворки, основанные на встроенных в Python.\n",
      "\n",
      "При попытке объяснить, что такое асинхронное программирование, проще всего думать об этом подходе как о чем-то похожем на потоки, но без системного планированщика. Это означает, что асинхронная программа может одновременно обрабатывать задачи, но ее контекст переключается внутри, а не системным планировщиком.\n",
      "\n",
      "Но, конечно, мы не используем потоки для параллельной обработки задач в асинхронной программе. Большинство решений используют разные концепции и, в зависимости от реализации, называются по-разному. Некоторые примеры имен, используемых для описания таких параллельных программных объектов:\n",
      "\n",
      "\n",
      "Green threads — Зеленые потоки (greenlet, gevent или eventlet проекты)\n",
      "Coroutines — сопрограммы (чистое асинхронное программирование на Python 3.5)\n",
      "Tasklets (Stackless Python) Это, в основном, те же понятия, но часто реализуемые немного по-другому. \n",
      "\n",
      "По понятным причинам в этом разделе мы сосредоточимся только на сопрограммах, которые изначально поддерживаются Python, начиная с версии 3.5.\n",
      "\n",
      "Совместная многозадачность и асинхронный ввод / вывод\n",
      "Совместная многозадачность является ядром асинхронного программирования. В этом смысле многозадачность в операционной системе не обязана инициировать переключение контекста (другому процессу или потоку), а вместо этого каждый процесс добровольно освобождает управление, когда находится в режиме ожидания, чтобы обеспечить одновременное выполнение нескольких программ. Вот почему это называется совместным. Все процессы должны сотрудничать для того, чтобы многозадачность осуществлялась успешно.\n",
      "\n",
      "Модель многозадачности иногда использовалась в операционных системах, но сейчас ее вряд ли можно найти в качестве решения системного уровня. Это связано с тем, что существует риск того, что одна плохо спроектированная служба может легко нарушить стабильность всей системы. Планирование потоков и процессов с помощью переключателей контекста, управляемых непосредственно операционной системой, в настоящее время является доминирующим подходом для параллелизма на системном уровне. Но совместная многозадачность все еще является отличным инструментом параллелизма на уровне приложений.\n",
      "\n",
      "Говоря о совместной многозадачности на уровне приложений, мы не имеем дело с потоками или процессами, которым необходимо освободить управление, поскольку все выполнение содержится в одном процессе и потоке. Вместо этого у нас есть несколько задач (сопрограммы, tasklets и зеленые потоки), которые передают управление одной функции, управляющей координацией задач. Эта функция обычно является своего рода циклом событий.\n",
      "\n",
      "Чтобы избежать путаницы (из-за терминологии Python), теперь мы будем называть такие параллельные задачи сопрограммами. Самая важная проблема в совместной многозадачности — когда передать контроль. В большинстве асинхронных приложений управление передается планировщику или циклу событий при операциях ввода-вывода. Независимо от того, читает ли программа данные из файловой системы или осуществляет связь через сокет, такая операция ввода-вывода всегда связана с некоторым временем ожидания, когда процесс становится бездействующим. Время ожидания зависит от внешнего ресурса, поэтому это хорошая возможность освободить управление, чтобы другие сопрограммы могли выполнять свою работу, пока им тоже не придется ждать, что такой подход несколько похожим по поведению на то, как многопоточность реализована в Python. Мы знаем, что GIL сериализует потоки Python, но он также освобождается при каждой операции ввода-вывода. Основное различие заключается в том, что потоки в Python реализованы как потоки системного уровня, поэтому операционная система может в любой момент выгрузить текущий запущенный поток и передать управление другому. \n",
      "\n",
      "В асинхронном программировании задачи никогда не прерываются главным циклом событий. Вот почему этот стиль многозадачности также называется не приоритетной многозадачностью.\n",
      "\n",
      "Конечно, каждое приложение Python работает в операционной системе, где есть другие процессы, конкурирующие за ресурсы. Это означает, что операционная система всегда имеет право выгрузить весь процесс и передать управление другому. Но когда наше асинхронное приложение запускается обратно, оно продолжается с того же места, где оно было приостановлено, когда системный планировщик вмешался. Именно поэтому сопрограммы в данном контексте считаются не вытесняющими.\n",
      "\n",
      "Ключевые слова async и await в Python\n",
      "Ключевые слова async и await являются основными строительными блоками в асинхронном программировании Python.\n",
      "\n",
      "Ключевое слово async, используемое перед оператором def, определяет новую сопрограмму. Выполнение функции сопрограммы может быть приостановлено и возобновлено в строго определенных обстоятельствах. Его синтаксис и поведение очень похожи на генераторы (см. Главу 2 «Рекомендации по синтаксису» ниже уровня класса). Фактически, генераторы должны использоваться в более старых версиях Python для реализации сопрограмм. Вот пример объявления функции, которая использует ключевое слово async:\n",
      "\n",
      "async def async_hello():\n",
      "       print(\"hello, world!\")\n",
      "\n",
      "Функции, определенные с помощью ключевого слова async, являются специальными. При вызове они не выполняют код внутри, а вместо этого возвращают объект сопрограммы:\n",
      "\n",
      ">>>> async def async_hello():\n",
      "...     print(\"hello, world!\")\n",
      "...\n",
      ">>> async_hello()\n",
      "<coroutine object async_hello at 0x1014129e8>\n",
      "\n",
      "Объект сопрограммы (coroutine object) ничего не делает, пока его выполнение не запланировано в цикле событий. Модуль asyncio доступен для предоставления базовой реализации цикла событий, а также множества других асинхронных утилит:\n",
      "\n",
      ">>> import asyncio\n",
      ">>> async def async_hello():\n",
      "...     print(\"hello, world!\")\n",
      "...\n",
      ">>> loop = asyncio.get_event_loop()\n",
      ">>> loop.run_until_complete(async_hello())\n",
      "hello, world!\n",
      ">>> loop.close()\n",
      "\n",
      "Естественно, создавая только одну простую сопрограмму, в нашей программе мы не осуществляем параллелизма. Чтобы увидеть что-то действительно параллельное, нам нужно создать больше задач, которые будут выполняться циклом событий.\n",
      "\n",
      "Новые задачи можно добавить в цикл, вызвав метод loop.create_task () или предоставив другой объект для ожидания использования функции asyncio.wait (). Мы будем использовать последний подход и попытаемся асинхронно напечатать последовательность чисел, сгенерированных с помощью функции range ():\n",
      "\n",
      "import asyncio\n",
      "\n",
      "async def print_number(number):\n",
      "       print(number)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "       loop = asyncio.get_event_loop()\n",
      "       loop.run_until_complete(\n",
      "           asyncio.wait([\n",
      "               print_number(number)\n",
      "               for number in range(10)\n",
      "           ])\n",
      "       )\n",
      "       loop.close()\n",
      "\n",
      "Функция asyncio.wait () принимает список объектов сопрограмм и немедленно возвращается. Результатом является генератор, который выдает объекты, представляющие будущие результаты (фьючерсы). Как следует из названия, он используется для ожидания завершения всех предоставленных сопрограмм. Причина, по которой он возвращает генератор вместо объекта сопрограммы, заключается в обратной совместимости с предыдущими версиями Python, что будет объяснено позже. Результат выполнения этого скрипта может быть следующим:\n",
      "\n",
      "$ python asyncprint.py\n",
      "0\n",
      "7\n",
      "8\n",
      "3 9 4 1 5 2 6 \n",
      "\n",
      "Как мы видим, числа печатаются не в том порядке, в котором мы создали наши сопрограммы. Но это именно то, чего мы хотели достичь.\n",
      "\n",
      "Второе важное ключевое слово, добавленное в Python 3.5, await. Он используется для ожидания результатов сопрограммы или будущего события (поясняется позже) и освобождения контроля над выполнением в цикле событий. Чтобы лучше понять, как это работает, нам нужно рассмотреть более сложный пример кода.\n",
      "\n",
      "Допустим, мы хотим создать две сопрограммы, которые будут выполнять некоторые простые задачи в цикле:\n",
      "\n",
      "\n",
      "Подождать случайное количество секунд\n",
      " Распечатать некоторый текст, предоставленный в качестве аргумента, и количество времени, проведенного в ожидании. Начнем с простой реализации, в которой есть некоторые проблемы параллелизма, которые мы позже попытаемся улучшить с помощью дополнительного использования await:\n",
      "\n",
      "import time\n",
      "import random\n",
      "import asyncio\n",
      " ",
      "\n",
      "async def waiter(name):\n",
      "       for _ in range(4):\n",
      " ",
      "           time_to_sleep = random.randint(1, 3) / 4\n",
      "           time.sleep(time_to_sleep)\n",
      "           print(\n",
      " ",
      "               \"{} waited {} seconds\"\n",
      " ",
      "               \"\".format(name, time_to_sleep)\n",
      "           )\n",
      " ",
      "   \n",
      "async def main():\n",
      "       await asyncio.wait([waiter(\"foo\"), waiter(\"bar\")])\n",
      " ",
      "   \n",
      "if __name__ == \"__main__\":\n",
      "       loop = asyncio.get_event_loop()\n",
      "       loop.run_until_complete(main())\n",
      "       loop.close()\n",
      "\n",
      "\n",
      "При выполнении в терминале (с помощью команды времени для измерения времени) можно увидеть:\n",
      "\n",
      "$ time python corowait.py\n",
      "bar waited 0.25 seconds\n",
      "bar waited 0.25 seconds\n",
      "bar waited 0.5 seconds\n",
      "bar waited 0.5 seconds\n",
      "foo waited 0.75 seconds\n",
      "foo waited 0.75 seconds\n",
      "foo waited 0.25 seconds\n",
      "foo waited 0.25 seconds\n",
      "real 0m3.734s user 0m0.153s sys 0m0.028s \n",
      "\n",
      "\n",
      "Как мы видим, обе сопрограммы завершили свое выполнение, но не асинхронно. Причина в том, что они обе используют функцию time.sleep (), которая блокирует, но не освобождает элемент управления в цикле событий. Это будет работать лучше в многопоточной установке, но мы не хотим сейчас использовать потоки. Итак, как мы можем это исправить?\n",
      "\n",
      "Ответ заключается в том, чтобы использовать asyncio.sleep (), которая является асинхронной версией time.sleep (), и ожидать результата с помощью ключевого слова await. Мы уже использовали это утверждение в первой версии функции main (), но это было только для улучшения ясности кода. Это явно не делало нашу реализацию более параллельной. Давайте посмотрим на улучшенную версию сопрограммы waiter (), которая использует await asyncio.sleep(): \n",
      "\n",
      "async def waiter(name):\n",
      "       for _ in range(4):\n",
      "           time_to_sleep = random.randint(1, 3) / 4\n",
      "           await asyncio.sleep(time_to_sleep)\n",
      "           print(\n",
      "               \"{} waited {} seconds\"\n",
      "               \"\".format(name, time_to_sleep)\n",
      "           )\n",
      "\n",
      "\n",
      "Запустив обновленный скрипт, мы увидим, как выходные данные двух функций чередуются друг с другом:\n",
      "\n",
      "$ time python corowait_improved.py\n",
      "bar waited 0.25 seconds\n",
      "foo waited 0.25 seconds\n",
      "bar waited 0.25 seconds\n",
      "foo waited 0.5 seconds\n",
      "foo waited 0.25 seconds\n",
      "bar waited 0.75 seconds\n",
      "foo waited 0.25 seconds\n",
      "bar waited 0.5 seconds\n",
      "real  0m1.953s\n",
      "user  0m0.149s\n",
      "sys   0m0.026s\n",
      "\n",
      "\n",
      "Дополнительным преимуществом этого простого улучшения является то, что код работает быстрее. Общее время выполнения было меньше, чем сумма всех времен сна, потому что сопрограммы поочередно освобождали контроль.\n",
      "\n",
      "Asyncio в предыдущих версиях Python\n",
      "Модуль asyncio появился в Python 3.4. Так что это единственная версия Python, которая имеет серьезную поддержку асинхронного программирования до Python 3.5. К сожалению, похоже, что этих двух последующих версий достаточно, чтобы представить проблемы совместимости.\n",
      "\n",
      "Как ни крути, ядро асинхронного программирования в Python было введено раньше, чем элементы синтаксиса, поддерживающие этот шаблон. Лучше поздно, чем никогда, но это создало ситуацию, когда есть два синтаксиса для работы с сопрограммами.\n",
      "\n",
      "Начиная с Python 3.5, вы можете использовать async и await: \n",
      "\n",
      "async def main ():\n",
      "    await asyncio.sleep(0)\n",
      "\n",
      "\n",
      "Однако в Python 3.4, прийдется дополнительно применить asyncio.coroutine декоратор и yield в тексте сопрограммы: \n",
      "\n",
      "@asyncio.couroutine\n",
      "def main():\n",
      "       yield from asyncio.sleep(0)\n",
      "\n",
      "\n",
      "Другим полезным фактом является то, что оператор yield from был введен в Python 3.3, а в PyPI имеется асинхронный бэкпорт. Это означает, что вы также можете использовать эту реализацию совместной многозадачности с Python 3.3.\n",
      "\n",
      "Практический пример асинхронного программирования\n",
      "Как уже неоднократно упоминалось в этой главе, асинхронное программирование является отличным инструментом для обработки операций ввода-вывода. Настало время создать что-то более практичное, чем простая печать последовательностей или асинхронное ожидание.\n",
      "\n",
      "В целях обеспечения согласованности мы попытаемся решить ту же проблему, которую решили с помощью многопоточности и многопроцессорности. Поэтому мы попытаемся асинхронно извлечь некоторые данные из внешних ресурсов через сетевое соединение. Было бы здорово, если бы мы могли использовать тот же пакет python-gmaps, что и в предыдущих разделах. К сожалению, мы не можем.\n",
      "\n",
      "Создатель python-gmaps был немного ленив и взял лишь название. Чтобы упростить разработку, он выбрал пакет запросов в качестве своей клиентской библиотеки HTTP. К сожалению, запросы не поддерживают асинхронный ввод-вывод с async и await. Есть некоторые другие проекты, которые нацелены на обеспечение некоторого параллелизма для проекта запросов, но они либо полагаются на Gevent (grequests, см. Https://github.com/ kennethreitz / grequests), либо на выполнение пула потоков / процессов (запросы-futures, обратитесь к github.com/ross/requests-futures). Ни один из них не решает нашу проблему.\n",
      "\n",
      "Прежде чем уперкать, что я ругаю невинного разработчика с открытым исходным кодом, успокойся. Человек, стоящий за пакетом python-gmaps, это я. Плохой выбор зависимостей является одной из проблем этого проекта. Мне просто нравится время от времени публично критиковать себя. Для меня это будет горьким уроком, так как python-gmaps в его последней версии (0.3.1 на момент написания этой книги) не может быть легко интегрирован с асинхронным вводом-выводом Python. В любом случае, это может измениться в будущем, поэтому ничего не потеряно.\n",
      " Зная об ограничениях библиотеки, которую было так легко использовать в предыдущих примерах, нам нужно создать что-то, что заполнит этот пробел. Google MapsAPI действительно прост в использовании, поэтому мы создадим на скорую руку асинхронную утилиту только для иллюстрации. В стандартной библиотеке Python версии 3.5 по-прежнему отсутствует библиотека, которая бы выполняла асинхронные HTTP-запросы так же просто, как вызов urllib.urlopen (). Мы определенно не хотим создавать полную поддержку протокола с нуля, поэтому мы будем использовать небольшую справку из пакета aiohttp, доступного в PyPI. Это действительно многообещающая библиотека, которая добавляет как клиентские, так и серверные реализации для асинхронного HTTP. Вот небольшой модуль, построенный поверх aiohttp, который создает одну вспомогательную функцию geocode (), которая выполняет запросы геокодирования к службе Google Maps API:\n",
      "\n",
      "import aiohttp\n",
      "\n",
      "session = aiohttp.ClientSession()\n",
      "async def geocode(place):\n",
      "       params = {\n",
      "           'sensor': 'false',\n",
      "           'address': place\n",
      "       }\n",
      "\n",
      "       async with session.get(\n",
      "           'https://maps.googleapis.com/maps/api/geocode/json',\n",
      "           params=params\n",
      "       ) as response:\n",
      "           result = await response.json()\n",
      "           return result['results']\n",
      "\n",
      "\n",
      "Давайте предположим, что этот код хранится в модуле с именем asyncgmaps, который мы будем использовать позже. Теперь мы готовы переписать пример, используемый при обсуждении многопоточности и многопроцессорности. Ранее мы использовали для разделения всей операции на два отдельных этапа:\n",
      "\n",
      "\n",
      "Выполните все запросы к внешней службе параллельно, используя функцию fetch_place ().\n",
      "Отобразите все результаты в цикле, используя функцию present_result ().\n",
      "\n",
      "Но поскольку совместная многозадачность совершенно отличается от использования нескольких процессов или потоков, мы можем немного изменить наш подход. Большинство проблем, поднятых в разделе Использование одного потока на элемент, больше не являются нашей заботой. \n",
      "Сопрограммы не являются вытесняющими, поэтому мы можем легко отображать результаты сразу после получения ответов HTTP. Это упростит наш код и сделает его более понятным:\n",
      "\n",
      "import asyncio\n",
      "# note: local module introduced earlier\n",
      "from asyncgmaps import geocode, session\n",
      "\n",
      "PLACES = (\n",
      "       'Reykjavik', 'Vien', 'Zadar', 'Venice',\n",
      "       'Wrocław', 'Bolognia', 'Berlin', 'Słubice',\n",
      "       'New York', 'Dehli',\n",
      ") \n",
      "async def fetch_place(place):\n",
      "       return (await geocode(place))[0]\n",
      "\n",
      "async def present_result(result):\n",
      "       geocoded = await result\n",
      "       print(\"{:>25s}, {:6.2f}, {:6.2f}\".format(\n",
      "           geocoded['formatted_address'],\n",
      "           geocoded['geometry']['location']['lat'],\n",
      "           geocoded['geometry']['location']['lng'],\n",
      "       )) \n",
      "\n",
      "async def main():\n",
      "       await asyncio.wait([\n",
      "           present_result(fetch_place(place))\n",
      "           for place in PLACES\n",
      "       ])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "       loop = asyncio.get_event_loop()\n",
      "       loop.run_until_complete(main())\n",
      "       # aiohttp will raise issue about unclosed\n",
      "       # ClientSession so we perform cleanup manually\n",
      "       loop.run_until_complete(session.close())\n",
      "       loop.close()\n",
      "\n",
      "\n",
      "Асинхронное программирование отлично подходит для бэкэнд-разработчиков, заинтересованных в создании масштабируемых приложений. На практике это один из наиболее важных инструментов для создания высококонкурентных серверов.\n",
      "\n",
      "Но реальность печальна. Многие популярные пакеты, которые имеют дело с проблемами ввода-вывода, не предназначены для использования с асинхронным кодом. Основными причинами этого являются:\n",
      "\n",
      "\n",
      "Все еще низкое внедрение Python 3 и некоторых его расширенных возможностей\n",
      "Низкое понимание различных концепций параллелизма среди начинающих изучать Python \n",
      "\n",
      "Это означает, что очень часто миграция существующих синхронных многопоточных приложений и пакетов либо невозможна (из-за архитектурных ограничений), либо слишком дорога. Многие проекты могли бы извлечь большую пользу от внедрения асинхронного стиля многозадачности, но только некоторые из них в конечном итоге сделают это. Это означает, что прямо сейчас вы будете испытывать много трудностей при попытке создать асинхронные приложения с самого начала. В большинстве случаев это будет похоже на проблему, упомянутую в разделе «Практический пример асинхронного программирования» — несовместимые интерфейсы и несинхронная блокировка операций ввода-вывода. Конечно, иногда вы можете отказаться от ожидания, когда вы испытываете такую несовместимость, и просто синхронно получать необходимые ресурсы. Но это будет блокировать выполнение каждой другой сопрограммой своего кода, пока вы ждете результатов. Технически это работает, но также разрушает все преимущества асинхронного программирования. Таким образом, в конце концов, объединение асинхронного ввода-вывода с синхронным вводом-выводом не вариант. Это игра типа «все или ничего». \n",
      "\n",
      "Другая проблема — длительные операции с привязкой к процессору. Когда вы выполняете операцию ввода-вывода, не проблема выпустить управление из сопрограммы. При записи / чтении из файловой системы или сокета вы, в конце концов, будете ждать, так что вызов с использованием await — лучшее, что вы можете сделать. Но что делать, если вам нужно что-то вычислить, и вы знаете, что это займет некоторое время? Конечно, вы можете разделить проблему на части и отменить контроль каждый раз, когда вы немного продвигаете работу. Но вскоре вы обнаружите, что это не очень хорошая модель. Такая вещь может сделать код беспорядочным, а также не гарантирует хорошие результаты. \n",
      "\n",
      "Временная привязка должна быть ответственностью интерпретатора или операционной системы.\n",
      "\n",
      "Объединение асинхронного кода с асинхронным использованием фьючерсов\n",
      "Так что делать, если у вас есть код, который выполняет длинные синхронные операции ввода-вывода, которые вы не можете или не хотите переписывать. Или что делать, когда вам приходится выполнять некоторые тяжелые операции с процессором в приложении, разработанном в основном с учетом асинхронного ввода-вывода? Ну… тебе нужно найти обходной путь. И под этим я подразумеваю многопоточность или многопроцессорность.\n",
      "\n",
      "Это может звучать не очень хорошо, но иногда лучшим решением может быть то, от чего мы пытались убежать. Параллельная обработка ресурсоемких задач в Python всегда выполняется лучше благодаря многопроцессорности. И многопоточность может справляться с операциями ввода-вывода одинаково хорошо (быстро и без больших затрат ресурсов), как асинхронное и ожидающее, если правильно настроена и обрабатывается с осторожностью.\n",
      "\n",
      "Поэтому иногда, когда вы не знаете, что делать, когда что-то просто не подходит вашему асинхронному приложению, используйте фрагмент кода, который откладывает его на отдельный поток или процесс. Вы можете сделать вид, что это сопрограмма, освободить управление для цикла событий и в конечном итоге обработать результаты, когда они будут готовы. \n",
      "\n",
      "К счастью для нас, стандартная библиотека Python предоставляет модуль concurrent.futures, который также интегрирован с модулем asyncio. Эти два модуля вместе позволяют вам планировать функции блокировки, выполняемые в потоках или дополнительных процессах, как если бы это были асинхронные неблокирующие сопрограммы.\n",
      "\n",
      "Исполнители (executors) и фьючерсы (futures)\n",
      "Прежде чем мы увидим, как внедрить потоки или процессы в асинхронный цикл обработки событий, мы подробнее рассмотрим модуль concurrent.futures, который позже станет основным компонентом нашего так называемого обходного пути.\n",
      "\n",
      "Наиболее важными классами в модуле concurrent.futures являются Executor и Future.\n",
      "\n",
      "Executor представляет собой пул ресурсов, которые могут обрабатывать рабочие элементы параллельно. Это может показаться очень похожим по назначению на классы из многопроцессорного модуля — Pool и dummy.Pool — но имеет совершенно другой интерфейс и семантику. Это базовый класс, не предназначенный для реализации и имеющий две конкретные реализации:\n",
      "\n",
      "\n",
      "ThreadPoolExecutor: который представляет пул потоков\n",
      "ProcessPoolExecutor: который представляет пул процессов\n",
      "\n",
      "Каждый executor представляет три метода: \n",
      "\n",
      "\n",
      "submit (fn, * args, ** kwargs): планирует функцию fn для выполнения в пуле ресурсов и возвращает объект Future, представляющий выполнение вызываемого объекта\n",
      "map (func, * iterables, timeout = None, chunksize = 1): функция func выполняется над итерацией аналогично многопроцессорной обработке. Метод Pool.map ()\n",
      "shutdown (wait = True): это выключает Executor и освобождает все его ресурсы.\n",
      "\n",
      "Наиболее интересен метод — submit () из-за объекта Future, который он возвращает. Он представляет асинхронное выполнение вызываемого и только косвенно представляет его результат. Чтобы получить фактическое возвращаемое значение отправляемого вызываемого объекта, необходимо вызвать метод Future.result (). И если вызываемый объект уже завершен, метод result () не заблокирует его и просто вернет выходные данные функции. Если это не так, он заблокирует его, пока результат не будет готов. Относитесь к нему как к обещанию результата (на самом деле это та же концепция, что и обещание в JavaScript). Вам не нужно распаковывать его сразу же после его получения (с помощью метода result ()), но если вы попытаетесь это сделать, это гарантированно в конечном итоге что-то вернет:\n",
      "\n",
      ">>> def loudy_return():\n",
      "\t\t...      print(\"processing\")\n",
      "\t\t ",
      "\n",
      "\t\t...      return 42\n",
      "\t\t...\n",
      "\t\t>>> from concurrent.futures import ThreadPoolExecutor\n",
      "\t\t>>> with ThreadPoolExecutor(1) as executor:\n",
      "\t\t ",
      "\n",
      "...     future = executor.submit(loudy_return)\n",
      "...\n",
      "processing\n",
      ">>> future\n",
      "<Future at 0x33cbf98 state=finished returned int>\n",
      ">>> future.result()\n",
      "42\n",
      "\n",
      "\n",
      "Если вы хотите использовать метод Executor.map (), он не отличается по использованию от метода Pool.map () класса Pool многопроцессорного модуля:\n",
      "\n",
      "def main():\n",
      "       with ThreadPoolExecutor(POOL_SIZE) as pool:\n",
      "           results = pool.map(fetch_place, PLACES)\n",
      "       for result in results:\n",
      "           present_result(result)\n",
      "\n",
      "\n",
      "Использование Executor в цикле событий\n",
      "Экземпляры класса Future, возвращаемые методом Executor.submit (), концептуально очень близки к сопрограммам, используемым в асинхронном программировании. Вот почему мы можем использовать исполнителей, чтобы сделать гибрид между совместной многозадачностью и многопроцессорностью или многопоточностью.\n",
      "\n",
      "Ядром этого обходного пути является метод BaseEventLoop.run_in_executor (executor, func, * args) класса цикла событий. Это позволяет планировать выполнение функции func в процессе или пуле потоков, представленном аргументом executor. Самым важным в этом методе является то, что он возвращает новый ожидаемый объект (объект, который можно ожидать с помощью оператора await). Таким образом, благодаря этому вы можете выполнить блокирующую функцию, которая не является сопрограммой в точности как сопрограмма, и она не будет блокировать, независимо от того, сколько времени потребуется, чтобы закончить. Он остановит только функцию, ожидающую результатов от такого вызова, но весь цикл событий будет продолжаться.\n",
      "\n",
      "И полезным фактом является то, что вам даже не нужно создавать свой экземпляр executor. Если вы передадите None в качестве аргумента executor, класс ThreadPoolExecutor будет использоваться с числом потоков по умолчанию (для Python 3.5 это число процессоров, умноженное на 5).\n",
      "\n",
      "Итак, давайте предположим, что мы не хотели переписывать проблемную часть пакета python-gmaps, которая была причиной нашей головной боли. Мы можем легко отложить блокирующий вызов до отдельного потока с помощью вызова loop.run_in_executor (), при этом оставляя функцию fetch_place () в качестве ожидаемой сопрограммы:\n",
      "\n",
      "async def fetch_place(place):\n",
      "       coro = loop.run_in_executor(None, api.geocode, place)\n",
      "       result = await coro\n",
      "       return result[0]\n",
      "\n",
      "\n",
      "Такое решение хуже, чем наличие полностью асинхронной библиотеки для выполнения этой работы, но вы знаете, что хоть что-то лучше, чем ничего.\n",
      "\n",
      "После объяснения того, что такое параллелизм на самом деле, мы приступили к действиям и проанализировали одну из типичных параллельных проблем с помощью многопоточности. Выявив основные недостатки нашего кода и исправив их, мы обратились к многопроцессорной обработке, чтобы посмотреть, как она будет работать в нашем случае.\n",
      "\n",
      "После этого мы обнаружили, что с многопроцессорным модулем использовать несколько процессов намного проще, чем базовые потоки с многопоточностью. Но только после этого мы поняли, что можем использовать один и тот же API с потоками, благодаря multiprocessing.dummy. Таким образом, выбор между многопроцессорностью и многопоточностью теперь зависит только от того, какое решение лучше соответствует проблеме, а не какое решение имеет лучший интерфейс.\n",
      "\n",
      "Говоря о подгонке проблемы, мы наконец-то попробовали асинхронное программирование, которое должно быть лучшим решением для приложений, связанных с вводом / выводом, только чтобы понять, что мы не можем полностью забыть о потоках и процессах. Таким образом, мы сделали круг, обратно в то место, где мы начали!\n",
      "\n",
      "И это приводит нас к окончательному выводу этой главы. Нет решения, устраивающего всех. Есть несколько подходов, которые вы можете предпочесть или любить больше. Есть некоторые подходы, которые лучше подходят для данного набора проблем, но вам нужно знать их все, чтобы быть успешным. В реалистичных сценариях вы можете использовать весь арсенал инструментов и стилей параллелизма в одном приложении, и это не редкость.\n",
      "\n",
      "Предыдущий вывод — отличное введение в тему следующей главы, Глава 14 «Полезные шаблоны проектирования». Так как нет единого шаблона, который решит все ваши проблемы. Вы должны знать как можно больше, потому что в конечном итоге вы будете использовать их каждый день.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "If you are landing on this page you might be looking for several questions like:\n",
      "\r\n",
      "– NodeJS or Python: which is the right choice for my next web app development project?\n",
      "\r\n",
      "– Which programming language cost me less?\n",
      "\r\n",
      "– Which programming language is suitable for which industry?\n",
      "\r\n",
      "– Which programming language is suitable for small business or large scale enterprises?\n",
      "\r\n",
      "– Which programming language is scalable, high performing and secure?\n",
      "\r\n",
      "In this blog post, I’m going to answer all these questions AND MORE THAN THAT! So, continue reading this blog post:\n",
      "_________________________________________________________________________________\n",
      "1. Node.js VS Python: A Brief Intro_________________________________________________________________________________\n",
      "\r\n",
      "Both Node.js (majorly used as a backend framework), and Python (front-end and back-end programming language) are used extensively for programming of a web app. It is vital to select a suitable framework or programming language for web app development because it is the backbone of every web app.\n",
      "\r\n",
      "Node.js and Python are extensively used for this purpose. When you talk about Node.js or python, you are actually comparing JavaScript with Python. This is because Node.js is actually a framework built on Google Chrome’s JavaScript. Python vs Javascript\n",
      "\r\n",
      "Both of them are among the top programming languages according to the TOIBE index.\n",
      "\r\n",
      "Here is the list with May 2018 and May 2019 rankings.\n",
      "\n",
      "\n",
      "Python is maintaining its fourth position whereas JavaScript has climbed up from eighth to the seventh position. Undoubtedly, Python is more popular than JavaScript run-time, Node.js. The web app using Python are more in number in comparison to Node.js.\n",
      "\n",
      "\n",
      "\r\n",
      "Source: SimilarTech\n",
      "\r\n",
      "The stats depict that the popularity of Python is more than Node.js. But this does not showcase that Python is better than Node.js in all aspects and circumstances. In fact, choosing the best technology for the development of your project is not that simple.\n",
      "\r\n",
      "It requires an in-depth analysis of various factors and facts. Let’s discuss the major aspects to solve the popular query of Node.js vs Python: Which is better?\n",
      "_________________________________________________________________________________\n",
      "2. Node.js vs Python: Usage Statistics Industry-Wise_________________________________________________________________________________\n",
      "\r\n",
      "According to the above representation, Python is a popular choice in all domains under consideration. Python is being used since its launch in 1991 and has more coverage in websites under all categories.\n",
      "\n",
      "\n",
      "\r\n",
      "Although, this does not show a complete picture. Do you know why? It is true that Python is more popular in the entire web but NodeJs is used by top sites. Yes! NodeJS is more popular among top 100K websites, Top 10K websites, and Top 1M sites.\n",
      "\n",
      "\n",
      "\n",
      "Summary: Python rules the entire web but NodeJS rules the top web apps.\r\n",
      "_________________________________________________________________________________\n",
      "3. Node.js vs Python: Scalability_________________________________________________________________________________\n",
      "\r\n",
      "It is obvious that you want many new people to use your applications without any hindrance. The capacity of an application to support an increasing number of users without any flaw in its performance is known as scalability.\n",
      "\r\n",
      "Scalability through Node.js programming can be achieved naturally as it creates asynchronous architecture in a single thread. This default feature of this Javascript framework ensures smooth scalability of application written in NodeJs code.\n",
      "\r\n",
      "Simple web apps can be created with a complete guarantee for scalability by using the NodeJS framework. If you wish to provide the same convenience in the complex app then you will have to hire Node.js app developers who have in-depth knowledge. \n",
      "\r\n",
      "On the other hand, Python does not have the default feature for supporting asynchronous programming. This means that Python’s architecture is not as scalable as that of NodeJS. But Python has some tools to offer which allow achieving scalability easily.\n",
      "\n",
      "Summary: NodeJS provides easy scalability as compared to Python.\r\n",
      "_________________________________________________________________________________\n",
      "4. Node.js vs Python: Learning Curve_________________________________________________________________________________\n",
      "\r\n",
      "The learning curve is a measurement of the ability of users to write code in a particular language or framework. It depicts the fluency of web app developers in syntax and tools. In the case of NodeJS, if you know JavaScript then it will not be a problem to learn this framework.\n",
      "\r\n",
      "This makes it easy to learn. Probably this is the reason the NodeJS has topped the list of most popular frameworks with a remarkable score of 49.9%.\n",
      "\n",
      "\n",
      "\r\n",
      "On the other hand, Python language is not written in a well-known language like NodeJS. So, its syntax is new to the python developers. But it has a cleaner and compact code. One can easily write its code as they have to write fewer lines.\n",
      "\r\n",
      "A few lines of code in Python can help you reach the same result as NodeJS. Moreover, being an old language, it has well-developed documentation also. A software developer will never run short of knowledge base when it comes to Python.\n",
      "\r\n",
      "In the StackOverflow survey, Python is among the topmost preferred languages. Full-stack developers love to use this easy, precise, and efficient language for building apps. Here is the graphical representation of the same:\n",
      "\n",
      "\n",
      "\n",
      "Summary: Python is easier to learn than NodeJS if you do not know JavaScript.\r\n",
      "_________________________________________________________________________________\r\n",
      "Read more: How to hire a Python developer at 60% lost?\n",
      "_________________________________________________________________________________\n",
      "5. Node.js vs Python: Suitable Projects_________________________________________________________________________________\n",
      "\n",
      "\n",
      "\r\n",
      "Node.js is ideal for small projects in web application development and website development. It is not recommended for web development companies to use this framework of javascript for large projects because it lacks clear coding standards.\n",
      "\r\n",
      "On the other hand, Python is suitable for a vast range of projects from web apps, numerical computations, to machine learning and network programming. It is an ideal programming language to perform various tasks.\n",
      "\r\n",
      "Python has several frameworks that can be used for backend like, Django, Flask, Pyramids. It also has frameworks like Tkinter/PySide which can be used at the front-end. Its precise coding makes it ideal for large projects also.\n",
      "\r\n",
      "Therefore, if you are planning to get a large project developed then choose python developers. But if you have a small business or startup then your ideal choice should be to hire NodeJS web developers.\n",
      "\n",
      "Summary: Node.js is suitable for small projects whereas python can be used for large projects also. Python can be used to do a variety of operations like machine learning etc.\r\n",
      "_________________________________________________________________________________\n",
      "6. Node.js vs Python: Data and memory-intensive applications_________________________________________________________________________________\n",
      "\n",
      "\n",
      "\r\n",
      "Node.js is probably the best framework of contemporary times to develop data-intensive run-time applications. For example, you can develop apps that involve chat functionality. Such apps developed by Nodejs development companies can proficiently handle proxy, queued inputs, and data-streaming.\n",
      "\r\n",
      "The high speed offered by NodeJS has made it an ideal choice for heavy traffic sites like e-commerce stores as well. It is also suitable for memory-intensive activities like developing applications using 3D graphics.\n",
      "\r\n",
      "On the other hand, because of the lower speed, Python is not an ideal choice for real-time applications. It is also not suggested to use Python in memory-intensive applications.\n",
      "\n",
      "Summary: NodeJS wins over Python for developing data and memory-intensive applications. \r\n",
      "_________________________________________________________________________________\n",
      "7. Node.js vs Python: Performance_________________________________________________________________________________\n",
      "\n",
      "\n",
      "\r\n",
      "Performance is directly related to the speed of the application. This speed is the rate at which your code can be executed. This is the chief feature that we look for in any programming tool.\n",
      "\r\n",
      "The comparison of Python and Node.js on this parameter is not tough. We all know that NodeJS is based on Google Chrome’s version 8. It is a very powerful and fast engine. This has made the speed and performance of NodeJS extremely high.\n",
      "\r\n",
      "In comparison to Python, NodeJS wins in speed and performance. This is why it is preferred for chatting or messaging app development. While Python is not recommended in data-intensive run-time applications.\n",
      "\r\n",
      "Therefore, if your idea revolves around chatting functionalities then NodeJS is a better choice than Python. Select according to the core purpose of your web development project.\n",
      "\n",
      "Summary: NodeJS is significantly faster than Python.\n",
      "_________________________________________________________________________________\n",
      "8. Node.js vs Python: Error Handling_________________________________________________________________________________\n",
      "\r\n",
      "When the developers write codes, errors are an unavoidable part. The convenience and transparency in detecting errors are quite important when it comes to a programming tool. The tools we are discussing here are efficient in handling errors.\n",
      "\r\n",
      "Although, Python has an upper hand here as it takes lesser time to fix the bugs as well as errors. Node.js also has a good command in catching exceptions that occur during code execution.\n",
      "\r\n",
      "So, error handling will not suffer if you choose any of the two programming tools for the development of your website. This will also reduce the overall completion time and increases the chances of getting a robust website in the first attempt.\n",
      "\n",
      "Summary: NodeJS and Python are almost equal in catching and throwing errors. \r\n",
      "_________________________________________________________________________________\n",
      "9. Node.js vs Python: Use cases_________________________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Node.js use case:\n",
      "\n",
      "LinkedIn: One of the largest platforms for interaction among professionals. Node.js has successfully managed its ever-growing user base. Loads of messages, as well as connections, are very well taken care of by the high-speed attribute of this Javascript framework.\n",
      "\n",
      "eBay: A famous e-commerce store for C2C and B2C sales operations is used worldwide. The ability of Node.js for handling heavy traffic sites and data-intensive applications has been appreciated by eBay.\n",
      "\n",
      "Mozilla: Node.js is a programming tool that has allowed the same language to be used on both sides. Mozilla considers it a team consolidation tool as Node.js is used for both the client-side development and server-side development for its browser.\n",
      "\n",
      "Python Use Cases:\n",
      "\n",
      "United Space Alliance: For the NASA shuttle program software development, they chose Python. The massive data processing and ability to serve complex programs of Python programming language impressed them.\n",
      "\n",
      "Frequentis: The straightforward syntax of Python allowed this air traffic management, transport, and public safety solution provider to use it. Python is used for developing navigation and weather condition monitoring instruments of the company.\n",
      "\n",
      "Industrial Light and Magic: Python eased the process of maintaining outstanding batch processing abilities of ILM. It has proved itself the best choice for the processing of thousands of frames per day. \n",
      "_________________________________________________________________________________\n",
      "\n",
      "Conclusion:\r\n",
      "In the end, these are the three things that we can conclude:\n",
      "\r\n",
      "Python is used more in the entire web but Node.js is more popular among top websites\r\n",
      "Node.js provides more scalability, speed, and better performance, so more suitable for data and memory-intensive apps. Python is suitable for more variety of tasks.\r\n",
      "Both Node.js and python are easy to learn and settle errors efficiently.\n",
      "\r\n",
      "I hope that after reading this complete post, you have a clear idea about the most suitable programming tool for your web app development project. All web technologies are efficient in one or the other way, you just need to find out the most suitable one according to your business goals.\n",
      "\r\n",
      "If you have any suggestions or additions for this blog, then please share them with me in the comment section below. I am open to a healthy and informative discussion with readers because I believe that knowledge increases with sharing.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Мы строили-строили, и наконец построили: расписание Moscow Python Conf++ собрано, проверено, перепроверено и опубликовано. Не то чтобы работа Программного комитета на этом заканчивалась (за два-то месяца до конференции, ну-ну), но 10 месяцев явно потрачено не зря, и я с нетерпением жду результата, заложив все возможное для общения разработчиков друг с другом.\n",
      "\r\n",
      "Сейчас расскажу, какой получилась программа конференции, и выбора у нас просто не останется. На площадке в центре Москвы будет: 3 потока докладов, поток воркшопов и митапов, 4 Core-разработчика (я до сих пор не знаю, считать ли Python Core-разработчиком заведующего разработкой Pytest и Hypothesis), 6 зарубежных спикеров с нетривиальным опытом, доклады от Microsoft, Wargaming, JetBrains, Parallels, EPAM, Booking.com, Tinkoff и других не менее интересных компаний. Ни одной проходной темы, я проверил. Каждый докладчик по-своему интересен, и каждая тема точно найдет тех, кому есть что обсудить со спикером. В этой статье я максимально кратко расскажу обо всех наших гостях: акцент на спикерах, по темам вы и сами сориентируетесь.\n",
      "\n",
      "\n",
      "\n",
      "Англоязычные доклады\r\n",
      "Core-разработчиков и еще нескольких крутых специалистов слушаем по-английски — у нас не будет перевода. Мы пробовали, получается плохо. Поэтому надеемся, что воспринять профессиональную информацию все справятся, а вот с вопросами уже поможем. Доклады будут идти в три параллельных потока — если не хотите англоязычные доклады, то до 17:00 избегайте третий зал.\n",
      "\n",
      "Developing and deploying Python for secured environments\r\n",
      "Kushal Das — Core-разработчик CPython, активный участник Tor Project и член совета директоров PSF — на нашей конференции будет рассказывать о том, как безопасно разрабатывать и деплоить Python-код. Отличительная черта выступлений Кушала в том, что он регулярно обнародует «секретные» способы сломать Python-код и в противовес показывает, как написать код так, чтобы АНБ не смогло его взломать. Уверен, что у этого спикера есть что спросить.\n",
      "\n",
      "Sufficiently Advanced Testing\r\n",
      "Zac Hatfield-Dodds — Core-разработчик библиотеки для написания unit-тестов и мэйнтейнер Pytest. В ходе доклада Зак поделится своим мнением о современном тестировании и обсудит с гостями конференции их вопросы.\n",
      "\n",
      "Introduction to low-level profiling and tracing\r\n",
      "Christian Heimes — Core-разработчик CPython, член Python security team и Python Software Foundation, работает в Red Hat и специализируется на профилировании и ускорении Python-кода. Поэтому на Moscow Python Conf++ Кристиан расскажет, как понять, почему код тормозит, и что с этим делать.\n",
      "\r\n",
      "Этот доклад отлично дополнит воркшоп Алексея Романова из Wargaming, в рамках которого мы посмотрим, что эта огромная компания делает для ускорения своего кода и обсудим, как это может помочь в коде, с которым работаете вы.\n",
      "\n",
      "Mastering a data pipeline with Python\r\n",
      "Robson Luis Monteiro Junior, во-первых, активный участник мирового Python-сообщества, давно и регулярно выступает на всевозможных митапах и конференциях, во-вторых, эксперт в приготовлении данных для ML из Microsoft. Вследствие такого комбо нас ждёт доклад о приёмах и подводных камнях построения пайплайнов обработки данных на Python, дистилирующий многолетний опыт спикера в Microsoft. Робсон сравнит PySpark, Dask, Pandas, Airflow и Apache Arrow и поделится своими собственными рецептами решения прикладных задач.\n",
      "\n",
      "Cerberus or Data Validation for Humans\r\n",
      "Nicola Iarocci — автор большого количества популярных библиотек и создатель REST-фреймворка Eve. Но о вебе с ним можно будет поговорить в кулуарах, а доклад Никола посвятит другой своей разработке: фреймворку для валидации данных Cerberus. С вопросами валидации данных рано или поздно сталкивается каждый проект, поэтому обратите пристальное внимание на этот доклад в расписании конференции.\n",
      "\n",
      "The platform is dead, long live the platform\r\n",
      "Luka Kladaric расскажет, как затащить в облако огромный Python-проект. Задача актуальная — будет интересно послушать об этапах и результатах её успешного решения.\n",
      "\n",
      "\n",
      "\n",
      "Machine Learning и работа с данными\r\n",
      "Какая Python-конференция без машинного обучения. Конечно, мы не обойдем эту тему совсем стороной, а самые горячие темы на начало 2020 года это подготовка данных для обучения и взаимодействия с дейта сайентистами, которые пишут странное.\n",
      "\n",
      "Обработка данных в функциональном стиле с mPyPl\r\n",
      "Дмитрий Сошников работает в Microsoft уже 13 лет, десять из которых в роли технологического евангелиста. Как технический специалист по машинному обучению Дмитрий расскажет о наработках Microsoft в этом направлении и презентует open source библиотеку mPyPl. Она упрощает обработку данных с использованием Python за счет функциональных конвейеров данных. В докладе Дмитрий покажет примеры использования такого подхода для классификации и обнаружения изображений, распознавания событий на видео, рендеринга видео, рисования когнитивных портретов и др.\n",
      "\r\n",
      "Еще Дмитрий проведет воркшоп «Создаём портрет в жанре Science Art при помощи когнитивных сервисов и креативности». Из названия ясно, что это однозначно категория «расширить кругозор». Но прелесть в том, что это практический воркшоп, то есть за два часа вы сами (с помощью аффинных преобразования и когнитивных сервисов Microsoft) попробуете создать произведение цифрового искусства и заодно прокачаться в обработке изображений.\n",
      "\n",
      "From Scikit-learn To PySpark MLlib\r\n",
      "Машинное обучение — тема горячая, и еще один доклад в этом направлении сделает Андрей Гаврилов из EPAM. В данном случае речь пойдет о том, как адаптировать Data Science решение к работе в распределенной среде, в частности мигрировать cо Scikit-learn на аналоги из MLlib (PySpark). Андрей покажет, какие трудности возникают, когда переносишь существующий пайплайн на рельсы PySpark, и что может помочь с ними справиться — от архитектуры решения до особенностей тюнинга гиперпараметров.\n",
      "\n",
      "Все возможности JupyterHub для более чем 20 студентов или R&D команды\r\n",
      "Пётр Ермаков занимается машинным обучением в Lamoda и параллельно преподает Data Sceince в собственной школе DataGym. Пётр столкнулся с нетривиальной задачей настройки JupyterHub для большой Data Science команды и знает, как правильно организовать работу на общих серверах. Он поделится рекомендациями и готовыми рецептами с гостями конференции, и даже если ваша команда состоит всего из одного человека, вы все равно вынесете новые хитрости из этого доклада.\n",
      "\n",
      "\n",
      "\n",
      "Рефакторинг и работа с legacy\r\n",
      "2020 — год победившего Python 3. Ну или мы очень хотим в это верить. Для многих окончание поддержки Python 2 означает неизбежный (примерно, как снег этой зимой) переезд и сопутствующий рефакторинг legacy-кода. На Moscow Python Conf++ будем разбираться, как продать рефаторинг бизнесу, как сделать его технически, и, наверное, в принципе похоливарим на тему расстановки запятых во фразе «Все переписать нельзя рефакторить».\n",
      "\n",
      "Как решиться на Python3 в проекте, которому больше 10 лет и уговорить остальных\r\n",
      "Сначала Левон Авакян поможет нам посмотреть на проблему перехода Python 2 на Python 3 с точки зрения бизнеса. Левон расскажет о дискуссии, которая возникла в проекте World Of Tanks по этому поводу, какие аргументы за и против высказывались и к какому решению пришли в итоге.\n",
      "\r\n",
      "Тема «Как продать бизнесу переход с Python 2 на Python 3» сейчас актуальна как никогда прежде, и доклад Левона один из немногих докладов, на которые я точно пойду на конференции, несмотря на то, что я куратор и у нас будут подготовительные обсуждения и прогоны. Спойлер: там интрига.\n",
      "\n",
      "С двойки на тройку за 72 часа\r\n",
      "Кирилл Борисов в прошлом году рассказывал о работе с legacy-кодом и попытках внедрить в него тесты. В этот раз Кирилл расскажет о технической стороне рефакторинга кода с Python 2 на Python 3. Что может быть актуальнее, да еще если подкреплено опытом такой крупной компании как Booking.com.\n",
      "\n",
      "Как выжить, если вам достался legacy, разработчик которого слился\r\n",
      "Владимир Филонов — один из организаторов сообщества MoscowPython — за годы работы прокачал сверхспособность разбираться в чужом legacy-коде. То есть за несколько дней Владимир начинает ориентироваться в legacy-проекте лучше, чем его (уволившиеся много лет назад) авторы. На конференции Владимир, во-первых, даст пошаговую инструкцию, что делать, если вам достался legacy, разработчик которого слился. Во-вторых, во время воркшопа на практике покажет, как он это делает. И судя по трейлеру, который изготовил Владимир, все это будет очень качественно подготовлено и классно представлено.\n",
      "\n",
      "Автоматизация рефакторинга\r\n",
      "Владимир Протасов из Parallels хорошо знаком участникам российского и не только Python-сообщества. На Moscow Python Conf++ Владимир расскажет, как быть, когда IDE не справляется с задачей рефакторинга, а бегать по всей кодовой базе и вносить однотипные изменения нецелесообразно от слова «совсем». Специально для ленивых программистов, которые не любят выполнять повторяющиеся задачи, можно будет на реальных примерах посмотреть, как автоматизировать сложный рефакторинг.\n",
      "\n",
      "\n",
      "\n",
      "Бэкенд, Сельдерей, Кафка\r\n",
      "В этот раз у нас всего один доклад прямо касающийся веб-разработки (который со словом Django в названии), зато есть очереди, потоковая обработка сообщений и, конечно же, async/await.\n",
      "\n",
      "Многопоточность и async/await: подход разный, проблемы общие\n",
      "Доклад Андрея Светлова — Python Core Developer, разработчика asyncio и автора aiohttp — традиционно будет посвящен асинхронности. Как человек, начинавший с C++, Андрей знает на собственном опыте, что проблемы современных разработчиков с использованием асинхронного подхода далеко не новы. Копнув немного вглубь истории разработки, узнаем, как похожие проблемы решались 10, 20 лет назад, и попробуем понять, как с ними удобнее всего справляться сейчас.\n",
      "\n",
      "Статическая типизация в Django\r\n",
      "Максим Курников расскажет о типах для Django: какие существуют решения для проверки типов в Python, какие грабли встретились в ходе написания библиотеки django-stubs, какие перспективы у proposal по добавлению типов в ядро языка.\n",
      "\n",
      "Apache Kafka Event Streaming Platform for Python developers\r\n",
      "Виктор Гамов — соавтор книги «Enterprise Web Development» от O’Reilly, известный спикер и автор статей, а еще developer advocate в Confluent — компании, которая делает платформу потоковой обработки событий на основе Apache Kafka. О Кафке и пойдет речь в докладе Виктора: он покажет, насколько возможности Apache Kafka шире, чем просто обмен сообщениями, разберет основные моменты внутренней архитектуры и объяснит, как Python- разработчики могут использовать Кафку для работы с потоковыми данными.\n",
      "\n",
      "Выбор брокера для Celery\r\n",
      "Celery — одна из самых популярных очередей задач, но Владимир Колясинский расскажет не о работе с самим очередями, а о выборе брокера. В Яндексе долгое время в качестве брокера использовали MongoDB, но столкнулись с недостатками и стали искать альтернативы. Какие варианты рассматривали и почему в итоге пришли к YMQ (спойлер: нет, не только потому, что ее сделали в Яндексе), спикер расскажет на докладе.\n",
      "\n",
      "Очереди в Python: как приготовить салат из сельдерея, редиски и их друзей\r\n",
      "А еще Celery станет одним из главных героев митапа Игоря Мосягина. На митапе Игорь рассмотрит обработку долгих пользовательских запросов на примере библиотек rq, huey, и celery. Познакомит с текущим состоянием dask и расскажет «а как там это всё делать в стандартной библиотеке». Потому что дать дейта сайентистам что-то сложное иногда просто не получается: лапки и все такое.\n",
      "\n",
      "Интеграция Python-приложений с Windows API\r\n",
      "Василий Панков поделится историей ужасов в представлении многих современных разработчиков и расскажет, как в кровавом enterprise Python живет на Windows. Разработка Python-приложений для компаний, которые не используют Docker или WSL, а предпочитают MS Windows в чистом виде имеет много, действительно много особенностей. И Василий расскажет, как взаимодействовать с большинством из них. И не надо думать, что вы никогда с таким не столкнетесь. Иногда у таких компаний много, ну просто очень много денег на разработку. И Windows.\n",
      "\n",
      "\n",
      "\n",
      "Сам Python, эволюция и использование\r\n",
      "У Python большое и деятельное сообщество. Как следствие в язык все время проникают новые идеи (иногда — оператор моржика), появляются новые концепции, идет борьба пакетов и библиотек. Следить за всем этим простому разработчику не обязательно, но тому, кто хочет видеть дальше и знать больше, пригодится.\n",
      "\n",
      "Dependency Hell\r\n",
      "Автор библиотеки DepHell Никита Воронов, как можно догадаться, хорошо разбирается в болезненном для Python вопросе управления зависимостями. Мы недавно публиковали разговор с Никитой о том, в чем там дело, а из его доклада вы узнаете, кто побеждает в конкуренции между Pipenv и Poetry и как решать текущие задачи, когда не хватает pip.\n",
      "\n",
      "Типы? Типы… Типы!\r\n",
      "Виталий Брагилевский — член комитета по стандартизации Haskell, автор книги «Haskell in Depth». Его выступление на Moscow Python Conf++ не обойдется без сравнения Python с Haskell: Виталий расскажет про работу с типами и о том, чем строгая типизация может быть полезна нам с вами. Будет сложно, но интересно, особенно для тех Python-разработчиков, которые хотят новых возможностей для прицельного решения специфичных задач.\n",
      "\n",
      "О чём мечтают Java-роботы, глядя на Python\r\n",
      "Паша Финкельштейн сравнит Python и Java для написания бизнес-логики. У Паши за плечами 10 лет разработки на Java, и за это время он ни разу не встречал ничего похожего на джанговские «толстые модели». В марте узнаем, как такие штуки делаются в мире бизнес-разработки, как в Python, почему это кажется странным и какие есть решения.\n",
      "\n",
      "Metaprogramming в Python: мечта о генерации unit-тестов из кода\r\n",
      "Мы пригласили Юлию Волкову из GridDynamics, чтобы обсудить необычное в мире тестирования. Юлия, встретившись с очередным проектом абсолютно без тестов, задумалась, а что если можно автоматизировать создание какого-то набора тестов. Каких результатов по генерации unit-тестов из код получилось добиться, какие уроки в итоге выучены, а самое главное, что это дало, Юлия и расскажет на конференции.\n",
      "\n",
      "Как мы внедряли Julia туда, где всегда жил Python\r\n",
      "В прошлом году выступление Глеба Ивашкевича о возможностях языка Julia получило очень высокие оценки участников конференции. Текстовая версия доклада, которую мы недавно опубликовали, тоже вызвала немалый интерес. А в марте Глеб продолжит историю и расскажет, как гибкий и быстрый в теории язык показал себя на практике.\n",
      "\n",
      "\n",
      "\r\n",
      "Итого 24 доклада и 6 воркшопов/митапов. Это ядро конференции. Но так как наша конференция «про поговорить», то выступления спикеров будут давать пищу для размышления и направлять обсуждения. Остальное сделает нетворкинг, для которого мы делаем все возможное: дискуссионные и экспертные зоны, стенды партнеров, встречи локальных сообществ, бесконечный кофе-брейк, афтепати и в целом атмосферу профессионального мероприятия.\n",
      "\r\n",
      "Об event-фишках конференции будем писать в телеграме, фейсбуке, твиттере, вконтакте, когда преодолеем стадию proof of concept. А вы уже можете зайти на сайт Moscow Python Conf++ и забронировать билет, пока цена не выросла окончательно.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Иногда во время работы над проектом на языке Python возникает желание использовать библиотеку, которая написана не на Python, а, например, на C или C++. Причины для этого могут быть разные Во-первых, Python — язык замечательный, но в некоторых ситуациях недостаточно быстрый. И если вы видите, что производительность ограничена особенностями языка Python, то имеет смысл часть программы написать на другом языке (в этой статье мы будем говорить про C и C++), оформить эту часть программы в виде библиотеки, сделать Python-обвязки (Python bindings) поверх нее и использовать полученный таким образом модуль как обычную Python-библиотеку. Во-вторых, часто случается ситуация, когда вы знаете, что есть библиотека, которая решает требуемую задачу, но, к сожалению, эта библиотека написана не на Python, а на тех же C или C++. В этом случае также мы можем сделать Python-обвязку над библиотекой и пользоваться ей, не задумываясь о том, что библиотека изначально не была написана на Python.\n",
      "\r\n",
      "Для создания Python-обвязок существуют разные инструменты, начиная от более низкоуровневых вроде Python/C API и до более высокоуровневых вроде SWIG и SIP. \n",
      "\r\n",
      "У меня не было цели сравнения разных способов создания Python-обвязок, а хотелось бы рассказать об основах использования одного инструмента, а именно SIP. Изначально SIP разрабатывался для создания обвязки вокруг библиотеки Qt — PyQt, а также используется при разработке других крупных Python-библиотек, например, wxPython.\n",
      "\r\n",
      "В этой статье в качестве компилятора для C будет использоваться gcc, а в качестве компилятора C++ — g++. Все примеры проверялись под Arch Linux и Python 3.8. Для того, чтобы не усложнять примеры, тема компиляции под разные операционные системы и с помощью разных компиляторов (например, Visual Studio) не входит в рамки этой статьи.\n",
      "\r\n",
      "Все примеры для данной статьи вы можете скачать из репозитория на github.\r\n",
      "Репозиторий с исходниками SIP расположен по адресу https://www.riverbankcomputing.com/hg/sip/. В качестве системы контроля версий для SIP используется Mercurial.\n",
      "\n",
      " Делаем обвязку над библиотекой на языке C\n",
      " Пишем библиотеку на C\r\n",
      "Этот пример находится в папке pyfoo_c_01 в исходниках, но в данной статье мы будем подразумевать, что мы все делаем с чистого листа.\n",
      "\r\n",
      "Начнем с простого примера. Для начала сделаем простую C-библиотеку, которую потом будем запускать из скрипта на Python. Пусть в нашей библиотеке будет единственная функция \n",
      "\n",
      "int foo(char*);\n",
      "\r\n",
      "которая будет принимать строку и возвращать ее длину, умноженную на 2.\n",
      "\r\n",
      "Заголовочный файл foo.h может выглядеть, например, так:\n",
      "\n",
      "#ifndef FOO_LIB\n",
      "#define FOO_LIB\n",
      "\n",
      "int foo(char* str);\n",
      "\n",
      "#endif\n",
      "\r\n",
      "И файл с реализацией foo.cpp:\n",
      "\n",
      "#include <string.h>\n",
      "\n",
      "#include \"foo.h\"\n",
      "\n",
      "int foo(char* str) {\n",
      "\treturn strlen(str) * 2;\n",
      "}\n",
      "\r\n",
      "Для проверки работоспособности библиотеки напишем простую программу main.c:\n",
      "\n",
      "#include <stdio.h>\n",
      "\n",
      "#include \"foo.h\"\n",
      "\n",
      "int main(int argc, char* argv[]) {\n",
      "\tchar* str = \"0123456789\";\n",
      "\tprintf(\"%d\\n\", foo(str));\n",
      "}\n",
      "\r\n",
      "Для аккуратности создадим Makefile:\n",
      "\n",
      "CC=gcc\n",
      "CFLAGS=-c\n",
      "DIR_OUT=bin\n",
      "\n",
      "all: main\n",
      "\n",
      "main: main.o libfoo.a\n",
      "\t$(CC) $(DIR_OUT)/main.o -L$(DIR_OUT) -lfoo -o $(DIR_OUT)/main\n",
      "\n",
      "main.o: makedir main.c\n",
      "\t$(CC) $(CFLAGS) main.c -o $(DIR_OUT)/main.o\n",
      "\n",
      "libfoo.a: makedir foo.c\n",
      "\t$(CC) $(CFLAGS) foo.c -o $(DIR_OUT)/foo.o\n",
      "\tar rcs $(DIR_OUT)/libfoo.a $(DIR_OUT)/foo.o\n",
      "\n",
      "makedir:\n",
      "\tmkdir -p $(DIR_OUT)\n",
      "\n",
      "clean:\n",
      "\trm -rf $(DIR_OUT)/*\n",
      "\r\n",
      "Пусть все исходники библиотеки foo расположены в подпапке foo в папке с исходниками:\n",
      "\n",
      "\n",
      "foo_c_01/\n",
      "└── foo\n",
      "    ├── foo.c\n",
      "    ├── foo.h\n",
      "    ├── main.c\n",
      "    └── Makefile\n",
      "\n",
      "\r\n",
      "Заходим в папку foo и компилируем исходники с помощью команды\n",
      "\n",
      "make\n",
      "\r\n",
      "В процессе компиляции будет выведен текст\n",
      "\n",
      "mkdir -p bin\n",
      "gcc -c main.c -o bin/main.o\n",
      "gcc -c foo.c -o bin/foo.o\n",
      "ar rcs bin/libfoo.a bin/foo.o\n",
      "gcc bin/main.o -Lbin -lfoo -o bin/main\n",
      "\r\n",
      "Результат компиляции будет помещен в папку bin внутри папки foo:\n",
      "\n",
      "\n",
      "foo_c_01/\n",
      "└── foo\n",
      "    ├── bin\n",
      "    │   ├── foo.o\n",
      "    │   ├── libfoo.a\n",
      "    │   ├── main\n",
      "    │   └── main.o\n",
      "    ├── foo.c\n",
      "    ├── foo.h\n",
      "    ├── main.c\n",
      "    └── Makefile\n",
      "\n",
      "\r\n",
      "Мы скомпилировали библиотеку для статической линковки и программу, которая ее использует под названием main. После компиляции можно убедиться, что программа main запускается.\n",
      "\r\n",
      "Давайте сделаем Python-обвязку над библиотекой foo.\n",
      "\n",
      " Основы работы с SIP\r\n",
      "Для начала SIP нужно установить. Делается это стандартно, как и для всех остальных библиотек с помощью pip:\n",
      "\n",
      "pip install --user sip\n",
      "\r\n",
      "Разумеется, если вы работаете в виртуальном окружении, то параметр --user, сообщающий о том, что библиотеку SIP нужно установить в папку пользователя, а не глобально в систему, указывать не надо.\n",
      "\r\n",
      "Что нам нужно сделать, чтобы библиотеку foo можно было бы вызывать из кода на Python? Как минимум нужно создать два файла: один из них в формате TOML и назвать его pyproject.toml, а второй — файл с расширением .sip. Давайте последовательно разбираться с каждым из них.\n",
      "\r\n",
      "Нам нужно договориться о структуре исходников. Внутри папки pyfoo_c содержится папка foo, в которой расположены исходники для библиотеки. После компиляции внутри папки foo создается папка bin, которая будет содержать все скомпилированные файлы. Позже мы добавим возможность пользователю указывать пути до заголовочных и объектных файлов библиотеки через командную строку.\n",
      "\r\n",
      "Файлы, необходимые для SIP, будут расположены в той же папке, что и папка foo.\n",
      "\n",
      " pyproject.toml\r\n",
      "Файл pyproject.toml — это не изобретение разработчиков SIP, а формат описания проекта на языке Python, описанный в PEP 517 «A build-system independent format for source trees» и в PEP 518 «Specifying Minimum Build System Requirements for Python Projects». Это файл в формате TOML, который можно рассматривать как более продвинутую версию формата ini, в котором параметры хранятся в виде «ключ=значение», при этом параметры могут располагаться не просто в разделах вроде [foo], которые в терминах TOML называются таблицами, но и в подразделах вида [foo.bar.spam]. Параметры могут могут содержать в качестве значения не только строки, но и списки, числа и булевы значения.\n",
      "\r\n",
      "Этот файл по задумке должен описывать все, что необходимо для сборки Python-пакета, причем не обязательно с помощью SIP. Правда, как мы увидим чуть позже, этого файла в некоторых случаях будет не достаточно, и ему в дополнение нужно будет создать небольшой скрипт на Python. Но давайте обо всем по порядку.\n",
      "\r\n",
      "Полное описание всех возможных параметров файла pyproject.toml, которые относятся к SIP, можно найти на странице документации SIP.\n",
      "\r\n",
      "Создадим для нашего примера файл pyproject.toml на том же уровне, что и папка foo:\n",
      "\n",
      "\n",
      "foo_c_01/\n",
      "├── foo\n",
      "│   ├── bin\n",
      "│   │   ├── foo.o\n",
      "│   │   ├── libfoo.a\n",
      "│   │   ├── main\n",
      "│   │   └── main.o\n",
      "│   ├── foo.c\n",
      "│   ├── foo.h\n",
      "│   ├── main.c\n",
      "│   └── Makefile\n",
      "└── pyproject.toml\n",
      "\n",
      "\r\n",
      "Содержимое pyproject.toml будет следующее:\n",
      "\n",
      "[build-system]\n",
      "requires = [\"sip >=5, <6\"]\n",
      "build-backend = \"sipbuild.api\"\n",
      "\n",
      "[tool.sip.metadata]\n",
      "name = \"pyfoo\"\n",
      "version = \"0.1\"\n",
      "license = \"MIT\"\n",
      "\n",
      "[tool.sip.bindings.pyfoo]\n",
      "headers = [\"foo.h\"]\n",
      "libraries = [\"foo\"]\n",
      "include-dirs = [\"foo\"]\n",
      "library-dirs = [\"foo/bin\"]\n",
      "\r\n",
      "Раздел [build-system] («таблица» в терминах TOML) является стандартным и описан в PEP 518. Он содержит два параметра:\n",
      "\n",
      "\n",
      "requires — список пакетов, необходимых для сборки нашего пакета. Формат описания зависимостей пакета описан в PEP 508 «Dependency specification for Python Software Packages». В данном случае нам требуется только пакет sip версии 5.x.\n",
      "build-backend описывает, с помощью чего мы будем собирать наш пакет. Строго говоря, этот параметр в виде строки должен содержать полное название Python-объекта, который будет заниматься сборкой. Если не задумываться над глубоким содержимым этого параметра, то для пакетов, собираемых с помощью SIP, это значение должно равняться «sipbuild.api».\n",
      "\r\n",
      "Другие параметры описаны в разделах [tool.sip.*]. \n",
      "\r\n",
      "Раздел [tool.sip.metadata] содержит общую информацию о пакете: имя собираемого пакета (у нас пакет будет называться pyfoo, но не путайте это имя с именем модуля, который мы потом будем импортировать в Python), номер версии пакета (в нашем случае номер версии «0.1») и лицензия (например, \"MIT\").\n",
      "\r\n",
      "Самое важное с точки зрения сборки описано в разделе [tool.sip.bindings.pyfoo]. \n",
      "\r\n",
      "Обратите внимание на имя пакета в заголовке раздела. В этот раздел мы добавили два параметра:\n",
      "\n",
      "\n",
      "headers — список заголовочных файлов, которые необходимы для использования библиотеки foo.\n",
      "libraries — список объектных файлов, скомпилированных для статической линковки.\n",
      "include-dirs — путь, где искать дополнительные заголовочные файлы помимо тех, что прилагаются к компилятору C. В данном случае, где искать файл foo.h.\n",
      "library-dirs — путь, где искать дополнительные объектные файлы помимо тех, что прилагаются к компилятору C. В данном случае это папка, в которой создается скомпилированный файл библиотеки foo.\n",
      "\r\n",
      "Итак, первый необходимый файл для SIP мы создали. Теперь переходим к созданию следующего файла, который будет описывать содержимое будущего Python-модуля.\n",
      "\n",
      " pyfoo.sip\r\n",
      "Создадим файл pyfoo.sip в той же папке, что и файл pyproject.toml:\n",
      "\n",
      "\n",
      "foo_c_01/\n",
      "├── foo\n",
      "│   ├── bin\n",
      "│   │   ├── foo.o\n",
      "│   │   ├── libfoo.a\n",
      "│   │   ├── main\n",
      "│   │   └── main.o\n",
      "│   ├── foo.c\n",
      "│   ├── foo.h\n",
      "│   ├── main.c\n",
      "│   └── Makefile\n",
      "├── pyfoo.sip\n",
      "└── pyproject.toml\n",
      "\n",
      "\r\n",
      "Файл с расширением .sip описывает интерфейс исходной библиотеки, который будет преобразован в модуль на Python. Этот файл имеет собственный формат, который мы сейчас рассмотрим, и напоминает заголовочный файл C/C++ с дополнительной разметкой, которая должна помочь SIP создать Python-модуль. \n",
      "\r\n",
      "В нашем примере этот файл должен называться pyfoo.sip, потому что до этого в файле pyproject.toml мы создали раздел [tool.sip.bindings.pyfoo]. В общем случае таких разделов может быть несколько и, соответственно, должно быть несколько файлов *.sip. Но если у нас несколько sip-файлов, то это особый случай с точки зрения SIP, и в этой статье мы его не рассматриваем. Обратите внимание, что в общем случае имя файла .sip (и, соответственно, имя раздела) может не совпадать с именем пакета, которое указано в параметре name в разделе [tool.sip.metadata].\n",
      "\r\n",
      "Рассмотрим файл pyfoo.sip из нашего примера:\n",
      "\n",
      "%Module(name=foo, language=\"C\")\n",
      "\n",
      "int foo(char*);\n",
      "\r\n",
      "Строки, которые начинаются с символа \"%\", называются директивами. Они должны подсказывать SIP, как нужно правильно собирать и оформлять Python-модуль. Полный список директив описан на этой странице документации. Некоторые директивы имеют дополнительные параметры. Параметры могут быть не обязательными.\n",
      "\r\n",
      "В этом примере мы используем две директивы, с некоторыми другими директивами познакомимся в следующих примерах.\n",
      "\r\n",
      "Файл pyfoo.sip начинается с директивы %Module(name=foo, language=«C»). Обратите внимание, что значение первого параметра (name) мы указали без кавычек, а значение второго параметра (language) с кавычками, как строки в C/C++. Это требование данной директивы, описанное в документации к директиве %Module.\n",
      "\r\n",
      "В директиве %Module обязательным является только параметр name, который задает имя Python-модуля, из которого мы будем импортировать функцию библиотеки. В данном случае модуль называется foo, он будет содержать функцию foo, поэтому после сборки и установки мы будем ее импортировать с помощью кода:\n",
      "\n",
      "from foo import foo\n",
      "\r\n",
      "Мы могли бы сделать этот модуль вложенным в другой модуль, заменив эту строку, например, такой:\n",
      "\n",
      "%Module(name=foo.bar, language=\"C\")\n",
      "...\n",
      "\r\n",
      "Тогда импортировать функцию foo нужно было бы следующим образом:\n",
      "\n",
      "from foo.bar import foo\n",
      "\r\n",
      "Параметр language директивы %Module указывает язык, на котором написана исходная библиотека. Значение этого параметра может быть либо «C», либо «C++». Если этот параметр не указать, то SIP будет считать, что библиотека написана на C++.\n",
      "\r\n",
      "Теперь посмотрим на последнюю строчку файла pyfoo.sip:\n",
      "\n",
      "int foo(char*);\n",
      "\r\n",
      "Это описание интерфейса функции из библиотеки, которую мы хотим поместить в Python-модуль. На основе этого объявления sip создаст Python-функцию. Думаю, что здесь все должно быть ясно.\n",
      "\n",
      " Собираем и проверяем\r\n",
      "Теперь все готово для того, чтобы собрать Python-пакет с обвязкой для библиотеки на C. В первую очередь нужно собрать саму библиотеку. Переходим в папку pyfoo_c_01/foo/ и запускаем сборку с помощью команды make:\n",
      "\n",
      "$ make\n",
      "\n",
      "mkdir -p bin\n",
      "gcc -c main.c -o bin/main.o\n",
      "gcc -c foo.c -o bin/foo.o\n",
      "ar rcs bin/libfoo.a bin/foo.o\n",
      "gcc bin/main.o -Lbin -lfoo -o bin/main\n",
      "\r\n",
      "Если все прошло успешно, то внутри папки foo будет создана папка bin, в котором среди прочих файлов будет собранная библиотека libfoo.a. Напомню, что здесь, чтобы не отвлекаться от основной темы, мы говорим только про сборку под Linux с помощью gcc.\n",
      "\r\n",
      "Переходим обратно в папку pyfoo_c_01. Теперь пришло время познакомиться с командами SIP. После установки SIP станут доступны следующие команды командной строки (страница документации):\n",
      "\n",
      "\n",
      "sip-build. Создает объектный файл Python-расширения (Python extension).\n",
      "sip-install. Создает объектный файл Python-расширения и устанавливает его.\n",
      "sip-sdist. Создает пакет в виде архива .tar.gz, который можно установить с помощью pip.\n",
      "sip-wheel. Создает пакет в формате wheel (файл с расширением .whl).\n",
      "sip-module. Создает модуль, в который включается только служебные инструменты, необходимые самому SIP. Это нужно, если вы создаете библиотеку, разбитую на несколько пакетов. В этой статье мы не будем рассматривать такой случай, мы будем создавать только так называемый standalone project, то есть наш пакет будет единый, он будет включать и библиотеку, для которой мы делаем обвязку, и все служебные инструменты.\n",
      "sip-distinfo. Создает и заполняет папку .dist-info, которая используется в пакете в формате wheel.\n",
      "\r\n",
      "Эти команды нужно запускать из папки, где расположен файл pyproject.toml.\n",
      "\r\n",
      "Для начала, чтобы лучше понять работу SIP, запустим команду sip-build, причем с параметром --verbose для более подробного вывода в консоль, и посмотрим, что происходит в процессе сборки.\n",
      "\r\n",
      "$ sip-build --verbose\n",
      "\r\n",
      "These bindings will be built: pyfoo.\r\n",
      "Generating the pyfoo bindings…\r\n",
      "Compiling the 'foo' module…\r\n",
      "building 'foo' extension\r\n",
      "creating build\r\n",
      "creating build/temp.linux-x86_64-3.8\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c sipfoocmodule.c -o build/temp.linux-x86_64-3.8/sipfoocmodule.o\r\n",
      "sipfoocmodule.c: В функции «func_foo»:\r\n",
      "sipfoocmodule.c:29:22: предупреждение: неявная декларация функции «foo» [-Wimplicit-function-declaration]\r\n",
      " 29 | sipRes = foo(a0);\r\n",
      " | ^~~\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c array.c -o build/temp.linux-x86_64-3.8/array.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c bool.cpp -o build/temp.linux-x86_64-3.8/bool.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c objmap.c -o build/temp.linux-x86_64-3.8/objmap.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c qtlib.c -o build/temp.linux-x86_64-3.8/qtlib.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c int_convertors.c -o build/temp.linux-x86_64-3.8/int_convertors.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c voidptr.c -o build/temp.linux-x86_64-3.8/voidptr.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c apiversions.c -o build/temp.linux-x86_64-3.8/apiversions.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c descriptors.c -o build/temp.linux-x86_64-3.8/descriptors.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c threads.c -o build/temp.linux-x86_64-3.8/threads.o\r\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DSIP_PROTECTED_IS_PUBLIC -Dprotected=public -I. -I../../foo -I/usr/include/python3.8 -c siplib.c -o build/temp.linux-x86_64-3.8/siplib.o\r\n",
      "siplib.c: В функции «slot_richcompare»:\r\n",
      "siplib.c:9536:16: предупреждение: «st», возможно, используется без инициализации в данной функции [-Wmaybe-uninitialized]\r\n",
      " 9536 | slot = findSlotInClass(ctd, st);\r\n",
      " | ^~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "siplib.c:10671:19: замечание: «st» было объявлено здесь\r\n",
      "10671 | sipPySlotType st;\r\n",
      " | ^~\r\n",
      "siplib.c: В функции «parsePass2»:\r\n",
      "siplib.c:5625:32: предупреждение: «owner», возможно, используется без инициализации в данной функции [-Wmaybe-uninitialized]\r\n",
      " 5625 | *owner = arg;\r\n",
      " | ~~~~~~~^~\r\n",
      "g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -fno-semantic-interposition -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-x86_64-3.8/sipfoocmodule.o build/temp.linux-x86_64-3.8/array.o build/temp.linux-x86_64-3.8/bool.o build/temp.linux-x86_64-3.8/objmap.o build/temp.linux-x86_64-3.8/qtlib.o build/temp.linux-x86_64-3.8/int_convertors.o build/temp.linux-x86_64-3.8/voidptr.o build/temp.linux-x86_64-3.8/apiversions.o build/temp.linux-x86_64-3.8/descriptors.o build/temp.linux-x86_64-3.8/threads.o build/temp.linux-x86_64-3.8/siplib.o -L../../foo/bin -L/usr/lib -lfoo -o /home/jenyay/projects/soft/sip-examples/pyfoo_c_01/build/foo/foo.cpython-38-x86_64-linux-gnu.so\r\n",
      "The project has been built.\n",
      "\r\n",
      "Мы не будем сильно углубляться в работу SIP, но из вывода видно, что происходит компиляция каких-то исходников. Эти исходники можно увидеть в созданной этой командой папке build/foo/:\n",
      "\n",
      "\n",
      "pyfoo_c_01\n",
      "├── build\n",
      "│   └── foo\n",
      "│       ├── apiversions.c\n",
      "│       ├── array.c\n",
      "│       ├── array.h\n",
      "│       ├── bool.cpp\n",
      "│       ├── build\n",
      "│       │   └── temp.linux-x86_64-3.8\n",
      "│       │       ├── apiversions.o\n",
      "│       │       ├── array.o\n",
      "│       │       ├── bool.o\n",
      "│       │       ├── descriptors.o\n",
      "│       │       ├── int_convertors.o\n",
      "│       │       ├── objmap.o\n",
      "│       │       ├── qtlib.o\n",
      "│       │       ├── sipfoocmodule.o\n",
      "│       │       ├── siplib.o\n",
      "│       │       ├── threads.o\n",
      "│       │       └── voidptr.o\n",
      "│       ├── descriptors.c\n",
      "│       ├── foo.cpython-38-x86_64-linux-gnu.so\n",
      "│       ├── int_convertors.c\n",
      "│       ├── objmap.c\n",
      "│       ├── qtlib.c\n",
      "│       ├── sipAPIfoo.h\n",
      "│       ├── sipfoocmodule.c\n",
      "│       ├── sip.h\n",
      "│       ├── sipint.h\n",
      "│       ├── siplib.c\n",
      "│       ├── threads.c\n",
      "│       └── voidptr.c\n",
      "├── foo\n",
      "│   ├── bin\n",
      "│   │   ├── foo.o\n",
      "│   │   ├── libfoo.a\n",
      "│   │   ├── main\n",
      "│   │   └── main.o\n",
      "│   ├── foo.c\n",
      "│   ├── foo.h\n",
      "│   ├── main.c\n",
      "│   └── Makefile\n",
      "├── pyfoo.sip\n",
      "└── pyproject.toml\n",
      "\n",
      "\r\n",
      "В папке build/foo появились вспомогательные исходники. Из любопытства посмотрим файл sipfoocmodule.c, поскольку он непосредственно относится к модулю foo, который будет создан:\n",
      "\n",
      "/*\n",
      " * Module code.\n",
      " *\n",
      " * Generated by SIP 5.1.1\n",
      " */\n",
      "\n",
      "#include \"sipAPIfoo.h\"\n",
      "\n",
      "/* Define the strings used by this module. */\n",
      "const char sipStrings_foo[] = {\n",
      "    'f', 'o', 'o', 0,\n",
      "};\n",
      "\n",
      "PyDoc_STRVAR(doc_foo, \"foo(str) -> int\");\n",
      "\n",
      "static PyObject *func_foo(PyObject *sipSelf,PyObject *sipArgs)\n",
      "{\n",
      "    PyObject *sipParseErr = SIP_NULLPTR;\n",
      "\n",
      "    {\n",
      "        char* a0;\n",
      "\n",
      "        if (sipParseArgs(&sipParseErr, sipArgs, \"s\", &a0))\n",
      "        {\n",
      "            int sipRes;\n",
      "\n",
      "            sipRes = foo(a0);\n",
      "\n",
      "            return PyLong_FromLong(sipRes);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    /* Raise an exception if the arguments couldn't be parsed. */\n",
      "    sipNoFunction(sipParseErr, sipName_foo, doc_foo);\n",
      "\n",
      "    return SIP_NULLPTR;\n",
      "}\n",
      "\n",
      "/* This defines this module. */\n",
      "sipExportedModuleDef sipModuleAPI_foo = {\n",
      "    0,\n",
      "    SIP_ABI_MINOR_VERSION,\n",
      "    sipNameNr_foo,\n",
      "    0,\n",
      "    sipStrings_foo,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    0,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    0,\n",
      "    SIP_NULLPTR,\n",
      "    0,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    {SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR,\n",
      "            SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR},\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR\n",
      "};\n",
      "\n",
      "/* The SIP API and the APIs of any imported modules. */\n",
      "const sipAPIDef *sipAPI_foo;\n",
      "\n",
      "/* The Python module initialisation function. */\n",
      "#if defined(SIP_STATIC_MODULE)\n",
      "PyObject *PyInit_foo(void)\n",
      "#else\n",
      "PyMODINIT_FUNC PyInit_foo(void)\n",
      "#endif\n",
      "{\n",
      "    static PyMethodDef sip_methods[] = {\n",
      "        {sipName_foo, func_foo, METH_VARARGS, doc_foo},\n",
      "        {SIP_NULLPTR, SIP_NULLPTR, 0, SIP_NULLPTR}\n",
      "    };\n",
      "\n",
      "    static PyModuleDef sip_module_def = {\n",
      "        PyModuleDef_HEAD_INIT,\n",
      "        \"foo\",\n",
      "        SIP_NULLPTR,\n",
      "        -1,\n",
      "        sip_methods,\n",
      "        SIP_NULLPTR,\n",
      "        SIP_NULLPTR,\n",
      "        SIP_NULLPTR,\n",
      "        SIP_NULLPTR\n",
      "    };\n",
      "\n",
      "    PyObject *sipModule, *sipModuleDict;\n",
      "    /* Initialise the module and get it's dictionary. */\n",
      "    if ((sipModule = PyModule_Create(&sip_module_def)) == SIP_NULLPTR)\n",
      "        return SIP_NULLPTR;\n",
      "\n",
      "    sipModuleDict = PyModule_GetDict(sipModule);\n",
      "\n",
      "    if ((sipAPI_foo = sip_init_library(sipModuleDict)) == SIP_NULLPTR)\n",
      "        return SIP_NULLPTR;\n",
      "\n",
      "    /* Export the module and publish it's API. */\n",
      "    if (sipExportModule(&sipModuleAPI_foo, SIP_ABI_MAJOR_VERSION, SIP_ABI_MINOR_VERSION, 0) < 0)\n",
      "    {\n",
      "        Py_DECREF(sipModule);\n",
      "        return SIP_NULLPTR;\n",
      "    }\n",
      "    /* Initialise the module now all its dependencies have been set up. */\n",
      "    if (sipInitModule(&sipModuleAPI_foo,sipModuleDict) < 0)\n",
      "    {\n",
      "        Py_DECREF(sipModule);\n",
      "        return SIP_NULLPTR;\n",
      "    }\n",
      "\n",
      "    return sipModule;\n",
      "}\n",
      "\r\n",
      "Если вы работали с Python/C API, то увидите знакомые функции. Особо обратите внимание на функцию func_foo, начинающейся с 18 строки.\n",
      "\r\n",
      "В результате компиляции этих исходников будет создан файл build/foo/foo.cpython-38-x86_64-linux-gnu.so, именно он и содержит Python-расширение, которое еще нужно правильно установить.\n",
      "\r\n",
      "Для того, чтобы одной командой скомпилировать расширение и сразу его установить, можно воспользоваться командой sip-install, но мы ей пользоваться не будем, потому что по умолчанию пытается установить созданное Python-расширение глобально в систему. У этой команды есть параметр --target-dir, с помощью которого можно указать путь, куда нужно устанавливать расширение, но мы лучше воспользуемся другими инструментами, создающими пакеты, которые затем можно будет установить с помощью pip.\n",
      "\r\n",
      "Сначала воспользуемся командой sip-sdist. Использовать ее очень просто:\n",
      "\n",
      "$ sip-sdist\n",
      "\n",
      "The sdist has been built.\n",
      "\r\n",
      "После этого будет создан файл pyfoo-0.1.tar.gz, который можно установить с помощью команды:\n",
      "\n",
      "pip install --user pyfoo-0.1.tar.gz\n",
      "\r\n",
      "В результате будет показана следующая информация и пакет установится:\n",
      "\n",
      "Processing ./pyfoo-0.1.tar.gz\n",
      "  Installing build dependencies ... done\n",
      "  Getting requirements to build wheel ... done\n",
      "    Preparing wheel metadata ... done\n",
      "Building wheels for collected packages: pyfoo\n",
      "  Building wheel for pyfoo (PEP 517) ... done\n",
      "  Created wheel for pyfoo: filename=pyfoo-0.1-cp38-cp38-manylinux1_x86_64.whl size=337289 sha256=762fc578...\n",
      "  Stored in directory: /home/jenyay/.cache/pip/wheels/54/dc/d8/cc534fff...\n",
      "Successfully built pyfoo\n",
      "Installing collected packages: pyfoo\n",
      "  Attempting uninstall: pyfoo\n",
      "    Found existing installation: pyfoo 0.1\n",
      "    Uninstalling pyfoo-0.1:\n",
      "      Successfully uninstalled pyfoo-0.1\n",
      "Successfully installed pyfoo-0.1\n",
      "\r\n",
      "Давайте убедимся, что нам удалось сделать Python-обвязку. Запускаем Python и пытаемся вызвать функцию. Напомню, что согласно нашим настройкам, пакет pyfoo содержит модуль foo, в котором имеется функция foo.\n",
      "\n",
      ">>> from foo import foo\n",
      ">>> foo(b'123456')\n",
      "12\n",
      "\r\n",
      "Обратите внимание, что в качестве параметра функции мы передаем не просто строку, а строку байтов b'123456' — прямой аналог char* в C. Чуть позже мы добавим преобразование char* в str и обратно. Результат получился ожидаемым. Напомню, что функция foo возвращает удвоенный размер массива типа char*, переданного ей в качестве параметра.\n",
      "\r\n",
      "Давайте попробуем передать в функцию foo обычную Python-строку вместо списка байтов.\n",
      "\n",
      ">>> from foo import foo\n",
      ">>> foo('123456')\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: foo(str): argument 1 has unexpected type 'str'\n",
      "\r\n",
      "Созданная обвязка не смогла преобразовать строку в char*, как ее научить это делать, мы рассмотрим в следующем разделе.\n",
      "\r\n",
      "Поздравляю, мы сделали первую обвязку над библиотекой, написанной на языке C.\n",
      "\r\n",
      "Выйдем из интерпретатора Python и соберем сборку в формате wheel. Как вы скорее всего знаете, wheel — это сравнительно новый формат пакетов, который в последнее время используется повсеместно. Описание формата содержится в PEP 427 «The Wheel Binary Package Format 1.0», но описание особенностей формата wheel — тема, достойная отдельной большой статьи. Для нас важно, что пакет в формате wheel пользователь может легко установить с помощью pip.\n",
      "\r\n",
      "Пакет в формате wheel собирается ничуть не сложнее, чем пакет в формате sdist. Для этого в папке с файлом pyproject.toml нужно выполнить команду\n",
      "\n",
      "sip-wheel\n",
      "\r\n",
      "После запуска этой команды будет показан процесс сборки и могут быть предупреждения от компилятора:\n",
      "\r\n",
      "$ sip-wheel \n",
      "\r\n",
      "These bindings will be built: pyfoo.\r\n",
      "Generating the pyfoo bindings…\r\n",
      "Compiling the 'foo' module…\r\n",
      "sipfoocmodule.c: В функции «func_foo»:\r\n",
      "sipfoocmodule.c:29:22: предупреждение: неявная декларация функции «foo» [-Wimplicit-function-declaration]\r\n",
      " 29 | sipRes = foo(a0);\r\n",
      " | ^~~\r\n",
      "siplib.c: В функции «slot_richcompare»:\r\n",
      "siplib.c:9536:16: предупреждение: «st», возможно, используется без инициализации в данной функции [-Wmaybe-uninitialized]\r\n",
      " 9536 | slot = findSlotInClass(ctd, st);\r\n",
      " | ^~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "siplib.c:10671:19: замечание: «st» было объявлено здесь\r\n",
      "10671 | sipPySlotType st;\r\n",
      " | ^~\r\n",
      "siplib.c: В функции «parsePass2»:\r\n",
      "siplib.c:5625:32: предупреждение: «owner», возможно, используется без инициализации в данной функции [-Wmaybe-uninitialized]\r\n",
      " 5625 | *owner = arg;\r\n",
      " | ~~~~~~~^~\r\n",
      "The wheel has been built.\n",
      "\r\n",
      "Когда сборка завершится (наш маленький проект компилируется быстро), в папке проекта появится файл с именем pyfoo-0.1-cp38-cp38-manylinux1_x86_64.whl или похожим. Имя созданного файла может отличаться в зависимости от вашей операционной системы и версии Python.\n",
      "\r\n",
      "Теперь мы можем установить этот пакет с помощью pip:\n",
      "\n",
      "pip install --user --upgrade pyfoo-0.1-cp38-cp38-manylinux1_x86_64.whl\n",
      "\r\n",
      "Здесь используется параметр --upgrade, чтобы pip заменил модуль pyfoo, установленный ранее.\n",
      "\r\n",
      "Дальше модуль foo и пакета pyfoo можно использовать, как было показано выше.\n",
      "\n",
      " Добавляем правила преобразования в char*\r\n",
      "В предыдущем разделе мы столкнулись с проблемой, что функция foo может принимать только набор байтов, но не строки. Сейчас мы исправим этот недостаток. Для этого мы воспользуемся еще одним инструментом SIP — аннотациями. Аннотации используются внутри файлов .sip и применяются к каким-то элементам кода: функциям, классам, аргументам функций, исключениям, переменным и др. Аннотации записываются между прямыми слешами: /аннотация/. \n",
      "\r\n",
      "Аннотация может работать в качестве флага, который может находиться в состоянии установлен или не установлен, например: /ReleaseGIL/, или некоторым аннотациям нужно присваивать какие-либо значения, например: /Encoding=«UTF-8»/. Если к какому-то объекту нужно применить несколько аннотаций, то они разделяются запятыми внутри слешей: /аннотация_1, аннотация_2/.\n",
      "\r\n",
      "В следующем примере, который находится в папке pyfoo_c_02, добавим в файл pyfoo.sip аннотацию для параметра функции foo:\n",
      "\n",
      "%Module(name=foo, language=\"C\")\n",
      "\n",
      "int foo(char* /Encoding=\"UTF-8\"/);\n",
      "\r\n",
      "Аннотация Encoding указывает, в какую кодировку должна быть закодирована строка, которая будет передаваться в функцию. Значения этой аннотации могут быть следующие: «ASCII», «Latin-1», «UTF-8» или «None». Если аннотация Encoding не указана или равна None, то параметр для такой функции не подвергается никакой кодировке и передается в функцию как есть, но в этом случае параметр в коде на Python должен тип bytes, т.е. массив байтов, что мы и видели в предыдущем примере. Если кодировка указана, то этот параметр может быть строкой (типом str в Python). Аннотация Encoding может применяться только к параметрам типа char, const char, char* или const char*.\n",
      "\r\n",
      "Проверим, как теперь работает функция foo из модуля foo. Для этого, как и ранее, нужно сначала скомпилировать библиотеку foo, вызвав внутри папки foo команду make, а затем из папки примера pyfoo_c_02 вызвать команду, например, sip-wheel. Будет создан файл pyfoo-0.2-cp38-cp38-manylinux1_x86_64.whl или с похожим названием, который можно установить с помощью команды:\n",
      "\n",
      "pip install --user --upgrade pyfoo-0.2-cp38-cp38-manylinux1_x86_64.whl\n",
      "\r\n",
      "Если все прошло успешно, запускаем интерпретатор Python и пробуем вызвать функцию foo со строковым аргументом:\n",
      "\n",
      ">>> from foo import foo\n",
      "\n",
      ">>> foo(b'qwerty')\n",
      "12\n",
      "\n",
      ">>> foo('qwerty')\n",
      "12\n",
      "\n",
      ">>> foo('йцукен')\n",
      "24\n",
      "\r\n",
      "Сначала мы убеждаемся, что использование типа bytes по-прежнему возможно. После этого убеждаемся, что теперь мы можем передавать в функцию foo также и строковые аргументы. Обратите внимание, что функция foo для строкового аргумента с русскими буквами вернула значение в два раза больше, чем для строки, содержащей только латинские буквы. Это произошло из-за того, что функция foo считает не длину строки в символах (и удваивает ее), а длину массива char*, а поскольку в кодировке UTF-8 русские буквы занимают 2 байта, то и размер массива char* после преобразования из строки Python получился в два раза длиннее. \n",
      "\r\n",
      "Отлично! Мы решили проблему с аргументом функции foo, но что, если у нас в библиотеке будут десятки или сотни таких функций, для каждой из них придется указывать кодировку параметров? Часто кодировка в программе используется одна и та же, и нет цели для разных функций указывать разные кодировки. В этом случае в SIP есть возможность указать кодировку по умолчанию, а если для какой-то функции кодировка нужна какая-то другая, то ее можно переопределить с помощью аннотации Encoding.\n",
      "\r\n",
      "Чтобы задать кодировку параметров функции по умолчанию предназначена директива %DefaultEncoding. Ее использование показано в примере, расположенном в папке pyfoo_c_03. \n",
      "\r\n",
      "Для того, чтобы воспользоваться директивой %DefaultEncoding, изменим файл pyfoo.sip, теперь его содержимое выглядит следующим образом:\n",
      "\n",
      "%Module(name=foo, language=\"C\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "\n",
      "int foo(char*);\n",
      "\r\n",
      "Теперь, если у аргумента функции типа char, char* и т.п. нет аннотации Encoding, то кодировка берется из директивы %DefaultEncoding, а если ее нет, то преобразование не производится, и для всех параметров char* и т.п. нужно передавать не строки, а bytes.\n",
      "\r\n",
      "Пример из папки pyfoo_c_03 собирается и проверяется так же, как и пример из папки pyfoo_c_02.\n",
      "\n",
      " Коротко о project.py. Автоматизируем сборку\r\n",
      "До сих пор для создания Python-обвязки мы использовали два служебных файла — pyproject.toml и pyfoo.sip. Теперь мы познакомимся с еще одним таким файлом, который должен называться project.py. С помощью этого скрипта мы можем влиять на процесс сборки нашего пакета. Давайте займемся автоматизацией сборки. Для того, чтобы собрать примеры pyfoo_c_01 — pyfoo_c_03 из предыдущих разделов, нужно было сначала зайти в папку foo/, выполнить там компиляцию с помощью команды make, вернуться в папку, где расположен файл pyproject.toml и только тогда запустить сборку пакета с помощью одной из команд sip-*.\n",
      "\r\n",
      "Теперь наша цель — сделать так, чтобы при выполнении команд sip-build, sip-sdist и sip-wheel сначала запускалась сборка C-библиотеки foo, а потом уже запускалась непосредственно сама команда.\n",
      "\r\n",
      "Пример, создаваемый в этом разделе, находится в папке pyfoo_c_04 исходников.\n",
      "\r\n",
      "Чтобы изменить процесс сборки, мы можем в файле project.py (имя файла должно быть именно таким) объявить класс, производный от класса sipbuild.Project. У этого класса есть методы, которые мы можем переопределить на свои. В данный момент нас интересуют следующие методы:\n",
      "\n",
      "\n",
      "build. Вызывается в процессе вызова команды sip-build.\n",
      "build_sdist. Вызывается в процессе вызова команды sip-sdist.\n",
      "build_wheel. Вызывается в процессе вызова команды sip-wheel.\n",
      "install. Вызывается в процессе вызова команды sip-install.\n",
      "\r\n",
      "То есть мы можем переопределить поведение этих команд. Строго говоря, перечисленные методы объявлены в абстрактном классе sipbuild.AbstractProject, от которого создан производный класс sipbuild.Project.\n",
      "\r\n",
      "Создадим файл project.py со следующим содержимым:\n",
      "\n",
      "import os\n",
      "import subprocess\n",
      "\n",
      "from sipbuild import Project\n",
      "\n",
      "class FooProject(Project):\n",
      "    def _build_foo(self):\n",
      "        cwd = os.path.abspath('foo')\n",
      "        subprocess.run(['make'], cwd=cwd, capture_output=True, check=True)\n",
      "\n",
      "    def build(self):\n",
      "        self._build_foo()\n",
      "        super().build()\n",
      "\n",
      "    def build_sdist(self, sdist_directory):\n",
      "        self._build_foo()\n",
      "        return super().build_sdist(sdist_directory)\n",
      "\n",
      "    def build_wheel(self, wheel_directory):\n",
      "        self._build_foo()\n",
      "        return super().build_wheel(wheel_directory)\n",
      "\n",
      "    def install(self):\n",
      "        self._build_foo()\n",
      "        super().install()\n",
      "\r\n",
      "Мы объявили класс FooProject, производный от класса sipbuild.Project и преопределили в нем методы build, build_sdist, build_wheel и install. Во всех этих методах мы вызываем одноименные методы из базового класса, вызвав перед этим метод _build_foo, который запускает выполнение команды make в папке foo.\n",
      "\r\n",
      "Обратите внимание, что методы build_sdist и build_wheel должны вернуть имя созданного ими файла. Это не написано в документации, но указано в исходниках SIP.\n",
      "\r\n",
      "Теперь нам не нужно запускать команду make вручную для сборки библиотеки foo, это будет сделано автоматически.\n",
      "\r\n",
      "Если теперь в папке pyfoo_c_04 выполнить команду sip-wheel, то будет создан файл с именем pyfoo-0.4-cp38-cp38-manylinux1_x86_64.whl или аналогичный в зависимости от вашей операционной системы и версии Python.\n",
      "\r\n",
      "Этот пакет можно установить с помощью команды:\n",
      "\n",
      "pip install --user --upgrade pyfoo-0.4-cp38-cp38-manylinux1_x86_64.whl\n",
      "\r\n",
      "После этого можно убедиться, что функция foo из модуля foo по-прежнему работает.\n",
      "\n",
      " Добавляем параметры командной строки для сборки\r\n",
      "Следующий пример содержится в папке pyfoo_c_05, а пакет имеет номер версии 0.5 (см. настройки в файле pyproject.toml). Этот пример создан на основе примера из документации с некоторыми исправлениями. В этом примере мы переделаем наш файл project.py и добавим новые параметры командной строки для сборки.\n",
      "\r\n",
      "В наших примерах мы собираем очень простую библиотеку foo, а в реальных проектах библиотека может быть достаточно большой и тогда не будет смысла ее включать в исходники проекта Python-обвязки. Напомню, что SIP изначально создавался для создания обвязки для такого огромной библиотеки как Qt. Можно, конечно, возразить, что для организации исходников могут помочь подмодули из git, но не в этом суть. Предположим, что библиотека может находиться не в папке с исходниками обвязки. В этом случае возникает вопрос, где сборщик SIP должен искать заголовочные и объектные файлы библиотеки? В этом случае пути размещения библиотеки у разных пользователей могут быть свои.\n",
      "\r\n",
      "Чтобы решить эту проблему, добавим два новых параметра командной строки в систему сборки, с помощью которых можно будет указывать путь до файла foo.h (параметр --foo-include-dir) и до объектного файла библиотеки (параметр --foo-library-dir). Кроме того будем подразумевать, что если эти параметры не указаны, то библиотека foo расположена по-прежнему вместе с исходниками обвязки.\n",
      "\r\n",
      "Нам нужно снова создать файл project.py, а в нем объявить класс, производный от sipbuild.Project. Давайте сначала посмотрим на новую версию файла project.py, а потом разберемся, как он работает.\n",
      "\n",
      "import os\n",
      "\n",
      "from sipbuild import Option, Project\n",
      "\n",
      "class FooProject(Project):\n",
      "    \"\"\" Проект с дополнительными параметрами командной строки для задания\n",
      "путей до заголовочных и объектных файлов библиотеки foo.\n",
      "    \"\"\"\n",
      "\n",
      "    def get_options(self):\n",
      "        \"\"\" Возвращает список опций командной строки. \"\"\"\n",
      "\n",
      "        tools = ['build', 'install', 'sdist', 'wheel']\n",
      "\n",
      "        # Получить стандартные опции.\n",
      "        options = super().get_options()\n",
      "\n",
      "        # Добавить новые опции\n",
      "        inc_dir_option = Option('foo_include_dir',\n",
      "                                help=\"the directory containing foo.h\",\n",
      "                                metavar=\"DIR\",\n",
      "                                default=os.path.abspath('foo'),\n",
      "                                tools=tools)\n",
      "        options.append(inc_dir_option)\n",
      "\n",
      "        lib_dir_option = Option('foo_library_dir',\n",
      "                                help=\"the directory containing the foo library\",\n",
      "                                metavar=\"DIR\",\n",
      "                                default=os.path.abspath('foo/bin'),\n",
      "                                tools=tools)\n",
      "\n",
      "        options.append(lib_dir_option)\n",
      "\n",
      "        return options\n",
      "\n",
      "    def apply_user_defaults(self, tool):\n",
      "        \"\"\" Применить настройки по умолчанию. \"\"\"\n",
      "\n",
      "        # Применить стандартные настройки по умолчанию\n",
      "        super().apply_user_defaults(tool)\n",
      "\n",
      "        # Чтобы гарантировать, что пути до заголовочных файлов и собранной библиотеки абсолютные\n",
      "        self.foo_include_dir = os.path.abspath(self.foo_include_dir)\n",
      "        self.foo_library_dir = os.path.abspath(self.foo_library_dir)\n",
      "\n",
      "    def update(self, tool):\n",
      "        \"\"\" Обновить конфигурацию проекта. \"\"\"\n",
      "\n",
      "        # Получить обвязки pyfoo\n",
      "        # (в файле pyproject.toml раздел [tool.sip.bindings.pyfoo])\n",
      "        foo_bindings = self.bindings['pyfoo']\n",
      "\n",
      "        # Установим параметр include_dirs для обвязки\n",
      "        if self.foo_include_dir is not None:\n",
      "            foo_bindings.include_dirs = [self.foo_include_dir]\n",
      "\n",
      "        # Установим параметр library_dirs для обвязки\n",
      "        if self.foo_library_dir is not None:\n",
      "            foo_bindings.library_dirs = [self.foo_library_dir]\n",
      "\n",
      "        super().update(tool)\n",
      "\r\n",
      "Мы снова создали класс FooProject, производный от sipbuild.Project. В этом примере отключена автоматическая сборка библиотеки foo, потому что теперь подразумевается, что она может находиться в каком-нибудь другом месте, и к моменту создания обвязки уже должны быть готовы заголовочные и объектные файлы.\n",
      "\r\n",
      "В классе FooProject переопределены три метода: get_options, apply_user_defaults и update. Рассмотрим их более внимательно. \n",
      "\r\n",
      "Начнем с метода get_options. Этот метод должен возвращать список экземпляров класса sipbuild.Option. Каждый элемент списка — это опция командной строки. Внутри переопределенного метода мы получаем список опций по умолчанию (переменная options) с помощью вызова одноименного метода базового класса, затем создаем две новые опции (--foo_include_dir и --foo_library_dir) и добавляем их в список, после чего возвращаем этот список из функции. \n",
      "\r\n",
      "Конструктор класса Option принимает один обязательный параметр (имя опции) и достаточно большое количество необязательных, описывающие тип значения для этого параметра, значение по умолчанию, описание параметра и некоторые другие. В этом примере используются следующие параметры конструктора Option: \n",
      "\n",
      "\n",
      "help задает описание параметра, которое можно увидеть, если запустить команду вроде sip-wheel -h\n",
      "metavar — строковое значение, которое для пользователя описывает, что должно представлять собой значение данного параметра. В нашем примере параметр metavar равен «DIR», чтобы подсказать пользователю, что значение этого параметра — директория.\n",
      "default — значение по умолчанию для параметра. В нашем примере подразумевается, что если не указаны пути к заголовочным и объектным файлам, то библиотека foo расположена там же, где и в предыдущих примерах (в папке с исходниками обвязки).\n",
      "tools — список строк, описывающих к каким командам должна применяться данная опция. В нашем примере мы добавляем параметры к sip-build, sip-install, sip-sdist и sip-wheel, поэтому tools = ['build', 'install', 'sdist', 'wheel'].\n",
      "\r\n",
      "Следующий перегруженный метод apply_user_defaults предназначен для установки значений параметров, которые пользователь может передать через командную строку. Метод apply_user_defaults из базового класса создает для каждого параметра командной строки, созданного в методе get_options, переменную (член класса), поэтому важно вызвать одноименный метод базового класса до использования созданных переменных, чтобы все созданные по параметрам командной строки переменные были созданы и проинициализированы значениями по умолчанию. После этого в нашем примере будут созданы переменные self.foo_include_dir и self.foo_library_dir. Если пользователь не указал соответствующие им параметры командной строки, то они будут принимать значения по умолчанию согласно параметрам конструктора класса Option (параметр default). Если параметр default не задан, то в зависимости от типа ожидаемого значения параметра он будет инициализирован либо None, либо пустым списком, либо 0.\n",
      "\r\n",
      "Внутри метода apply_user_defaults делаем так, чтобы пути в переменных self.foo_include_dir и self.foo_library_dir всегда были абсолютными. Это нужно чтобы не зависеть от того, какой будет рабочая папка в момент запуска сборки.\n",
      "\r\n",
      "Последний перегруженный метод в этом классе — update. Этот метод вызывается, когда нужно применить к проекту выполненные до этого изменения. Например, изменить или добавить параметры, заданные в файле pyproject.toml. В предыдущих примерах мы устанавливали пути до заголовочных и объектных файлов с помощью параметров include-dirs и library-dirs соответственно внутри раздела [tool.sip.bindings.pyfoo]. Теперь эти параметры мы будем устанавливать из скрипта project.py, поэтому в файле pyproject.toml эти параметры удалим:\n",
      "\n",
      "[build-system]\n",
      "requires = [\"sip >=5, <6\"]\n",
      "build-backend = \"sipbuild.api\"\n",
      "\n",
      "[tool.sip.metadata]\n",
      "name = \"pyfoo\"\n",
      "version = \"0.3\"\n",
      "license = \"MIT\"\n",
      "\n",
      "[tool.sip.bindings.pyfoo]\n",
      "headers = [\"foo.h\"]\n",
      "libraries = [\"foo\"]\n",
      "\r\n",
      "Внутри метода update мы из словаря self.bindings по ключу pyfoo достаем экземпляр класса sipbuild.Bindings. Имя ключа соответствует разделу [tool.sip.bindings.pyfoo] из файла pyproject.toml, и полученный таким образом экземпляр класса описывает настройки, описанные в этом разделе. Затем членам этого класса include_dirs и library_dirs (имена членов соответствуют параметрам include-dirs и library-dirs с заменой дефиса на нижнее подчеркивание) присваиваем списки, содержащие пути, хранящиеся в членах self.foo_include_dir и self.foo_library_dir. В этом примере для аккуратности производится проверка на то, что значения self.foo_include_dir и self.foo_library_dir не равны None, но в данном примере это условие всегда выполняется, потому что у созданных нами параметров командной строки есть значения по умолчанию.\n",
      "\r\n",
      "Таким образом мы подготовили файлы настроек для того, чтобы при сборке можно было указывать пути до заголовочных и объектных файлов. Проверим, что получилось.\n",
      "\r\n",
      "Для начала убедимся, что работают значения по умолчанию. Для этого нужно зайти в папку pyfoo_c_05/foo и собрать библиотеку с помощью команды make, поскольку мы отключили автоматическую сборку библиотеки в этом примере.\n",
      "\r\n",
      "После этого заходим в папку pyfoo_c_05 и запускаем команду sip-wheel. В результате выполнения этой команды будет создан файл pyfoo-0.5-cp38-cp38-manylinux1_x86_64.whl или с похожим названием.\n",
      "\r\n",
      "Теперь перенесем папку foo куда-нибудь за пределы папки pyfoo_c_05 и снова запустим команду sip-wheel. В результате получим ожидаемую ошибку, сообщающую, что у нас нет объектного файла библиотеки:\n",
      "\n",
      "usr/bin/ld: невозможно найти -lfoo\n",
      "collect2: ошибка: выполнение ld завершилось с кодом возврата 1\n",
      "sip-wheel: Unable to compile the 'foo' module: command 'g++' failed with exit status 1\n",
      "\r\n",
      "После этого запустим sip-wheel с использованием новых параметром командной строки:\n",
      "\n",
      "sip-wheel --foo-include-dir \".../foo\" --foo-library-dir \".../foo/bin\"\n",
      "\r\n",
      "Вместо многоточия нужно указать путь до папки, куда вы перенесли папку foo с собранной библиотекой. В результате сборка должна закончиться успешно созданием файла .whl. Созданный модуль можно установить и протестировать так же, как это делали в предыдущих разделах.\n",
      "\n",
      " Проверяем порядок вызова методов из project.py\r\n",
      "Следующий пример, который мы рассмотрим, будет совсем простым, он продемонстрирует порядок вызова методов класса Project, которые мы перегружали в предыдущих разделах. Это может быть полезно для того, чтобы понять, когда можно инициализировать переменные. Данный пример находится в папке pyfoo_c_06 в репозитории с исходниками.\n",
      "\r\n",
      "Суть этого примера состоит в том, чтобы в классе FooProject, который расположен в файле project.py, перегрузить все методы, которые мы использовали до этого, и добавить в них вызовы функции print, которая бы выводила имя метода, в котором она находится:\n",
      "\n",
      "from sipbuild import Project\n",
      "\n",
      "class FooProject(Project):\n",
      "    def get_options(self):\n",
      "        print('get_options()')\n",
      "        options = super().get_options()\n",
      "        return options\n",
      "\n",
      "    def apply_user_defaults(self, tool):\n",
      "        print('apply_user_defaults()')\n",
      "        super().apply_user_defaults(tool)\n",
      "\n",
      "    def apply_nonuser_defaults(self, tool):\n",
      "        print('apply_nonuser_defaults()')\n",
      "        super().apply_nonuser_defaults(tool)\n",
      "\n",
      "    def update(self, tool):\n",
      "        print('update()')\n",
      "        super().update(tool)\n",
      "\n",
      "    def build(self):\n",
      "        print('build()')\n",
      "        super().build()\n",
      "\n",
      "    def build_sdist(self, sdist_directory):\n",
      "        print('build_sdist()')\n",
      "        return super().build_sdist(sdist_directory)\n",
      "\n",
      "    def build_wheel(self, wheel_directory):\n",
      "        print('build_wheel()')\n",
      "        return super().build_wheel(wheel_directory)\n",
      "\n",
      "    def install(self):\n",
      "        print('install()')\n",
      "        super().install()\n",
      "\r\n",
      "Внимательные читатели должны заметить, что помимо ранее использованных методов, в этом примере перегружен еще метод apply_nonuser_defaults(), о котором мы раньше не говорили. В этом методе рекомендуют устанавливать значения по умолчанию для всех переменных, которые нельзя изменить через параметры командной строки.\n",
      "\r\n",
      "В файле pyproject.toml вернем явное указание пути до библиотеки:\n",
      "\n",
      "[build-system]\n",
      "requires = [\"sip >=5, <6\"]\n",
      "build-backend = \"sipbuild.api\"\n",
      "\n",
      "[tool.sip.metadata]\n",
      "name = \"pyfoo\"\n",
      "version = \"0.4\"\n",
      "license = \"MIT\"\n",
      "\n",
      "[tool.sip.bindings.pyfoo]\n",
      "headers = [\"foo.h\"]\n",
      "libraries = [\"foo\"]\n",
      "include-dirs = [\"foo\"]\n",
      "library-dirs = [\"foo/bin\"]\n",
      "\r\n",
      "Чтобы проект успешно собрался, нужно войти в папку foo и собрать там библиотеку с помощью команды make. После этого вернуться в папку pyfoo_c_06 и запустить, например, команду sip-wheel. В результате, если отбросить предупреждения компилятора, будет выведен следующий текст:\n",
      "\n",
      "get_options()\n",
      "apply_nonuser_defaults()\n",
      "get_options()\n",
      "get_options()\n",
      "apply_user_defaults()\n",
      "get_options()\n",
      "update()\r\n",
      "These bindings will be built: pyfoo.\n",
      "build_wheel()\r\n",
      "Generating the pyfoo bindings…\r\n",
      "Compiling the 'foo' module…\r\n",
      "The wheel has been built.\n",
      "\r\n",
      "Полужирным шрифтом выделены строки, которые выводятся из нашего файла project.py. Таким образом мы видим, что метод get_options вызывается несколько раз, и это надо учитывать, если вы собираетесь инициализировать какую-нибудь переменную-член в классе, производный от Project. Метод get_options для этого — не лучшее место.\n",
      "\r\n",
      "Также полезно запомнить, что метод apply_nonuser_defaults вызывается до метода apply_user_defaults, т.е. в методе apply_user_defaults уже можно использовать переменные, значения которых установлены в методе apply_nonuser_defaults.\n",
      "\r\n",
      "После этого вызывается метод update, а в самом конце метод, отвечающий непосредственно за сборку, в нашем случае — build_wheel.\n",
      "\n",
      " Заключение к первой части\r\n",
      "В этой статье мы начали изучать инструмент SIP, предназначенный для создания Python-обвязок (Python bindings) для библиотек, написанных на языках C или C++. В этой первой части статьи мы рассмотрели основы использования SIP на примере создания Python-обвязки для очень простой библиотеки, написанной на языке C. \n",
      "\r\n",
      "Мы разобрались с файлами, которые необходимо создать для работы с SIP. В файле pyproject.toml содержится информация о пакете (название, номер версии, лицензия и пути до заголовочных и объектных файлов). С помощью файла project.py можно влиять на процесс сборки пакета Python, например, запускать сборку C-библиотеки или дать возможность пользователю указывать расположение заголовочных и объектных файлов библиотеки.\n",
      "\r\n",
      "В файле *.sip описывается интерфейс Python-модуля с перечислением функций и классов, которые будут содержаться в модуле. Для описания интерфейса в файле *.sip используются директивы и аннотации.\n",
      "\r\n",
      "Во второй части статьи мы создадим обвязку над объектно-ориентированной библиотекой, написанной на C++, и на ее примере изучим приемы, которые будут полезны при описании интерфейса классов C++, а заодно разберемся с новыми для нас директивами и аннотациями.\n",
      "\r\n",
      "Продолжение следует.\n",
      "\n",
      " Ссылки\n",
      "\n",
      "Примеры для данной статьи\n",
      "Домашняя страница SIP\n",
      "Документация для SIP\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " В первой части статьи мы рассмотрели основы работы с утилитой SIP, предназначенной для создания Python-обвязок (Python bindings) для библиотек, написанных на языках C и C++. Мы рассмотрели основные файлы, которые нужно создать для работы с SIP и начали рассматривать директивы и аннотации. До сих пор мы делали обвязку для простой библиотеки, написанной на языке C. В этой части мы разберемся, как делать обвязку для библиотеки на языке C++, которая содержит классы. На примере этой библиотеки мы посмотрим, какие приемы могут быть полезны при работе с объектно-ориентированной библиотекой, а заодно разберемся с новыми для нас директивами и аннотациями.\n",
      "\r\n",
      "Все примеры для статьи доступны в репозитории на github по ссылке: https://github.com/Jenyay/sip-examples.\n",
      "\n",
      "Делаем обвязку для библиотеки на языке C++\r\n",
      "Следующий пример, который мы будем рассматривать, находится в папке pyfoo_cpp_01.\n",
      "\r\n",
      "Для начала создадим библиотеку, для которой мы будем делать обвязку. Библиотека будет по-прежнему располагаться в папке foo и содержать один класс — Foo. Заголовочный файл foo.h с объявлением этого класса выглядит следующим образом:\n",
      "\n",
      "#ifndef FOO_LIB\n",
      "#define FOO_LIB\n",
      "\n",
      "class Foo {\n",
      "    private:\n",
      "        int _int_val;\n",
      "        char* _string_val;\n",
      "    public:\n",
      "        Foo(int int_val, const char* string_val);\n",
      "        virtual ~Foo();\n",
      "\n",
      "        void set_int_val(int val);\n",
      "        int get_int_val();\n",
      "\n",
      "        void set_string_val(const char* val);\n",
      "        char* get_string_val();\n",
      "};\n",
      "\n",
      "#endif\n",
      "\r\n",
      "Это простой класс с двумя геттерами и сеттерами, устанавливающие и возвращающие значения типа int и char*. Реализация класса выглядит следующим образом:\n",
      "\n",
      "#include <string.h>\n",
      "\n",
      "#include \"foo.h\"\n",
      "\n",
      "Foo::Foo(int int_val, const char* string_val): _int_val(int_val) {\n",
      "    _string_val = nullptr;\n",
      "    set_string_val(string_val);\n",
      "}\n",
      "\n",
      "Foo::~Foo(){\n",
      "    delete[] _string_val;\n",
      "    _string_val = nullptr;\n",
      "}\n",
      "\n",
      "void Foo::set_int_val(int val) {\n",
      "    _int_val = val;\n",
      "}\n",
      "\n",
      "int Foo::get_int_val() {\n",
      "    return _int_val;\n",
      "}\n",
      "\n",
      "void Foo::set_string_val(const char* val) {\n",
      "    if (_string_val != nullptr) {\n",
      "        delete[] _string_val;\n",
      "    }\n",
      "\n",
      "    auto count = strlen(val) + 1;\n",
      "    _string_val = new char[count];\n",
      "    strcpy(_string_val, val);\n",
      "}\n",
      "\n",
      "char* Foo::get_string_val() {\n",
      "    return _string_val;\n",
      "}\n",
      "\r\n",
      "Для проверки работоспособности библиотеки в папке foo также содержится файл main.cpp, использующий класс Foo:\n",
      "\n",
      "#include <iostream>\n",
      "\n",
      "#include \"foo.h\"\n",
      "\n",
      "using std::cout;\n",
      "using std::endl;\n",
      "\n",
      "int main(int argc, char* argv[]) {\n",
      "    auto foo = Foo(10, \"Hello\");\n",
      "    cout << \"int_val: \" << foo.get_int_val() << endl;\n",
      "    cout << \"string_val: \" << foo.get_string_val() << endl;\n",
      "\n",
      "    foo.set_int_val(0);\n",
      "    foo.set_string_val(\"Hello world!\");\n",
      "\n",
      "    cout << \"int_val: \" << foo.get_int_val() << endl;\n",
      "    cout << \"string_val: \" << foo.get_string_val() << endl;\n",
      "}\n",
      "\r\n",
      "Для сборки библиотеки foo используется следующий Makefile:\n",
      "\n",
      "CC=g++\n",
      "CFLAGS=-c -fPIC\n",
      "DIR_OUT=bin\n",
      "\n",
      "all: main\n",
      "\n",
      "main: main.o libfoo.a\n",
      "    $(CC) $(DIR_OUT)/main.o -L$(DIR_OUT) -lfoo -o $(DIR_OUT)/main\n",
      "\n",
      "main.o: makedir main.cpp\n",
      "    $(CC) $(CFLAGS) main.cpp -o $(DIR_OUT)/main.o\n",
      "\n",
      "libfoo.a: makedir foo.cpp\n",
      "    $(CC) $(CFLAGS) foo.cpp -o $(DIR_OUT)/foo.o\n",
      "    ar rcs $(DIR_OUT)/libfoo.a $(DIR_OUT)/foo.o\n",
      "\n",
      "makedir:\n",
      "    mkdir -p $(DIR_OUT)\n",
      "\n",
      "clean:\n",
      "    rm -rf $(DIR_OUT)/*\n",
      "\r\n",
      "Отличие от Makefile в предыдущих примерах, помимо изменение компилятора с gcc на g++, заключается в том, что для компиляции был добавлен еще один параметр -fPIC, который указывает компилятору размещать код в библиотеке определенным образом (так называемый «позиционно-независимый код»). Поскольку эта статья не про компиляторы, то не будем более подробно разбираться с тем, что этот параметр делает и зачем он нужен.\n",
      "\r\n",
      "Начнем делать обвязку для этой библиотеки. Файлы pyproject.toml и project.py почти не изменятся по сравнению с предыдущими примерами. Вот как теперь выглядит файл pyproject.toml:\n",
      "\n",
      "[build-system]\n",
      "requires = [\"sip >=5, <6\"]\n",
      "build-backend = \"sipbuild.api\"\n",
      "\n",
      "[tool.sip.metadata]\n",
      "name = \"pyfoocpp\"\n",
      "version = \"0.1\"\n",
      "license = \"MIT\"\n",
      "\n",
      "[tool.sip.bindings.pyfoocpp]\n",
      "headers = [\"foo.h\"]\n",
      "libraries = [\"foo\"]\n",
      "\r\n",
      "Теперь наши примеры, написанные на языке C++ будут упаковываться в Python-пакет pyfoocpp, это, пожалуй, единственное заметное изменение в этом файле. \n",
      "\r\n",
      "Файл project.py остался такой же, как и в примере pyfoo_c_04:\n",
      "\n",
      "import os\n",
      "import subprocess\n",
      "\n",
      "from sipbuild import Project\n",
      "\n",
      "class FooProject(Project):\n",
      "    def _build_foo(self):\n",
      "        cwd = os.path.abspath('foo')\n",
      "        subprocess.run(['make'], cwd=cwd, capture_output=True, check=True)\n",
      "\n",
      "    def build(self):\n",
      "        self._build_foo()\n",
      "        super().build()\n",
      "\n",
      "    def build_sdist(self, sdist_directory):\n",
      "        self._build_foo()\n",
      "        return super().build_sdist(sdist_directory)\n",
      "\n",
      "    def build_wheel(self, wheel_directory):\n",
      "        self._build_foo()\n",
      "        return super().build_wheel(wheel_directory)\n",
      "\n",
      "    def install(self):\n",
      "        self._build_foo()\n",
      "        super().install()\n",
      "\r\n",
      "А вот файл pyfoocpp.sip мы рассмотрим более подробно. Напомню, что этот файл описывает интерфейс для будущего Python-модуля: что он должен в себя включать, как должен выглядеть интерфейс классов и т.д. Файл .sip не обязан повторять заголовочный файл библиотеки, хоть у них и будет много общего. Внутри этого класса могут добавляться новые методы, которых не было в исходном классе. Т.е. интерфейс, описанный в файле .sip может подстраивать классы библиотеки под принципы, принятые в языке Python, если это необходимо. В файле pyfoocpp.sip мы увидим новые для нас директивы. \n",
      "\r\n",
      "Для начала посмотрим, что этот файл содержит:\n",
      "\n",
      "%Module(name=foocpp, language=\"C++\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "\n",
      "class Foo {\n",
      "    %TypeHeaderCode\n",
      "    #include <foo.h>\n",
      "    %End\n",
      "\n",
      "    public:\n",
      "        Foo(int, const char*);\n",
      "\n",
      "        void set_int_val(int);\n",
      "        int get_int_val();\n",
      "\n",
      "        void set_string_val(const char*);\n",
      "        char* get_string_val();\n",
      "};\n",
      "\r\n",
      "Первые строки нам уже должны быть понятны по предыдущим примерам. В директиве %Module мы указываем имя Python-модуля, который будет создан (т.е. для использования этого модуля мы должны будем использовать команды import foocpp или from foocpp import .... В этой же директиве мы указываем, что язык у нас теперь — C++. Директива %DefaultEncoding задает кодировку, которая будет использоваться для преобразования строки Python в типы char, const char, char* и const char*.\n",
      "\r\n",
      "Затем следует объявление интерфейса класса Foo. Сразу после объявления класса Foo встречается не используемая до сих пор директива %TypeHeaderCode, которая заканчивается директивой %End. Директива %TypeHeaderCode должна содержать код, объявляющий интерфейс класса C++, для которого создается обертка. Как правило, в этой директиве достаточно подключить заголовочный файл с объявлением класса.\n",
      "\r\n",
      "После этого перечислены методы класса, которые будут преобразованы в методы класса Foo для языка Python. Важно отметить, что в этом месте мы объявляем только публичные методы, которые будут доступны из класса Foo в Python (поскольку в Python нет приватных и защищенных членов). Поскольку мы в самом начале использовали директиву %DefaultEncoding, то в методах, принимающих аргументы типа const char*, можно не использовать аннотацию Encoding для указания кодировки для преобразования этих параметров в строки Python и обратно.\n",
      "\r\n",
      "Теперь нам остается собрать Python-пакет pyfoocpp и проверить его. Но прежде чем собирать полноценный wheel-пакет, давайте воспользуемся командой sip-build и посмотрим, какие исходные файлы для последующей компиляции создаст SIP, и попытаемся найти в них что-то похожее на тот класс, который будет создаваться в коде на языке Python. Для этого вышеуказанную команду sip-build нужно вызвать в папке pyfoo_cpp_01. В результате будет создана папка build со следующим содержимым:\n",
      "\n",
      "\n",
      "build\n",
      "└── foocpp\n",
      "    ├── apiversions.c\n",
      "    ├── array.c\n",
      "    ├── array.h\n",
      "    ├── bool.cpp\n",
      "    ├── build\n",
      "    │   └── temp.linux-x86_64-3.8\n",
      "    │       ├── apiversions.o\n",
      "    │       ├── array.o\n",
      "    │       ├── bool.o\n",
      "    │       ├── descriptors.o\n",
      "    │       ├── int_convertors.o\n",
      "    │       ├── objmap.o\n",
      "    │       ├── qtlib.o\n",
      "    │       ├── sipfoocppcmodule.o\n",
      "    │       ├── sipfoocppFoo.o\n",
      "    │       ├── siplib.o\n",
      "    │       ├── threads.o\n",
      "    │       └── voidptr.o\n",
      "    ├── descriptors.c\n",
      "    ├── foocpp.cpython-38-x86_64-linux-gnu.so\n",
      "    ├── int_convertors.c\n",
      "    ├── objmap.c\n",
      "    ├── qtlib.c\n",
      "    ├── sipAPIfoocpp.h\n",
      "    ├── sipfoocppcmodule.cpp\n",
      "    ├── sipfoocppFoo.cpp\n",
      "    ├── sip.h\n",
      "    ├── sipint.h\n",
      "    ├── siplib.c\n",
      "    ├── threads.c\n",
      "    └── voidptr.c\n",
      "\n",
      "\r\n",
      "В качестве дополнительного задания рассмотрите внимательнее файл sipfoocppFoo.cpp (мы его не будем подробно обсуждать в этой статье):\n",
      "\n",
      "/*\n",
      " * Interface wrapper code.\n",
      " *\n",
      " * Generated by SIP 5.1.1\n",
      " */\n",
      "\n",
      "#include \"sipAPIfoocpp.h\"\n",
      "\n",
      "#line 6 \"/home/jenyay/temp/2/pyfoocpp.sip\"\n",
      "    #include <foo.h>\n",
      "#line 12 \"/home/jenyay/temp/2/build/foocpp/sipfoocppFoo.cpp\"\n",
      "\n",
      "PyDoc_STRVAR(doc_Foo_set_int_val, \"set_int_val(self, int)\");\n",
      "\n",
      "extern \"C\" {static PyObject *meth_Foo_set_int_val(PyObject *, PyObject *);}\n",
      "static PyObject *meth_Foo_set_int_val(PyObject *sipSelf, PyObject *sipArgs)\n",
      "{\n",
      "    PyObject *sipParseErr = SIP_NULLPTR;\n",
      "\n",
      "    {\n",
      "        int a0;\n",
      "         ::Foo *sipCpp;\n",
      "\n",
      "        if (sipParseArgs(&sipParseErr, sipArgs, \"Bi\", &sipSelf, sipType_Foo, &sipCpp, &a0))\n",
      "        {\n",
      "            sipCpp->set_int_val(a0);\n",
      "\n",
      "            Py_INCREF(Py_None);\n",
      "            return Py_None;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    /* Raise an exception if the arguments couldn't be parsed. */\n",
      "    sipNoMethod(sipParseErr, sipName_Foo, sipName_set_int_val, doc_Foo_set_int_val);\n",
      "\n",
      "    return SIP_NULLPTR;\n",
      "}\n",
      "\n",
      "PyDoc_STRVAR(doc_Foo_get_int_val, \"get_int_val(self) -> int\");\n",
      "\n",
      "extern \"C\" {static PyObject *meth_Foo_get_int_val(PyObject *, PyObject *);}\n",
      "static PyObject *meth_Foo_get_int_val(PyObject *sipSelf, PyObject *sipArgs)\n",
      "{\n",
      "    PyObject *sipParseErr = SIP_NULLPTR;\n",
      "\n",
      "    {\n",
      "         ::Foo *sipCpp;\n",
      "\n",
      "        if (sipParseArgs(&sipParseErr, sipArgs, \"B\", &sipSelf, sipType_Foo, &sipCpp))\n",
      "        {\n",
      "            int sipRes;\n",
      "\n",
      "            sipRes = sipCpp->get_int_val();\n",
      "\n",
      "            return PyLong_FromLong(sipRes);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    /* Raise an exception if the arguments couldn't be parsed. */\n",
      "    sipNoMethod(sipParseErr, sipName_Foo, sipName_get_int_val, doc_Foo_get_int_val);\n",
      "\n",
      "    return SIP_NULLPTR;\n",
      "}\n",
      "\n",
      "PyDoc_STRVAR(doc_Foo_set_string_val, \"set_string_val(self, str)\");\n",
      "\n",
      "extern \"C\" {static PyObject *meth_Foo_set_string_val(PyObject *, PyObject *);}\n",
      "static PyObject *meth_Foo_set_string_val(PyObject *sipSelf, PyObject *sipArgs)\n",
      "{\n",
      "    PyObject *sipParseErr = SIP_NULLPTR;\n",
      "\n",
      "    {\n",
      "        const char* a0;\n",
      "        PyObject *a0Keep;\n",
      "         ::Foo *sipCpp;\n",
      "\n",
      "        if (sipParseArgs(&sipParseErr, sipArgs, \"BA8\", &sipSelf, sipType_Foo, &sipCpp, &a0Keep, &a0))\n",
      "        {\n",
      "            sipCpp->set_string_val(a0);\n",
      "            Py_DECREF(a0Keep);\n",
      "\n",
      "            Py_INCREF(Py_None);\n",
      "            return Py_None;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    /* Raise an exception if the arguments couldn't be parsed. */\n",
      "    sipNoMethod(sipParseErr, sipName_Foo, sipName_set_string_val, doc_Foo_set_string_val);\n",
      "\n",
      "    return SIP_NULLPTR;\n",
      "}\n",
      "\n",
      "PyDoc_STRVAR(doc_Foo_get_string_val, \"get_string_val(self) -> str\");\n",
      "\n",
      "extern \"C\" {static PyObject *meth_Foo_get_string_val(PyObject *, PyObject *);}\n",
      "static PyObject *meth_Foo_get_string_val(PyObject *sipSelf, PyObject *sipArgs)\n",
      "{\n",
      "    PyObject *sipParseErr = SIP_NULLPTR;\n",
      "\n",
      "    {\n",
      "         ::Foo *sipCpp;\n",
      "\n",
      "        if (sipParseArgs(&sipParseErr, sipArgs, \"B\", &sipSelf, sipType_Foo, &sipCpp))\n",
      "        {\n",
      "            char*sipRes;\n",
      "\n",
      "            sipRes = sipCpp->get_string_val();\n",
      "\n",
      "            if (sipRes == SIP_NULLPTR)\n",
      "            {\n",
      "                Py_INCREF(Py_None);\n",
      "                return Py_None;\n",
      "            }\n",
      "\n",
      "            return PyUnicode_FromString(sipRes);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    /* Raise an exception if the arguments couldn't be parsed. */\n",
      "    sipNoMethod(sipParseErr, sipName_Foo, sipName_get_string_val, doc_Foo_get_string_val);\n",
      "\n",
      "    return SIP_NULLPTR;\n",
      "}\n",
      "\n",
      "/* Call the instance's destructor. */\n",
      "extern \"C\" {static void release_Foo(void *, int);}\n",
      "static void release_Foo(void *sipCppV, int)\n",
      "{\n",
      "    delete reinterpret_cast< ::Foo *>(sipCppV);\n",
      "}\n",
      "\n",
      "extern \"C\" {static void dealloc_Foo(sipSimpleWrapper *);}\n",
      "static void dealloc_Foo(sipSimpleWrapper *sipSelf)\n",
      "{\n",
      "    if (sipIsOwnedByPython(sipSelf))\n",
      "    {\n",
      "        release_Foo(sipGetAddress(sipSelf), 0);\n",
      "    }\n",
      "}\n",
      "\n",
      "extern \"C\" {static void *init_type_Foo(sipSimpleWrapper *, PyObject *, \n",
      "                 PyObject *, PyObject **, PyObject **, PyObject **);}\n",
      "static void *init_type_Foo(sipSimpleWrapper *, PyObject *sipArgs, PyObject *sipKwds,\n",
      "                                   PyObject **sipUnused, PyObject **, PyObject **sipParseErr)\n",
      "{\n",
      "     ::Foo *sipCpp = SIP_NULLPTR;\n",
      "\n",
      "    {\n",
      "        int a0;\n",
      "        const char* a1;\n",
      "        PyObject *a1Keep;\n",
      "\n",
      "        if (sipParseKwdArgs(sipParseErr, sipArgs, sipKwds, SIP_NULLPTR, sipUnused, \"iA8\", &a0, &a1Keep, &a1))\n",
      "        {\n",
      "            sipCpp = new  ::Foo(a0,a1);\n",
      "            Py_DECREF(a1Keep);\n",
      "\n",
      "            return sipCpp;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    {\n",
      "        const  ::Foo* a0;\n",
      "\n",
      "        if (sipParseKwdArgs(sipParseErr, sipArgs, sipKwds, SIP_NULLPTR, sipUnused, \"J9\", sipType_Foo, &a0))\n",
      "        {\n",
      "            sipCpp = new  ::Foo(*a0);\n",
      "\n",
      "            return sipCpp;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    return SIP_NULLPTR;\n",
      "}\n",
      "\n",
      "static PyMethodDef methods_Foo[] = {\n",
      "    {sipName_get_int_val, meth_Foo_get_int_val, METH_VARARGS, doc_Foo_get_int_val},\n",
      "    {sipName_get_string_val, meth_Foo_get_string_val, METH_VARARGS, doc_Foo_get_string_val},\n",
      "    {sipName_set_int_val, meth_Foo_set_int_val, METH_VARARGS, doc_Foo_set_int_val},\n",
      "    {sipName_set_string_val, meth_Foo_set_string_val, METH_VARARGS, doc_Foo_set_string_val}\n",
      "};\n",
      "\n",
      "PyDoc_STRVAR(doc_Foo, \"\\1Foo(int, str)\\n\"\n",
      "\"Foo(Foo)\");\n",
      "\n",
      "sipClassTypeDef sipTypeDef_foocpp_Foo = {\n",
      "    {\n",
      "        -1,\n",
      "        SIP_NULLPTR,\n",
      "        SIP_NULLPTR,\n",
      "        SIP_TYPE_CLASS,\n",
      "        sipNameNr_Foo,\n",
      "        SIP_NULLPTR,\n",
      "        SIP_NULLPTR\n",
      "    },\n",
      "    {\n",
      "        sipNameNr_Foo,\n",
      "        {0, 0, 1},\n",
      "        4, methods_Foo,\n",
      "        0, SIP_NULLPTR,\n",
      "        0, SIP_NULLPTR,\n",
      "        {SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR,\n",
      "         SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR, SIP_NULLPTR},\n",
      "    },\n",
      "    doc_Foo,\n",
      "    -1,\n",
      "    -1,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    init_type_Foo,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    dealloc_Foo,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    release_Foo,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR,\n",
      "    SIP_NULLPTR\n",
      "};\n",
      "\r\n",
      "Теперь соберем пакет с помощью команды sip-wheel. После выполнения этой команды, если все пройдет успешно, будет создан файл pyfoocpp-0.1-cp38-cp38-manylinux1_x86_64.whl или с похожим именем. Установим его с помощью команды pip install --user pyfoocpp-0.1-cp38-cp38-manylinux1_x86_64.whl и запустим интерпретатор Python для проверки:\n",
      "\n",
      ">>> from foocpp import Foo\n",
      ">>> x = Foo(10, 'Hello')\n",
      "\n",
      ">>> x.get_int_val()\n",
      "10\n",
      "\n",
      ">>> x.get_string_val()\n",
      "'Hello'\n",
      "\n",
      ">>> x.set_int_val(50)\n",
      ">>> x.set_string_val('Привет')\n",
      "\n",
      ">>> x.get_int_val()\n",
      "50\n",
      "\n",
      ">>> x.get_string_val()\n",
      "'Привет'\n",
      "\r\n",
      "Работает! Таким образом, мы с вами только что сделали Python-модуль с обвязкой для класса на C++. Дальше будем наводить в этом классе красоту и добавлять разные удобства.\n",
      "\n",
      "Добавляем свойства\r\n",
      "Классы, созданные с помощью SIP не обязаны в точности повторять интерфейс классов C++. Например, в нашем классе Foo имеется два геттера и два сеттера, которые явно можно объединить в свойство, чтобы класс стал более «питоновским». Добавить свойства с помощью сип достаточно легко, как это делается, показывает пример в папке pyfoo_cpp_02. \n",
      "\r\n",
      "Этот пример аналогичен предыдущему, главное отличие заключается в файле pyfoocpp.sip, который теперь выглядит следующим образом:\n",
      "\n",
      "%Module(name=foocpp, language=\"C++\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "\n",
      "class Foo {\n",
      "    %TypeHeaderCode\n",
      "    #include <foo.h>\n",
      "    %End\n",
      "\n",
      "    public:\n",
      "        Foo(int, const char*);\n",
      "\n",
      "        void set_int_val(int);\n",
      "        int get_int_val();\n",
      "        %Property(name=int_val, get=get_int_val, set=set_int_val)\n",
      "\n",
      "        void set_string_val(const char*);\n",
      "        char* get_string_val();\n",
      "        %Property(name=string_val, get=get_string_val, set=set_string_val)\n",
      "};\n",
      "\r\n",
      "Как видите, все достаточно просто. Чтобы добавить свойство, предназначена директива %Property, у которой имеется два обязательных параметра: name для задания имени свойства, а также get для указания метода, который возвращает какое-либо значение (геттер). Сеттера может не быть, но если свойству нужно также присваивать значения, то метод-сеттер указывается в качестве значения параметра set. В нашем примере свойства создаются достаточно прямолинейно, поскольку уже имеются функции, работающие как геттеры и сеттеры.\n",
      "\r\n",
      "Нам остается только лишь собрать пакет с помощью команды sip-wheel, установить его, после этого проверим работу свойств в командном режиме интерпретатора python:\n",
      "\n",
      ">>> from foocpp import Foo\n",
      ">>> x = Foo(10, \"Hello\")\n",
      ">>> x.int_val\n",
      "10\n",
      "\n",
      ">>> x.string_val\n",
      "'Hello'\n",
      "\n",
      ">>> x.int_val = 50\n",
      ">>> x.string_val = 'Привет'\n",
      "\n",
      ">>> x.get_int_val()\n",
      "50\n",
      "\n",
      ">>> x.get_string_val()\n",
      "'Привет'\n",
      "\r\n",
      "Как видно из примера использования класса Foo, свойства int_val и string_val работают и на чтение, и на запись.\n",
      "\n",
      "Добавляем строки документации\r\n",
      "Продолжим улучшать наш класс Foo. Следующий пример, который расположен в папке pyfoo_cpp_03 показывает, как добавлять к различным элементам класса строки документации (docstring). Этот пример сделан на основе предыдущего, и главное изменение в нем касается файла pyfoocpp.sip. Вот его содержимое:\n",
      "\n",
      "%Module(name=foocpp, language=\"C++\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "\n",
      "class Foo {\n",
      "%Docstring\n",
      "Class example from C++ library\n",
      "%End\n",
      "\n",
      "    %TypeHeaderCode\n",
      "    #include <foo.h>\n",
      "    %End\n",
      "\n",
      "    public:\n",
      "        Foo(int, const char*);\n",
      "\n",
      "        void set_int_val(int);\n",
      "        %Docstring(format=\"deindented\", signature=\"prepended\")\n",
      "            Set integer value\n",
      "        %End\n",
      "\n",
      "        int get_int_val();\n",
      "        %Docstring(format=\"deindented\", signature=\"prepended\")\n",
      "            Return integer value\n",
      "        %End\n",
      "\n",
      "        %Property(name=int_val, get=get_int_val, set=set_int_val)\n",
      "        {\n",
      "            %Docstring \"deindented\"\n",
      "                The property for integer value\n",
      "            %End\n",
      "        };\n",
      "\n",
      "        void set_string_val(const char*);\n",
      "        %Docstring(format=\"deindented\", signature=\"appended\")\n",
      "            Set string value\n",
      "        %End\n",
      "\n",
      "        char* get_string_val();\n",
      "        %Docstring(format=\"deindented\", signature=\"appended\")\n",
      "            Return string value\n",
      "        %End\n",
      "\n",
      "        %Property(name=string_val, get=get_string_val, set=set_string_val)\n",
      "        {\n",
      "            %Docstring \"deindented\"\n",
      "                The property for string value\n",
      "            %End\n",
      "        };\n",
      "};\n",
      "\r\n",
      "Как вы уже поняли, для того, чтобы добавить строки документации к какому-либо элементу класса, нужно воспользоваться директивой %Docstring. В этом примере показано несколько способов использования этой директивы. Для лучшего понимания этого примера давайте сразу скомпилируем пакет pyfoocpp с помощью команды sip-wheel, установим его и будем последовательно разбираться с тем, какой параметр этой директивы на что влияет, рассматривая получившиеся строки документации в командном режиме Python. Напомню, что строки документации сохраняются в члены __doc__ объектов, к которым относятся эти строки.\n",
      "\r\n",
      "Первая строка документации относится к классу Foo. Как вы видите, все строки документации расположены между директивами %Docstring и %End. В строках 5-7 этого примера не используются никакие дополнительные параметры директивы %Docstring, поэтому строка документации будет записана в класс Foo как есть. Именно поэтому в строках 5-7 нет отступов, иначе отступы перед строкой документации тоже попали бы в Foo.__doc__. Убедимся в том, что класс Foo действительно содержит ту строку документации, которую мы ввели:\n",
      "\n",
      ">>> from foocpp import Foo\n",
      ">>> Foo.__doc__\n",
      "'Class example from C++ library'\n",
      "\r\n",
      "Следующая директива %Docstring, расположенная на 17-19 строках, использует сразу два параметра. Параметр format может принимать одно из двух значений: «raw» или «deindented». В первом случае строки документации сохраняются в том виде, как они записаны, а во втором — удаляются начальные символы пробелов (но не табуляции). Значение по умолчанию для случая, если параметр format не указан, можно задать с помощью директивы %DefaultDocstringFormat (мы ее рассмотрим чуть позже), а если она не указана, то считается, что format=«raw».\n",
      "\r\n",
      "Помимо заданных строк документации, SIP добавляет к строкам документации функций описание ее сигнатуры (какие типы переменных ожидаются на входе и какой тип функция возвращает). Параметр signature указывает, куда помещать такую сигнатуру: до указанной строки документации (signature=«prepended»), после нее (signature=«appended») или не добавлять сигнатуру (signature=«discarded»). \n",
      "\r\n",
      "Наш пример устанавливает параметр signature=«prepended» для функций get_int_val и set_int_val, а также signature=«appended» для функций get_string_val и set_string_val. Также был добавлен параметр format=«deindented» для того, чтобы удалить пробелы в начале строки документации. Проверим работу этих параметров в Python:\n",
      "\n",
      ">>> Foo.get_int_val.__doc__\n",
      "'get_int_val(self) -> int\\nReturn integer value'\n",
      "\n",
      ">>> Foo.set_int_val.__doc__\n",
      "'set_int_val(self, int)\\nSet integer value'\n",
      "\n",
      ">>> Foo.get_string_val.__doc__\n",
      "'Return string value\\nget_string_val(self) -> str'\n",
      "\n",
      ">>> Foo.set_string_val.__doc__\n",
      "'Set string value\\nset_string_val(self, str)'\n",
      "\r\n",
      "Как видим, с помощью параметра signature директивы %Docstring можно менять положение описания сигнатуры функции в строке документации.\n",
      "\r\n",
      "Теперь рассмотрим добавление строки документации в свойства. Обратите внимание, что в этом случае директивы %Docstring...%End заключены в фигурные скобки после директивы %Property. Такой формат записи описан в документации к директиве %Property. \n",
      "\r\n",
      "Также обратите внимание, как мы указываем параметр директивы %Docstring. Такой формат записи директив возможен, если мы устанавливаем только первый параметр директивы (в данном случае параметр format). Таким образом, в этом примере используются сразу три способа использования директив. \n",
      "\r\n",
      "Убедимся, что строка документации для свойств установлена:\n",
      "\n",
      ">>> Foo.int_val.__doc__\n",
      "'The property for integer value'\n",
      "\n",
      ">>> Foo.string_val.__doc__\n",
      "'The property for string value'\n",
      "\n",
      ">>> help(Foo)\n",
      "Help on class Foo in module foocpp:\n",
      "\n",
      "class Foo(sip.wrapper)\n",
      " |  Class example from C++ library\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Foo\n",
      " |      sip.wrapper\n",
      " |      sip.simplewrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_int_val(...)\n",
      " |      get_int_val(self) -> int\n",
      " |      Return integer value\n",
      " |  \n",
      " |  get_string_val(...)\n",
      " |      Return string value\n",
      " |      get_string_val(self) -> str\n",
      " |  \n",
      " |  set_int_val(...)\n",
      " |      set_int_val(self, int)\n",
      " |      Set integer value\n",
      " |  \n",
      " |  set_string_val(...)\n",
      " |      Set string value\n",
      " |      set_string_val(self, str)\n",
      "...\n",
      "\n",
      "\r\n",
      "Давайте упростим этот пример, установив значения по умолчанию для параметров format и signature с помощью директив %DefaultDocstringFormat и %DefaultDocstringSignature. Использование этих директив показано в примере из папки pyfoo_cpp_04. Файл pyfoocpp.sip в этом примере содержит следующий код:\n",
      "\n",
      "%Module(name=foocpp, language=\"C++\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "%DefaultDocstringFormat \"deindented\"\n",
      "%DefaultDocstringSignature \"prepended\"\n",
      "\n",
      "class Foo {\n",
      "    %Docstring\n",
      "    Class example from C++ library\n",
      "    %End\n",
      "\n",
      "    %TypeHeaderCode\n",
      "    #include <foo.h>\n",
      "    %End\n",
      "\n",
      "    public:\n",
      "        Foo(int, const char*);\n",
      "\n",
      "        void set_int_val(int);\n",
      "        %Docstring\n",
      "            Set integer value\n",
      "        %End\n",
      "\n",
      "        int get_int_val();\n",
      "        %Docstring\n",
      "            Return integer value\n",
      "        %End\n",
      "\n",
      "        %Property(name=int_val, get=get_int_val, set=set_int_val)\n",
      "        {\n",
      "            %Docstring\n",
      "                The property for integer value\n",
      "            %End\n",
      "        };\n",
      "\n",
      "        void set_string_val(const char*);\n",
      "        %Docstring\n",
      "            Set string value\n",
      "        %End\n",
      "\n",
      "        char* get_string_val();\n",
      "        %Docstring\n",
      "            Return string value\n",
      "        %End\n",
      "\n",
      "        %Property(name=string_val, get=get_string_val, set=set_string_val)\n",
      "        {\n",
      "            %Docstring\n",
      "                The property for string value\n",
      "            %End\n",
      "        };\n",
      "};\n",
      "\r\n",
      "В начале файла добавлены строки %DefaultDocstringFormat «deindented» и %DefaultDocstringSignature «prepended», а далее все параметры из директивы %Docstring были убраны.\n",
      "\r\n",
      "После сборки и установки этого примера можем посмотреть, как теперь выглядит описание класса Foo, которое выводит команда help(Foo):\n",
      "\n",
      ">>> from foocpp import Foo\n",
      ">>> help(Foo)\n",
      "\n",
      "class Foo(sip.wrapper)\n",
      " |  Class example from C++ library\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Foo\n",
      " |      sip.wrapper\n",
      " |      sip.simplewrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_int_val(...)\n",
      " |      get_int_val(self) -> int\n",
      " |      Return integer value\n",
      " |  \n",
      " |  get_string_val(...)\n",
      " |      get_string_val(self) -> str\n",
      " |      Return string value\n",
      " |  \n",
      " |  set_int_val(...)\n",
      " |      set_int_val(self, int)\n",
      " |      Set integer value\n",
      " |  \n",
      " |  set_string_val(...)\n",
      " |      set_string_val(self, str)\n",
      " |      Set string value\n",
      "...\n",
      "\r\n",
      "Все выглядит достаточно аккуратно и однотипно.\n",
      "\n",
      "Переименовываем классы и методы\r\n",
      "Как мы уже говорили, интерфейс, предоставляемый обвязкой на языке Python не обязательно должен совпадать с тем интерфейсом, который предоставляет библиотека на языке C/C++. Выше мы добавляли свойства в классы, а сейчас рассмотрим еще один прием, который может быть полезен, если возникают конфликты имен классов или функций, например, если имя функции совпадает с каким-нибудь ключевым словом языка Python. Для этого предусмотрена возможность переименования классов, функций, исключений и других сущностей.\n",
      "\r\n",
      "Для переименования сущности используется аннотация PyName, значению которой нужно присвоить новое имя сущности. Работа с аннотацией PyName показана в примере из папки pyfoo_cpp_05. Этот пример создан на основе предыдущего примера pyfoo_cpp_04 и отличается от него файлом pyfoocpp.sip, содержимое которого теперь выглядит следующим образом:\n",
      "\n",
      "%Module(name=foocpp, language=\"C++\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "%DefaultDocstringFormat \"deindented\"\n",
      "%DefaultDocstringSignature \"prepended\"\n",
      "\n",
      "class Foo /PyName=Bar/ {\n",
      "    %Docstring\n",
      "    Class example from C++ library\n",
      "    %End\n",
      "\n",
      "    %TypeHeaderCode\n",
      "    #include <foo.h>\n",
      "    %End\n",
      "\n",
      "    public:\n",
      "        Foo(int, const char*);\n",
      "\n",
      "        void set_int_val(int) /PyName=set_integer_value/;\n",
      "        %Docstring\n",
      "            Set integer value\n",
      "        %End\n",
      "\n",
      "        int get_int_val() /PyName=get_integer_value/;\n",
      "        %Docstring\n",
      "            Return integer value\n",
      "        %End\n",
      "\n",
      "        %Property(name=int_val, get=get_integer_value, set=set_integer_value)\n",
      "        {\n",
      "            %Docstring\n",
      "                The property for integer value\n",
      "            %End\n",
      "        };\n",
      "\n",
      "        void set_string_val(const char*) /PyName=set_string_value/;\n",
      "        %Docstring\n",
      "            Set string value\n",
      "        %End\n",
      "\n",
      "        char* get_string_val() /PyName=get_string_value/;\n",
      "        %Docstring\n",
      "            Return string value\n",
      "        %End\n",
      "\n",
      "        %Property(name=string_val, get=get_string_value, set=set_string_value)\n",
      "        {\n",
      "            %Docstring\n",
      "                The property for string value\n",
      "            %End\n",
      "        };\n",
      "};\n",
      "\r\n",
      "В этом примере мы переименовали класс Foo в класс Bar, а также присвоили другие имена всем методам с помощью аннотации PyName. Думаю, что все здесь достаточно просто и понятно, единственное, на что стоит обратить внимание — это создание свойств. В директиве %Property в качестве параметров get и set нужно указывать имена методов, как они будут называться в Python-классе, а не те имена, как они назывались изначально к коде на C++.\n",
      "\r\n",
      "Скомпилируем пример, установим его и посмотрим, как этот класс будет выглядеть в языке Python:\n",
      "\n",
      ">>> from foocpp import Bar\n",
      ">>> help(Bar)\n",
      "\n",
      "Help on class Bar in module foocpp:\n",
      "\n",
      "class Bar(sip.wrapper)\n",
      " |  Class example from C++ library\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Bar\n",
      " |      sip.wrapper\n",
      " |      sip.simplewrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_integer_value(...)\n",
      " |      get_integer_value(self) -> int\n",
      " |      Return integer value\n",
      " |  \n",
      " |  get_string_value(...)\n",
      " |      get_string_value(self) -> str\n",
      " |      Return string value\n",
      " |  \n",
      " |  set_integer_value(...)\n",
      " |      set_integer_value(self, int)\n",
      " |      Set integer value\n",
      " |  \n",
      " |  set_string_value(...)\n",
      " |      set_string_value(self, str)\n",
      " |      Set string value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  int_val\n",
      " |      The property for integer value\n",
      " |  \n",
      " |  string_val\n",
      " |      The property for string value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      "...\n",
      "\r\n",
      "Сработало! Нам удалось переименовать сам класс и его методы.\n",
      "\r\n",
      "Иногда в библиотеках используется договоренность, что имена всех классов начинаются с какого-либо префикса, например, с буквы «Q» в Qt или «wx» в wxWidgets. Если в своей Python-обвязке вы хотите переименовать все классы, избавившись от таких префиксов, то для того, чтобы не задавать аннотацию PyName для каждого класса, можно воспользоваться директивой %AutoPyName. Мы не будем рассматривать эту директиву в данной статье, скажем только, что директива %AutoPyName должна располагаться внутри директивы %Module и ограничимся примером из документации:\n",
      "\n",
      "%Module PyQt5.QtCore\n",
      "{\n",
      "    %AutoPyName(remove_leading=\"Q\")\n",
      "}\n",
      "\n",
      "Добавляем преобразование типов\n",
      "Пример с использованием класса std::wstring\r\n",
      "До сих пор мы рассматривали функции и классы, которые работали с простейшими типами вроде int и char*. Для таких типов SIP автоматически создавал конвертер в классы Python и обратно. В следующем примере, который расположен в папке pyfoo_cpp_06, мы рассмотрим случай, когда методы класса принимают и возвращают более сложные объекты, например, строки из STL. Чтобы упростить пример и не усложнять преобразование байтов в Unicode и обратно, в этом примере будет использоваться класс строк std::wstring. Идея этого примера — показать, как можно вручную задавать правила преобразования классов C++ в классы Python и обратно.\n",
      "\r\n",
      "Для этого примера мы изменим класс Foo из библиотеки foo. Теперь определение класса будет выглядеть следующим образом (файл foo.h):\n",
      "\n",
      "#ifndef FOO_LIB\n",
      "#define FOO_LIB\n",
      "\n",
      "#include <string>\n",
      "\n",
      "using std::wstring;\n",
      "\n",
      "class Foo {\n",
      "    private:\n",
      "        int _int_val;\n",
      "        wstring _string_val;\n",
      "    public:\n",
      "        Foo(int int_val, wstring string_val);\n",
      "\n",
      "        void set_int_val(int val);\n",
      "        int get_int_val();\n",
      "\n",
      "        void set_string_val(wstring val);\n",
      "        wstring get_string_val();\n",
      "};\n",
      "\n",
      "#endif\n",
      "\r\n",
      "Реализация класса Foo в файле foo.cpp:\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"foo.h\"\n",
      "\n",
      "using std::wstring;\n",
      "\n",
      "Foo::Foo(int int_val, wstring string_val):\n",
      "    _int_val(int_val), _string_val(string_val) {}\n",
      "\n",
      "void Foo::set_int_val(int val) {\n",
      "    _int_val = val;\n",
      "}\n",
      "\n",
      "int Foo::get_int_val() {\n",
      "    return _int_val;\n",
      "}\n",
      "\n",
      "void Foo::set_string_val(wstring val) {\n",
      "    _string_val = val;\n",
      "}\n",
      "\n",
      "wstring Foo::get_string_val() {\n",
      "    return _string_val;\n",
      "}\n",
      "\r\n",
      "И файл main.cpp для проверки работоспособности библиотеки:\n",
      "\n",
      "#include <iostream>\n",
      "\n",
      "#include \"foo.h\"\n",
      "\n",
      "using std::cout;\n",
      "using std::endl;\n",
      "\n",
      "int main(int argc, char* argv[]) {\n",
      "    auto foo = Foo(10, L\"Hello\");\n",
      "    cout << L\"int_val: \" << foo.get_int_val() << endl;\n",
      "    cout << L\"string_val: \" << foo.get_string_val().c_str() << endl;\n",
      "\n",
      "    foo.set_int_val(0);\n",
      "    foo.set_string_val(L\"Hello world!\");\n",
      "\n",
      "    cout << L\"int_val: \" << foo.get_int_val() << endl;\n",
      "    cout << L\"string_val: \" << foo.get_string_val().c_str() << endl;\n",
      "}\n",
      "\r\n",
      "Файлы foo.h, foo.cpp и main.cpp, как и раньше, располагаются в папке foo. Makefile и процесс сборки библиотеки не изменился. Также нет существенных изменений в файлах pyproject.toml и project.py.\n",
      "\r\n",
      "А вот файл pyfoocpp.sip стал заметно сложнее:\n",
      "\n",
      "%Module(name=foocpp, language=\"C++\")\n",
      "%DefaultEncoding \"UTF-8\"\n",
      "\n",
      "class Foo {\n",
      "    %TypeHeaderCode\n",
      "    #include <foo.h>\n",
      "    %End\n",
      "\n",
      "    public:\n",
      "        Foo(int, std::wstring);\n",
      "\n",
      "        void set_int_val(int);\n",
      "        int get_int_val();\n",
      "        %Property(name=int_val, get=get_int_val, set=set_int_val)\n",
      "\n",
      "        void set_string_val(std::wstring);\n",
      "        std::wstring get_string_val();\n",
      "        %Property(name=string_val, get=get_string_val, set=set_string_val)\n",
      "};\n",
      "\n",
      "%MappedType std::wstring\n",
      "{\n",
      "%TypeHeaderCode\n",
      "#include <string>\n",
      "%End\n",
      "\n",
      "%ConvertFromTypeCode\n",
      "    // Convert an std::wstring to a Python (Unicode) string\n",
      "    PyObject* newstring;\n",
      "    newstring = PyUnicode_FromWideChar(sipCpp->data(), -1);\n",
      "    return newstring;\n",
      "%End\n",
      "\n",
      "%ConvertToTypeCode\n",
      "    // Convert a Python (Unicode) string to an std::wstring\n",
      "    if (sipIsErr == NULL) {\n",
      "        return PyUnicode_Check(sipPy);\n",
      "    }\n",
      "    if (PyUnicode_Check(sipPy)) {\n",
      "        *sipCppPtr = new std::wstring(PyUnicode_AS_UNICODE(sipPy));\n",
      "        return 1;\n",
      "    }\n",
      "    return 0;\n",
      "%End\n",
      "};\n",
      "\r\n",
      "Для наглядности файл pyfoocpp.sip не добавляет строки документации. Если бы мы в файле pyfoocpp.sip оставили только объявление класса Foo без последующей директивы %MappedType, то с процессе сборки получили бы следующую ошибку:\n",
      "\n",
      "$ sip-wheel\n",
      "\n",
      "These bindings will be built: pyfoocpp.\n",
      "Generating the pyfoocpp bindings...\n",
      "sip-wheel: std::wstring is undefined\n",
      "\r\n",
      "Нам нужно явно описать, как объект типа std::wstring будет преобразовываться в какой-либо Python-объект, а также описать обратное преобразование. Для описания преобразования нам нужно будет работать на достаточно низком уровне на языке C и использовать Python/C API. Поскольку Python/C API — это большая тема, достойная даже не отдельной статьи, а книги, то в этом разделе мы рассмотрим только те функции, которые используются в примере, не особо углубляясь в подробности.\n",
      "\r\n",
      "Для объявления преобразований из объектов C++ в Python и наоборот предназначена директива %MappedType, внутри которой могут находиться три другие директивы: %TypeHeaderCode, %ConvertToTypeCode и %ConvertFromTypeCode. После выражения %MappedType нужно указать тип, для которого будут создаваться конвертеры. В нашем случае директива начинается с выражения %MappedType std::wstring.\n",
      "\r\n",
      "С директивой %TypeHeaderCode мы уже встречались в разделе Делаем обвязку для библиотеки на языке C++. Напомню, что эта директива предназначена для того, чтобы объявить используемые типы или подключить заголовочные файлы, в которых они объявлены. В данном примере внутри директивы %TypeHeaderCode подключается заголовочный файл string, где объявлен класс std::string.\n",
      "\r\n",
      "Теперь нам нужно описать преобразования\n",
      "\n",
      "%ConvertFromTypeCode. Преобразование объектов C++ в Python\r\n",
      "Начнем с преобразования объектов std::wstring в класс str языка Python. Данное преобразование в примере выглядит следующим образом:\n",
      "\n",
      "%ConvertFromTypeCode\n",
      "    // Convert an std::wstring to a Python (Unicode) string\n",
      "    PyObject* newstring;\n",
      "    newstring = PyUnicode_FromWideChar(sipCpp->data(), -1);\n",
      "    return newstring;\n",
      "%End\n",
      "\r\n",
      "Внутри этой директивы у нас имеется переменная sipCpp — указатель на объект из кода на C++, по которому нужно создать Python-объект и вернуть созданный объект из директивы с помощью оператора return. В данном случае переменная sipCpp имеет тип std::wstring*. Чтобы создать класс str, используется функция PyUnicode_FromWideChar из Python/C API. Эта функция в качестве первого параметра принимает массив (указатель) типа const wchar_t *w, а в качестве второго параметра — размер этого массива. Если в качестве второго параметра передать значение -1, то функция PyUnicode_FromWideChar сама рассчитает длину с помощью функции wcslen. \n",
      "\r\n",
      "Чтобы получить массив wchar_t* используется метод data из класса std::wstring.\n",
      "\r\n",
      "Функция PyUnicode_FromWideChar возвращает указатель на PyObject или NULL в случае ошибки. PyObject представляет собой любой Python-объект, в данном случае это будет класс str. В Python/C API работа с объектами происходит обычно через указатели PyObject*, поэтому и в данном случае из директивы %ConvertFromTypeCode мы возвращаем указатель PyObject*.\n",
      "\n",
      "%ConvertToTypeCode. Преобразование объектов Python в C++\r\n",
      "Обратное преобразование из объекта Python (по сути из PyObject*) в класс std::wstring описывается в директиве %ConvertToTypeCode. В примере pyfoo_cpp_06 преобразование выглядит следующим образом:\n",
      "\n",
      "%ConvertToTypeCode\n",
      "    // Convert a Python (Unicode) string to an std::wstring\n",
      "    if (sipIsErr == NULL) {\n",
      "        return PyUnicode_Check(sipPy);\n",
      "    }\n",
      "    if (PyUnicode_Check(sipPy)) {\n",
      "        *sipCppPtr = new std::wstring(PyUnicode_AS_UNICODE(sipPy));\n",
      "        return 1;\n",
      "    }\n",
      "    return 0;\n",
      "%End\n",
      "\r\n",
      "Код директивы %ConvertToTypeCode выглядит более сложно, потому что в процессе преобразования он вызывается несколько раз с разными целями. Внутри директивы %ConvertToTypeCode SIP создает несколько переменных, которые мы можем (или должны) использовать. \n",
      "\r\n",
      "Одна из таких переменных PyObject *sipPy представляет собой Python-объект, по которому нужно создать в данном случае экземпляр класса std::wstring. Результат нужно будет записать в другую переменную — sipCppPtr — это двойной указатель на создаваемый объект, т.е. в нашем случае эта переменная будет иметь тип std::wstring**.\n",
      "\r\n",
      "Еще одна создаваемая внутри директивы %ConvertToTypeCode переменная — int *sipIsErr. Если значение этой переменной равно NULL, значит директива %ConvertToTypeCode вызывается только с целью проверки, возможно ли преобразование типа. В этом случае мы не обязаны выполнять преобразование, а должны только проверить, возможно ли оно в принципе. Если возможно, то из директивы должны вернуть не нулевое значение, в противном случае, если преобразование невозможно, должны вернуть 0. Если этот указатель не равен NULL, значит нужно выполнить преобразование, а в случае возникновения ошибки в процессе преобразования, целочисленный код ошибки можно сохранить в эту переменную (с учетом того, что эта переменная является указателем на int*).\n",
      "\r\n",
      "В данном примере для проверки того, что sipPy представляет собой юникодную строку (класс str) используется макрос PyUnicode_Check, который принимает в качестве параметра аргумент типа PyObject*, если переданный аргумент представляет собой юникодную строку или класс, производный от нее.\n",
      "\r\n",
      "Преобразование в объект C++ осуществляется с помощью строки *sipCppPtr = new std::wstring(PyUnicode_AS_UNICODE(sipPy));. Здесь вызывается макрос PyUnicode_AS_UNICODE из Python/C API, который возвращает массив типа Py_UNICODE*, что эквивалентно wchar_t*. Этот массив передается в конструктор класса std::wstring. Как уже было сказано выше, результат сохраняется в переменной sipCppPtr.\n",
      "\r\n",
      "В данный момент директива PyUnicode_AS_UNICODE объявлена устаревшей и рекомендуется использовать другие макросы, но для упрощения примера используется именно этот макрос.\n",
      "\r\n",
      "Если преобразование прошло успешно, директива %ConvertToTypeCode должна вернуть не нулевое значение (в данном случае 1), а в случае ошибки должна вернуть 0.\n",
      "\n",
      " Проверка \r\n",
      "Мы описали преобразование типа std::wstring в str и обратно, теперь можем убедиться, что пакет успешно собирается и обвязка работает, как надо. Для сборки вызываем sip-wheel, затем устанавливаем пакет с помощью pip и проверяем работоспособность в командном режиме Python:\n",
      "\n",
      ">>> from foocpp import Foo\n",
      ">>> x = Foo(10, 'Hello')\n",
      "\n",
      ">>> x.string_val\n",
      "'Hello'\n",
      "\n",
      ">>> x.string_val = 'Привет'\n",
      ">>> x.string_val\n",
      "'Привет'\n",
      "\n",
      ">>> x.get_string_val()\n",
      "'Привет'\n",
      "\r\n",
      "Как видим, все работает, с русским языком тоже проблем нет, т.е. преобразования юникодных строк выполнено корректно.\n",
      "\n",
      "Заключение\r\n",
      "В этой статье мы рассмотрели основы использования SIP для создания Python-обвязок для библиотек, написанных на C и C++. Сначала (в первой части) мы создали простую библиотеку на языке C и разобрались с файлами, которые необходимо создать для работы с SIP. В файле pyproject.toml содержится информация о пакете (название, номер версии, лицензия и пути до заголовочных и объектных файлов). С помощью файла project.py можно влиять на процесс сборки пакета Python, например, запускать сборку C/C++-библиотеки или дать возможность пользователю указывать расположение заголовочных и объектных файлов библиотеки.\n",
      "\r\n",
      "В файле *.sip описывается интерфейс Python-модуля с перечислением функций и классов, которые будут содержаться в модуле. Для описания интерфейса в файле *.sip используются директивы и аннотации. Интерфейс классов Python не обязательно должен совпадать с интерфейсом классов C++. Например, в классы можно добавлять свойства с помощью директивы %Property, переименовывать сущности с помощью аннотации /PyName/, добавлять строки документации с помощью директивы %Docstring.\n",
      "\r\n",
      "Элементарные типы вроде int, char, char* и т.п. SIP автоматически преобразует в аналогичные классы Python, но если нужно выполнять более сложное преобразование, то его нужно запрограммировать самостоятельно внутри директивы %MappedType, используя Python/C API. Преобразование из класса Python в C++ должно осуществляться во вложенной директиве %ConvertToTypeCode. Преобразование из типа C++ в класс Python должно осуществляться во вложенной директиве %ConvertFromTypeCode.\n",
      "\r\n",
      "Некоторые директивы вроде %DefaultEncoding, %DefaultDocstringFormat и %DefaultDocstringSignature являются вспомогательными и позволяют устанавливать значения по умолчанию для случаев, когда какие-то параметры аннотаций не установлены явно.\n",
      "\r\n",
      "В этой статье мы рассмотрели только лишь основные и самые простые директивы и аннотации, но многие из них обошли вниманием. Например, существуют директивы для управления GIL, для создания новых Python-исключений, для ручного управления памятью и сборщиком мусора, для подстройки классов под разные операционные системы и многие другие, которые могут быть полезны при создании обвязок сложных C/C++-библиотек. Также мы обошли вопрос сборки пакетов под разные операционные системы, ограничившись сборкой под Linux с помощью компиляторов gcc/g++. \n",
      "\n",
      "Ссылки\n",
      "\n",
      "Первая часть статьи\n",
      "Примеры для данной статьи\n",
      "Домашняя страница SIP\n",
      "Документация для SIP\n",
      "Документация для Python/C API\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет, меня зовут Александр Васин, я бэкенд-разработчик в Едадиле. Идея этого материала началась с того, что я хотел разобрать вступительное задание (Я.Диск) в Школу бэкенд-разработки Яндекса. Я начал описывать все тонкости выбора тех или иных технологий, методику тестирования… Получался совсем не разбор, а очень подробный гайд по тому, как писать бэкенды на Python. От первоначальной идеи остались только требования к сервису, на примере которых удобно разбирать инструменты и технологии. В итоге я очнулся на сотне тысяч символов. Ровно столько потребовалось, чтобы рассмотреть всё в мельчайших подробностях. Итак, программа на следующие 100 килобайт: как строить бэкенд сервиса, начиная от выбора инструментов и заканчивая деплоем.\n",
      "\n",
      "\n",
      "\n",
      "TL;DR: Вот репка на GitHub с приложением, а кто любит (настоящие) лонгриды — прошу под кат.\n",
      "\r\n",
      "Мы разработаем и протестируем REST API-сервис на Python, упакуем его в легкий Docker-контейнер и развернем с помощью Ansible.\n",
      "\n",
      "Реализовать REST API-сервис можно по-разному, с помощью разных инструментов. Описанное решение не единственно верное, реализацию и инструменты я выбирал исходя из своего личного опыта и предпочтений.\n",
      "\n",
      "Что будем делать?\n",
      "Какие инструменты выбрать?\n",
      " Разработка\n",
      "\n",
      "Почему нужно начать с setup.py?\n",
      "Как указать версии зависимостей?\n",
      " База данных\n",
      "\n",
      "Проектируем схему\n",
      "Описываем схему в SQLAlchemy\n",
      "Настраиваем Alembic\n",
      "Генерируем миграции\n",
      "\n",
      " Приложение\n",
      "Сериализация данных\n",
      "Обработчики\n",
      "POST /imports\n",
      "GET /imports/$import_id/citizens\n",
      "PATCH /imports/$import_id/citizens/$citizen_id\n",
      "GET /imports/$import_id/citizens/birthdays\n",
      "GET /imports/$import_id/towns/stat/percentile/age\n",
      "\n",
      "\n",
      "\n",
      "Тестирование\n",
      "Обработчики\n",
      "GET /imports/$import_id/citizens\n",
      "POST /imports\n",
      "PATCH /imports/$import_id/citizens/$citizen_id\n",
      "GET /imports/$import_id/citizens/birthdays\n",
      "GET /imports/$import_id/towns/stat/percentile/age\n",
      "\n",
      "Миграции\n",
      "\n",
      "Сборка\n",
      "CI\n",
      "Деплой\n",
      "Нагрузочное тестирование\n",
      "Что еще можно сделать?\n",
      "В заключение\n",
      "\n",
      "Что будем делать?\r\n",
      "Представим, что интернет-магазин подарков планирует запустить акцию в разных регионах. Чтобы стратегия продаж была эффективной, необходим анализ рынка. У магазина есть поставщик, регулярно присылающий (например, на почту) выгрузки данных с информацией о жителях.\n",
      "\r\n",
      "Давайте разработаем REST API-сервис на Python, который будет анализировать предоставленные данные и выявлять спрос на подарки у жителей разных возрастных групп в разных городах по месяцам.\n",
      "\r\n",
      "В сервисе реализуем следующие обработчики:\n",
      "\n",
      "\n",
      " POST /imports\r\n",
      "Добавляет новую выгрузку с данными;\n",
      "\n",
      " GET /imports/$import_id/citizens\r\n",
      "Возвращает жителей указанной выгрузки;\n",
      "\n",
      " PATCH /imports/$import_id/citizens/$citizen_id\r\n",
      "Изменяет информацию о жителе (и его родственниках) в указанной выгрузке;\n",
      "\n",
      " GET /imports/$import_id/citizens/birthdays\r\n",
      "Вычисляет число подарков, которое приобретет каждый житель выгрузки своим родственникам (первого порядка), сгруппированное по месяцам;\n",
      "\n",
      " GET /imports/$import_id/towns/stat/percentile/age\r\n",
      "Вычисляет 50-й, 75-й и 99-й перцентили возрастов (полных лет) жителей по городам в указанной выборке.\n",
      "\n",
      "\n",
      "Какие инструменты выбрать?\r\n",
      "Итак, пишем сервис на Python, используя знакомые фреймворки, библиотеки и СУБД.\n",
      "\r\n",
      "В 4 лекции видеокурса рассказывается о различных СУБД и их особенностях. Для моей реализации я выбрал СУБД PostgreSQL, зарекомендовавшую себя как надежное решение c отличной документацией на русском языке, сильным русским сообществом (всегда можно найти ответ на вопрос на русском языке) и даже бесплатными курсами. Реляционная модель достаточно универсальна и хорошо понятна многим разработчикам. Хотя то же самое можно было сделать на любой NoSQL СУБД, в этой статье будем рассматривать именно PostgreSQL.\n",
      "\r\n",
      "Основная задача сервиса — передача данных по сети между БД и клиентами — не предполагает большой нагрузки на процессор, но требует возможности обрабатывать несколько запросов в один момент времени. В 10 лекции рассматривается асинхронный подход. Он позволяет эффективно обслуживать нескольких клиентов в рамках одного процесса ОС (в отличие, например, от используемой во Flask/Django pre-fork-модели, которая создает несколько процессов для обработки запросов от пользователей, каждый из них потребляет память, но простаивает большую часть времени). Поэтому в качестве библиотеки для написания сервиса я выбрал асинхронный aiohttp.\n",
      "\n",
      "\n",
      "\r\n",
      "В 5 лекции видеокурса рассказывается, что SQLAlchemy позволяет декомпозировать сложные запросы на части, переиспользовать их, генерировать запросы с динамическим набором полей (например, PATCH-обработчик позволяет частичное обновление жителя с произвольными полями) и сосредоточиться непосредственно на бизнес-логике. С выполнением этих запросов и передачей данных быстрее всех справится драйвер asyncpg, а подружить их поможет asyncpgsa.\n",
      "\r\n",
      "Мой любимый инструмент для управления состоянием БД и работы с миграциями — Alembic. Кстати, я недавно рассказывал о нем на Moscow Python.\n",
      "\r\n",
      "Логику валидации получилось лаконично описать схемами Marshmallow (включая проверки на родственные связи). С помощью модуля aiohttp-spec я связал aiohttp-обработчики и схемы для валидации данных, а бонусом получилось сгенерировать документацию в формате Swagger и отобразить ее в графическом интерфейсе.\n",
      "\r\n",
      "Для написания тестов я выбрал pytest, подробнее о нем — в 3 лекции.\n",
      "\r\n",
      "Для отладки и профилирования этого проекта я использовал отладчик PyCharm (лекция 9).\n",
      "\r\n",
      "В 7 лекции рассказывается, как на любом компьютере с Docker (и даже на разных ОС) можно запускать упакованное приложение без необходимости настраивать окружение для запуска и легко устанавливать/обновлять/удалять приложение на сервере.\n",
      "\r\n",
      "Для деплоя я выбрал Ansible. Он позволяет декларативно описывать желаемое состояние сервера и его сервисов, работает по ssh и не требует специального софта.\n",
      "\n",
      "Разработка\r\n",
      "Я решил дать Python-пакету название analyzer и использовать следующую структуру:\n",
      "\n",
      "\n",
      "\r\n",
      "В файле analyzer/__init__.py я разместил общую информацию о пакете: описание (docstring), версию, лицензию, контакты разработчиков.\n",
      "\n",
      "\n",
      "Ее можно посмотреть встроенной командой help\n",
      "$ python\n",
      ">>> import analyzer\n",
      ">>> help(analyzer)\n",
      "\n",
      "Help on package analyzer:\n",
      "\n",
      "NAME\n",
      "    analyzer\n",
      "\n",
      "DESCRIPTION\n",
      "    Сервис с REST API, анализирующий рынок для промоакций.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api (package)\n",
      "    db (package)\n",
      "    utils (package)\n",
      "\n",
      "DATA\n",
      "    __all__ = ('__author__', '__email__', '__license__', '__maintainer__',...\n",
      "    __email__ = 'alvassin@yandex.ru'\n",
      "    __license__ = 'MIT'\n",
      "    __maintainer__ = 'Alexander Vasin'\n",
      "\n",
      "VERSION\n",
      "    0.0.1\n",
      "\n",
      "AUTHOR\n",
      "    Alexander Vasin\n",
      "\n",
      "FILE\n",
      "    /Users/alvassin/Work/backendschool2019/analyzer/__init__.py\n",
      "\r\n",
      "Пакет имеет две входных точки — REST API-сервис (analyzer/api/__main__.py) и утилита управления состоянием БД (analyzer/db/__main__.py). Файлы называются __main__.py неспроста — во-первых, такое название привлекает внимание, по нему понятно, что файл является входной точкой. \n",
      "\r\n",
      "Во-вторых, благодаря этому подходу к входным точкам можно обращаться с помощью команды python -m:\n",
      "\n",
      "# REST API\n",
      "$ python -m analyzer.api --help\n",
      "\n",
      "# Утилита управления состоянием БД\n",
      "$ python -m analyzer.db --help\n",
      "Почему нужно начать с setup.py?\r\n",
      "Забегая вперед, подумаем, как можно распространять приложение: оно может быть упаковано в zip- (а также wheel/egg-) архив, rpm-пакет, pkg-файл для macOS и установлено на удаленный компьютер, в виртуальную машину, MacBook или Docker-контейнер.\n",
      "\r\n",
      "Главная цель файла setup.py — описать пакет с приложением для distutils/setuptools. \n",
      "\r\n",
      "В файле необходимо указать общую информацию о пакете (название, версию, автора и т. д.), но также в нем можно указать требуемые для работы модули, «экстра»-зависимости (например для тестирования), точки входа (например, исполняемые команды) и требования к интерпретатору.\n",
      "\r\n",
      "Плагины setuptools позволяют собирать из описанного пакета артефакт. Есть встроенные плагины: zip, egg, rpm, macOS pkg. Остальные плагины распространяются через PyPI: wheel, xar, pex. \n",
      "\r\n",
      "В сухом остатке, описав один файл, мы получаем огромные возможности. Именно поэтому разработку нового проекта нужно начинать с setup.py.\n",
      "\r\n",
      "В функции setup() зависимые модули указываются списком:\n",
      "\n",
      "setup(..., install_requires=[\"aiohttp\", \"SQLAlchemy\"])\r\n",
      "Но я описал зависимости в отдельных файлах requirements.txt и requirements.dev.txt, содержимое которых используется в setup.py. Мне это кажется более гибким, плюс тут есть секрет: впоследствии это позволит собирать Docker-образ быстрее. Зависимости будут ставиться отдельным шагом до установки самого приложения, а при пересборке Docker-контейнера попадать в кеш.\n",
      "\r\n",
      "Чтобы setup.py смог прочитать зависимости из файлов requirements.txt и requirements.dev.txt, написана функция:\n",
      "\n",
      "def load_requirements(fname: str) -> list:\n",
      "    requirements = []\n",
      "    with open(fname, 'r') as fp:\n",
      "        for req in parse_requirements(fp.read()):\n",
      "            extras = '[{}]'.format(','.join(req.extras)) if req.extras else ''\n",
      "            requirements.append(\n",
      "                '{}{}{}'.format(req.name, extras, req.specifier)\n",
      "            )\n",
      "    return requirements\r\n",
      "Стоит отметить, что setuptools при сборке source distribution по умолчанию включает в сборку только файлы .py, .c, .cpp и .h. Чтобы файлы с зависимостями requirements.txt и requirements.dev.txt попали в пакет, их необходимо явно указать в файле MANIFEST.in.\n",
      "\n",
      "\n",
      "setup.py целиком\n",
      "import os\n",
      "from importlib.machinery import SourceFileLoader\n",
      "\n",
      "from pkg_resources import parse_requirements\n",
      "from setuptools import find_packages, setup\n",
      "\n",
      "module_name = 'analyzer'\n",
      "\n",
      "# Возможно, модуль еще не установлен (или установлена другая версия), поэтому\n",
      "# необходимо загружать __init__.py с помощью machinery.\n",
      "module = SourceFileLoader(\n",
      "    module_name, os.path.join(module_name, '__init__.py')\n",
      ").load_module()\n",
      "\n",
      "def load_requirements(fname: str) -> list:\n",
      "    requirements = []\n",
      "    with open(fname, 'r') as fp:\n",
      "        for req in parse_requirements(fp.read()):\n",
      "            extras = '[{}]'.format(','.join(req.extras)) if req.extras else ''\n",
      "            requirements.append(\n",
      "                '{}{}{}'.format(req.name, extras, req.specifier)\n",
      "            )\n",
      "    return requirements\n",
      "\n",
      "setup(\n",
      "    name=module_name,\n",
      "    version=module.__version__,\n",
      "    author=module.__author__,\n",
      "    author_email=module.__email__,\n",
      "    license=module.__license__,\n",
      "    description=module.__doc__,\n",
      "    long_description=open('README.rst').read(),\n",
      "    url='https://github.com/alvassin/backendschool2019',\n",
      "    platforms='all',\n",
      "    classifiers=[\n",
      "        'Intended Audience :: Developers',\n",
      "        'Natural Language :: Russian',\n",
      "        'Operating System :: MacOS',\n",
      "        'Operating System :: POSIX',\n",
      "        'Programming Language :: Python',\n",
      "        'Programming Language :: Python :: 3',\n",
      "        'Programming Language :: Python :: 3.8',\n",
      "        'Programming Language :: Python :: Implementation :: CPython'\n",
      "    ],\n",
      "    python_requires='>=3.8',\n",
      "    packages=find_packages(exclude=['tests']),\n",
      "    install_requires=load_requirements('requirements.txt'),\n",
      "    extras_require={'dev': load_requirements('requirements.dev.txt')},\n",
      "    entry_points={\n",
      "        'console_scripts': [\n",
      "            # f-strings в setup.py не используются из-за соображений\n",
      "            # совместимости.\n",
      "            # Несмотря на то, что этот пакет требует Python 3.8, технически\n",
      "            # source distribution для него может собираться с помощью более\n",
      "            # ранних версий Python. Не стоит лишать пользователей этой\n",
      "            # возможности.\n",
      "            '{0}-api = {0}.api.__main__:main'.format(module_name),\n",
      "            '{0}-db = {0}.db.__main__:main'.format(module_name)\n",
      "        ]\n",
      "    },\n",
      "    include_package_data=True\n",
      ")\n",
      "\r\n",
      "Установить проект в режиме разработки можно следующей командой (в editable-режиме Python не установит пакет целиком в папку site-packages, а только создаст ссылки, поэтому любые изменения, вносимые в файлы пакета, будут видны сразу):\n",
      "\n",
      "# Установить пакет с обычными и extra-зависимостями \"dev\"\n",
      "pip install -e '.[dev]'\n",
      "\n",
      "# Установить пакет только с обычными зависимостями\n",
      "pip install -e .\n",
      "Как указать версии зависимостей?\r\n",
      "Здорово, когда разработчики активно занимаются своими пакетами — в них активнее исправляются ошибки, появляется новая функциональность и можно быстрее получить обратную связь. Но иногда изменения в зависимых библиотеках не имеют обратной совместимости и могут привести к ошибкам в вашем приложении, если не подумать об этом заранее.\n",
      "\r\n",
      "Для каждого зависимого пакета можно указать определенную версию, например aiohttp==3.6.2. Тогда приложение будет гарантированно собираться именно с теми версиями зависимых библиотек, с которыми оно было протестировано. Но у этого подхода есть и недостаток — если разработчики исправят критичный баг в зависимом пакете, не влияющий на обратную совместимость, в приложение это исправление не попадет.\n",
      "\r\n",
      "Существует подход к версионированию Semantic Versioning, который предлагает представлять версию в формате MAJOR.MINOR.PATCH:\n",
      "\n",
      "\n",
      "MAJOR — увеличивается при добавлении обратно несовместимых изменений;\n",
      "MINOR — увеличивается при добавлении новой функциональности с поддержкой обратной совместимости;\n",
      "PATCH — увеличивается при добавлении исправлений багов с поддержкой обратной совместимости.\n",
      "\r\n",
      "Если зависимый пакет следует этому подходу (о чем авторы обычно сообщают в файлах README или CHANGELOG), то достаточно зафиксировать значения MAJOR, MINOR и ограничить минимальное значение для PATCH-версии: >= MAJOR.MINOR.PATCH, == MAJOR.MINOR.*. \n",
      "\r\n",
      "Такое требование можно реализовать с помощью оператора ~=. Например, aiohttp~=3.6.2 позволит PIP установить для aiohttp версию 3.6.3, но не 3.7. \n",
      "\r\n",
      "Если указать интервал версий зависимостей, это даст еще одно преимущество — не будет конфликтов версий между зависимыми библиотеками. \n",
      "\r\n",
      "Если вы разрабатываете библиотеку, которая требует другой пакет-зависимость, то разрешите для него не одну определенную версию, а интервал. Тогда потребителям вашей библиотеки будет намного легче ее использовать (вдруг их приложение требует этот же пакет-зависимость, но уже другой версии).\n",
      "\r\n",
      "Semantic Versioning — лишь соглашение между авторами и потребителями пакетов. Оно не гарантирует, что авторы пишут код без багов и не могут допустить ошибку в новой версии своего пакета. \n",
      "\n",
      "База данных\n",
      "Проектируем схему\r\n",
      "В описании обработчика POST /imports приведен пример выгрузки с информацией о жителях:\n",
      "\n",
      "\n",
      "Пример выгрузки\n",
      "{\n",
      "  \"citizens\": [\n",
      "    {\n",
      "      \"citizen_id\": 1,\n",
      "      \"town\": \"Москва\",\n",
      "      \"street\": \"Льва Толстого\",\n",
      "      \"building\": \"16к7стр5\",\n",
      "      \"apartment\": 7,\n",
      "      \"name\": \"Иванов Иван Иванович\",\n",
      "      \"birth_date\": \"26.12.1986\",\n",
      "      \"gender\": \"male\",\n",
      "      \"relatives\": [2]\n",
      "    },\n",
      "    {\n",
      "      \"citizen_id\": 2,\n",
      "      \"town\": \"Москва\",\n",
      "      \"street\": \"Льва Толстого\",\n",
      "      \"building\": \"16к7стр5\",\n",
      "      \"apartment\": 7,\n",
      "      \"name\": \"Иванов Сергей Иванович\",\n",
      "      \"birth_date\": \"01.04.1997\",\n",
      "      \"gender\": \"male\",\n",
      "      \"relatives\": [1]\n",
      "    },\n",
      "    {\n",
      "      \"citizen_id\": 3,\n",
      "      \"town\": \"Керчь\",\n",
      "      \"street\": \"Иосифа Бродского\",\n",
      "      \"building\": \"2\",\n",
      "      \"apartment\": 11,\n",
      "      \"name\": \"Романова Мария Леонидовна\",\n",
      "      \"birth_date\": \"23.11.1986\",\n",
      "      \"gender\": \"female\",\n",
      "      \"relatives\": []\n",
      "    },\n",
      "    ...\n",
      "  ]\n",
      "}\n",
      "\r\n",
      "Первой мыслью было хранить всю информацию о жителе в одной таблице citizens, где родственные связи были бы представлены полем relatives в виде списка целых чисел.\n",
      "\n",
      "\n",
      "Но у этого способа есть ряд недостатков\n",
      "\n",
      " В обработчике GET /imports/$import_id/citizens/birthdays для получения месяцев, на которые приходятся дни рождения родственников, потребуется выполнить слияние таблицы citizens с самой собой. Для этого будет необходимо развернуть список с идентификаторами родственников relatives с помощью фунции UNNEST.\n",
      "\r\n",
      "Такой запрос будет выполняться сравнительно медленно, и обработчик не уложится в 10-секундный таймаут:\n",
      "SELECT \n",
      "    relations.citizen_id, \n",
      "    relations.relative_id, \n",
      "    date_part('month', relatives.birth_date) as relative_birth_month\n",
      "FROM (\n",
      "\tSELECT\n",
      "        citizens.import_id, \n",
      "        citizens.citizen_id,\n",
      "        UNNEST(citizens.relatives) as relative_id\n",
      "\tFROM citizens\n",
      "    WHERE import_id = 1\n",
      ") as relations\n",
      "INNER JOIN citizens as relatives ON\n",
      "    relations.import_id = relatives.import_id AND\n",
      "    relations.relative_id = relatives.citizen_id\n",
      "\n",
      "\n",
      " В таком подходе целостность данных в поле relatives не обеспечивается PostgreSQL, а контролируется приложением: технически в список relatives можно добавить любое целое число, в том числе идентификатор несуществующего жителя. Ошибка в коде или человеческий фактор (редактирование записей напрямую в БД администратором) обязательно рано или поздно приведут к несогласованному состоянию данных.\n",
      "\n",
      "\r\n",
      "Далее, я решил привести все требуемые для работы данные к третьей нормальной форме, и получилась следующая структура:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Таблица imports состоит из автоматически инкрементируемого столбца import_id. Он нужен для создания проверки по внешнему ключу в таблице citizens.\n",
      "\n",
      " В таблице citizens хранятся скалярные данные о жителе (все поля за исключением информации о родственных связях).\n",
      "\r\n",
      "В качестве первичного ключа используется пара (import_id, citizen_id), гарантирующая уникальность жителей citizen_id в рамках import_id. \n",
      "\r\n",
      "Внешний ключ citizens.import_id -> imports.import_id гарантирует, что поле citizens.import_id будет содержать только существующие выгрузки.\n",
      "\n",
      " Таблица relations содержит информацию о родственных связях. \n",
      "\n",
      "Одна родственная связь представлена двумя записями (от жителя к родственнику и обратно): эта избыточность позволяет использовать более простое условие при слиянии таблиц citizens и relations и получать информацию более эффективно.\r\n",
      "Первичный ключ состоит из столбцов (import_id, citizen_id, relative_id) и гарантирует, что в рамках одной выгрузки import_id у жителя citizen_id будут родственники c уникальными relative_id. \n",
      "\r\n",
      "Также в таблице используются два составных внешних ключа: (relations.import_id, relations.citizen_id) -> (citizens.import_id, citizens.citizen_id) и (relations.import_id, relations.relative_id) -> (citizens.import_id, citizens.citizen_id), гарантирующие, что в таблице будут указаны существующие житель citizen_id и родственник relative_id из одной выгрузки.\n",
      "\r\n",
      "Такая структура обеспечивает целостность данных средствами PostgreSQL, позволяет эффективно получать жителей с родственниками из базы данных, но подвержена состоянию гонки во время обновления информации о жителях конкурентными запросами (подробнее рассмотрим при реализации обработчика PATCH).\n",
      "\n",
      "Описываем схему в SQLAlchemy\r\n",
      "В лекции 5 я рассказывал, что для создания запросов с помощью SQLAlchemy необходимо описать схему базы данных с помощью специальных объектов: таблицы описываются с помощью sqlalchemy.Table и привязываются к реестру sqlalchemy.MetaData, который хранит всю метаинформацию о базе данных. К слову, реестр MetaData способен не только хранить описанную в Python метаинформацию, но и представлять реальное состояние базы данных в виде объектов SQLAlchemy.\n",
      "\r\n",
      "Эта возможность в том числе позволяет Alembic сравнивать состояния и генерировать код миграций автоматически.\n",
      "\r\n",
      "Кстати, у каждой базы данных своя схема именования constraints по умолчанию. Чтобы вы не тратили время на именование новых constraints или на воспоминания/поиски того, как назван constraint, который вы собираетесь удалить, SQLAlchemy предлагает использовать шаблоны именования naming conventions. Их можно определить в реестре MetaData.\n",
      "\n",
      "\n",
      "Создаем реестр MetaData и передаем в него шаблоны именования\n",
      "# analyzer/db/schema.py\n",
      "from sqlalchemy import MetaData\n",
      "\n",
      "convention = {\n",
      "    'all_column_names': lambda constraint, table: '_'.join([\n",
      "        column.name for column in constraint.columns.values()\n",
      "    ]),\n",
      "\n",
      "    # Именование индексов\n",
      "    'ix': 'ix__%(table_name)s__%(all_column_names)s',\n",
      "\n",
      "    # Именование уникальных индексов\n",
      "    'uq': 'uq__%(table_name)s__%(all_column_names)s',\n",
      "\n",
      "    # Именование CHECK-constraint-ов\n",
      "    'ck': 'ck__%(table_name)s__%(constraint_name)s',\n",
      "\n",
      "    # Именование внешних ключей\n",
      "    'fk': 'fk__%(table_name)s__%(all_column_names)s__%(referred_table_name)s',\n",
      "\n",
      "    # Именование первичных ключей\n",
      "    'pk': 'pk__%(table_name)s'\n",
      "}\n",
      "metadata = MetaData(naming_convention=convention)\n",
      "\r\n",
      "Если указать шаблоны именования, Alembic воспользуется ими во время автоматической генерации миграций и будет называть все constraints в соответствии с ними. В дальнейшем cозданный реестр MetaData потребуется для описания таблиц:\n",
      "\n",
      "\n",
      "Описываем схему базы данных объектами SQLAlchemy\n",
      "# analyzer/db/schema.py\n",
      "from enum import Enum, unique\n",
      "\n",
      "from sqlalchemy import (\n",
      "    Column, Date, Enum as PgEnum, ForeignKey, ForeignKeyConstraint, Integer,\n",
      "    String, Table\n",
      ")\n",
      "\n",
      "\n",
      "@unique\n",
      "class Gender(Enum):\n",
      "    female = 'female'\n",
      "    male = 'male'\n",
      "\n",
      "\n",
      "imports_table = Table(\n",
      "    'imports',\n",
      "    metadata,\n",
      "    Column('import_id', Integer, primary_key=True)\n",
      ")\n",
      "\n",
      "citizens_table = Table(\n",
      "    'citizens',\n",
      "    metadata,\n",
      "    Column('import_id', Integer, ForeignKey('imports.import_id'),\n",
      "           primary_key=True),\n",
      "    Column('citizen_id', Integer, primary_key=True),\n",
      "    Column('town', String, nullable=False, index=True),\n",
      "    Column('street', String, nullable=False),\n",
      "    Column('building', String, nullable=False),\n",
      "    Column('apartment', Integer, nullable=False),\n",
      "    Column('name', String, nullable=False),\n",
      "    Column('birth_date', Date, nullable=False),\n",
      "    Column('gender', PgEnum(Gender, name='gender'), nullable=False),\n",
      ")\n",
      "\n",
      "relations_table = Table(\n",
      "    'relations',\n",
      "    metadata,\n",
      "    Column('import_id', Integer, primary_key=True),\n",
      "    Column('citizen_id', Integer, primary_key=True),\n",
      "    Column('relative_id', Integer, primary_key=True),\n",
      "    ForeignKeyConstraint(\n",
      "        ('import_id', 'citizen_id'),\n",
      "        ('citizens.import_id', 'citizens.citizen_id')\n",
      "    ),\n",
      "    ForeignKeyConstraint(\n",
      "        ('import_id', 'relative_id'),\n",
      "        ('citizens.import_id', 'citizens.citizen_id')\n",
      "    ),\n",
      ")\n",
      "\n",
      "\n",
      "Настраиваем Alembic\r\n",
      "Когда схема базы данных описана, необходимо сгенерировать миграции, но для этого сначала нужно настроить Alembic, об этом тоже рассказывается в лекции 5.\n",
      "\r\n",
      "Чтобы воспользоваться командой alembic, необходимо выполнить следующие шаги:\n",
      "\n",
      "\n",
      "Установить пакет: pip install alembic\n",
      " Инициализировать Alembic: cd analyzer && alembic init db/alembic.\n",
      "\r\n",
      "Эта команда создаст файл конфигурации analyzer/alembic.ini и папку analyzer/db/alembic со следующим содержимым:\n",
      "\n",
      " env.py — вызывается каждый раз при запуске Alembic. Подключает в Alembic реестр sqlalchemy.MetaData с описанием желаемого состояния БД и содержит инструкции по запуску миграций.\n",
      "\n",
      "script.py.mako — шаблон, на основе которого генерируются миграции.\n",
      "versions — папка, в которой Alembic будет искать (и генерировать) миграции.\n",
      "\n",
      " Указать адрес базы данных в файле alembic.ini:\n",
      "\n",
      "; analyzer/alembic.ini\n",
      "[alembic] \n",
      "sqlalchemy.url = postgresql://user:hackme@localhost/analyzer\n",
      " Указать описание желаемого состояния базы данных (реестр sqlalchemy.MetaData), чтобы Alembic мог генерировать миграции автоматически:\n",
      "\n",
      "# analyzer/db/alembic/env.py\n",
      "from analyzer.db import schema\n",
      "target_metadata = schema.metadata\n",
      "\r\n",
      "Alembic настроен и им уже можно пользоваться, но в нашем случае такая конфигурация имеет ряд недостатков:\n",
      "\n",
      "\n",
      "Утилита alembic ищет alembic.ini в текущей рабочей директории. Путь к alembic.ini можно указать аргументом командной строки, но это неудобно: хочется иметь возможность вызывать команду из любой папки без дополнительных параметров.\n",
      "Чтобы настроить Alembic на работу с определенной базой данных, требуется менять файл alembic.ini. Гораздо удобнее было бы указать настройки БД переменной окружения и/или аргументом командной строки, например --pg-url.\n",
      "Название утилиты alembic не очень хорошо коррелирует с названием нашего сервиса (а пользователь фактически может вообще не владеть Python и ничего не знать об Alembic). Конечному пользователю было бы намного удобнее, если бы все исполняемые команды сервиса имели общий префикс, например analyzer-*.\n",
      "\r\n",
      "Эти проблемы решаются с помощью небольшой обертки analyzer/db/__main__.py:\n",
      "\n",
      "\n",
      " Для обработки аргументов командной строки Alembic использует стандартный модуль argparse. Он позволяет добавить необязательный аргумент --pg-url со значением по умолчанию из переменной окружения ANALYZER_PG_URL.\n",
      "\n",
      "\n",
      "Код\n",
      "import os\n",
      "from alembic.config import CommandLine, Config\n",
      "from analyzer.utils.pg import DEFAULT_PG_URL\n",
      "\n",
      "\n",
      "def main():\n",
      "    alembic = CommandLine()\n",
      "    alembic.parser.add_argument(\n",
      "        '--pg-url', default=os.getenv('ANALYZER_PG_URL', DEFAULT_PG_URL),\n",
      "        help='Database URL [env var: ANALYZER_PG_URL]'\n",
      "    )\n",
      "    options = alembic.parser.parse_args()\n",
      "\n",
      "    # Создаем объект конфигурации Alembic\n",
      "    config = Config(file_=options.config, ini_section=options.name,\n",
      "                    cmd_opts=options)\n",
      "\n",
      "    # Меняем значение sqlalchemy.url из конфига Alembic\n",
      "    config.set_main_option('sqlalchemy.url', options.pg_url)\n",
      "\n",
      "    # Запускаем команду alembic\n",
      "    exit(alembic.run_cmd(config, options))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "Путь до файла alembic.ini можно рассчитывать относительно расположения исполняемого файла, а не текущей рабочей директории пользователя.\n",
      "\n",
      "\n",
      "Код\n",
      "import os\n",
      "from alembic.config import CommandLine, Config\n",
      "from pathlib import Path\n",
      "\n",
      "\n",
      "PROJECT_PATH = Path(__file__).parent.parent.resolve()\n",
      "\n",
      "\n",
      "def main():\n",
      "    alembic = CommandLine()\n",
      "    options = alembic.parser.parse_args()\n",
      "\n",
      "    # Если указан относительный путь (alembic.ini), добавляем в начало\n",
      "    # абсолютный путь до приложения\n",
      "    if not os.path.isabs(options.config):\n",
      "        options.config = os.path.join(PROJECT_PATH, options.config)\n",
      "\n",
      "    # Создаем объект конфигурации Alembic\n",
      "    config = Config(file_=options.config, ini_section=options.name,\n",
      "                    cmd_opts=options)\n",
      "\n",
      "    # Подменяем путь до папки с alembic на абсолютный (требуется, чтобы alembic\n",
      "    # мог найти env.py, шаблон для генерации миграций и сами миграции)\n",
      "    alembic_location = config.get_main_option('script_location')\n",
      "    if not os.path.isabs(alembic_location):\n",
      "        config.set_main_option('script_location',\n",
      "                               os.path.join(PROJECT_PATH, alembic_location))\n",
      "\n",
      "    # Запускаем команду alembic\n",
      "    exit(alembic.run_cmd(config, options))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "\r\n",
      "Когда утилита для управления состоянием БД готова, ее можно зарегистрировать в setup.py как исполняемую команду с понятным конечному пользователю названием, например analyzer-db:\n",
      "\n",
      "\n",
      "Регистрация исполняемой команды в setup.py\n",
      "from setuptools import setup\n",
      "\n",
      "setup(..., entry_points={\n",
      "    'console_scripts': [\n",
      "        'analyzer-db = analyzer.db.__main__:main'\n",
      "    ]\n",
      "})\n",
      "\n",
      "\r\n",
      "После переустановки модуля будет сгенерирован файл env/bin/analyzer-db и команда analyzer-db станет доступной:\n",
      "\n",
      "$ pip install -e '.[dev]'\n",
      "Генерируем миграции\r\n",
      "Чтобы сгенерировать миграции, требуется два состояния: желаемое (которое мы описали объектами SQLAlchemy) и реальное (база данных, в нашем случае пустая).\n",
      "\r\n",
      "Я решил, что проще всего поднять Postgres с помощью Docker и для удобства добавил команду make postgres, запускающую в фоновом режиме контейнер с PostgreSQL на 5432 порту:\n",
      "\n",
      "\n",
      "Поднимаем PostgreSQL и генерируем миграцию\n",
      "$ make postgres\n",
      "...\n",
      "$ analyzer-db revision --message=\"Initial\" --autogenerate\n",
      "INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume transactional DDL.\n",
      "INFO  [alembic.autogenerate.compare] Detected added table 'imports'\n",
      "INFO  [alembic.autogenerate.compare] Detected added table 'citizens'\n",
      "INFO  [alembic.autogenerate.compare] Detected added index 'ix__citizens__town' on '['town']'\n",
      "INFO  [alembic.autogenerate.compare] Detected added table 'relations'\n",
      "  Generating /Users/alvassin/Work/backendschool2019/analyzer/db/alembic/versions/d5f704ed4610_initial.py ...  done\n",
      "\r\n",
      "Alembic в целом хорошо справляется с рутинной работой генерации миграций, но я хотел бы обратить внимание на следующее:\n",
      "\n",
      "\n",
      "Пользовательские типы данных, указанные в создаваемых таблицах, создаются автоматически (в нашем случае — gender), но код для их удаления в downgrade не генерируется. Если применить, откатить и потом еще раз применить миграцию, это вызовет ошибку, так как указанный тип данных уже существует.\n",
      "\n",
      "\n",
      "Удаляем тип данных gender в методе downgrade\n",
      "from alembic import op\n",
      "from sqlalchemy import Column, Enum\n",
      "\n",
      "GenderType = Enum('female', 'male', name='gender')\n",
      "\n",
      "\n",
      "def upgrade():\n",
      "    ...\n",
      "    # При создании таблицы тип данных GenderType будет создан автоматически\n",
      "    op.create_table('citizens', ...,\n",
      "                    Column('gender', GenderType, nullable=False))\n",
      "    ...\n",
      "\n",
      "\n",
      "def downgrade():\n",
      "    op.drop_table('citizens')\n",
      "\n",
      "    # После удаления таблицы тип данных необходимо удалить\n",
      "    GenderType.drop(op.get_bind())\n",
      "\n",
      "В методе downgrade некоторые действия иногда можно убрать (если мы удаляем таблицу целиком, можно не удалять ее индексы отдельно):\n",
      "\n",
      "\n",
      "Например\n",
      "def downgrade():\n",
      "op.drop_table('relations')\n",
      "\n",
      "# Следующим шагом мы удаляем таблицу citizens, индекс будет удален автоматически\n",
      "# эту строчку можно удалить\n",
      "op.drop_index(op.f('ix__citizens__town'), table_name='citizens')\n",
      "op.drop_table('citizens')\n",
      "op.drop_table('imports')\n",
      "\n",
      "\r\n",
      "Когда миграция исправлена и готова, применим ее:\n",
      "\n",
      "$ analyzer-db upgrade head\n",
      "INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> d5f704ed4610, Initial\n",
      "Приложение\r\n",
      "Прежде чем приступить к созданию обработчиков, необходимо сконфигурировать приложение aiohttp.\n",
      "\n",
      "\n",
      "Если посмотреть aiohttp quickstart, можно написать приблизительно такой код\n",
      "import logging\n",
      "\n",
      "from aiohttp import web\n",
      "\n",
      "\n",
      "def main():\n",
      "    # Настраиваем логирование\n",
      "    logging.basicConfig(level=logging.DEBUG)\n",
      "\n",
      "    # Создаем приложение\n",
      "    app = web.Application()\n",
      "\n",
      "    # Регистрируем обработчики\n",
      "    app.router.add_route(...)\n",
      "\n",
      "    # Запускаем приложение\n",
      "    web.run_app(app)\n",
      "\n",
      "\r\n",
      "Этот код вызывает ряд вопросов и имеет ряд недостатков:\n",
      "\n",
      "\n",
      " Как конфигурировать приложение? Как минимум, необходимо указать хост и порт для подключения клиентов, а также информацию для подключения к базе данных.\n",
      "\r\n",
      "Мне очень нравится решать эту задачу с помощью модуля ConfigArgParse: он расширяет стандартный argparse и позволяет использовать для конфигурации аргументы командной строки, переменные окружения (незаменимые для конфигурации Docker-контейнеров) и даже файлы конфигурации (а также совмещать эти способы). C помощью ConfigArgParse также можно валидировать значения параметров конфигурации приложения.\n",
      "\n",
      "\n",
      "Пример обработки параметров с помощью ConfigArgParse\n",
      "from aiohttp import web\n",
      "from configargparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
      "\n",
      "from analyzer.utils.argparse import positive_int\n",
      "\n",
      "parser = ArgumentParser(\n",
      "    # Парсер будет искать переменные окружения с префиксом ANALYZER_,\n",
      "    # например ANALYZER_API_ADDRESS и ANALYZER_API_PORT\n",
      "    auto_env_var_prefix='ANALYZER_',\n",
      "\n",
      "    # Покажет значения параметров по умолчанию\n",
      "    formatter_class=ArgumentDefaultsHelpFormatter\n",
      ")\n",
      "\n",
      "parser.add_argument('--api-address', default='0.0.0.0',\n",
      "                    help='IPv4/IPv6 address API server would listen on')\n",
      "\n",
      "# Разрешает только целые числа больше нуля\n",
      "parser.add_argument('--api-port', type=positive_int, default=8081,\n",
      "                    help='TCP port API server would listen on')\n",
      "\n",
      "\n",
      "def main():\n",
      "    # Получаем параметры конфигурации, которые можно передать как аргументами\n",
      "    # командной строки, так и переменными окружения\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    # Запускаем приложение на указанном порту и адресе\n",
      "    app = web.Application()\n",
      "    web.run_app(app, host=args.api_address, port=args.api_port)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\r\n",
      "Кстати, ConfigArgParse, как и argparse, умеет генерировать подсказку по запуску команды с описанием всех аргументов (необходимо позвать команду с аргументом -h или --help). Это невероятно облегчает жизнь пользователям вашего ПО:\n",
      "\n",
      "\n",
      "Например\n",
      "$ python __main__.py --help\n",
      "usage: __main__.py [-h] [--api-address API_ADDRESS] [--api-port API_PORT]\n",
      "\n",
      "If an arg is specified in more than one place, then commandline values override environment variables which override defaults.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --api-address API_ADDRESS\n",
      "                        IPv4/IPv6 address API server would listen on [env var: ANALYZER_API_ADDRESS] (default: 0.0.0.0)\n",
      "  --api-port API_PORT   TCP port API server would listen on [env var: ANALYZER_API_PORT] (default: 8081)\n",
      "\n",
      " После получения переменные окружения больше не нужны и даже могут представлять опасность — например, они могут случайно «утечь» с отображением информации об ошибке. Злоумышленники в первую очередь будут пытаться получить информацию об окружении, поэтому очистка переменных окружения считается хорошим тоном. \n",
      "\r\n",
      "Можно было бы воспользоваться os.environ.clear(), но Python позволяет управлять поведением модулей стандартной библиотеки с помощью многочисленных переменных окружения (например, вдруг потребуется включить режим отладки asyncio?), поэтому разумнее очищать переменные окружения по префиксу приложения, указанного в ConfigArgParser.\n",
      "\n",
      "\n",
      "Пример\n",
      "import os\n",
      "from typing import Callable\n",
      "from configargparse import ArgumentParser\n",
      "from yarl import URL\n",
      "\n",
      "from analyzer.api.app import create_app\n",
      "from analyzer.utils.pg import DEFAULT_PG_URL\n",
      "\n",
      "ENV_VAR_PREFIX = 'ANALYZER_'\n",
      "\n",
      "parser = ArgumentParser(auto_env_var_prefix=ENV_VAR_PREFIX)\n",
      "parser.add_argument('--pg-url', type=URL, default=URL(DEFAULT_PG_URL),\n",
      "                   help='URL to use to connect to the database')\n",
      "\n",
      "\n",
      "def clear_environ(rule: Callable):\n",
      "    \"\"\"\n",
      "    Очищает переменные окружения, переменные для очистки определяет переданная\n",
      "    функция rule\n",
      "    \"\"\"\n",
      "    # Ключи из os.environ копируются в новый tuple, чтобы не менять объект\n",
      "    # os.environ во время итерации\n",
      "    for name in filter(rule, tuple(os.environ)):\n",
      "        os.environ.pop(name)\n",
      "\n",
      "\n",
      "def main():\n",
      "    # Получаем аргументы\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    # Очищаем переменные окружения по префиксу ANALYZER_\n",
      "    clear_environ(lambda i: i.startswith(ENV_VAR_PREFIX))\n",
      "\n",
      "    # Запускаем приложение\n",
      "    app = create_app(args)\n",
      "    ...\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      " Запись логов в stderr/файл в основном потоке блокирует цикл событий.\n",
      "\r\n",
      "В лекции 9 рассказывается, что по умолчанию logging.basicConfig() настраивает запись логов в stderr.\n",
      "\r\n",
      "Чтобы логирование не мешало эффективной работе асинхронного приложения, необходимо выполнять запись логов в отдельном потоке. Для этого можно воспользоваться готовым методом из модуля aiomisc.\n",
      "\n",
      "\n",
      "Настраиваем логирование с помощью aiomisc\n",
      "import logging\n",
      "\n",
      "from aiomisc.log import basic_config\n",
      "\n",
      "basic_config(logging.DEBUG, buffered=True)    \n",
      "\n",
      "\n",
      " Как масштабировать приложение, если одного процесса станет недостаточно для обслуживания входящего трафика? Можно сначала аллоцировать сокет, затем с помощью fork создать несколько новых отдельных процессов, и соединения на сокете будут распределяться между ними механизмами ядра (конечно, под Windows это не работает).\n",
      "\n",
      "\n",
      "Пример\n",
      "import os\n",
      "from sys import argv\n",
      "\n",
      "import forklib\n",
      "from aiohttp.web import Application, run_app\n",
      "from aiomisc import bind_socket\n",
      "from setproctitle import setproctitle\n",
      "\n",
      "\n",
      "def main():\n",
      "    sock = bind_socket(address='0.0.0.0', port=8081, proto_name='http')\n",
      "    setproctitle(f'[Master] {os.path.basename(argv[0])}')\n",
      "\n",
      "    def worker():\n",
      "        setproctitle(f'[Worker] {os.path.basename(argv[0])}')\n",
      "        app = Application()\n",
      "        run_app(app, sock=sock)\n",
      "\n",
      "    forklib.fork(os.cpu_count(), worker, auto_restart=True)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "\n",
      " Требуется ли приложению обращаться или аллоцировать какие-либо ресурсы во время работы? Если нет, по соображениям безопасности все ресурсы (в нашем случае — сокет для подключения клиентов) можно аллоцировать на старте, а затем сменить пользователя на nobody. Он обладает ограниченным набором привиллегий — это здорово усложнит жизнь злоумышленникам.\n",
      "\n",
      "\n",
      "Пример\n",
      "import os\n",
      "import pwd\n",
      "\n",
      "from aiohttp.web import run_app\n",
      "from aiomisc import bind_socket\n",
      "\n",
      "from analyzer.api.app import create_app\n",
      "\n",
      "\n",
      "def main():\n",
      "    # Аллоцируем сокет\n",
      "    sock = bind_socket(address='0.0.0.0', port=8085, proto_name='http')\n",
      "\n",
      "    user = pwd.getpwnam('nobody')\n",
      "    os.setgid(user.pw_gid)\n",
      "    os.setuid(user.pw_uid)\n",
      "\n",
      "    app = create_app(...)\n",
      "    run_app(app, sock=sock)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "В конце концов я решил вынести создание приложения в отдельную параметризуемую функцию create_app, чтобы можно было легко создавать идентичные приложения для тестирования.\n",
      "\n",
      "Сериализация данных\r\n",
      "Все успешные ответы обработчиков будем возвращать в формате JSON. Информацию об ошибках клиентам тоже было бы удобно получать в сериализованном виде (например, чтобы увидеть, какие поля не прошли валидацию).\n",
      "\r\n",
      "Документация aiohttp предлагает метод json_response, который принимает объект, сериализует его в JSON и возвращает новый объект aiohttp.web.Response с заголовком Content-Type: application/json и сериализованными данными внутри.\n",
      "\n",
      "\n",
      "Как сериализовать данные с помощью json_response\n",
      "from aiohttp.web import Application, View, run_app\n",
      "from aiohttp.web_response import json_response\n",
      "\n",
      "\n",
      "class SomeView(View):\n",
      "    async def get(self):\n",
      "        return json_response({'hello': 'world'})\n",
      "\n",
      "\n",
      "app = Application()\n",
      "app.router.add_route('*', '/hello', SomeView)\n",
      "run_app(app)\n",
      "\n",
      "\r\n",
      "Но существует и другой способ: aiohttp позволяет зарегистрировать произвольный сериализатор для определенного типа данных ответа в реестре aiohttp.PAYLOAD_REGISTRY. Например, можно указать сериализатор aiohttp.JsonPayload для объектов типа Mapping. \n",
      "\r\n",
      "В этом случае обработчику будет достаточно вернуть объект Response с данными ответа в параметре body. aiohttp найдет сериализатор, соответствующий типу данных и сериализует ответ. \n",
      "\r\n",
      "Помимо того, что сериализация объектов описана в одном месте, этот подход еще и более гибкий — он позволяет реализовывать очень интересные решения (мы рассмотрим один из вариантов использования в обработчике GET /imports/$import_id/citizens). \n",
      "\n",
      "\n",
      "Как сериализовать данные с помощью aiohttp.PAYLOAD_REGISTRY\n",
      "from types import MappingProxyType\n",
      "from typing import Mapping\n",
      "\n",
      "from aiohttp import PAYLOAD_REGISTRY, JsonPayload\n",
      "from aiohttp.web import run_app, Application, Response, View\n",
      "\n",
      "PAYLOAD_REGISTRY.register(JsonPayload, (Mapping, MappingProxyType))\n",
      "\n",
      "\n",
      "class SomeView(View):\n",
      "    async def get(self):\n",
      "        return Response(body={'hello': 'world'})\n",
      "\n",
      "\n",
      "app = Application()\n",
      "app.router.add_route('*', '/hello', SomeView)\n",
      "run_app(app)\n",
      "\n",
      "\r\n",
      "Важно понимать, что метод json_response, как и aiohttp.JsonPayload, используют стандартный json.dumps, который не умеет сериализовать сложные типы данных, например datetime.date или asyncpg.Record (asyncpg возвращает записи из БД в виде экземпляров этого класса). Более того, одни сложные объекты могут содержать другие: в одной записи из БД может быть поле типа datetime.date.\n",
      "\r\n",
      "Разработчики Python предусмотрели эту проблему: метод json.dumps позволяет с помощью аргумента default указать функцию, которая вызывается, когда необходимо сериализовать незнакомый объект. Ожидается, что функция приведет незнакомый объект к типу, который умеет сериализовать модуль json.\n",
      "\n",
      "\n",
      "Как расширить JsonPayload для сериализации произвольных объектов\n",
      "import json\n",
      "from datetime import date\n",
      "from functools import partial, singledispatch\n",
      "from typing import Any\n",
      "\n",
      "from aiohttp.payload import JsonPayload as BaseJsonPayload\n",
      "from aiohttp.typedefs import JSONEncoder\n",
      "\n",
      "@singledispatch\n",
      "def convert(value):\n",
      "    raise NotImplementedError(f'Unserializable value: {value!r}')\n",
      "\n",
      "\n",
      "@convert.register(Record)\n",
      "def convert_asyncpg_record(value: Record):\n",
      "    \"\"\"\n",
      "    Позволяет автоматически сериализовать результаты запроса, возвращаемые\n",
      "    asyncpg\n",
      "    \"\"\"\n",
      "    return dict(value)\n",
      "\n",
      "\n",
      "@convert.register(date)\n",
      "def convert_date(value: date):\n",
      "    \"\"\"\n",
      "    В проекте объект date возвращается только в одном случае — если необходимо\n",
      "    отобразить дату рождения. Для отображения даты рождения должен\n",
      "    использоваться формат ДД.ММ.ГГГГ\n",
      "    \"\"\"\n",
      "    return value.strftime('%d.%m.%Y')\n",
      "    \n",
      " \n",
      "dumps = partial(json.dumps, default=convert)\n",
      "\n",
      "\n",
      "class JsonPayload(BaseJsonPayload):\n",
      "    def __init__(self,\n",
      "                 value: Any,\n",
      "                 encoding: str = 'utf-8',\n",
      "                 content_type: str = 'application/json',\n",
      "                 dumps: JSONEncoder = dumps,\n",
      "                 *args: Any,\n",
      "                 **kwargs: Any) -> None:\n",
      "        super().__init__(value, encoding, content_type, dumps, *args, **kwargs)\n",
      "\n",
      "Обработчики\r\n",
      "aiohttp позволяет реализовать обработчики асинхронными функциями и классами. Классы более расширяемы: во-первых, код, относящийся к одному обработчику, можно разместить в одном месте, а во вторых, классы позволяют использовать наследование для избавления от дублирования кода (например, каждому обработчику требуется соединение с базой данных).\n",
      "\n",
      "\n",
      "Базовый класс обработчика\n",
      "from aiohttp.web_urldispatcher import View\n",
      "from asyncpgsa import PG\n",
      "\n",
      "\n",
      "class BaseView(View):\n",
      "    URL_PATH: str\n",
      "\n",
      "    @property\n",
      "    def pg(self) -> PG:\n",
      "        return self.request.app['pg']\n",
      "\n",
      "\r\n",
      "Так как один большой файл читать сложно, я решил разнести обработчики по файлам. Маленькие файлы поощряют слабую связность, а если, например, есть кольцевые импорты внутри хэндлеров — значит, возможно, что-то не так с композицией сущностей.\n",
      "\n",
      "POST /imports\r\n",
      "На вход обработчик получает json с данными о жителях. Максимально допустимый размер запроса в aiohttp регулируется опцией client_max_size и по умолчанию равен 2 МБ. При превышении лимита aiohttp вернет HTTP-ответ со статусом 413: Request Entity Too Large Error. \n",
      "\r\n",
      "В то же время корректный json c максимально длинными строчками и цифрами будет весить ~63 мегабайта, поэтому ограничения на размер запроса необходимо расширить.\n",
      "\r\n",
      "Далее, необходимо проверить и десериализовать данные. Если они некорректные, нужно вернуть HTTP-ответ 400: Bad Request. \n",
      "\r\n",
      "Мне потребовались две схемы Marhsmallow. Первая, CitizenSchema, проверяет данные каждого отдельного жителя, а также десериализует строку с днем рождения в объект datetime.date:\n",
      "\n",
      "\n",
      "Тип данных, формат и наличие всех обязательных полей;\n",
      "Отсутствие незнакомых полей;\n",
      "Дата рождения должна быть указана в формате DD.MM.YYYY и не может иметь значения из будущего;\n",
      "Список родственников каждого жителя должен содержать уникальные существующие в этой выгрузке идентификаторы жителей.\n",
      "\r\n",
      "Вторая схема, ImportSchema, проверяет выгрузку в целом:\n",
      "\n",
      "\n",
      "citizen_id каждого жителя в рамках выгрузки должен быть уникален;\n",
      "Родственные связи должны быть двусторонними (если у жителя #1 в списке родственников указан житель #2, то и у жителя #2 должен быть родственник #1).\n",
      "\n",
      "Если данные корректные, их необходимо добавить в БД с новым уникальным import_id. \r\n",
      "Для добавления данных потребуется выполнить несколько запросов в разные таблицы. Чтобы в БД не осталось частично добавленных данных в случае возникновения ошибки или исключения (например, при отключении клиента, который не получил ответ полностью, aiohttp бросит исколючение CancelledError), необходимо использовать транзакцию.\n",
      "\n",
      "Добавлять данные в таблицы необходимо частями, так как в одном запросе к PostgreSQL может быть не более 32 767 аргументов. В таблице citizens 9 полей. Соответственно, за 1 запрос в эту таблицу можно вставить только 32 767 / 9 = 3640 строк, а в одной выгрузке может быть до 10 000 жителей.\n",
      "\n",
      "GET /imports/$import_id/citizens\r\n",
      "Обработчик возвращает всех жителей для выгрузки с указанным import_id. Если указанная выгрузка не существует, необходимо вернуть HTTP-ответ 404: Not Found. Это поведение выглядит общим для обработчиков, которым требуется существующая выгрузка, поэтому я вынес код проверки в отдельный класс.\n",
      "\n",
      "\n",
      "Базовый класс для обработчиков с выгрузками\n",
      "from aiohttp.web_exceptions import HTTPNotFound\n",
      "from sqlalchemy import select, exists\n",
      "\n",
      "from analyzer.db.schema import imports_table\n",
      "\n",
      "\n",
      "class BaseImportView(BaseView):\n",
      "    @property\n",
      "    def import_id(self):\n",
      "        return int(self.request.match_info.get('import_id'))\n",
      "\n",
      "    async def check_import_exists(self):\n",
      "        query = select([\n",
      "            exists().where(imports_table.c.import_id == self.import_id)\n",
      "        ])\n",
      "        if not await self.pg.fetchval(query):\n",
      "            raise HTTPNotFound()\n",
      "\n",
      "\r\n",
      "Чтобы получить список родственников для каждого жителя, потребуется выполнить LEFT JOIN из таблицы citizens в таблицу relations, агрегируя поле relations.relative_id с группировкой по import_id и citizen_id. \n",
      "\r\n",
      "Если у жителя нет родственников, то LEFT JOIN вернет для него в поле relations.relative_id значение NULL и в результате агрегации список родственников будет выглядеть как [NULL].\n",
      "\r\n",
      "Чтобы исправить это некорректное значение, я воспользовался функцией array_remove.\n",
      "\r\n",
      "БД хранит дату в формате YYYY-MM-DD, а нам нужен формат DD.MM.YYYY. \n",
      "\r\n",
      "Технически форматировать дату можно либо SQL-запросом, либо на стороне Python в момент сериализации ответа с json.dumps (asyncpg возвращает значение поля birth_date как экземпляр класса datetime.date). \n",
      "\r\n",
      "Я выбрал сериализацию на стороне Python, учитывая, что birth_date — единственный объект datetime.date в проекте с единым форматом (см. раздел «Сериализация данных»).\n",
      "\r\n",
      "Несмотря на то, что в обработчике выполняется два запроса (проверка на существование выгрузки и запрос на получение списка жителей), использовать транзакцию необязательно. По умолчанию PostgreSQL использует уровень изоляции READ COMMITTED и даже в рамках одной транзакции будут видны все изменения других, успешно завершенных транзакций (добавление новых строк, изменение существующих).\n",
      "\r\n",
      "Самая большая выгрузка в текстовом представлении может занимать ~63 мегабайта — это достаточно много, особенно учитывая, что одновременно может прийти несколько запросов на получение данных. Есть достаточно интересный способ получать данные из БД с помощью курсора и отправлять их клиенту по частям. \n",
      "\r\n",
      "Для этого нам потребуется реализовать два объекта: \n",
      "\n",
      "\n",
      " Объект SelectQuery типа AsyncIterable, возвращающий записи из базы данных. При первом обращении подключается к базе, открывает транзакцию и создает курсор, при дальнейшей итерации возвращает записи из БД. Возвращается обработчиком.\n",
      "\n",
      "\n",
      "Код SelectQuery\n",
      "from collections import AsyncIterable\n",
      "from asyncpgsa.transactionmanager import ConnectionTransactionContextManager\n",
      "from sqlalchemy.sql import Select\n",
      "\n",
      "\n",
      "class SelectQuery(AsyncIterable):\n",
      "    \"\"\"\n",
      "    Используется, чтобы отправлять данные из PostgreSQL клиенту сразу после\n",
      "    получения, по частям, без буфферизации всех данных\n",
      "    \"\"\"\n",
      "    PREFETCH = 500\n",
      "\n",
      "    __slots__ = (\n",
      "        'query', 'transaction_ctx', 'prefetch', 'timeout'\n",
      "    )\n",
      "\n",
      "    def __init__(self, query: Select,\n",
      "                 transaction_ctx: ConnectionTransactionContextManager,\n",
      "                 prefetch: int = None,\n",
      "                 timeout: float = None):\n",
      "        self.query = query\n",
      "        self.transaction_ctx = transaction_ctx\n",
      "        self.prefetch = prefetch or self.PREFETCH\n",
      "        self.timeout = timeout\n",
      "\n",
      "    async def __aiter__(self):\n",
      "        async with self.transaction_ctx as conn:\n",
      "            cursor = conn.cursor(self.query, prefetch=self.prefetch,\n",
      "                                 timeout=self.timeout)\n",
      "            async for row in cursor:\n",
      "                yield row\n",
      "\n",
      "\n",
      " Сериализатор AsyncGenJSONListPayload, который умеет итерироваться по асинхронным генераторам, сериализовать данные из асинхронного генератора в JSON и отправлять данные клиентам по частям. Регистрируется в aiohttp.PAYLOAD_REGISTRY как сериализатор объектов AsyncIterable.\n",
      "\n",
      "\n",
      "Код AsyncGenJSONListPayload\n",
      "import json\n",
      "from functools import partial\n",
      "\n",
      "from aiohttp import Payload\n",
      "\n",
      "\n",
      "# Функция, умеющая сериализовать в JSON объекты asyncpg.Record и datetime.date\n",
      "dumps = partial(json.dumps, default=convert, ensure_ascii=False)\n",
      "\n",
      "\n",
      "class AsyncGenJSONListPayload(Payload):\n",
      "    \"\"\"\n",
      "    Итерируется по объектам AsyncIterable, частями сериализует данные из них\n",
      "    в JSON и отправляет клиенту\n",
      "    \"\"\"\n",
      "    def __init__(self, value, encoding: str = 'utf-8',\n",
      "                 content_type: str = 'application/json',\n",
      "                 root_object: str = 'data',\n",
      "                 *args, **kwargs):\n",
      "        self.root_object = root_object\n",
      "        super().__init__(value, content_type=content_type, encoding=encoding,\n",
      "                         *args, **kwargs)\n",
      "\n",
      "    async def write(self, writer):\n",
      "        # Начало объекта\n",
      "        await writer.write(\n",
      "            ('{\"%s\":[' % self.root_object).encode(self._encoding)\n",
      "        )\n",
      "\n",
      "        first = True\n",
      "        async for row in self._value:\n",
      "            # Перед первой строчкой запятая не нужнаа\n",
      "            if not first:\n",
      "                await writer.write(b',')\n",
      "            else:\n",
      "                first = False\n",
      "\n",
      "            await writer.write(dumps(row).encode(self._encoding))\n",
      "\n",
      "        # Конец объекта\n",
      "        await writer.write(b']}')\n",
      "\n",
      "\r\n",
      "Далее, в обработчике можно будет создать объект SelectQuery, передать ему SQL запрос и функцию для открытия транзакции и вернуть его в Response body:\n",
      "\n",
      "\n",
      "Код обработчика\n",
      "# analyzer/api/handlers/citizens.py\n",
      "from aiohttp.web_response import Response\n",
      "from aiohttp_apispec import docs, response_schema\n",
      "\n",
      "from analyzer.api.schema import CitizensResponseSchema\n",
      "from analyzer.db.schema import citizens_table as citizens_t\n",
      "from analyzer.utils.pg import SelectQuery\n",
      "\n",
      "from .query import CITIZENS_QUERY\n",
      "from .base import BaseImportView\n",
      "\n",
      "\n",
      "class CitizensView(BaseImportView):\n",
      "    URL_PATH = r'/imports/{import_id:\\d+}/citizens'\n",
      "\n",
      "    @docs(summary='Отобразить жителей для указанной выгрузки')\n",
      "    @response_schema(CitizensResponseSchema())\n",
      "    async def get(self):\n",
      "        await self.check_import_exists()\n",
      "\n",
      "        query = CITIZENS_QUERY.where(\n",
      "            citizens_t.c.import_id == self.import_id\n",
      "        )\n",
      "        body = SelectQuery(query, self.pg.transaction())\n",
      "        return Response(body=body)\n",
      "\n",
      "\n",
      "aiohttp обнаружит в реестре aiohttp.PAYLOAD_REGISTRY зарегистрированный сериализатор AsyncGenJSONListPayload для объектов типа AsyncIterable. Затем сериализатор будет итерироваться по объекту SelectQuery и отправлять данные клиенту. При первом обращении объект SelectQuery получает соединение к БД, открывает транзакцию и создает курсор, при дальнейшей итерации будет получать данные из БД курсором и возвращать их построчно.\n",
      "\r\n",
      "Этот подход позволяет не выделять память на весь объем данных при каждом запросе, но у него есть особенность: приложение не сможет вернуть клиенту соответствующий HTTP-статус, если возникнет ошибка (ведь клиенту уже был отправлен HTTP-статус, заголовки, и пишутся данные). \n",
      "\r\n",
      "При возникновении исключения не остается ничего, кроме как разорвать соединение. Исключение, конечно, можно залогировать, но клиент не сможет понять, какая именно ошибка произошла. \n",
      "\r\n",
      "С другой стороны, похожая ситуация может возникнуть, даже если обработчик получит все данные из БД, но при передаче данных клиенту моргнет сеть — от этого никто не застрахован.\n",
      "\n",
      "PATCH /imports/$import_id/citizens/$citizen_id\r\n",
      "Обработчик получает на вход идентификатор выгрузки import_id, жителя citizen_id, а также json с новыми данными о жителе. В случае обращения к несуществующей выгрузке или жителю необходимо вернуть HTTP-ответ 404: Not Found.\n",
      "\r\n",
      "Переданные клиентом данные требуется проверить и десериализовать. Если они некорректные — необходимо вернуть HTTP-ответ 400: Bad Request. Я реализовал Marshmallow-схему PatchCitizenSchema, которая проверяет:\n",
      "\n",
      "\n",
      "Тип и формат данных для указанных полей.\n",
      "Дату рождения. Она должна быть указана в формате DD.MM.YYYY и не может иметь значения из будущего.\n",
      "Список родственников каждого жителя. Он должен иметь уникальные идентификаторы жителей\n",
      "\r\n",
      "Существование родственников, указанных в поле relatives, можно отдельно не проверять: при добавлении в таблицу relations несуществующего жителя PostgreSQL вернет ошибку ForeignKeyViolationError, которую можно обработать и вернуть HTTP-статус 400: Bad Request.\n",
      "\r\n",
      "Какой статус возвращать, если клиент прислал некорректные данные для несуществующего жителя или выгрузки? Семантически правильнее проверять сначала существование выгрузки и жителя (если такого нет — возвращать 404: Not Found) и только потом —корректные ли данные прислал клиент (если нет — возвращать 400: Bad Request). На практике часто бывает дешевле сначала проверить данные, и только если они корректные, обращаться к базе. \n",
      "\r\n",
      "Оба варианта приемлемы, но я решил выбрать более дешевый второй вариант, так как в любом случае результат операции — ошибка, которая ни на что не влияет (клиент исправит данные и потом так же узнает, что житель не существует). \n",
      "\r\n",
      "Если данные корректные, необходимо обновить информацию о жителе в БД. В обработчике потребуется сделать несколько запросов к разным таблицам. Если возникнет ошибка или исключение, изменения в базе данных должны быть отменены, поэтому запросы необходимо выполнять в транзакции.\n",
      "\r\n",
      "Метод PATCH позволяет передавать лишь некоторые поля для изменяемого жителя. \n",
      "\r\n",
      "Обработчик необходимо написать таким образом, чтобы он не падал при обращении к данным, которые не указал клиент, а также не выполнял запросы к таблицам, данные в которых не изменились.\n",
      "\r\n",
      "Если клиент указал поле relatives, необходимо получить список существующих родственников. Если он изменился — определить, какие записи из таблицы relatives необходимо удалить, а какие добавить, чтобы привести базу данных в соответствие с запросом клиента. По умолчанию в PostgreSQL для изоляции транзакций используется уровень READ COMMITTED. Это означает, что в рамках текущей транзакции будут видны изменения существующих (а также добавления новых) записей других завершенных транзакций. Это может привести к состоянию гонки между конкурентными запросами.\n",
      "\r\n",
      "Предположим, существует выгрузка с жителями #1, #2, #3, без родственных связей. Сервис получает два одновременных запроса на изменение жителя #1: {\"relatives\": [2]} и {\"relatives\": [3]}. aiohttp создаст два обработчика, которые одновременно получат текущее состояние жителя из PostgreSQL. \n",
      "\r\n",
      "Каждый обработчик не обнаружит ни одной родственной связи и примет решение добавить новую связь с указанным родственником. В результате у жителя #1 поле relatives равно [2,3].\n",
      "\n",
      "\n",
      "\r\n",
      "Такое поведение нельзя назвать очевидным. Есть два варианта ожидаемо решить исход гонки: выполнить только первый запрос, а для второго вернуть HTTP-ответ \n",
      "409: Conflict (чтобы клиент повторил запрос), либо выполнить запросы по очереди (второй запрос будет обработан только после завершения первого).\n",
      "\r\n",
      "Первый вариант можно реализовать, включив режим изоляции SERIALIZABLE. Если во время обработки запроса кто-то уже успел изменить и закоммитить данные, будет брошено исключение, которое можно обработать и вернуть соответствующий HTTP-статус. \n",
      "\r\n",
      "Минус такого решения — большое число блокировок в PostgreSQL, SERIALIZABLE будет вызывать исключение, даже если конкурентные запросы меняют записи жителей из разных выгрузок.\n",
      "\r\n",
      "Также можно воспользоваться механизмом рекомендательных блокировок. Если получить такую блокировку по import_id, конкурентные запросы для разных выгрузок смогут выполняться параллельно. \n",
      "\r\n",
      "Для обработки конкурентных запросов в одной выгрузке можно реализовать поведение любого из вариантов: функция pg_try_advisory_xact_lock пытается получить блокировку и \r\n",
      "возвращает результат boolean немедленно (если блокировку получить не удалось — можно бросить исключение), а pg_advisory_xact_lock ожидает, пока \r\n",
      "ресурс не станет доступен для блокировки (в этом случае запросы выполнятся последовательно, я остановился на этом варианте).\n",
      "\r\n",
      "В итоге обработчик должен вернуть актуальную информацию об обновленном жителе. Можно было ограничиться возвращением клиенту данных из его же запроса (раз мы возвращаем ответ клиенту, значит, исключений не было и все запросы успешно выполнены). Или — воспользоваться ключевым словом RETURNING в запросах, изменяющих БД, и сформировать ответ из полученных результатов. Но оба этих подхода не позволили бы увидеть и протестировать случай с гонкой состояний. \n",
      "\r\n",
      "К сервису не предъявлялись требования по высокой нагрузке, поэтому я решил запрашивать все данные о жителе заново и возвращать клиенту честный результат из БД. \n",
      "\n",
      "GET /imports/$import_id/citizens/birthdays\r\n",
      "Обработчик вычисляет число подарков, которое приобретет каждый житель выгрузки своим родственникам (первого порядка). Число сгруппировано по месяцам для выгрузки с указанным import_id. В случае обращения к несуществующей выгрузке необходимо вернуть HTTP-ответ 404: Not Found.\n",
      "\r\n",
      "Есть два варианта реализации: \n",
      "\n",
      "\n",
      "Получить данные для жителей с родственниками из базы, а на стороне Python агрегировать данные по месяцам и сгенерировать списки для тех месяцев, для которых нет данных в БД.\n",
      "Cоставить json-запрос в базу и дописать для отсутствующих месяцев заглушки.\n",
      "\r\n",
      "Я остановился на первом варианте — визуально он выглядит более понятным и поддерживаемым. Число дней рождений в определенном месяце можно получить, сделав JOIN из таблицы с родственными связями (relations.citizen_id — житель, для которого мы считаем дни рождения родственников) в таблицу citizens (содержит дату рождения, из которой требуется получить месяц).\n",
      "\r\n",
      "Значения месяцев не должны содержать ведущих нулей. Месяц, получаемый из поля birth_date c помощью функции date_part, может содержать ведущий ноль. Чтобы убрать его, я выполнил cast к integer в SQL-запросе.\n",
      "\r\n",
      "Несмотря на то, что в обработчике требуется выполнить два запроса (проверить существование выгрузки и получить информации о днях рождения и подарках), транзакция не требуется. \n",
      "\r\n",
      "По умолчанию PostgreSQL использует режим READ COMMITTED, при котором в текущей транзакции видны все новые (добавляемые другими транзакциями) и существующие (изменяемые другими транзакциями) записи после их успешного завершения.\n",
      "\r\n",
      "Например, если в момент получения данных будет добавлена новая выгрузка — она никак не повлияет на существующие. Если в момент получения данных будет выполнен запрос на изменение жителя — то либо данные еще не будут видны (если транзакция, меняющая данные, не завершилась), либо транзакция полностью завершится и станут видны сразу все изменения. Целостность получаемых из базы данных не нарушится.\n",
      "\n",
      "GET /imports/$import_id/towns/stat/percentile/age\r\n",
      "Обработчик вычисляет 50-й, 75-й и 99-й перцентили возрастов (полных лет) жителей по городам в выборке с указанным import_id. В случае обращения к несуществующей выгрузке необходимо вернуть HTTP-ответ 404: Not Found.\n",
      "\r\n",
      "Несмотря на то, что в обработчике выполняется два запроса (проверка на существование выгрузки и получение списка жителей), использовать транзакцию необязательно.\n",
      "\r\n",
      "Есть два варианта реализации: \n",
      "\n",
      "\n",
      "Получить из БД возраста жителей, сгруппированные по городам, а затем на стороне Python вычислить перцентили с помощью numpy (который в задании указан как эталонный) и округлить до двух знаков после запятой.\n",
      "Сделать всю работу на стороне PostgreSQL: функция percentile_cont вычисляет перцентиль с линейной интерполяцией, затем округляем полученные значения до двух знаков после запятой в рамках одного SQL-запроса, а numpy используем для тестирования.\n",
      " \r\n",
      "Второй вариант требует передавать меньше данных между приложением и PostgreSQL, но у него есть не очень очевидный подводный камень: в PostgreSQL округление математическое, (SELECT ROUND(2.5) вернет 3), а в Python — бухгалтерское, к ближайшему целому (round(2.5) вернет 2).\n",
      "\r\n",
      "Чтобы тестировать обработчик, реализация должна быть одинаковой и в PostgreSQL, и в Python (реализовать функцию с математическим округлением в Python выглядит проще). Стоит отметить, что при вычислении перцентилей numpy и PostgreSQL могут возвращать немного отличающиеся числа, но с учетом округления эта разница будет незаметна.\n",
      "\n",
      "Тестирование\r\n",
      "Что нужно проверить в этом приложении? Во-первых, что обработчики отвечают требованиям и выполняют требуемую работу в окружении, максимально близком к боевому. Во-вторых, что миграции, которые изменяют состояние базы данных, работают без ошибок. В-третьих, есть ряд вспомогательных функций, которые тоже было бы правильно покрыть тестами.\n",
      "\r\n",
      "Я решил воспользоваться фреймворком pytest из-за его гибкости и простоты в использовании. Он предлагает мощный механизм подготовки окружения для тестов — фикстуры, то есть функции с декоратором pytest.mark.fixture, названия которых можно указать параметром в тесте. Если pytest обнаружит в аннотации теста параметр с названием фикстуры, он выполнит эту фикстуру и передаст результат в значении этого параметра. А если фикстура является генератором, то параметр теста примет значение, возвращаемое yield, и после окончания теста выполнится вторая часть фикстуры, которая может очистить ресурсы или закрыть соединения.\n",
      "\r\n",
      "Для большинства тестов нам потребуется база данных PostgreSQL. Чтобы изолировать тесты друг от друга, можно перед выполнением каждого теста создавать отдельную базу данных, а после выполнения — удалять ее.\n",
      "\n",
      "\n",
      "Создаем базу данных фикстурой для каждого теста\n",
      "import os\n",
      "import uuid\n",
      "\n",
      "import pytest\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy_utils import create_database, drop_database\n",
      "from yarl import URL\n",
      "\n",
      "from analyzer.utils.pg import DEFAULT_PG_URL\n",
      "\n",
      "PG_URL = os.getenv('CI_ANALYZER_PG_URL', DEFAULT_PG_URL)\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def postgres():\n",
      "    tmp_name = '.'.join([uuid.uuid4().hex, 'pytest'])\n",
      "    tmp_url = str(URL(PG_URL).with_path(tmp_name))\n",
      "    create_database(tmp_url)\n",
      "\n",
      "    try:\n",
      "        # Это значение будет иметь параметр postgres в функции-тесте\n",
      "        yield tmp_url\n",
      "    finally:\n",
      "        drop_database(tmp_url)\n",
      "\n",
      "\n",
      "def test_db(postgres):\n",
      "    \"\"\"\n",
      "    Пример теста, использующего PostgreSQL\n",
      "    \"\"\"\n",
      "    engine = create_engine(postgres)\n",
      "    assert engine.execute('SELECT 1').scalar() == 1\n",
      "    engine.dispose()\n",
      "\r\n",
      "C этой задачей здорово справился модуль sqlalchemy_utils, учитывающий особенности разных баз данных и драйверов. Например, PostgreSQL не разрешает выполнение CREATE DATABASE в блоке транзакции. При создании БД sqlalchemy_utils переводит psycopg2 (который обычно выполняет все запросы в транзакции) в режим autocommit.\n",
      "\r\n",
      "Другая важная особенность: если к PostgreSQL подключен хотя бы один клиент — базу данных нельзя удалить, а sqlalchemy_utils отключает всех клиентов перед удалением базы. БД будет успешно удалена, даже если зависнет какой-нибудь тест, имеющий активные подключения к ней. \n",
      "\r\n",
      "PostgreSQL потребуется нам в разных состояниях: для тестирования миграций необходима чистая база данных, в то время как обработчики требуют, чтобы все миграции были применены. Изменять состояние базы данных можно программно с помощью команд Alembic, для их вызова требуется объект конфигурации Alembic.\n",
      "\n",
      "\n",
      "Создаем фикстурой объект конфигурации Alembic\n",
      "from types import SimpleNamespace\n",
      "\n",
      "import pytest\n",
      "\n",
      "from analyzer.utils.pg import make_alembic_config\n",
      "\n",
      "\n",
      "@pytest.fixture()\n",
      "def alembic_config(postgres):\n",
      "    cmd_options = SimpleNamespace(config='alembic.ini', name='alembic',\n",
      "                                  pg_url=postgres, raiseerr=False, x=None)\n",
      "    return make_alembic_config(cmd_options)\n",
      "\r\n",
      "Обратите внимание, что у фикстуры alembic_config есть параметр postgres — pytest позволяет не только указывать зависимость теста от фикстур, но и зависимости между фикстурами. \n",
      "\r\n",
      "Этот механизм позволяет гибко разделять логику и писать очень краткий и переиспользуемый код.\n",
      "\n",
      "Обработчики\r\n",
      "Для тестирования обработчиков требуется база данных с созданными таблицами и типами данных. Чтобы применить миграции, необходимо программно вызвать команду upgrade Alembic. Для ее вызова потребуется объект с конфигурацией Alembic, который мы уже определили фикстурой alembic_config. База данных с миграциями выглядит как вполне самостоятельная сущность, и ее можно представить в виде фикстуры:\n",
      "\n",
      "from alembic.command import upgrade\n",
      "\n",
      "@pytest.fixture\n",
      "async def migrated_postgres(alembic_config, postgres):\n",
      "    upgrade(alembic_config, 'head')\n",
      "    # Возвращаем DSN базы данных, которая была смигрирована \n",
      "    return postgres\r\n",
      "Когда миграций в проекте становится много, их применение для каждого теста может занимать слишком много времени. Чтобы ускорить процесс, можно один раз создать базу данных с миграциями и затем использовать ее в качестве шаблона.\n",
      "\r\n",
      "Помимо базы данных для тестирования обработчиков, потребуется запущенное приложение, а также клиент, настроенный на работу с этим приложением. Чтобы приложение было легко тестировать, я вынес его создание в функцию create_app, которая принимает параметры для запуска: базу данных, порт для REST API и другие.\n",
      "\r\n",
      "Аргументы для запуска приложения можно также представить в виде отдельной фикстуры. Для их создания потребуется определить свободный порт для запуска тестируемого приложения и адрес до смигрированной временной базы данных.\n",
      "\r\n",
      "Для определения свободного порта я воспользовался фикстурой aiomisc_unused_port из пакета aiomisc. \n",
      "\r\n",
      "Стандартная фикстура aiohttp_unused_port тоже вполне бы подошла, но она возвращает функцию для определения свободых портов, в то время как aiomisc_unused_port возвращает сразу номер порта. Для нашего приложения требуется определить только один свободный порт, поэтому я решил не писать лишнюю строчку кода с вызовом aiohttp_unused_port.\n",
      "\n",
      "@pytest.fixture\n",
      "def arguments(aiomisc_unused_port, migrated_postgres):\n",
      "    return parser.parse_args(\n",
      "        [\n",
      "            '--log-level=debug',\n",
      "            '--api-address=127.0.0.1',\n",
      "            f'--api-port={aiomisc_unused_port}',\n",
      "            f'--pg-url={migrated_postgres}'\n",
      "        ]\n",
      "    )\r\n",
      "Все тесты с обработчиками подразумевают запросы к REST API, работа напрямую с приложением aiohttp не требуется. Поэтому я сделал одну фикстуру, которая запускает приложение и с помощью фабрики aiohttp_client создает и возвращает подключенный к приложению стандартный тестовый клиент aiohttp.test_utils.TestClient.\n",
      "\n",
      "from analyzer.api.app import create_app\n",
      "\n",
      "@pytest.fixture\n",
      "async def api_client(aiohttp_client, arguments):\n",
      "    app = create_app(arguments)\n",
      "    client = await aiohttp_client(app, server_kwargs={\n",
      "        'port': arguments.api_port\n",
      "    })\n",
      "\n",
      "    try:\n",
      "        yield client\n",
      "    finally:\n",
      "        await client.close()\n",
      "\r\n",
      "Теперь, если в параметрах теста указать фикстуру api_client, произойдет следующее:\n",
      "\n",
      "\n",
      "Фикстура postgres создаст базу данных (зависимость для migrated_postgres).\n",
      "Фикстура alembic_config создаст объект конфигурации Alembic, подключенный к временной базе данных (зависимость для migrated_postgres).\n",
      "Фикстура migrated_postgres применит миграции (зависимость для arguments).\n",
      "Фикстура aiomisc_unused_port обнаружит свободный порт (зависимость для arguments).\n",
      "Фикстура arguments создаст аргументы для запуска (зависимость для api_client).\n",
      "Фикстура api_client создаст и запустит приложение и вернет клиента для выполнения запросов.\n",
      "Выполнится тест.\n",
      "Фикстура api_client отключит клиента и остановит приложение.\n",
      "Фикстура postgres удалит базу данных.\n",
      "\r\n",
      "Фикстуры позволяют избежать дублирования кода, но помимо подготовки окружения в тестах есть еще одно потенциальное место, в котором будет очень много одинакового кода — запросы к приложению.\n",
      "\r\n",
      "Во-первых, сделав запрос, мы ожидаем получить определенный HTTP-статус. Во-вторых, если статус совпадает с ожидаемым, то перед работой с данными необходимо убедиться, что они имеют правильный формат. Здесь легко ошибиться и написать обработчик, который делает правильные вычисления и возвращает правильный результат, но не проходит автоматическую валидацию из-за неправильного формата ответа (например, забыть обернуть ответ в словарь с ключом data). Все эти проверки можно было бы сделать в одном месте.\n",
      "\r\n",
      "В модуле analyzer.testing я подготовил для каждого обработчика функцию-помощник, которая проверяет статус HTTP, а также формат ответа с помощью Marshmallow. \n",
      "\n",
      "GET /imports/$import_id/citizens\r\n",
      "Я решил начать с обработчика, возвращающего жителей, потому что он очень полезен для проверки результатов работы других обработчиков, изменяющих состояние базы данных.\n",
      "\r\n",
      "Я намеренно не использовал код, добавляющий данные в базу из обработчика POST /imports, хотя вынести его в отдельную функцию несложно. Код обработчиков имеет свойство меняться, а если в коде, добавляющем в базу, будет какая-либо ошибка, есть вероятность, что тест перестанет работать как задумано и неявно для разработчиков перестанет показывать ошибки.\n",
      "\r\n",
      "Для этого теста я определил следующие наборы данных для тестирования:\n",
      "\n",
      "\n",
      "Выгрузка с несколькими родственниками. Проверяет, что для каждого жителя будет правильно сформирован список с идентификаторами родственников.\n",
      "Выгрузка с одним жителем без родственников. Проверяет, что поле relatives — пустой список (из-за LEFT JOIN в SQL-запросе список родственников может быть равен [None]).\n",
      "Выгрузка с жителем, который сам себе родственник.\n",
      "Пустая выгрузка. Проверяет, что обработчик разрешает добавить пустую выгрузку и не падает с ошибкой.\n",
      "\r\n",
      "Чтобы запустить один и тот же тест отдельно на каждой выгрузке, я воспользовался еще одним очень мощным механизмом pytest — параметризацией. Этот механизм позволяет обернуть функцию-тест в декоратор pytest.mark.parametrize и описать в нем, какие параметры должна принимать функция-тест для каждого отдельного тестируемого случая.\n",
      "\n",
      "\n",
      "Как параметризовать тест\n",
      "import pytest\n",
      "\n",
      "from analyzer.utils.testing import generate_citizen\n",
      "\n",
      "datasets = [\n",
      "    # Житель с несколькими родственниками\n",
      "    [\n",
      "        generate_citizen(citizen_id=1, relatives=[2, 3]),\n",
      "        generate_citizen(citizen_id=2, relatives=[1]),\n",
      "        generate_citizen(citizen_id=3, relatives=[1])\n",
      "    ],\n",
      "\n",
      "    # Житель без родственников\n",
      "    [\n",
      "        generate_citizen(relatives=[])\n",
      "    ],\n",
      "\n",
      "    # Выгрузка с жителем, который сам себе родственник\n",
      "    [\n",
      "        generate_citizen(citizen_id=1, name='Джейн', gender='male',\n",
      "                         birth_date='17.02.2020', relatives=[1])\n",
      "    ],\n",
      "\n",
      "    # Пустая выгрузка\n",
      "    [],\n",
      "]\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize('dataset', datasets)\n",
      "async def test_get_citizens(api_client, dataset):\n",
      "    \"\"\"\n",
      "    Этот тест будет вызван 4 раза, отдельно для каждого датасета\n",
      "    \"\"\"\n",
      "\r\n",
      "Итак, тест добавит выгрузку в базу данных, затем с помощью запроса к обработчику получит информацию о жителях и сравнит эталонную выгрузку с полученной. Но как сравнить жителей? \n",
      "\r\n",
      "Каждый житель состоит из скалярных полей и поля relatives — списка идентификаторов родственников. Список в Python — упорядоченный тип, и при сравнении порядок элементов каждого списка имеет значение, но при сравнении списков с родственниками порядок не должен иметь значение.\n",
      "\r\n",
      "Если привести relatives к множеству перед сравнением, то при сравнении не получится обнаружить ситуацию, когда у одного из жителей в поле relatives есть дубли. Если отсортировать список с идентификаторами родственников, это позволит обойти проблему разного порядка идентификаторов родственников, но при этом обнаружить дубли.\n",
      "\r\n",
      "При сравнении двух списков с жителями можно столкнуться с похожей проблемой: технически, порядок жителей в выгрузке не важен, но важно обнаружить, если в одной выгрузке будет два жителя с одинаковыми идентификаторами, а в другой нет. Так что помимо упорядочивания списка с родственниками relatives для каждого жителя необходимо упорядочить жителей в каждой выгрузке. \n",
      "\r\n",
      "Так как задача сравнения жителей возникнет еще не раз, я реализовал две функции: одну для сравнения двух жителей, а вторую для сравнения двух списков с жителями:\n",
      "\n",
      "\n",
      "Сравниваем жителей\n",
      "from typing import Iterable, Mapping\n",
      "\n",
      "def normalize_citizen(citizen):\n",
      "    \"\"\"\n",
      "    Возвращает жителя с упорядоченным списком родственников\n",
      "    \"\"\"\n",
      "    return {**citizen, 'relatives': sorted(citizen['relatives'])}\n",
      "\n",
      "\n",
      "def compare_citizens(left: Mapping, right: Mapping) -> bool:\n",
      "    \"\"\"\n",
      "    Сравнивает двух жителей\n",
      "    \"\"\"\n",
      "    return normalize_citizen(left) == normalize_citizen(right)\n",
      "\n",
      "\n",
      "def compare_citizen_groups(left: Iterable, right: Iterable) -> bool:\n",
      "    \"\"\"\n",
      "    Упорядочивает списки с родственниками для каждого жителя, списки с жителями\n",
      "    и сравнивает их\n",
      "    \"\"\"\n",
      "    left = [normalize_citizen(citizen) for citizen in left]\n",
      "    left.sort(key=lambda citizen: citizen['citizen_id'])\n",
      "\n",
      "    right = [normalize_citizen(citizen) for citizen in right]\n",
      "    right.sort(key=lambda citizen: citizen['citizen_id'])\n",
      "    return left == right\n",
      "\n",
      "\r\n",
      "Чтобы убедиться, что этот обработчик не возвращает жителей других выгрузок, я решил перед каждым тестом добавлять дополнительную выгрузку с одним жителем.\n",
      "\n",
      "POST /imports\r\n",
      "Я определил следующие наборы данных для тестирования обработчика:\n",
      "\n",
      "\n",
      " Корректные данные, ожидается успешное добавление в БД.\n",
      "\n",
      "\n",
      " Житель без родственников (самый простой).\n",
      "\r\n",
      "Обработчику необходимо добавить данные в две таблицы. Если не обрабатывается ситуация, когда у жителя нет родственников, будет выполнен пустой insert в таблицу родственных связей, что приведет к ошибке.\n",
      "Житель с родственниками (более сложный, обычный).\n",
      "\r\n",
      "Проверяет, что обработчик корректно сохраняет данные и о жителе и его родственных связях.\n",
      "\n",
      " Житель сам себе родственник.\n",
      "\r\n",
      "Про этот случай было много вопросов, поэтому в шутку решил добавить и его. :)\n",
      "\n",
      " Выгрузка с максимального размера\n",
      "\r\n",
      "Проверяет, что aiohttp позволяет загружать такие объемы данных и что при большом количестве данных в PostgreSQL не отправляется больше 32 767 аргументов (обработчик должен выполнить несколько запросов).\n",
      "\n",
      " Пустая выгрузка\n",
      "\r\n",
      "Обработчик должен учитывать такой случай и не падать, пытаясь выполнить пустой insert в таблицу с жителями.\n",
      "\n",
      "\n",
      "\n",
      " Данные с ошибками, ожидаем HTTP-ответ 400: Bad Request.\n",
      "\n",
      "\n",
      "Дата рождения некорректная (будущее время).\n",
      "citizen_id в рамках выгрузки не уникален.\n",
      "Родственная связь указана неверно (есть только от одного жителя к другому, но нет обратной).\n",
      "У жителя указан несуществующий в выгрузке родственник.\n",
      "Родственные связи не уникальны.\n",
      "\n",
      "\r\n",
      "Если обработчик отработал успешно и данные были добавлены, необходимо получить добавленных в БД жителей и сравнить их с эталонной выгрузки. Для получения жителей я воспользовался уже протестированным обработчиком GET /imports/$import_id/citizens, а для сравнения — функцией compare_citizen_groups.\n",
      "\n",
      "PATCH /imports/$import_id/citizens/$citizen_id\r\n",
      "Валидация данных во многом похожа на описанную в обработчике POST /imports с небольшими исключениями: есть только один житель и клиент может передать только те поля, которые пожелает. \n",
      "\r\n",
      "Я решил использовать следующие наборы с некорректными данными, чтобы проверить, что обработчик вернет HTTP-ответ 400: Bad request:\n",
      "\n",
      "\n",
      "Поле указано, но имеет некорректный тип и/или формат данных\n",
      "Указана некорректная дата рождения (будущее время).\n",
      "Поле relatives содержит несуществующего в выгрузке родственника.\n",
      "\r\n",
      "Также необходимо проверить, что обработчик корректно обновляет информацию о жителе и его родственниках. \n",
      "\r\n",
      "Для этого создадим выгрузку с тремя жителями, два из которых — родственники, и отправим запрос с новыми значениями всех скалярных полей и новым идентификатором родственника в поле relatives.\n",
      "\r\n",
      "Чтобы убедиться, что обработчик различает жителей разных выгрузок перед тестом (и, например, не изменит жителей с одинаковыми идентификаторами из другой выгрузки), я создал дополнительную выгрузку с тремя жителями, которые имеют такие же идентификаторы.\n",
      "\r\n",
      "Обработчик должен сохранить новые значения скалярных полей, добавить нового указанного родственника и удалить связь со старым, не указанным родственником. Все изменения родственных связей должны быть двусторонними. Изменений в других выгрузках быть не должно.\n",
      "\r\n",
      "Поскольку такой обработчик может быть подвержен состоянию гонки (это рассматривалось в разделе «Разработка»), я добавил два дополнительных теста. Один воспроизводит проблему с состоянием гонки (расширяет класс обработчика и убирает блокировку), второй доказывает, что проблема с состоянием гонки не воспроизводится.\n",
      "\n",
      "GET /imports/$import_id/citizens/birthdays\r\n",
      "Для тестирования этого обработчика я выбрал следующие наборы данных:\n",
      "\n",
      "\n",
      "Выгрузка, в которой у жителя есть один родственник в одном месяце и два родственника в другом.\n",
      "Выгрузка с одним жителем без родственников. Проверяет, что обработчик не учитывает его при расчетах.\n",
      "Пустая выгрузка. Проверяет, что обработчик не упадет с ошибкой и вернет в ответе корректный словарь с 12 месяцами.\n",
      "Выгрузка с жителем, который сам себе родственник. Проверяет, что житель купит себе подарок в месяц своего рождения.\n",
      "\r\n",
      "Обработчик должен возвращать в ответе все месяцы, даже если в эти месяцы нет дней рождений. Чтобы избежать дублирования, я сделал функцию, которой можно передать словарь, чтобы она дополнила его значениями для отсутствующих месяцев.\n",
      "\r\n",
      "Чтобы убедиться, что обработчик различает жителей разных выгрузок, я добавил дополнительную выгрузку с двумя родственниками. Если обработчик по ошибке использует их при расчетах, то результаты будут некорректными и обработчик упадет с ошибкой.\n",
      "\n",
      "GET /imports/$import_id/towns/stat/percentile/age\r\n",
      "Особенность этого теста в том, что результаты его работы зависят от текущего времени: возраст жителей вычисляется исходя из текущей даты. Чтобы результаты тестирования не менялись с течением времени, текущую дату, даты рождения жителей и ожидаемые результаты необходимо зафиксировать. Это позволит легко воспроизвести любые, даже краевые случаи.\n",
      "\r\n",
      "Как лучше зафиксировать дату? В обработчике для вычисления возраста жителей используется PostgreSQL-функция AGE, принимающая первым параметром дату, для которой необходимо рассчитать возраст, а вторым — базовую дату (определена константой TownAgeStatView.CURRENT_DATE).\n",
      "\n",
      "\n",
      "Подменяем базовую дату в обработчике на время теста\n",
      "from unittest.mock import patch\n",
      "\n",
      "import pytz\n",
      "\n",
      "CURRENT_DATE = datetime(2020, 2, 17, tzinfo=pytz.utc)\n",
      "\n",
      "\n",
      "@patch('analyzer.api.handlers.TownAgeStatView.CURRENT_DATE', new=CURRENT_DATE)\n",
      "async def test_get_ages(...):\n",
      "    ...\n",
      "\r\n",
      "Для тестирования обработчика я выбрал следующие наборы данных (для всех жителей указывал один город, потому что обработчик агрегирует результаты по городам):\n",
      "\n",
      "\n",
      "Выгрузка с несколькими жителями, у которых завтра день рождения (возраст — несколько лет и 364 дня). Проверяет, что обработчик использует в расчетах только количество полных лет.\n",
      "Выгрузка с жителем, у которого сегодня день рождения (возраст — ровно несколько лет). Проверяет краевой случай — возраст жителя, у которого сегодня день рождения, не должен рассчитаться как уменьшенный на 1 год.\n",
      "Пустая выгрузка. Обработчик не должен на ней падать.\n",
      "\r\n",
      "Эталон для расчета перцентилей — numpy с линейной интерполяцией, и эталонные результаты для тестирования я рассчитал именно им.\n",
      "\r\n",
      "Также нужно округлять дробные значения перцентилей до двух знаков после запятой. Если вы использовали в обработчике для округления PostgreSQL, а для расчета эталонных данных — Python, то могли заметить, что округление в Python 3 и PostgreSQL может давать разные результаты.\n",
      "\n",
      "\n",
      "Например\n",
      "# Python 3\n",
      "round(2.5)\n",
      "> 2\n",
      "\n",
      "-- PostgreSQL\n",
      "SELECT ROUND(2.5)\n",
      "> 3\n",
      "\n",
      "\r\n",
      "Дело в том, что Python использует банковское округление до ближайшего четного, а PostgreSQL — математическое (half-up). В случае, если расчеты и округление производятся в PostgreSQL, было бы правильным в тестах также использовать математическое округление.\n",
      "\r\n",
      "Сначала я описал наборы данных с датами рождения в текстовом формате, но читать тест в таком формате было неудобно: приходилось каждый раз вычислять в уме возраст каждого жителя, чтобы вспомнить, что проверяет тот или иной набор данных. Конечно, можно было обойтись комментариями в коде, но я решил пойти чуть дальше и написал функцию age2date, которая позволяет описать дату рождения в виде возраста: количества лет и дней.\n",
      "\n",
      "\n",
      "Например, вот так\n",
      "import pytz\n",
      "\n",
      "from analyzer.utils.testing import generate_citizen\n",
      "\n",
      "\n",
      "CURRENT_DATE = datetime(2020, 2, 17, tzinfo=pytz.utc)\n",
      "\n",
      "def age2date(years: int, days: int = 0, base_date=CURRENT_DATE) -> str:\n",
      "    birth_date = copy(base_date).replace(year=base_date.year - years)\n",
      "    birth_date -= timedelta(days=days)\n",
      "    return birth_date.strftime(BIRTH_DATE_FORMAT)\n",
      "\n",
      "# Сколько лет этому жителю? Посчитать несложно, но если их будет много?\n",
      "generate_citizen(birth_date='17.02.2009')\n",
      "\n",
      "# Жителю ровно 11 лет и у него сегодня день рождения\n",
      "generate_citizen(birth_date=age2date(years=11))\n",
      "\n",
      "\r\n",
      "Чтобы убедиться, что обработчик различает жителей разных выгрузок, я добавил дополнительную выгрузку с одним жителем из другого города: если обработчик по ошибке использует его, в результатах появится лишний город и тест сломается.\n",
      "\n",
      "Интересный факт: когда я писал этот тест 29 февраля 2020 года, у меня внезапно перестали генерироваться выгрузки с жителями из-за бага в Faker (2020-й — високосный год, а другие годы, которые выбирал Faker, не всегда были високосными и в них не было 29 февраля). Не забывайте фиксировать даты и тестировать краевые случаи!\n",
      "Миграции\r\n",
      "Код миграций на первый взгляд кажется очевидным и наименее подверженным ошибкам, зачем его тестировать? Это очень опасное заблуждение: самые коварные ошибки миграций могут проявить себя в самый неподходящий момент. Даже если они не испортят данные, то могут стать причиной лишнего даунтайма.\n",
      "\r\n",
      "Существующая в проекте initial миграция изменяет структуру базы данных, но не изменяет данные. От каких типовых ошибок можно защититься в подобных миграциях?\n",
      "\n",
      "\n",
      " Метод downgrade не реализован или не удалены все созданные в миграции сущности (особенно это касается пользовательских типов данных, которые создаются автоматически при создании таблицы, я про них уже упоминал).\n",
      "\r\n",
      "Это приведет к тому, что миграцию нельзя будет применить два раза (применить-откатить-применить): при откате не будут удалены все созданные миграцией сущности, при повторном создании миграция пройдет с ошибкой — тип данных уже существует.\n",
      "\n",
      "Cинтаксические ошибки и опечатки.\n",
      "Ошибки в связях миграций (цепочка нарушена).\n",
      "\r\n",
      "Большинство этих ошибок обнаружит stairway-тест. Его идея — применять миграции по одной, последовательно выполняя методы upgrade, downgrade, upgrade для каждой миграции. Такой тест достаточно один раз добавить в проект, он не требует поддержки и будет служить верой и правдой.\n",
      "\r\n",
      "А вот если миграция, помимо структуры, изменяла бы данные, то потребовалось бы написать хотя бы один отдельный тест, проверяющий, что данные корректно изменяются в методе upgrade и возвращаются к изначальному состоянию в downgrade. На всякий случай: проект с примерами тестирования разных миграций, который я подготовил для доклада про Alembic на Moscow Python.\n",
      "\n",
      "Сборка\r\n",
      "Конечный артефакт, который мы собираемся разворачивать и который хотим получить в результате сборки, — Docker-образ. Для сборки необходимо выбрать базовый образ c Python. Официальный образ python:latest весит ~1 ГБ и, если его использовать в качестве базового, образ с приложением будет огромным. Существуют образы на основе ОС Alpine, размер которых намного меньше. Но с растущим количеством устанавливаемых пакетов размер конечного образа вырастет, и в итоге даже образ, собранный на основе Alpine, будет не таким уж и маленьким. Я выбрал в качестве базового образа snakepacker/python — он весит немного больше Alpine-образов, но основан на Ubuntu, которая предлагает огромный выбор пакетов и библиотек.\n",
      "\r\n",
      "Еще один способ уменьшить размер образа с приложением — не включать в итоговый образ компилятор, библиотеки и файлы с заголовками для сборки, которые не потребуются для работы приложения. \n",
      "\r\n",
      "Для этого можно воспользоваться многоступенчатой сборкой Docker:\n",
      "\n",
      "\n",
      " С помощью «тяжелого» образа snakepacker/python:all (~1 ГБ, в сжатом виде ~500 МБ) создаем виртуальное окружение, устанавливаем в него все зависимости и пакет с приложением. Этот образ нужен исключительно для сборки, он может содержать компилятор, все необходимые библиотеки и файлы с заголовками.\n",
      "\n",
      "FROM snakepacker/python:all as builder\n",
      "\n",
      "# Создаем виртуальное окружение\n",
      "RUN python3.8 -m venv /usr/share/python3/app\n",
      "\n",
      "# Копируем source distribution в контейнер и устанавливаем его\n",
      "COPY dist/ /mnt/dist/\n",
      "RUN /usr/share/python3/app/bin/pip install /mnt/dist/*\n",
      " Готовое виртуальное окружение копируем в «легкий» образ snakepacker/python:3.8 (~100 МБ, в сжатом виде ~50 МБ), который содержит только интерпретатор требуемой версии Python.\n",
      "\r\n",
      "Важно: в виртуальном окружении используются абсолютные пути, поэтому его необходимо скопировать по тому же адресу, по которому оно было собрано в контейнере-сборщике. \n",
      "\n",
      "FROM snakepacker/python:3.8 as api\n",
      "\n",
      "# Копируем готовое виртуальное окружение из контейнера builder\n",
      "COPY --from=builder /usr/share/python3/app /usr/share/python3/app\n",
      "\n",
      "# Устанавливаем ссылки, чтобы можно было воспользоваться командами\n",
      "# приложения\n",
      "RUN ln -snf /usr/share/python3/app/bin/analyzer-* /usr/local/bin/\n",
      "\n",
      "# Устанавливаем выполняемую при запуске контейнера команду по умолчанию\n",
      "CMD [\"analyzer-api\"]\n",
      "\r\n",
      "Чтобы сократить время на сборку образа, зависимые модули приложения можно установить до его установки в виртуальное окружение. Тогда Docker закеширует их и не будет устанавливать заново, если они не менялись.\n",
      "\n",
      "\n",
      "Dockerfile целиком\n",
      "############### Образ для сборки виртуального окружения ################\n",
      "# Основа — «тяжелый» (~1 ГБ, в сжатом виде ~500 ГБ) образ со всеми необходимыми\n",
      "# библиотеками для сборки модулей\n",
      "FROM snakepacker/python:all as builder\n",
      "\n",
      "# Создаем виртуальное окружение и обновляем pip\n",
      "RUN python3.8 -m venv /usr/share/python3/app\n",
      "RUN /usr/share/python3/app/bin/pip install -U pip\n",
      "\n",
      "# Устанавливаем зависимости отдельно, чтобы закешировать. При последующей сборке\n",
      "# Docker пропустит этот шаг, если requirements.txt не изменится\n",
      "COPY requirements.txt /mnt/\n",
      "RUN /usr/share/python3/app/bin/pip install -Ur /mnt/requirements.txt\n",
      "\n",
      "# Копируем source distribution в контейнер и устанавливаем его\n",
      "COPY dist/ /mnt/dist/\n",
      "RUN /usr/share/python3/app/bin/pip install /mnt/dist/* \\\n",
      "    && /usr/share/python3/app/bin/pip check\n",
      "\n",
      "########################### Финальный образ ############################\n",
      "# За основу берем «легкий» (~100 МБ, в сжатом виде ~50 МБ) образ с Python\n",
      "FROM snakepacker/python:3.8 as api\n",
      "\n",
      "# Копируем в него готовое виртуальное окружение из контейнера builder\n",
      "COPY --from=builder /usr/share/python3/app /usr/share/python3/app\n",
      "\n",
      "# Устанавливаем ссылки, чтобы можно было воспользоваться командами\n",
      "# приложения\n",
      "RUN ln -snf /usr/share/python3/app/bin/analyzer-* /usr/local/bin/\n",
      "\n",
      "# Устанавливаем выполняемую при запуске контейнера команду по умолчанию\n",
      "CMD [\"analyzer-api\"]\n",
      "\r\n",
      "Для удобства сборки я добавил команду make upload, которая собирает Docker-образ и загружает его на hub.docker.com.\n",
      "\n",
      "CI\r\n",
      "Теперь, когда код покрыт тестами и мы умеем собирать Docker-образ, самое время автоматизировать эти процессы. Первое, что приходит в голову: запускать тесты на создание пул-реквестов, а при добавлении изменений в master-ветку собирать новый Docker-образ и загружать его на Docker Hub (или GitHub Packages, если вы не собираетесь распространять образ публично).\n",
      "\r\n",
      "Я решил эту задачу с помощью GitHub Actions. Для этого потребовалось создать YAML-файл в папке .github/workflows и описать в нем workflow (c двумя задачами: test и publish), которое я назвал CI.\n",
      "\r\n",
      "Задача test выполняется при каждом запуске workflow CI, с помощью services поднимает контейнер с PostgreSQL, ожидает, когда он станет доступен, и запускает pytest в контейнере snakepacker/python:all.\n",
      "\r\n",
      "Задача publish выполняется, только если изменения были добавлены в ветку master и если задача test была выполнена успешно. Она собирает source distribution контейнером snakepacker/python:all, затем собирает и загружает Docker-образ с помощью docker/build-push-action@v1.\n",
      "\n",
      "\n",
      "Полное описание workflow\n",
      "name: CI\n",
      "\n",
      "# Workflow должен выполняться при добавлении изменений \n",
      "# или новом пул-реквесте в master\n",
      "on:\n",
      "  push:\n",
      "    branches: [ master ]\n",
      "  pull_request:\n",
      "    branches: [ master ]\n",
      "\n",
      "jobs:\n",
      "  # Тесты должны выполняться при каждом запуске workflow\n",
      "  test:\n",
      "    runs-on: ubuntu-latest\n",
      "\n",
      "    services:\n",
      "      postgres:\n",
      "        image: docker://postgres\n",
      "        ports:\n",
      "          - 5432:5432\n",
      "        env:\n",
      "          POSTGRES_USER: user\n",
      "          POSTGRES_PASSWORD: hackme\n",
      "          POSTGRES_DB: analyzer\n",
      "\n",
      "    steps:\n",
      "      - uses: actions/checkout@v2\n",
      "      - name: test\n",
      "        uses: docker://snakepacker/python:all\n",
      "        env:\n",
      "          CI_ANALYZER_PG_URL: postgresql://user:hackme@postgres/analyzer\n",
      "        with:\n",
      "          args: /bin/bash -c \"pip install -U '.[dev]' && pylama && wait-for-port postgres:5432 && pytest -vv --cov=analyzer --cov-report=term-missing tests\"\n",
      "\n",
      "  # Сборка и загрузка Docker-образа с приложением\n",
      "  publish:\n",
      "    # Выполняется только если изменения попали в ветку master\n",
      "    if: github.event_name == 'push' && github.ref == 'refs/heads/master'\n",
      "    # Требует, чтобы задача test была выполнена успешно\n",
      "    needs: test\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "      - uses: actions/checkout@v2\n",
      "      - name: sdist\n",
      "        uses: docker://snakepacker/python:all\n",
      "        with:\n",
      "          args: make sdist\n",
      "\n",
      "      - name: build-push\n",
      "        uses: docker/build-push-action@v1\n",
      "        with:\n",
      "          username: ${{ secrets.REGISTRY_LOGIN }}\n",
      "          password: ${{ secrets.REGISTRY_TOKEN }}\n",
      "          repository: alvassin/backendschool2019\n",
      "          target: api\n",
      "          tags: 0.0.1, latest\n",
      "\n",
      "\r\n",
      "Теперь при добавлении изменений в master во вкладке Actions на GitHub можно увидеть запуск тестов, сборку и загрузку Docker-образа:\n",
      "\n",
      "\n",
      "\r\n",
      "А при создании пул-реквеста в master-ветку в нем также будут отображаться результаты выполнения задачи test:\n",
      "\n",
      "\n",
      "\n",
      "Деплой\r\n",
      "Чтобы развернуть приложение на предоставленном сервере, нужно установить Docker, Docker Compose, запустить контейнеры с приложением и PostgreSQL и применить миграции.\n",
      "\r\n",
      "Эти шаги можно автоматизировать с помощью системы управления конфигурациями Ansible. Она написана на Python, не требует специальных агентов (подключается прямо по ssh), использует jinja-шаблоны и позволяет декларативно описывать желаемое состояние в YAML-файлах. Декларативный подход позволяет не задумываться о текущем состоянии системы и действиях, необходимых, чтобы привести систему к желаемому состоянию. Вся эта работа ложится на плечи модулей Ansible.\n",
      "\r\n",
      "Ansible позволяет сгруппировать логически связанные задачи в роли и затем переиспользовать. Нам потребуются две роли: docker (устанавливает и настраивает Docker) и analyzer (устанавливает и настраивает приложение).\n",
      "\n",
      "Роль docker добавляет в систему репозиторий с Docker, устанавливает и настраивает пакеты docker-ce и docker-compose.\n",
      "\r\n",
      "Опционально можно наладить автоматическое возобновление работы REST API после перезагрузки сервера. Ubuntu позволяет решить эту задачу силами системы инициализации systemd. Она управляет юнитами, представляющими собой различные ресурсы (демоны, сокеты, точки монтирования и другие). Чтобы добавить новый юнит в systemd, необходимо описать его конфигурацию в отдельном файле .service и разместить этот файл в одной из специальных папок, например в /etc/systemd/system. Затем юнит можно запустить, а также включить для него автозагрузку.\n",
      "\r\n",
      "Пакет docker-ce при установке автоматически создаст файл с конфигурацией юнита — необходимо только убедиться, что он запущен и включается при запуске системы. Для Docker Compose файл конфигурации docker-compose@.service будет создан силами Ansible. Символ @ в названии указывает systemd, что юнит является шаблоном. Это позволяет запускать сервис docker-compose с параметром — например, с названием нашего сервиса, который будет подставлен вместо %i в файле конфигурации юнита:\n",
      "\n",
      "[Unit]\n",
      "Description=%i service with docker compose\n",
      "Requires=docker.service\n",
      "After=docker.service\n",
      "\n",
      "[Service]\n",
      "Type=oneshot\n",
      "RemainAfterExit=true\n",
      "WorkingDirectory=/etc/docker/compose/%i\n",
      "ExecStart=/usr/local/bin/docker-compose up -d --remove-orphans\n",
      "ExecStop=/usr/local/bin/docker-compose down\n",
      "\n",
      "[Install]\n",
      "WantedBy=multi-user.target\n",
      "Роль analyzer сгенерирует из шаблона файл docker-compose.yml по адресу /etc/docker/compose/analyzer, зарегистрирует приложение как автоматически запускаемый сервис в systemd и применит миграции. Когда роли готовы, необходимо описать playbook.\n",
      "\n",
      "---\n",
      "\n",
      "- name: Gathering facts\n",
      "  hosts: all\n",
      "  become: yes\n",
      "  gather_facts: yes\n",
      "\n",
      "- name: Install docker\n",
      "  hosts: docker\n",
      "  become: yes\n",
      "  gather_facts: no\n",
      "  roles:\n",
      "    - docker\n",
      "\n",
      "- name: Install analyzer\n",
      "  hosts: api\n",
      "  become: yes\n",
      "  gather_facts: no\n",
      "  roles:\n",
      "    - analyzer\r\n",
      "Список хостов, а также переменные, использованные в ролях, можно указать в inventory-файле hosts.ini.\n",
      "\n",
      "[api]\n",
      "# Хосты, на которые Ansible задеплоит проект.\n",
      "# Необходимо поменять на свои.\n",
      "1.2.3.4\n",
      "\n",
      "[docker:children]\n",
      "api\n",
      "\n",
      "[api:vars]\n",
      "analyzer_image = alvassin/backendschool2019\n",
      "analyzer_pg_user = user\n",
      "analyzer_pg_password = hackme\n",
      "analyzer_pg_dbname = analyzer\r\n",
      "После того, как все файлы Ansible будут готовы, запустим его:\n",
      "\n",
      "$ ansible-playbook -i hosts.ini deploy.yml\n",
      "\n",
      "Про нагрузочное тестирование\n",
      "Итак, приложение покрыто тестами, развернуто и готово к эксплуатации. Для полноты картины на минутку вспомним, что поводом для построения сервиса когда-то было техническое задание. В нем были указаны ограничения: на выгрузке с десятью тысячами жителей, из которых тысяча — родственники первого порядка, каждый обработчик должен обрабатывать запрос менее чем за 10 секунд. Безусловно, такое тестирование целесообразно производить именно на конечном сервере (а, скажем, не на CI-сервере): результаты тестирования напрямую зависят от конфигурации сервера и количества доступных ресурсов.\n",
      "\r\n",
      "Допустим, мы сгенерировали выгрузку с жителями, вызвали друг за другом все обработчики, каждый из них отработал менее чем за 10 секунд. Достаточно ли этого? Можно предположить, что скорость обработки данных будет деградировать при увеличении количества данных, загружаемых в сервис. Важно понимать, сколько выгрузок сможет обработать сервис, прежде чем обработчики перестанут укладываться в ограничения.\n",
      "\r\n",
      "Хоть для тестирования данного сервиса и не требуется генерировать высокий RPS, его нагрузочное тестирование имеет свою особенность: использовать статический набор запросов не получится. Например, чтобы получить список жителей, необходимо иметь идентификатор выгрузки import_id, который возвращается обработчиком POST /imports и может оказаться любым целым числом. Этот подход называется тестированием по сценарию. \n",
      "\r\n",
      "Учитывая, что генерация данных уже реализована на Python 3, я решил воспользоваться фреймворком Locust. \n",
      "\r\n",
      "Чтобы выполнить нагрузочное тестирование, необходимо описать сценарий в файле locustfile.py и запустить модуль командой locust. Затем результаты тестирования можно наблюдать на графиках в веб-интерфейсе или таблице результатов в консоли.\n",
      "\r\n",
      "Графики Locust показывают общую информацию. Мне было интересно узнать, на каком раунде сервис не уложится в таймаут. Я добавил переменную с номером текущей \r\n",
      "итерации self.round и логивание каждого запроса с указанием итерации тестирования и времени выполнения.\n",
      "\n",
      "\n",
      "Описываем сценарий в файле locustfile.py\n",
      "# locustfile.py\n",
      "import logging\n",
      "from http import HTTPStatus\n",
      "\n",
      "from locust import HttpLocust, constant, task, TaskSet\n",
      "from locust.exception import RescheduleTask\n",
      "\n",
      "from analyzer.api.handlers import (\n",
      "    CitizenBirthdaysView, CitizensView, CitizenView, TownAgeStatView\n",
      ")\n",
      "from analyzer.utils.testing import generate_citizen, generate_citizens, url_for\n",
      "\n",
      "\n",
      "class AnalyzerTaskSet(TaskSet):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self.round = 0\n",
      "\n",
      "    def make_dataset(self):\n",
      "        citizens = [\n",
      "            # Первого жителя создаем с родственником. В запросе к\n",
      "            # PATCH-обработчику список relatives будет содержать только другого\n",
      "            # жителя, что потребует выполнения максимального кол-ва запросов\n",
      "            # (как на добавление новой родственной связи, так и на удаление\n",
      "            # существующей).\n",
      "            generate_citizen(citizen_id=1, relatives=[2]),\n",
      "            generate_citizen(citizen_id=2, relatives=[1]),\n",
      "            *generate_citizens(citizens_num=9998, relations_num=1000,\n",
      "                               start_citizen_id=3)\n",
      "        ]\n",
      "        return {citizen['citizen_id']: citizen for citizen in citizens}\n",
      "\n",
      "    def request(self, method, path, expected_status, **kwargs):\n",
      "        with self.client.request(\n",
      "                method, path, catch_response=True, **kwargs\n",
      "        ) as resp:\n",
      "            if resp.status_code != expected_status:\n",
      "                resp.failure(f'expected status {expected_status}, '\n",
      "                             f'got {resp.status_code}')\n",
      "            logging.info(\n",
      "                'round %r: %s %s, http status %d (expected %d), took %rs',\n",
      "                self.round, method, path, resp.status_code, expected_status,\n",
      "                resp.elapsed.total_seconds()\n",
      "            )\n",
      "            return resp\n",
      "\n",
      "    def create_import(self, dataset):\n",
      "        resp = self.request('POST', '/imports', HTTPStatus.CREATED,\n",
      "                            json={'citizens': list(dataset.values())})\n",
      "        if resp.status_code != HTTPStatus.CREATED:\n",
      "            raise RescheduleTask\n",
      "        return resp.json()['data']['import_id']\n",
      "\n",
      "    def get_citizens(self, import_id):\n",
      "        url = url_for(CitizensView.URL_PATH, import_id=import_id)\n",
      "        self.request('GET', url, HTTPStatus.OK,\n",
      "                     name='/imports/{import_id}/citizens')\n",
      "\n",
      "    def update_citizen(self, import_id):\n",
      "        url = url_for(CitizenView.URL_PATH, import_id=import_id, citizen_id=1)\n",
      "        self.request('PATCH', url, HTTPStatus.OK,\n",
      "                     name='/imports/{import_id}/citizens/{citizen_id}',\n",
      "                     json={'relatives': [i for i in range(3, 10)]})\n",
      "\n",
      "    def get_birthdays(self, import_id):\n",
      "        url = url_for(CitizenBirthdaysView.URL_PATH, import_id=import_id)\n",
      "        self.request('GET', url, HTTPStatus.OK,\n",
      "                     name='/imports/{import_id}/citizens/birthdays')\n",
      "\n",
      "    def get_town_stats(self, import_id):\n",
      "        url = url_for(TownAgeStatView.URL_PATH, import_id=import_id)\n",
      "        self.request('GET', url, HTTPStatus.OK,\n",
      "                     name='/imports/{import_id}/towns/stat/percentile/age')\n",
      "\n",
      "    @task\n",
      "    def workflow(self):\n",
      "        self.round += 1\n",
      "        dataset = self.make_dataset()\n",
      "\n",
      "        import_id = self.create_import(dataset)\n",
      "        self.get_citizens(import_id)\n",
      "        self.update_citizen(import_id)\n",
      "        self.get_birthdays(import_id)\n",
      "        self.get_town_stats(import_id)\n",
      "\n",
      "\n",
      "class WebsiteUser(HttpLocust):\n",
      "    task_set = AnalyzerTaskSet\n",
      "    wait_time = constant(1)\n",
      "\r\n",
      "Выполнив 100 итераций c максимальными выгрузками, я убедился, что время работы всех обработчиков укладывается в ограничения:\n",
      "\n",
      "\n",
      "\r\n",
      "Как видно на графике распределения времени ответов обработчиков, скорость обработки запросов почти не деградирует с ростом количества данных (желтый — 95 перцентиль, зеленый — медиана). Даже со ста выгрузками сервис будет работать эффективно.\n",
      "\n",
      "\n",
      "\r\n",
      "На графиках потребления ресурсов виден всплеск — установка приложения с помощью Ansible и далее ровное потребление ресурсов с ~20.15 до ~20.30 под нагрузкой от Locust.\n",
      "\n",
      "\n",
      "\n",
      "Что еще можно сделать?\r\n",
      "Профилирование приложения показало, что около четверти всего времени выполнения запросов уходит на сериализацию и десериализацию JSON: данных, отправляемых и получаемых из сервиса, достаточно много. Эти процессы можно существенно ускорить с помощью библиотеки orjson, но сервис придется немного подготовить — orjson не является drop-in-заменой для стандартного модуля json \n",
      "\r\n",
      "Обычно для продакшена требуется несколько копий сервиса, чтобы обеспечить отказоустойчивость и справиться с нагрузкой. Для управления группой сервисов нужен инструмент, показывающий, «жива» ли копия сервиса. Решить эту задачу можно обработчиком /health, который опрашивает все требуемые для работы ресурсы, в нашем случае — базу данных. Если SELECT 1 выполняется меньше чем за секунду, то сервис жив. Если нет — нужно обратить на него внимание.\n",
      "\r\n",
      "Когда приложение очень интенсивно работает с сетью, uvloop может здорово увеличить производительность.\n",
      "\r\n",
      "Немаловажным фактором является и читабельность кода. Один мой коллега, Юрий Шиканов, написал объединяющий несколько инструментов модуль gray для автоматической проверки и оформления кода, который легко добавить в pre-commit Git-хук, настроить одним файлом конфигурации или переменными окружения. Gray позволяет сортировать импорты (isort), оптимизирует выражения python в соответствии с новыми версиями языка (pyupgrade), добавляет запятые в конце вызовов функций, импортов, списков и т. д. (add-trailing-comma), а также приводит кавычки к единому виду (unify).\n",
      "\n",
      "* * *\r\n",
      "На этом у меня все: мы разработали, покрыли тестами, собрали и развернули сервис, а также провели нагрузочное тестирование.\n",
      "\n",
      "Благодарности\r\n",
      "Я хотел бы выразить огромную благодарность ребятам, которые нашли время принять участие в написании этой статьи, поревьювить код, внести свои идеи и замечания: Марии Зеленовой zelma, Владимиру Соломатину leenr, Анастасии Семёновой morkov, Юрию Шиканову dizballanze, Михаилу Шушпанову mishush, Павлу Мосеину pavkazzz и особенно Дмитрию Орлову orlovdl.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      "Привет, Хабр. Перевод этой статьи занял намного больше времени, чем ожидалось. Мне очень хотелось сделать всё качественно и без обмана, но если найдёте неточности, буду рад услышать о них. Также я буду сам перечитывать и исправлять ошибки предыдущих статей, если где-то оказался не прав. Мне предстоит перевести ещё около 4-5 статей такого объёма, поэтому прошу оценить мой труд, если вам понравилось. \n",
      "\n",
      "Оглавление\n",
      "\n",
      "1 часть\n",
      "2 часть\n",
      "\n",
      "Компиляция исходного кода Python\r\n",
      "Python обычно не рассматривается как компилируемый язык, но на самом деле он является таковым. Во время компиляции исходный код, написанный на Python, преобразуется в байт-код, который потом выполняется виртуальной машиной. Однако, процесс компиляции в Python является довольно простым и не включает в себя множество сложных этапов. Он состоит из следующих шагов в указанном порядке:\n",
      "\n",
      "\n",
      "Преобразование исходного python кода в «парсинговые» деревья.\n",
      "Преобразование «парсинговых» деревьев в абстрактные синтаксические деревья (AST).\n",
      "Генерация таблицы символов.\n",
      "Создание объектов кода из AST. Этот шаг включает в себя:\n",
      "\n",
      "Преобразование AST в граф потока управления.\n",
      "Получение объекта кода из графа потока управления.\n",
      "\n",
      "\r\n",
      "Создание «парсинговых» деревьев и их преобразование в AST является стандартным процессом. Python не вносит в него каких-либо сложных нюансов, поэтому в этой главе основное внимание уделяется преобразованию AST в граф потока управления и получению из этого графа объектов кода. Для тех, кто заинтересован в парсинговых деревях и генерации AST, существует «драконья» книга, которая даёт более углубленный tour de force [прим. с французского: «Большое усилие»] по обоим из этих тем.\n",
      "\n",
      "От исходников к дереву парсинга\r\n",
      "Парсер Python — это синтаксический анализатор LL (1), он основывается на принципах, которые изложены в «драконьей» книге. Модуль Grammar/Grammar содержит расширенную форму Бэкуса — Наура (Extended Backus-Naur Form (EBNF)) со спецификацией грамматики языка Python. Отрывок этой спецификации показан в листинге 3.0.\n",
      "\n",
      " 1  stmt: simple_stmt | compound_stmt\n",
      " 2  simple_stmt: small_stmt (';' small_stmt)* [';'] NEWLINE\n",
      " 3  small_stmt: (expr_stmt | del_stmt | pass_stmt | flow_stmt |\n",
      " 4          import_stmt | global_stmt | nonlocal_stmt | assert_stmt)\n",
      " 5  expr_stmt: testlist_star_expr (augassign (yield_expr|testlist) |\n",
      " 6                  ('=' (yield_expr|testlist_star_expr))*)\n",
      " 7  testlist_star_expr: (test|star_expr) (',' (test|star_expr))* [',']\n",
      " 8  augassign: ('+=' | '-=' | '*=' | '@=' | '/=' | '%=' | '&=' | '|=' | '^=' \n",
      " 9          | '<<=' | '>>=' | '**=' | '//=')\n",
      "10      \n",
      "11  del_stmt: 'del' exprlist\n",
      "12  pass_stmt: 'pass'\n",
      "13  flow_stmt: break_stmt | continue_stmt | return_stmt | raise_stmt | \n",
      "14          yield_stmt\n",
      "15  break_stmt: 'break'\n",
      "16  continue_stmt: 'continue'\n",
      "17  return_stmt: 'return' [testlist]\n",
      "18  yield_stmt: yield_expr\n",
      "19  raise_stmt: 'raise' [test ['from' test]]\n",
      "20  import_stmt: import_name | import_from\n",
      "21  import_name: 'import' dotted_as_names\n",
      "22  import_from: ('from' (('.' | '...')* dotted_name | ('.' | '...')+)\n",
      "23          'import' ('*' | '(' import_as_names ')' | import_as_names))\n",
      "24  import_as_name: NAME ['as' NAME]\n",
      "25  dotted_as_name: dotted_name ['as' NAME]\n",
      "26  import_as_names: import_as_name (',' import_as_name)* [',']\n",
      "27  dotted_as_names: dotted_as_name (',' dotted_as_name)*\n",
      "28  dotted_name: NAME ('.' NAME)*\n",
      "29  global_stmt: 'global' NAME (',' NAME)*\n",
      "30  nonlocal_stmt: 'nonlocal' NAME (',' NAME)*\n",
      "31  assert_stmt: 'assert' test [',' test]\n",
      "32     \n",
      "33  ...\n",
      "Листинг 3.0. Отрывок синтаксиса BNF в Python\n",
      "\r\n",
      "При выполнении модуля, переданного интерпретатору в командной строке, совершается вызов PyParser_ParseFileObject. Эта функция инициирует синтаксический анализ файла. Она же вызывает функцию токенизации PyTokenizer_FromFile, передавая ей имя файла-модуля в качестве аргумента. Функция токенизации в Python разбивает содержимое модуля на правильные токены или же выдает исключение при обнаружении недопустимых.\n",
      "\n",
      "Python токены\r\n",
      "Исходный код Python состоит из токенов. Например, слово return является ключевым токеном, а литерал — 2 числовым и так далее. Первая задача при синтаксическом анализе состоит в том, чтобы токенизировать исходный код, разбив его на токены-компоненты. В Python есть несколько видов токенов:\n",
      "\n",
      "\n",
      "Идентификаторы — это имена, которые определены программистом. Они включают имена функций, переменных, классов и т.д. Они должны соответствовать правилам именования идентификаторов, указанным в документации python.\n",
      "Операторы — это специальные символы, такие как: + и *, которые работают со значениями данных и возвращают какой-то результат.\n",
      "Ограничители — эти символы служат для группировки выражений, пунктуации и присваивания. Члены этой категории включают в себя: (, ), {,}, =, *= и т.д.\n",
      "Литералы — это символы, которые обеспечивают постоянное значение для некоторого типа. У нас есть строковые и байтовые литералы, такие как «Fred» и b«Fred», числовые литералы, которые включают: целочисленные литералы, такие как 2, литералы с плавающей запятой: 1e100 и мнимые литералы: 10j.\n",
      "Комментарии — это строковые литералы, которые начинаются с символа #. Токены комментариев всегда заканчиваются «физическим» концом строки.\n",
      "NEWLINE — это специальный токен, который обозначает конец «логической» строки.\n",
      "INDENT и DEDENT — эти токены используются для представления уровней отступов, которые группируют инструкции.\n",
      "\r\n",
      "Группа токенов, отделённая NEWLINE, образует логическую линию, которая соотносится с python инструкциями. То есть, мы можем сказать, что python-программа состоит из последовательности логических строк, каждая из которых отделена токеном NEWLINE. Каждая из этих логических строк состоит из нескольких физических, которые в свою очередь, оканчиваются символом перевода строки. Но в Python логические строки чаще всего совпадают с физическими, поэтому в большинстве случаев мы можем сказать, что логические строки разделяются символами конца строки. Составные инструкции могут занимать несколько физических строк, как это показано на рисунке 3.0. Логические строки могут быть образованы неявно (через заключение выражения в круглые, квадратные или фигурные скобки) или явно, с помощью символа обратной косой черты. Отступ также играет центральную роль в группировке операторов Python. Таким образом, одна из строк в грамматике питона может выглядеть так: suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT. Как следствие, одна из основных задач токенайзера Python заключается в генерировании токенов отступа и обратного-отступа, которые добавляются в дерево парсинга. Токенайзер использует стек для отслеживания отступов и использует алгоритм из листинга 3.1\n",
      "\n",
      "Инициируйте стек отступов значением 0.\n",
      " Для каждой логической строки с учетом объединения строк:\n",
      "   A. Если отступ текущей строки больше\n",
      "   отступа в верхней части стека\n",
      "       1. Добавьте отступ текущей строки в верхнюю часть стека.\n",
      "       2. Сгенерируйте токен INDENT.\n",
      "   B. Если отступ текущей строки меньше отступа\n",
      "    в верхней части стека\n",
      "       1. Если в стеке\n",
      "        нет уровня отступа, соответствующего отступу текущей строки, сообщите об ошибке.\n",
      "       2.  Для каждого значения в верхней части стека, которое не равно\n",
      "       отступу текущей строки.\n",
      "           a.  Удалить значение из верхней части стека.\n",
      "           b.  Создайте токен DEDENT.\n",
      "   C.  Токенизация текущей строки.\n",
      "Для каждого отступа в стеке, кроме 0, создайте токен DEDENT.\n",
      "Листинг 3.1: Python алгоритм для генерации токенов INDENT и DEDENT.\n",
      "\r\n",
      "Функция PyTokenizer_FromFile из модуля Parser/parsetok.c сканирует исходный Python файл слева-направо и сверху-вниз производя токенизацию содержимого. Символы пробелов (отличные от терминаторов) служат для разграничения токенов, но не являются обязательными. Там, где есть некоторая двусмысленность, такая как: 2+2, токенайзер пытается выделить максимально длинную строку, которая формирует легальный токен при чтении справа налево. В данном примере токенами являются литерал 2, оператор + и ещё один литерал 2.\n",
      "\r\n",
      "Сгенерированные токены передаются парсеру, который пытается построить из них дерево «парсинга» (в соответствии с грамматикой python, подмножество которой указано в листинге 3.0). Когда анализатор обнаруживает токен, нарушающий грамматику, генерируется исключение SyntaxError. Модуль parser предоставляет ограниченный доступ к дереву конкретного кода Python и используется в листинге 3.2 для получения примера синтаксического дерева.\n",
      "\n",
      " 1  >>>code_str = \"\"\"def hello_world():\n",
      " 2                      return 'hello world'\n",
      " 3                \"\"\"\n",
      " 4  >>> import parser\n",
      " 5  >>> from pprint import pprint \n",
      " 6  >>> st = parser.suite(code_str)\n",
      " 7  >>> pprint(parser.st2list(st))\n",
      " 8  [257,\n",
      " 9  [269,\n",
      "10  [294,\n",
      "11  [263,\n",
      "12      [1, 'def'],\n",
      "13      [1, 'hello_world'],\n",
      "14      [264, [7, '('], [8, ')']],\n",
      "15      [11, ':'],\n",
      "16      [303,\n",
      "17      [4, ''],\n",
      "18      [5, ''],\n",
      "19      [269,\n",
      "20      [270,\n",
      "21      [271,\n",
      "22          [277,\n",
      "23          [280,\n",
      "24          [1, 'return'],\n",
      "25          [330,\n",
      "26          [304,\n",
      "27              [308,\n",
      "28              [309,\n",
      "29              [310,\n",
      "30              [311,\n",
      "31                  [314,\n",
      "32                  [315,\n",
      "33                  [316,\n",
      "34                  [317,\n",
      "35                      [318,\n",
      "36                      [319,\n",
      "37                      [320,\n",
      "38                      [321,\n",
      "39                          [322, [323, [3, '\"hello world\"']]]]]]]]]]]]]]]]]]]],\n",
      "40      [4, '']]],\n",
      "41      [6, '']]]]],\n",
      "42  [4, ''],\n",
      "43  [0, '']]\n",
      "44  >>>  \n",
      "Листинг 3.2. Использование модуля parser для получения дерева в Python\n",
      "\r\n",
      "Вызов parser.suite(source) в листинге 3.2 возвращает объект дерева (ST), который является промежуточным представлением дерева парсинга из исходного кода (предполагается, что исходный код синтаксически корректен). Вызов parser.st2list возвращает фактическое синтаксическое дерево, представленное в виде списка python. Первые элементы в списках — целые числа, которое идентифицируют продукционные правила в грамматике Python.\n",
      "\n",
      "\n",
      "Если я ещё сам правильно понял термин production rules...\n",
      "Продукционных правило в информатике — специальное правило, определяющее рекурсивную замену одних символов на другие.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Рисунок 3.0: Дерево парсинга для листинга 3.2 (функция, возвращающая строку 'hello world')\n",
      "\r\n",
      "Рисунок 3.0 — это древовидная диаграмма, показывающая то же дерево из листинга 3.2, но уже с некоторыми удаленными токенами, а также часть команд была заменена строковыми описаниями в соответствии числовым номерам. Все эти продукционные правила указаны в заголовочных файлах Include/token.h и Include/graminit.h.\n",
      "\r\n",
      "В виртуальной машине CPython для представления дерева парсинга используется древовидная структура данных. Каждое продукционное правило — это узел в ней. Структура данных нода (узла) из Include/node.h показана в листинге 3.3.\n",
      "\n",
      "1  typedef struct _node {\n",
      "2      short n_type;\n",
      "3      char\t*n_str;\n",
      "4      int n_lineno;\n",
      "5      int n_col_offset;\n",
      "6      int n_nchildren;\n",
      "7      struct _node *n_child;\n",
      "8  } node;\n",
      "Листинг 3.3: Структура данных нода в виртуальной машине\n",
      "\r\n",
      "По мере обхода дерева парсинга узлы могут запрашиваться по их типу, дочерним элементам (если таковые имеются), номеру строки исходного файла (которая привела к созданию данного узла) и так далее. Макросы для взаимодействия с узлами дерева парсинга также определены в файле Include/node.h.\n",
      "\n",
      "От дерева парсинга к абстрактному синтаксическому дереву\r\n",
      "Следующим этапом процесса компиляции в python является преобразование деревьев парсинга в абстрактное синтаксическое дерево (AST). Абстрактное синтаксическое дерево является представлением кода, которое не зависит от тонкостей синтаксиса python. Например, дерево парсинга на рисунке 3.0 содержит узел «двоеточие» [прим. двоеточие объявления функции], потому что это синтаксическая конструкция, но, как показано в листинге 3.4, AST не будет содержать таких «подробностей».\n",
      "\n",
      "1  >>> import ast\n",
      "2  >>> import pprint\n",
      "3  >>> node = ast.parse(code_str, mode=\"exec\")\n",
      "4  >>> ast.dump(node)\n",
      "5  (\"Module(body=[FunctionDef(name='hello_world', args=arguments(args=[], \"\n",
      "6  'vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), '\n",
      "7  \"body=[Return(value=Str(s='hello world'))], decorator_list=[], \"\n",
      "8  'returns=None)])')\n",
      "Листинг 3.4. Использование astмодуля для управления AST исходного кода Python\n",
      "\r\n",
      "Различные определения узлов AST находятся в файле Parser/Python.asdl. Большинство определений в AST соответствуют конкретной исходной конструкции, такой как оператор if или поиск атрибута. Модуль ast вместе с Python-интерпретатором дает нам возможность манипулировать AST. Такие инструменты как codegen могут по конкретному AST вывести исходный код python. В реализации CPython AST-узлы представлены C-структурами, которые определены в Include/Python-ast.h. Эти структуры фактически генерируются кодом Python. Модуль Parser/asdl_c.py генерирует этот файл из AST определения ASDL. Например, вот кусок из определения нода statement, который показан в листинге 3.5.\n",
      "\n",
      " 1  struct _stmt {\n",
      " 2      enum _stmt_kind kind;\n",
      " 3      union {\n",
      " 4          struct {\n",
      " 5              identifier name;\n",
      " 6              arguments_ty args;\n",
      " 7              asdl_seq *body;\n",
      " 8              asdl_seq *decorator_list;\n",
      " 9              expr_ty returns;\n",
      "10          } FunctionDef;\n",
      "11 \n",
      "12          struct {\n",
      "13              identifier name;\n",
      "14              arguments_ty args;\n",
      "15              asdl_seq *body;\n",
      "16              asdl_seq *decorator_list;\n",
      "17              expr_ty returns;\n",
      "18          } AsyncFunctionDef;\n",
      "19 \n",
      "20          struct {\n",
      "21              identifier name;\n",
      "22              asdl_seq *bases;\n",
      "23              asdl_seq *keywords;\n",
      "24              asdl_seq *body;\n",
      "25              asdl_seq *decorator_list;\n",
      "26          } ClassDef;\n",
      "27          ...\n",
      "28      }v;\n",
      "29      int lineno;\n",
      "30      int col_offset\n",
      "31  }\n",
      "Union в листинге 3.5 — это ключевое слово языка C, которое используется для создания атрибута, который может принимать один из любых типов, перечисленных в самом union. Функция PyAST_FromNode в модуле Python/ast.c отвечает за генерацию AST из данного дерева парсинга. Теперь пришло время создавать байт-код из сгенерированного AST.\n",
      "\n",
      "Построение таблицы символов\r\n",
      "Следующим шагом после создания AST является генерация таблицы символов. Таблица символов, как и предполагает название, представляет собой набор имен, используемый в блоке кода и другом контекст. Процесс построения таблицы символов включает в себя анализ имен (содержащихся в блоке кода) и присвоение таким именам правильной области видимости. Возможно, прежде чем обсуждать тонкости построения таблиц символов, стоит повторить имена и связывания (binding) в python.\n",
      "Имена и Связывания\r\n",
      "В python на объекты ссылаются по именам. «Names» похожи на переменные в C++ или Java, но это не совсем так.\n",
      "\n",
      ">>> x = 5\r\n",
      "В приведенном выше примере x — это имя, которое ссылается на объект: 5. Процесс присвоения ссылки на значение 5 к x называется биндингом. Связывание приводит к тому, что имя начинает ссылаться на объект, расположенный внутри самой вложенной области видимости текущей исполняемой программы. Связывание происходит во многих случаях, например, при присваивании переменной, функции, а также метода, когда переданный параметр привязан к аргументу и т.д. Важно отметить, что имена — это просто символы и они не имеют типа, который ассоциируется с ними. Тип существует у самих объектов, на которые эти имена ссылаются.\n",
      "\n",
      "Блоки кода\r\n",
      "Блоки кода являются центральной частью для Python программы и их понимание имеет первостепенное значение для изучения внутреннего устройства виртуальной машины Python. Блок кода — это фрагмент программного кода, который выполняется в Python как единое целое. Примерами блоков кода являются модули, функции и классы. Команды, введенные в интерактивном режиме в REPL, команды сценария запускаемые с флагом -c, также являются блоками кода. Блок кода имеет несколько пространств имен, связанных с ним. Например, блок кода модуля имеет доступ к пространству имен global, а блок кода функции имеет также доступ к пространству имен local, по-мимо пространства global.\n",
      "\n",
      "Пространства имен\r\n",
      "Пространство имен (namespace) является контекстом, в котором какой-то набор имен связан с объектами. Пространства имен Python реализованы в виде словаря. Встроенное пространство имен является примером namespace, которое содержит все встроенные функции и доступ к нему можно получить через ввод __builtins__.__dict__ в терминале (выходной текст будет довольно большим). Интерпретатор имеет доступ к нескольким пространствам имен, в том числе к глобальному, встроенному и локальному. Данные в namespace создаются в разное время и также имеют разное время жизни. Например, новое локальное пространство имен создается при вызове функции и удаляется при её окончании или выходе из функции. Глобальное пространство имен создаётся в начале выполнения модуля и все имена, определенные в этом namespace, доступны во всём модуле. Встроенная область видимости создаётся, когда вызывается интерпретатор и он уже содержит все встроенные имена. Эти три пространства имен являются основными namespace доступными интерпретатору.\n",
      "\n",
      "Области видимости\r\n",
      "Область видимости — это часть программы, в которой набор биндингов (пространство имен) виден и доступен напрямую без использования точечной нотации. [прим. кажется, здесь опечатка и имеется ввиду доступ через функцию globals()]. Во время выполнения программы могут быть доступны следующие области видимости:\n",
      "\n",
      "\n",
      "Самая «внутренняя» область видимости, содержащая локальные переменные\n",
      "При вложенных функциях, во вложенной функции можно получить доступ к пространству имён более «внешней».\n",
      "Глобальная область видимости текущего модуля\n",
      "Область видимости, содержащая встроенное пространство имен\n",
      "\r\n",
      "Когда в python упоминается имя, интерпретатор ищет пространство имен области видимости в порядке возрастания (как указано выше) и если имя не найдено ни в одном из пространств имен, возникает исключение. Python поддерживает статическую область видимости (также известную как лексическая область видимости). Это означает, что видимость биндингов имен может быть определена только путем проверки текста программы.\n",
      "\n",
      "Примечание\r\n",
      "В Python есть своеобразное правило для области видимости, которое предотвращает изменение ссылки на объект глобальной области видимости в локальной. Такое действие приведет к исключению UnboundLocalError. Следующий пример иллюстрирует это:\n",
      "\n",
      "    >>> a = 1\n",
      "    >>> def inc_a(): a += 2\n",
      "    ... \n",
      "    >>> inc_a()\n",
      "    Traceback (most recent call last):\n",
      "     File \"<stdin>\", line 1, in <module>\n",
      "     File \"<stdin>\", line 1, in inc_a\n",
      "    UnboundLocalError: local variable 'a' referenced before assignment\n",
      "Листинг A3.0. Попытка изменить глобальную переменную из функции\n",
      "\r\n",
      "Чтобы изменить «глобальный» объект в локальной области, необходимо использовать ключевое слово global с именем объекта:\n",
      "\n",
      ">>> a = 1\n",
      ">>> def inc_a():\n",
      "...     global a\n",
      "...     a += 1\n",
      "... \n",
      ">>> inc_a()\n",
      ">>> a\n",
      "2\n",
      "Листинг A3.1. Использование ключевого слова global для изменения глобальной переменной из функции\n",
      "\r\n",
      "Python также имеет ключевое слово nonlocal. Оно используется, когда необходимо изменить переменную, находящуюся во внешней, но «не глобальной» области видимости. Это очень удобно при работе с вложенными функциями (также называемыми замыканиями). Очень простая иллюстрация ключевого слова nonlocal показана в следующем фрагменте, который определяет простой счетчик для объекта:\n",
      "\n",
      ">>> def make_counter():\n",
      "...     count = 0\n",
      "...     def counter():\n",
      "...         nonlocal count #capture count binding from enclosing not global scope\n",
      "...         count += 1\n",
      "...         return count\n",
      "...     return counter\n",
      "... \n",
      ">>> counter_1 = make_counter()\n",
      ">>> counter_2 = make_counter()\n",
      ">>> counter_1()\n",
      "1\n",
      ">>> counter_1()\n",
      "2\n",
      ">>> counter_2()\n",
      "1\n",
      ">>> counter_2()\n",
      "2\n",
      "Листинг A3.2. Вложенные функции со счётчиком\n",
      "\r\n",
      "Последовательность вызовов функций run_mod -> PyAST_CompileObject -> PySymtable_BuildObject запускает процесс построения таблицы символов. Два аргумента функции PySymtable_BuildObject — это ранее сгенерированный AST и имя модуля. Алгоритм построения таблицы символов разбит на две части. В первой «посещается» каждый узел AST, чтобы создать коллекцию символов, используемых в AST. Очень простое описание этого процесса приведено в листинге 3.6, и термины, используемые в нем, станут более очевидными, когда мы обсудим структуры данных, используемые при построении таблицы символов.\n",
      "\n",
      "for каждый_узел in AST\n",
      "    if узел == начало_блока_кода:\n",
      "        1. Cоздайть новую запись в таблице символов и\n",
      "           установите это значение текущей таблице символов\n",
      "        2. Сделайте \"push\" новой таблицы в st_stack.\n",
      "        3. Добавьте новую таблицу символов в список\n",
      "           дочерних предыдущей таблицы.\n",
      "        4. Замените текущую таблицу только что созданной\n",
      "        5. for все_узлы in узлы_блока_кода:\n",
      "            a. Рекурсивно посетить каждый узел функцией\n",
      "               \"symtable_visit_XXX\", где \"XXX\" это тип узла.\n",
      "        6. Выйти из блока кода, через удаление текущей таблицы\n",
      "           символов из стека.\n",
      "        7. Сделайте \"pop\" следующей таблицы символов из стека\n",
      "           и обновите значение текующей таблицы символов этим значением.\n",
      "    else:\n",
      "        рекурсивно посетить узел и под-узел.\n",
      "Листинг 3.6. Создание таблицы символов из AST\n",
      "\r\n",
      "После первого прохода алгоритма таблица символов содержит все имена, которые использовались в модуле, но не содержит контекстной информации о них. Например, интерпретатор не может определить, является ли данная переменная глобальной, локальной или свободной. Вызов функции symtable_analyze из Parser/symtable.c инициирует вторую фазу генерации таблицы символов. Эта фаза алгоритма определяет область видимости (локальную, глобальную или свободную) для символов, полученных на первом этапе. Комментарии в файле Parser/symtable.c достаточно информативны и перефразированы ниже, чтобы дать представление о втором этапе процесса построения таблицы символов:\n",
      "\n",
      "\n",
      "Чтобы определить область видимости каждого имени необходимо два этапа. Первый собирает необработанные «факты» из AST через функции symtable_visit_ *, а второй анализирует эти «факты» во время прохода над объектами PySTEntryObject, созданными в первом этапе.\n",
      "Когда второй проход заходит внутрь функции, родительский элемент передает набор всех биндингов, видимых для его дочерних элементов. Эти связывания используются, что выяснить: являются ли non-local переменные свободными или неявными глобальными. Имена, которые явно объявлены non-local, должны существовать в этом наборе видимых имен — если их нет, возникает синтаксическая ошибка. После локального анализа функция анализирует каждый из своих дочерних блоков, используя обновленный набор биндингов.\n",
      "Есть также два вида глобальных переменных: явные и неявные. Явные глобальные переменные объявляются оператором global. Неявная глобальная переменная — это свободная переменная, для которой компилятор не нашел биндинга в локальной области видимости текующей функции. Неявные глобальные переменные является либо глобальным, либо встроенным.\r\n",
      "Модули Python и блоки классов используют опкоды xxx_NAME для обработки этих имен, чтобы реализовать слегка странную семантику. В таком блоке имя обрабатывается как глобальное, пока оно не будет «переприсвоено». После этого переменная трактуется как локальная.\n",
      "Потомки обновляют набор свободных переменных. Если локальная переменная добавляется в набор свободных переменных c пометкой «установлена дочерним элементом», то переменная помечается как ячейка. Объект функции должен обеспечивать runtime хранилище для переменной, которая может пережить фрейм функции. Поэтому переменные-ячейки удаляются из набора свободных переменных прежде, чем функция анализа возвращается к своему родителю.\n",
      "\r\n",
      "Комментарии в модуле пытаются объяснить процесс построения таблицы символов понятным языком, но есть некоторые запутанные моменты. Например: родитель передает набор всех биндингов, видимых его дочерним элементам — но на какого родителя и на какие дочерние элементы они конкретно ссылаются? Чтобы получить представление об этом, нам нужно взглянуть на структуры данных, которые используются в процессе создания таблицы символов.\n",
      "\n",
      "Структуры данных таблицы символов\r\n",
      "Существует две структуры данных, которые являются центральными для генерации таблицы символов:\n",
      "\n",
      "\n",
      "Структура данных таблицы символов.\n",
      "Структура данных записи таблицы символов.\n",
      "\r\n",
      "Структура данных таблицы символов приведена в листинге 3.7. Можно думать о ней, как о таблице состоящей из записей и содержащей информацию об именах, используемых в различных блоках кода данного модуля.\n",
      "\n",
      " 1  struct symtable {\n",
      " 2      PyObject *st_filename;          /* name of file being compiled */\n",
      " 3      struct _symtable_entry *st_cur; /* current symbol table entry */\n",
      " 4      struct _symtable_entry *st_top; /* symbol table entry for module */\n",
      " 5      PyObject *st_blocks;            /* dict: map AST node addresses\n",
      " 6                                      *       to symbol table entries */\n",
      " 7      PyObject *st_stack;             /*list: stack of namespace info */\n",
      " 8      PyObject *st_global;            /*borrowed ref to \n",
      " 9                                      st_top->ste_symbols*/\n",
      "10      int st_nblocks;                 /* number of blocks used. kept for\n",
      "11                                      consistency with the corresponding\n",
      "12                                      compiler structure */\n",
      "13      PyObject *st_private;           /* name of current class or NULL */\n",
      "14      PyFutureFeatures *st_future;    /* module's future features that \n",
      "15                                      affect the symbol table */\n",
      "16      int recursion_depth;            /* current recursion depth */\n",
      "17      int recursion_limit;            /* recursion limit */\n",
      "18  };\n",
      "Листинг 3.7. Структура данных таблицы символов.\n",
      "\r\n",
      "Модуль Python может содержать несколько блоков кода — например, несколько определений функций. Поле st_blocks является отображением всех существующих блоков кода в один элемент таблицы символов. Запись таблицы st_top — это таблица символов для компилируемого модуля (напомним, что модуль также является блоком кода), поэтому она будет содержать имена, определенные в глобальном пространстве имен модуля. Поле st_cur относится к записи таблицы символов для кодового блока, который обрабатывается в данный момент. Каждый блок кода внутри «блока кода модуля» имеет свою собственную запись таблицы символов, которая содержит символы, определенные в этом блоке кода.\n",
      "\n",
      "\n",
      "Рисунок 3.1: таблица символов и записи в ней.\n",
      "\r\n",
      "В очередной раз, просмотр структуры данных _symtable_entry из файла Include/symtable.h очень полезен, чтобы понять, как она работает. Данная структура данных показана в листинге 3.8.\n",
      "\n",
      " 1  typedef struct _symtable_entry {\n",
      " 2      PyObject_HEAD\n",
      " 3      PyObject *ste_id;               /* int: key in ste_table->st_blocks */\n",
      " 4      PyObject *ste_symbols;          /* dict: variable names to flags */\n",
      " 5      PyObject *ste_name;             /* string: name of current block */\n",
      " 6      PyObject *ste_varnames;         /* list of function parameters */\n",
      " 7      PyObject *ste_children;         /* list of child blocks */\n",
      " 8      PyObject *ste_directives;       /* locations of global and nonlocal \n",
      " 9                                       statements */\n",
      "10      _Py_block_ty ste_type;          /* module, class, or function */\n",
      "11      int ste_nested;                 /* true if block is nested */\n",
      "12      unsigned ste_free : 1;          /*true if block has free variables*/\n",
      "13      unsigned ste_child_free : 1;    /* true if a child block has free \n",
      "14                                      vars including free refs to globals*/\n",
      "15      unsigned ste_generator : 1;     /* true if namespace is a generator */\n",
      "16      unsigned ste_varargs : 1;       /* true if block has varargs */\n",
      "17      unsigned ste_varkeywords : 1;   /* true if block has varkeywords */\n",
      "18      unsigned ste_returns_value : 1; /* true if namespace uses return with\n",
      "19                                          an argument */\n",
      "20      unsigned ste_needs_class_closure : 1; /* for class scopes, true if a\n",
      "21                                              closure over __class__\n",
      "22                                              should be created */\n",
      "23      int ste_lineno;          /* first line of block */\n",
      "24      int ste_col_offset;      /* offset of first line of block */\n",
      "25      int ste_opt_lineno;      /* lineno of last exec or import * */\n",
      "26      int ste_opt_col_offset;  /* offset of last exec or import * */\n",
      "27      int ste_tmpname;         /* counter for listcomp temp vars */\n",
      "28      struct symtable *ste_table;\n",
      "29  } PySTEntryObject;\n",
      "Листинг 3.8. Структура данных _symtable_entry\n",
      "\r\n",
      "Комментарии в исходном коде объясняют, что делает каждое поле. Поле ste_symbols содержит отображение символов/имен, которые встречаются при анализе блока кода. Флаги, на которые отображаются символы, представляют собой числовые значения, которые дают нам информацию о контексте, в котором используется символ/имя. Например, символ может быть аргументом функции или определением глобального оператора. Некоторые примеры этих флагов, определенных в модуле Include/symtable.h, приведены в листинге 3.9.\n",
      "\n",
      "1  /* Flags for def-use information */\n",
      "2  #define DEF_GLOBAL 1           /* global stmt */\n",
      "3  #define DEF_LOCAL 2            /* assignment in code block */\n",
      "4  #define DEF_PARAM 2<<1         /* formal parameter */\n",
      "5  #define DEF_NONLOCAL 2<<2      /* nonlocal stmt */\n",
      "6  #define DEF_FREE 2<<4          /* name used but not defined in \n",
      "7                                    nested block */\n",
      "Листинг 3.9. Флаги, которые определяют контекст определения имени\n",
      "\r\n",
      "Возвратимся же к обсуждению таблиц символов. Предположим, что компилируется модуль, содержащий код из листинга 3.10. После того, как таблица символов построена, есть три записи таблицы символов.\n",
      "\n",
      "def make_counter():\n",
      "    count = 0\n",
      "    def counter():\n",
      "        nonlocal count \n",
      "        count += 1\n",
      "        return count\n",
      "    return counter\n",
      "Листинг 3.10. Простая функция Python\n",
      "\r\n",
      "Первая запись make_counter является замыканием модуля и будет определена областью видимости local. Следующая запись таблицы символов будет о том, что функция make_counter содержит имена count и counter, помеченные как локальные. Последняя запись таблицы символов будет о вложенной функции counter. Она будет иметь переменную count, помеченную как free. Следует отметить, что хоть make_counter и определена как локальная в записи таблицы символов, но она рассматривается как глобальная в самом блоке кода модуля, поскольку *st_global указывает на символы *st_top, которые в данном случае являются символами замыкающего модуля.\n",
      "\n",
      "От AST к объектам кода\r\n",
      "Следующим шагом для компилятора является генерация объектов кода из информации полученной благодаря AST и таблицам символов. Отвечающие за это функции, реализованы в модуле Python/compile.c. Процесс создания объектов кода является многоэтапным. На первом шаге AST преобразуется в базовые блоки инструкций байт-кода Python. Алгоритм преобразования похож на тот, который используется при генерации таблиц символов — функции с именами compiler_visit_xx (где xx этот тип узла) рекурсивно посещают каждый узел и генерируют базовые блоки инструкций байт-кода. Базовые блоки и связи между ними представляются в виде графа — графа потока управления [прим. именуемый также CFG — control flow graph]. Он показывает «пути» кода, которые могут быть использованы во время выполнения программы. На втором этапе сгенерированный граф потока управления «сглаживается» с использованием поиска по графу в глубину (DFS). После того как граф сглажен, рассчитывается смещения перехода и оно используется в качестве аргумента для инструкции jump байт-кода. Объекта кода генерируется из этого набора инструкций. Чтобы лучше разобраться в этом процессе, рассмотрим функцию fizzbuzz в листинге 3.11.\n",
      "\n",
      "1     def fizzbuzz(n):\n",
      "2         if n % 3 == 0 and n % 5 == 0:\n",
      "3             return 'FizzBuzz'\n",
      "4         elif n % 3 == 0:\n",
      "5             return 'Fizz'\n",
      "6         elif n % 5 == 0:\n",
      "7             return 'Buzz'\n",
      "8         else:\n",
      "9             return str(n)\n",
      "Листинг 3.11. Простая python функция\n",
      "\r\n",
      "AST для этой функции показан на рисунке 3.2.\n",
      "\n",
      "Рисунок 3.2: Очень простой AST для листинга 3.2\n",
      "\r\n",
      "Этот AST из рисунка 3.2 при компиляции в CFG возвращает граф, аналогичный показанному на рисунке 3.3. Пустые блоки на рисунке были опущены. Рассмотрение этого графа обеспечит некоторую информацию о том, что скрывается за базовыми блоками. Базовые блоки имеют одну точку входа, но могут иметь несколько выходов. Эти блоки описаны более подробно далее.\n",
      "\n",
      "Рисунок 3.3: Граф потока управления для функции fizzbuzz из листинга 3.11. Прямая линия представляет нормальное, прямолинейное выполнение кода, в то время как изогнутые линии представляют «прыжки».\n",
      "\r\n",
      "В следующие описания включены только фактические инструкции. Для некоторых инструкций нужны аргументы, но он были удалены, поскольку сейчас нас не интересуют.\n",
      "\n",
      "\n",
      "Блок 1 содержит инструкции, являющиеся узлом BoolOp в AST на рисунке 3.2. Инструкции в этом блоке реализуют операцию: n%3==0 and n%5==0, используя следующий набор из одиннадцати команд:\n",
      "\n",
      "LOAD_FAST               \n",
      "LOAD_CONST              \n",
      "BINARY_MODULO\n",
      "LOAD_CONST              \n",
      "COMPARE_OP              \n",
      "JUMP_IF_FALSE_OR_POP    \n",
      "LOAD_FAST               \n",
      "LOAD_CONST              \n",
      "BINARY_MODULO\n",
      "LOAD_CONST              \n",
      "COMPARE_OP\r\n",
      "Удивительно, но остальная часть узла if (фактический тест, который определяет, нужно ли выполнить код ниже) не включен в этот блок. Причина станет более понятной, когда мы обсудим второй блок. Как показано на рисунке 3.3, есть два способа выйти из этого блока: либо через прямое выполнение всех опкодов, либо путем перехода к блоку 2 через инструкцию JUMP_IF_FALSE_OR_POP.\n",
      "Блок 2 отображается на первый узел if, инкапсулирует тест if и последующий код. Второй блок содержит следующие четыре инструкции:\n",
      "\n",
      "POP_JUMP_IF_FALSE\n",
      "LOAD_CONST\n",
      "RETURN_VALUE\n",
      "JUMP_FORWARD\r\n",
      "Как мы увидим в следующих главах, когда интерпретатор выполняет инструкции байт-кода для оператора if, он производит считывание из стека значения и в зависимости от истинности данного объекта либо выполняет следующую инструкцию байт-кода, либо переходит к другой части набора инструкций и продолжает выполнение оттуда. Именно инструкция POP_JUMP_IF_FALSE отвечает за это. Данный опкод принимает аргумент, который указывает место назначения такого перехода. Можно задаться вопросом: почему инструкции для узла BoolOp и операторов if находятся в разных блоках? Чтобы понять это напомним, что python использует ленивые вычисления для логических операций. Таким образом, если значение n%3==0 равно false, то n%5==0 даже не будет вычислено. Посмотрев на инструкции из первого блока можно заметить инструкцию JUMP_IF_FALSE_OR_POP сразу после первого сравнения. Она как раз и является вариантом jump, а следовательно, нуждается «в цели». Задумайтесь об этом на секунду и потребность в разных блоках станет очевидной. JUMP_IF_FALSE_OR_POP нуждается в «цели», чтобы продолжить выполнение инструкций, когда первое логический выражение принимает значение «ложь» и из-за ленивых вычислений идёт переход к инструкции POP_JUMP_IF_FALSE в самом блоке if. Для того чтобы «прыжок» был возможен, мы оставляем инструкции тела if в другом блоке. Если все компоненты логического выражения оценены, то после выполнения инструкций в блоке BoolOp, вычисления продолжатся как обычно, по инструкциям в блоке if.\n",
      "\n",
      "Блок 3 соответствует первому узлу orElse в AST и содержит следующие 9 инструкций:\n",
      "\n",
      "LOAD_FAST\n",
      "LOAD_CONST\n",
      "BINARY_MODULO\n",
      "LOAD_CONST\n",
      "COMPARE_OP\n",
      "POP_JUMP_IF_FALSE\n",
      "LOAD_CONST\n",
      "return_value\n",
      "JUMP_FORWARD\r\n",
      "Заметьте, что здесь оператор elif, условие n%3==0, а также тело оператора находятся в одном блоке. Теперь легко понять, почему это так. Единственный вход в этот блок — через «прыжок» из первого if, а выход может быть выполнен либо с помощью инструкции возврата, либо также с помощью прыжка, если тест единственного условия в if не пройден.\n",
      "Блок 4 является зеркальным по отношению к блоку 3 с точки зрения инструкций, но аргументы к инструкциям отличаются.\n",
      "Блок 5 является отображением конечного узла orElse и содержит следующие 4 инструкции:\n",
      "\n",
      "LOAD_GLOBAL\n",
      "LOAD_FAST\n",
      "call_function\n",
      "return_value\r\n",
      "Функция LOAD_GLOBAL принимает классическую функцию str в качестве аргумента и загружает ее в стек значений. LOAD_FAST загружает аргумент n в стек, а return_value возвращает значение, полученное через CALL_FUNCTION т.е. через инструкцию str(n).\n",
      "\r\n",
      "Как и в предыдущем разделе, мы рассмотрим структуры данных, которые используются при построении базовых блоков, чтобы лучше понять данный процесс.\n",
      "\n",
      "Структура данных: compiler\r\n",
      "На рисунке 3.4 показана взаимосвязь между основными структурами данных, используемыми в процессе генерации базовых блоков, которые составляют граф потока управления.\n",
      "\n",
      "\n",
      "Рисунок 3.4: Четыре основные структуры данных, используемые при создании объекта кода.\n",
      "\r\n",
      "На самом верхнем уровне находится структура данных compiler, которая отвечает за глобальный процесс компиляции модуля. Эта структура данных определена в листинге 3.12.\n",
      "\n",
      " 1  struct compiler {\n",
      " 2      PyObject *c_filename;\n",
      " 3      struct symtable *c_st;\n",
      " 4      PyFutureFeatures *c_future; /* pointer to module's __future__ */\n",
      " 5      PyCompilerFlags *c_flags;\n",
      " 6          \n",
      " 7      int c_optimize;              /* optimization level */\n",
      " 8      int c_interactive;           /* true if in interactive mode */\n",
      " 9      int c_nestlevel;\n",
      "10          \n",
      "11      struct compiler_unit *u; /* compiler state for current block */\n",
      "12      PyObject *c_stack;           /* Python list holding compiler_unit \n",
      "13                                  ptrs */\n",
      "14      PyArena *c_arena;            /* pointer to memory allocation arena */\n",
      "15  };\n",
      "Листинг 3.12. Структура данных compiler\n",
      "\r\n",
      "Поля, которые представляют для нас здесь интерес, следующие:\n",
      "\n",
      "\n",
      "*c_st — ссылка на таблицу символов, созданную в предыдущем разделе.\n",
      "*u — ссылка на структуру данных compiler unit. Эта инкапсулированная информация необходима для работы с блоком кода. Данное поле указывает на compiler unit выполняемого сейчас блока кода.\n",
      "*c_stack — ссылка на стек структур данных compiler_unit. Когда блок кода является составным, это поле управляет сохранением и восстановлением структур данных compile_unit при обнаружении новых блоков. Когда происходит вхождение в новый блок кода, создаётся новая область видимости, а затем compiler_enter_scope() делает «push» текущего compiler_unit (*u) в стек *c_stack, а затем создает новый объект compiler_unit и устанавливает его в качестве текущего. Когда происходит выход из блока, совершается операция «pop» из стека *c_stack, тем самым идёт восстановление предыдущего состояния.\n",
      "\r\n",
      "Структура compiler инициализируется для каждого компилируемого модуля. В точности, как AST генерируется для каждого используемого модуля, также и структура compiler_unit создаётся для каждого блока кода в AST.\n",
      "\n",
      "Структура данных: compiler_unit\r\n",
      "Структура данных compiler_unit, показанная ниже в листинге 3.13, собирает информацию необходимую для генерации требуемых инструкций байт-кода в блоках кода. Большинство полей, определенных в compiler_unit, встретится нам при изучении объектов кода.\n",
      "\n",
      " 1  struct compiler_unit {\n",
      " 2      PySTEntryObject *u_ste;\n",
      " 3        \n",
      " 4      PyObject *u_name;\n",
      " 5      PyObject *u_qualname;  /* dot-separated qualified name (lazy) */\n",
      " 6      int u_scope_type;\n",
      " 7      \n",
      " 8      /* The following fields are dicts that map objects to\n",
      " 9      the index of them in co_XXX.      The index is used as\n",
      "10      the argument for opcodes that refer to those collections.\n",
      "11      */\n",
      "12      PyObject *u_consts;    /* all constants */\n",
      "13      PyObject *u_names;     /* all names */\n",
      "14      PyObject *u_varnames;  /* local variables */\n",
      "15      PyObject *u_cellvars;  /* cell variables */\n",
      "16      PyObject *u_freevars;  /* free variables */\n",
      "17         \n",
      "18      PyObject *u_private;        /* for private name mangling */\n",
      "19            \n",
      "20      Py_ssize_t u_argcount;        /* number of arguments for block */\n",
      "21      Py_ssize_t u_kwonlyargcount; /* number of keyword only arguments \n",
      "22                                      for block */\n",
      "23      /* Pointer to the most recently allocated block.  By following b_list\n",
      "24      members, you can reach all early allocated blocks. */\n",
      "25      basicblock *u_blocks;\n",
      "26      basicblock *u_curblock; /* pointer to current block */\n",
      "27            \n",
      "28      int u_nfblocks;\n",
      "29      struct fblockinfo u_fblock[CO_MAXBLOCKS];\n",
      "30          \n",
      "31      int u_firstlineno; /* the first lineno of the block */\n",
      "32      int u_lineno;          /* the lineno for the current stmt */\n",
      "33      int u_col_offset;      /* the offset of the current stmt */\n",
      "34      int u_lineno_set;  /* boolean to indicate whether instr\n",
      "35                          has been generated with current lineno */\n",
      "36  };\n",
      "Листинг 3.13. Структура данных compiler_unit\n",
      "\r\n",
      "Поля u_blocks и u_curblock ссылаются на базовые блоки, которые вместе составляют компилируемый блок кода. Поле *u_ste является ссылкой на запись таблицы символов для компилируемого блока кода. Остальные поля имеют довольно понятные имена, которые говорят сами за себя. Во время компиляции происходит обход различных узлов, составляющих блок кода. В зависимости от того, начинает ли данный тип узла новый базовый блок или нет, создается базовый блок (содержащий инструкции этих узлов), или же инструкции для узла добавляются в существующий базовый блок. Вот самые частые типы узлов, которые могут начинать новый базовый блок:\n",
      "\n",
      "\n",
      "Функциональные узлы.\n",
      "Узлы, где нужно совершить «прыжок».\n",
      "Обработчики исключений.\n",
      "Булевы операции и т.п.\n",
      "\n",
      "Структуры данных basic_block и instruction\r\n",
      "Структура данных базового блока довольно интересна в рамках процесса генерации графа потока управления. Базовый блок — это последовательность инструкций, которая имеет одну точку входа, но несколько точек выхода. Определение структуры данных basic_block, используемой в виртуальной машине python, приведено в листинге 3.14.\n",
      "\n",
      " 1  typedef struct basicblock_ {\n",
      " 2      /* Each basicblock in a compilation unit is linked via b_list in the\n",
      " 3      reverse order that the block are allocated.  b_list points to the next\n",
      " 4      block, not to be confused with b_next, which is next by control flow. */\n",
      " 5      struct basicblock_ *b_list;\n",
      " 6      /* number of instructions used */\n",
      " 7      int b_iused;\n",
      " 8      /* length of instruction array (b_instr) */\n",
      " 9      int b_ialloc;\n",
      "10      /* pointer to an array of instructions, initially NULL */\n",
      "11      struct instr *b_instr;\n",
      "12      /* If b_next is non-NULL, it is a pointer to the next\n",
      "13      block reached by normal control flow. */\n",
      "14      struct basicblock_ *b_next;\n",
      "15      /* b_seen is used to perform a DFS of basicblocks. */\n",
      "16      unsigned b_seen : 1;\n",
      "17      /* b_return is true if a RETURN_VALUE opcode is inserted. */\n",
      "18      unsigned b_return : 1;\n",
      "19      /* depth of stack upon entry of block, computed by stackdepth() */\n",
      "20      int b_startdepth;\n",
      "21      /* instruction offset for block, computed by assemble_jump_offsets() */\n",
      "22      int b_offset;\n",
      "23  } basicblock;\n",
      "Листинг 3.14. Структура данных basicblock_\n",
      "\r\n",
      "Как упоминалось ранее, CFG в основном состоит из базовых блоков и соединительных «путей» между ними. Поле *b_instr ссылается на массив структур данных instruction и каждая из этих структур данных содержит байт-код инструкцию. Эти байт-коды можно найти в заголовочном файле Include/opcode.h. Структура данных instruction показана в листинге 3.15.\n",
      "\n",
      "1  struct instr {\n",
      "2      unsigned i_jabs : 1;\n",
      "3      unsigned i_jrel : 1;\n",
      "4      unsigned char i_opcode;\n",
      "5      int i_oparg;\n",
      "6      struct basicblock_ *i_target; /* target block (if jump instruction) */\n",
      "7      int i_lineno;\n",
      "8  };\n",
      "Листинг 3.15. Структура данных instruction\n",
      "\r\n",
      "Ещё раз взгляните на CFG для функции fizzbuzz. Мы видим, что на самом деле есть два пути способа перейти от выполнения блока 1 к блоку 2. Первый — через нормальное выполнение, когда все инструкции в блоке 1 выполняются и поэтому поток выполнения автоматически продолжается в блоке 2. Второй способ — инструкция перехода, мы видели такую сразу после первой операции сравнения. «Целью» такой инструкции перехода является другой базовый блок, но на самом деле виртуальная машина выполняет объекты кода, которые ничего не знают о базовых блоках. Блок кода содержит только поток байт-кодов, который индексируется с помощью смещения. Получается, мы должны взять блоки с «целями» прыжка и заменить их на смещения в массиве инструкций. Это то, что делает процесс сборки базовых блоков.\n",
      "\n",
      "Сборка базовых блоков\r\n",
      "После того как CFG сгенерирован, базовые блоки содержат инструкции байт-кода, являющиеся репрезентацией AST. Но блоки не упорядочены линейно, а инструкции перехода все ещё содержат базовые блоки в качестве целей перехода, вместо относительного или абсолютного смещения в потоке команд. Функция assemble обрабатывает линеаризацию CFG и создание объекта кода из CFG.\n",
      "\r\n",
      "Во-первых, функция сборки базовых блоков добавляет инструкции return None в любой блок, который заканчивается без оператора RETURN. Теперь вы знаете, почему вы можете определять методы без добавления RETURN. Затем следует предварительный post-order обход графа CFG (дочерние элементы посещаются перед их корневым узлом), чтобы «сгладить» блоки.\n",
      "\n",
      "Обход графа\n",
      "\r\n",
      "При post-order depth-first обходе графа мы рекурсивно посещаем левый дочерний узел графа, за которым следует правый дочерний узел графа, а затем сам узел. В нашем графе на рисунке 3.5, когда график сглажен с использованием этого подхода, порядок узлов равен H -> D -> I -> J -> E -> B -> K -> L -> F -> G -> C -> A. При использовании pre-order обхода, мы получили бы A -> B -> D -> H -> E -> I -> J -> C -> F -> K -> L -> G. Также существует способ обхода in-order, где мы получили бы: H -> D -> B -> I -> E -> J -> A -> K -> L -> F -> C -> G\n",
      "\r\n",
      "CFG для функции fizzbuzz, приведённый в листинге 3.3, довольно простой граф, поэтому результат при post-order обходе будет таковым: block 5 -> block 4 -> block 3 -> block 2 -> block 1. Как только граф линеаризирован (т.е. сглажен), смещения для «прыжков» можно рассчитать, вызвав функцию assemble_jump_offsets, передав ей полученный граф.\n",
      "\r\n",
      "Расчёт смещения прыжка происходит в два этапа. На первом этапе смещение каждой инструкции в массиве инструкций рассчитывается, как показано во фрагменте из листинга 3.16. Это простой цикл, который работает с конца сглаженного массива, создавая смещение от 0.\n",
      "\n",
      "1  ...\n",
      "2  totsize = 0;\n",
      "3  for (i = a->a_nblocks - 1; i >= 0; i--) {\n",
      "4      b = a->a_postorder[i];\n",
      "5      bsize = blocksize(b);\n",
      "6      b->b_offset = totsize;\n",
      "7      totsize += bsize;\n",
      "8  }\n",
      "9  ...\n",
      "Листинг 3.16. Расчет смещения байт-кода\n",
      "\r\n",
      "На втором этапе, цели прыжка для команд перехода рассчитываются, как показано в листинге 3.17. Происходит вычисление относительных смещений при переходах и замена ими абсолютных значений.\n",
      "\n",
      " 1  ...\n",
      " 2   for (b = c->u->u_blocks; b != NULL; b = b->b_list) {\n",
      " 3      bsize = b->b_offset;\n",
      " 4      for (i = 0; i < b->b_iused; i++) {\n",
      " 5          struct instr *instr = &b->b_instr[i];\n",
      " 6          int isize = instrsize(instr->i_oparg);\n",
      " 7          /* Relative jumps are computed relative to\n",
      " 8             the instruction pointer after fetching\n",
      " 9             the jump instruction.\n",
      "10          */\n",
      "11          bsize += isize;\n",
      "12          if (instr->i_jabs || instr->i_jrel) {\n",
      "13              instr->i_oparg = instr->i_target->b_offset;\n",
      "14              if (instr->i_jrel) {\n",
      "15                  instr->i_oparg -= bsize;\n",
      "16              }\n",
      "17              instr->i_oparg *= sizeof(_Py_CODEUNIT);\n",
      "18              if (instrsize(instr->i_oparg) != isize) {\n",
      "19                  extended_arg_recompile = 1;\n",
      "20              }\n",
      "21          }\n",
      "22      }\n",
      "23  }\n",
      "24  ...\n",
      "Листинг 3.17. Сборка смещений перехода\n",
      "\r\n",
      "Вычисленные смещения «прыжка» добавляются в сглаженный граф в порядке, обратном порядке обхода при линеаризации. Обратный post-order является топологической сортировкой CFG. Это означает, что для каждого ребра от вершины u до вершины v, u идет перед v в отсортированном порядке. Причина этого очевидна: мы хотим, чтобы узел, который перепрыгивает на другой, всегда был раньше «цели перехода». После завершения передачи байт-кода, объекты кода могут быть собраны для каждого блока кода, используя полученный байт-код и информацию из таблицы символов. Сгенерированный объект кода возвращается в вызывающую функцию, тем самым отмечая конец процесса компиляции.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Официальный Docker-образ Python весьма популярен. Кстати, я и сам рекомендовал одну из его вариаций в качестве базового образа. Но многие программисты не вполне понимают того, как именно он работает. А это может привести к путанице и к возникновению различных проблем.\n",
      "\n",
      "\n",
      "\r\n",
      "В этом материале я собираюсь поговорить о том, как создан этот образ, о том, какую он может принести пользу, о его правильном использовании и о его ограничениях. В частности, я разберу тут его вариант python:3.8-slim-buster (в состоянии, представленном файлом Dockerfile от 19 августа 2020 года) и по ходу дела остановлюсь на самых важных деталях.\n",
      "\n",
      "Читаем файл Dockerfile\n",
      "▍Базовый образ\r\n",
      "Начнём с базового образа:\n",
      "\n",
      "FROM debian:buster-slim\n",
      "\r\n",
      "Оказывается, что базовым образом для python:3.8-slim-buster является Debian GNU/Linux 10 — текущий стабильный релиз Debian, известный ещё как Buster (релизы Debian называют именами персонажей из «Истории игрушек»). Бастер — это, если кому интересно, собака Энди.\n",
      "\r\n",
      "Итак, в основе интересующего нас образа лежит дистрибутив Linux, который гарантирует его стабильную работу. Для этого дистрибутива периодически выходят исправления ошибок. В варианте slim установлено меньше пакетов, чем в обычном варианте. Там, например, нет компиляторов.\n",
      "\n",
      "▍Переменные среды\r\n",
      "Теперь взглянем на переменные среды. Первая обеспечивает как можно более раннее добавление /usr/local/bin в $PATH.\n",
      "\n",
      "# обеспечивает выбор локальной версии python, а не версии, входящей в состав дистрибутива\n",
      "ENV PATH /usr/local/bin:$PATH\n",
      "\r\n",
      "Образ устроен так, что установка Python выполняется в /usr/local. В результате данная конструкция обеспечивает то, что по умолчанию будут использоваться установленные исполняемые файлы.\n",
      "\r\n",
      "Далее — взглянем на настройки языка:\n",
      "\n",
      "# http://bugs.python.org/issue19846\n",
      "# > В настоящий момент настройка \"LANG=C\" в Linux *полностью выводит из строя Python 3*, а это плохо.\n",
      "ENV LANG C.UTF-8\n",
      "\r\n",
      "Насколько я знаю, современный Python 3, по умолчанию, и без этой настройки, использует UTF-8. Поэтому я не уверен в том, что в наши дни в исследуемом Dockerfile нужна эта строка.\n",
      "\r\n",
      "Здесь есть и переменная окружения, содержащая сведения о текущей версии Python:\n",
      "\n",
      "ENV PYTHON_VERSION 3.8.5\n",
      "\r\n",
      "В Dockerfile есть ещё переменная окружения с GPG-ключом, используемая для верификации загружаемого исходного кода Python.\n",
      "\n",
      "▍Зависимости времени выполнения\r\n",
      "Python'у для работы нужны некоторые дополнительные пакеты:\n",
      "\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "    ca-certificates \\\n",
      "    netbase \\\n",
      "  && rm -rf /var/lib/apt/lists/*\n",
      "\r\n",
      "Первый пакет, ca-certificates, содержит список сертификатов стандартных центров сертификации. Нечто подобное используется браузером для проверки -адресов. Это позволяет Python, wget и другим инструментам проверять сертификаты, предоставляемые серверами.\n",
      "\r\n",
      "Второй пакет, netbase, выполняет установку в /etc нескольких файлов, необходимых для настройки соответствия определённых имён с некоторыми портами и протоколами. Например, /etc/services отвечает за настройку соответствия имён сервисов, вроде https, с номерами портов. В данном случае это 443/tcp.\n",
      "\n",
      "▍Установка Python\r\n",
      "Теперь выполняется установка набора инструментальных средств компиляции. А именно, загружается и компилируется исходный код Python, после чего деинсталлируются ненужные пакеты Debian:\n",
      "\n",
      "RUN set -ex \\\n",
      "  \\\n",
      "  && savedAptMark=\"$(apt-mark showmanual)\" \\\n",
      "  && apt-get update && apt-get install -y --no-install-recommends \\\n",
      "    dpkg-dev \\\n",
      "    gcc \\\n",
      "    libbluetooth-dev \\\n",
      "    libbz2-dev \\\n",
      "    libc6-dev \\\n",
      "    libexpat1-dev \\\n",
      "    libffi-dev \\\n",
      "    libgdbm-dev \\\n",
      "    liblzma-dev \\\n",
      "    libncursesw5-dev \\\n",
      "    libreadline-dev \\\n",
      "    libsqlite3-dev \\\n",
      "    libssl-dev \\\n",
      "    make \\\n",
      "    tk-dev \\\n",
      "    uuid-dev \\\n",
      "    wget \\\n",
      "    xz-utils \\\n",
      "    zlib1g-dev \\\n",
      "# с релиза Stretch \"gpg\" больше по умолчанию в дистрибутив не входит\n",
      "    $(command -v gpg > /dev/null || echo 'gnupg dirmngr') \\\n",
      "  \\\n",
      "  && wget -O python.tar.xz \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz\" \\\n",
      "  && wget -O python.tar.xz.asc \"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc\" \\\n",
      "  && export GNUPGHOME=\"$(mktemp -d)\" \\\n",
      "  && gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys \"$GPG_KEY\" \\\n",
      "  && gpg --batch --verify python.tar.xz.asc python.tar.xz \\\n",
      "  && { command -v gpgconf > /dev/null && gpgconf --kill all || :; } \\\n",
      "  && rm -rf \"$GNUPGHOME\" python.tar.xz.asc \\\n",
      "  && mkdir -p /usr/src/python \\\n",
      "  && tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \\\n",
      "  && rm python.tar.xz \\\n",
      "  \\\n",
      "  && cd /usr/src/python \\\n",
      "  && gnuArch=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" \\\n",
      "  && ./configure \\\n",
      "    --build=\"$gnuArch\" \\\n",
      "    --enable-loadable-sqlite-extensions \\\n",
      "    --enable-optimizations \\\n",
      "    --enable-option-checking=fatal \\\n",
      "    --enable-shared \\\n",
      "    --with-system-expat \\\n",
      "    --with-system-ffi \\\n",
      "    --without-ensurepip \\\n",
      "  && make -j \"$(nproc)\" \\\n",
      "    LDFLAGS=\"-Wl,--strip-all\" \\\n",
      "  && make install \\\n",
      "  && rm -rf /usr/src/python \\\n",
      "  \\\n",
      "  && find /usr/local -depth \\\n",
      "    \\( \\\n",
      "      \\( -type d -a \\( -name test -o -name tests -o -name idle_test \\) \\) \\\n",
      "      -o \\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' -o -name '*.a' \\) \\) \\\n",
      "      -o \\( -type f -a -name 'wininst-*.exe' \\) \\\n",
      "    \\) -exec rm -rf '{}' + \\\n",
      "  \\\n",
      "  && ldconfig \\\n",
      "  \\\n",
      "  && apt-mark auto '.*' > /dev/null \\\n",
      "  && apt-mark manual $savedAptMark \\\n",
      "  && find /usr/local -type f -executable -not \\( -name '*tkinter*' \\) -exec ldd '{}' ';' \\\n",
      "    | awk '/=>/ { print $(NF-1) }' \\\n",
      "    | sort -u \\\n",
      "    | xargs -r dpkg-query --search \\\n",
      "    | cut -d: -f1 \\\n",
      "    | sort -u \\\n",
      "    | xargs -r apt-mark manual \\\n",
      "  && apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false \\\n",
      "  && rm -rf /var/lib/apt/lists/* \\\n",
      "  \\\n",
      "  && python3 --version\n",
      "\r\n",
      "Тут происходит много всего, но самое важное — это следующее:\n",
      "\n",
      "\n",
      "Python устанавливается в /usr/local.\n",
      "Удаляются все .pyc-файлы.\n",
      "Пакеты, в частности — gcc и прочие подобные, которые были нужны для компиляции Python, удаляются после того, как необходимость в них пропадает.\n",
      "\r\n",
      "Из-за того, что всё это происходит в единственной команде RUN, в итоге компилятор не сохраняется ни в одном из слоёв, что помогает поддерживать компактный размер образа.\n",
      "\r\n",
      "Тут можно обратить внимание на то, что Python для компиляции нужна библиотека libbluetooth-dev. Мне это показалось необычным, поэтому я решил в этом разобраться. Как оказалось, Python может создавать Bluetooth-сокеты, но только в том случае, если он скомпилирован с использованием этой библиотеки.\n",
      "\n",
      "▍Настройка символьных ссылок\r\n",
      "На следующем шаге работы /usr/local/bin/python3 назначается символьная ссылка /usr/local/bin/python, что позволяет вызывать Python разными способами:\n",
      "\n",
      "# создание некоторых полезных символьных ссылок, присутствие которых ожидается в системе\n",
      "RUN cd /usr/local/bin \\\n",
      "  && ln -s idle3 idle \\\n",
      "  && ln -s pydoc3 pydoc \\\n",
      "  && ln -s python3 python \\\n",
      "  && ln -s python3-config python-config\n",
      "\n",
      "▍Установка pip\r\n",
      "У менеджера пакетов pip имеется собственный график выхода релизов, отличающийся от графика релизов Python. Например, в этом Dockerfile выполняется установка Python 3.8.5, выпущенного в июле 2020. А pip 20.2.2 вышел в августе, уже после выхода Python, но Dockerfile устроен так, чтобы была бы установлена свежая версия pip:\n",
      "\n",
      "# если эту переменную назвать \"PIP_VERSION\", то pip выдаёт ошибку: \"ValueError: invalid truth value '<VERSION>'\"\n",
      "ENV PYTHON_PIP_VERSION 20.2.2\n",
      "# https://github.com/pypa/get-pip\n",
      "ENV PYTHON_GET_PIP_URL https://github.com/pypa/get-pip/raw/5578af97f8b2b466f4cdbebe18a3ba2d48ad1434/get-pip.py\n",
      "ENV PYTHON_GET_PIP_SHA256 d4d62a0850fe0c2e6325b2cc20d818c580563de5a2038f917e3cb0e25280b4d1\n",
      "\n",
      "RUN set -ex; \\\n",
      "  \\\n",
      "  savedAptMark=\"$(apt-mark showmanual)\"; \\\n",
      "  apt-get update; \\\n",
      "  apt-get install -y --no-install-recommends wget; \\\n",
      "  \\\n",
      "  wget -O get-pip.py \"$PYTHON_GET_PIP_URL\"; \\\n",
      "  echo \"$PYTHON_GET_PIP_SHA256 *get-pip.py\" | sha256sum --check --strict -; \\\n",
      "  \\\n",
      "  apt-mark auto '.*' > /dev/null; \\\n",
      "  [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark; \\\n",
      "  apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\\n",
      "  rm -rf /var/lib/apt/lists/*; \\\n",
      "  \\\n",
      "  python get-pip.py \\\n",
      "    --disable-pip-version-check \\\n",
      "    --no-cache-dir \\\n",
      "    \"pip==$PYTHON_PIP_VERSION\" \\\n",
      "  ; \\\n",
      "  pip --version; \\\n",
      "  \\\n",
      "  find /usr/local -depth \\\n",
      "    \\( \\\n",
      "      \\( -type d -a \\( -name test -o -name tests -o -name idle_test \\) \\) \\\n",
      "      -o \\\n",
      "      \\( -type f -a \\( -name '*.pyc' -o -name '*.pyo' \\) \\) \\\n",
      "    \\) -exec rm -rf '{}' +; \\\n",
      "  rm -f get-pip.py\n",
      "\r\n",
      "После выполнения этих операций, как и прежде, удаляются все .pyc-файлы.\n",
      "\n",
      "▍Точка входа в образ\r\n",
      "В итоге в Dockerfile указывается точка входа в образ:\n",
      "\n",
      "CMD [\"python3\"]\n",
      "\r\n",
      "Используя CMD вместо ENTRYPOINT мы, запуская образ, по умолчанию получаем доступ к python:\n",
      "\n",
      "$ docker run -it python:3.8-slim-buster\n",
      "Python 3.8.5 (default, Aug  4 2020, 16:24:08)\n",
      "[GCC 8.3.0] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>>\n",
      "\r\n",
      "Но, при необходимости, можно указывать при запуске образа и другие исполняемые файлы:\n",
      "\n",
      "$ docker run -it python:3.8-slim-buster bash\n",
      "root@280c9b73e8f9:/#\n",
      "\n",
      "Итоги\r\n",
      "Вот что мы узнали, разобрав Dockerfile официального Python-образа slim-buster.\n",
      "\n",
      "▍В состав образа входит Python\r\n",
      "Хотя это и может показаться очевидным, стоит обратить внимание на то, как именно Python включён в состав образа. А именно, сделано это путём его самостоятельной установки в /usr/local.\n",
      "\r\n",
      "Программисты, использующие этот образ, порой совершают одну и ту же ошибку, которая заключается в повторной установке Debian-версии Python:\n",
      "\n",
      "FROM python:3.8-slim-buster\n",
      "\n",
      "# Делать этого не нужно:\n",
      "RUN apt-get update && apt-get install python3-dev\n",
      "\r\n",
      "При выполнении этой команды RUN Python будет установлен ещё раз, но в /usr, а не в /usr/local. И это, как правило, будет не та версия Python, которая установлена в /usr/local. А программисту, который воспользовался вышеприведённым Docker-файлом, вероятно, не нужны две разные версии Python в одном и том же образе. Это, в основном, является причиной путаницы.\n",
      "\r\n",
      "А если же кому-то и правда нужна Debian-версия Python, то лучше будет использовать в качестве базового образа debian:buster-slim.\n",
      "\n",
      "▍В образ входит самая свежая версия pip\r\n",
      "Например, самый свежий релиз Python 3.5 состоялся в ноябре 2019, но Docker-образ python:3.5-slim-buster включает в себя pip, который вышел в августе 2020. Это (обычно) хорошо, так как означает, что в нашем распоряжении оказываются самые свежие исправления ошибок и улучшения производительности. Это, кроме того, значит, что мы можем пользоваться поддержкой более новых вариантов «колёс».\n",
      "\n",
      "▍Из образа удаляются все .pyc-файлы\r\n",
      "Если хочется немного ускорить загрузку системы, то можно самостоятельно скомпилировать исходный код стандартной библиотеки в формат .pyc. Делается это с помощью модуля compileall.\n",
      "\n",
      "▍Образ не выполняет установку обновлений безопасности Debian\r\n",
      "Хотя базовые образы debian:buster-slim и python часто обновляются, имеется определённый промежуток между моментами выхода обновлений безопасности Debian и включением их в образы. Поэтому нужно самостоятельно устанавливать обновления безопасности для базового дистрибутива Linux.\n",
      "\n",
      "Какими Docker-образами вы пользуетесь для выполнения Python-кода?\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет, Хабр!\n",
      "\r\n",
      "У нас возможен предзаказ долгожданного второго издания книги \"Простой Python\". Перевод первого издания вышел в 2016 году и по сей день остается в числе бестселлеров.\n",
      "\n",
      "\n",
      "\r\n",
      "Поскольку мы убеждены, что Python — лучший язык программирования для начинающих, а также для работы с data science и машинным обучением, сегодня предлагаем вам перевод несколько мировоззренческого поста с сайта Dropbox, где завершил свою карьеру Гвидо ван Россум — о смысле и незаменимости языка Python.\n",
      "\r\n",
      "Ничто так ярко не характеризует XXI век, как всепроникающее влияние программирования. Почти все, чем мы занимаемся, в особенности, на работе, делается через монитор, на котором выводятся результаты колоссальной вычислительной работы, что мы сегодня принимаем как данность. Для тех 99,7% представителей человеческого рода, кто не занимается программированием, все это происходит как будто по волшебству. Как отметил великий писатель-фантаст Артур Кларк, «Любая достаточно развитая технология неотличима от магии».\n",
      "\r\n",
      "Разумеется, никакая это не магия. Но программирование – одновременно сложный и многоуровневый процесс, объем баз кода в серьезных технологических компаниях измеряется миллионами строк кода. Рассуждая о том, как может быть реализована на уровне кода реальная система, вы думаете о сложном взаимовлиянии различных функций с течением времени. Ваш код может быть более или менее сложен на уровне структуры, но неотъемлемая сложность присуща и той задаче, которую вы решаете, причем, такую сложность невозможно свести к чему-то более простому. \n",
      "\r\n",
      "Быть программистом — это не только генерировать идеи, и вы не продержитесь в этой профессии долго, если не умеете с хирургической точностью описывать в коде ваши идеи. «Я немного скептически отношусь к мнению, что важнее всего в данном случае системное мышление, поскольку гораздо проще обрисовать идею системы, чем взять эту идею и превратить ее в рабочий код», — говорит Гвидо ван Россум, создатель и отставной Великодушный Пожизненный Диктатор языка Python. Веб по-прежнему остается за JavaScript, а на Java работает 2,5 миллиарда телефонов под Android, но в программировании широкого профиля и в образовании Python де-факто стал стандартом.\n",
      "\r\n",
      "Тот, кто сделал больше всего, чтобы облегчить множеству людей реализацию их идей в коде – это Гвидо ван Россум, отдавший 30 лет жизни языку Python. Причем, он сделал это с непритязательным изяществом и приглушенным юмором. Язык назван в честь гротескного юмористического шоу «Монти Пайтон», а не в честь бирманской змеи. Без лишнего шума язык программирования Python позволил упростить некоторые вещи в программировании, особенно для осмысления.\n",
      "\r\n",
      "Чтобы понять, как ван Россуму удалось подобное, давайте обратимся к истории вычислительной техники и перенесемся в эру мейнфреймов и машинного языка. «Мейнфрейм — это машина стоимостью миллионы долларов, и совокупная зарплата всех его программистов – это просто копейки по сравнению со стоимостью мейнфрейма», — говорит Россум. Поэтому логично, что машинное время расценивалось как более приоритетное, нежели человеческое. «Но, имея дело с настольными рабочими станциями и ПК, я осознал, что давно пора поменять отношение к соотношению стоимости человеческого и машинного времени». Ван Россум не считает себя первым, кто заметил эту перемену, но он по-настоящему акцентировал ее при проектировании языка Python.\n",
      "\r\n",
      "Столь простая идея «машина для человека, а не для человек для машины» — суть всей философии языка Python. Определенно, тот факт, что это интерпретируемый, а не компилируемый язык, подразумевает, что программист видит эффект от написанного кода сразу же, как написал его, без необходимости перекомпилировать его после каждого изменения. Сегодня такая практика весьма распространена, но многие годы назад она воспринималась неоднозначно, так как казалось несомненным, что чем быстрее компьютер — тем лучше. Пересмотр этого убеждения произвел значительный положительный эффект на продуктивность программистов.\n",
      "\r\n",
      "«Существует целая куча распространенных задач программирования, которые на Python решаются проще», — говорит ван Россум. — «Для тех, кто пока не программист, а только хочет им стать, порог вхождения в Python особенно невысок». Действительно, на многих курсах по программированию начинают отказываться от Java в пользу Python, поскольку он гораздо понятнее для начинающих. Причины, лежащие в основе такого перехода, сложны, в них много факторов, каждый из которых позволил устранить немного шероховатостей. Суть философии в основе всех улучшений: у всего обязательно должно быть назначение. Когда избыточного кода нет, проще сосредоточиться на том, на чем действительно нужно сфокусироваться. «В Python важен каждый символ, который вы вводите», — говорит ван Россум. \n",
      "\r\n",
      "Такая лаконичность гарантирует, что в Python несложно добиться ощутимого результата, благодаря чему этот язык и находит столь широкое применение. «Также важно, каким именно образом мы знакомим с Python начинающих программистов. Можно показать человеку совсем небольшие фрагменты кода, с которыми можно работать, почти не понимая терминологии и концепций программирования как таковых, пока не разберешься с ними», — объясняет ван Россум, — «тогда как в самой небольшой программе на Java вы найдете целый ворох символов, которые покажутся непосвященному глазу какой-то рябью».\n",
      "\r\n",
      "Благодаря столь спокойному и простому дизайну языка, проще понять, что происходит в коде. «Python кажется мне невероятно наглядным», — говорит ван Россум. — «Читая Python, я определенно воспринимаю его структуру как двухуровневую, а не как одноуровневую. Вероятно, это потому, что в Python важны отступы, а, возможно, и потому, что мне просто нравится мыслить визуально».\n",
      "\r\n",
      "Разумеется, не ему одному свойственно визуальное мышление. Все мы в какой-то степени мыслим именно так. Но он особенно акцентирует роль визуального компонента в познавательной деятельности. «Если текст плохо отформатирован, меня это может привести в бешенство. Тогда у меня в голове сбивается разбор текстового потока, и в таком смысле действительно можно утверждать, что я мыслю на Python», — признает ван Россум. – «Я гораздо лучше усваиваю код, если он хорошо отформатирован». Если отступы в коде расставлены произвольно, то требуется больше информации, чтобы распутать смысл кода, чем если у каждого отступа четкое назначение, как в Python. Поэтому с Python и удобно работать: вам приходится переварить меньше информации, чтобы понять, что происходит.\n",
      "\r\n",
      "Python удобочитаем не только с точки зрения типографики, но и концептуально. Ван Россум полагает, что Python, возможно, точнее других языков программирования соответствует нашему визуальному восприятию структур, представленных в коде, поскольку «в Python соблюдение структуры является обязательным». \n",
      "\r\n",
      "Клайв Томпсон, автор книги «CODERS», рассказывает, что «готовясь к созданию книги, разговаривал со множеством разработчиков, которые совершенно влюблены в Python. Практически все без исключения говорили что-то вроде «Python красив»». Им нравилась удобочитаемость, они находили, что не составляет труда мельком взглянуть на код Python и понять его назначение. Этот язык, лишенный фигурных скобок, со строками, уложенными как аккуратные полочки, в самом деле напоминает современную поэзию». Также они считают, что на Python интересно писать, а это гораздо важнее, чем может показаться на первый взгляд. Томпсон пишет, что, «знакомясь с кодером, вы видите перед собой человека, чья основная повседневная работа полна бесконечных неудач и гнетущей фрустрации».\n",
      "\r\n",
      "Когда уважение к времени программиста встроено в язык, это интересным образом отражается на сообществе специалистов по этому языку. Существует социальная философия, проистекающая из Python: программист отвечает за свой код, зная, что пишет его для других людей. Существует негласный тезис, который в своих текстах и лекциях активно продвигает сам ван Россум: потратьте немного больше времени и добейтесь, чтобы человеку, который станет читать ваш код в будущем, было немного легче. Выражать собственное уважение коллегам и показывать, что вы цените их время – это этика, которую ван Россум без лишнего шума продвигает во всем мире. «Вы пишете код, прежде всего, для общения с другими программистами и в меньшей степени – для навязывания своей воли компьютеру», — говорит он. \r\n",
      "Универсальность культуры, распространяющейся вокруг Python, позволила в некоторой степени воплотить замысел, появившийся у ван Россума еще около двадцати лет назад, когда он запустил недолговечный проект CP4E (Компьютерное Программирование Для Каждого). «Обычно я не могу похвастаться провидческим талантом. Меня спрашивают, какое будущее ждет Python, а я и не знаю. Но своим важнейшим пророчеством я считаю тезис о том, что наступит время, когда изучить программирование станет целесообразно для каждого». На тот момент персональные компьютеры существовали уже около 20 лет, но большинство из них представляли собой распиаренные пишущие машинки и калькуляторы. Тогда ван Россум задался вопросом: «не безумие ли это, что у такого множества людей есть компьютеры, но настолько мало тех, кто учится программировать»?\n",
      "\r\n",
      "С тех пор он занимался постоянным упрощением программирования, и эту работу легко проследить по развитию Python, который ныне достиг версии 3.7. Вам Россум продолжает считать, что программирование прививает общеполезные навыки, в частности, умение решать задачи, аккуратно следовать в том или ином направлении и понимать, в чем смысл данного направления. Но он также обнаружил, что «существуют варианты введения в программирование, которые интересны детям, правда, не всем; поэтому не думаю, что программирование нужно включать в школьный курс в качестве обязательного предмета».\n",
      "\r\n",
      "В то же время, сегодня человеку уже не так актуально программировать свой компьютер, так как появилось множество готовых программ, особенно в Интернете, и они позволяют интуитивно справиться с задачами, решения для которых ранее требовали что-либо программировать.\n",
      "\r\n",
      "Тем не менее, сейчас все больше людей, использующих Python в различных дисциплинах. «Сегодня господствует одна теория, объясняющая неожиданный успех Python», — говорит ван Россум, — «согласно которой, ему повезло закрепиться в сфере data science и машинного обучения, а также обработки научных данных в целом. Как только вы наработаете критическую массу умений, связанных с Python, вам станет проще работать с той же системой, что и ваши коллеги, и ваши конкуренты, чем изучать что-то новое». Причем, по мнению ван Россума, хотя Python и начинался как обычный инструмент для программистов, сегодня это, в том числе, язык для любителей, и, по мнению ван Россума, это наилучшее применение для Python.\n",
      "\r\n",
      "Успешный опенсорсный программный проект, подобный Python, был бы прост в изучении для новичков, но также находил бы практическое применение при решении реальных задач, что потребовалось бы более продвинутым пользователям. Точно, как при работе с начинающими мы стремимся не усложнять язык, чтобы они могли бросить все умственные силы на изучение сложностей, присущих среде разработки, так и при работе с продвинутыми пользователями простота языка оказывается кстати, поскольку так человеку становится проще усвоить конкурирующие абстракции. Стремление сделать реализацию Python максимально простой отчасти связано с тем, чтобы при работе с этим языком было легко менять мнение, учиться, повторять. «Написать на Python прототип и пустить его в работу зачастую не так сложно», — говорит ван Россум, — «а затем можно себе позволить выбросить этот прототип и реализовать то же самое на основе уже изученного. Вторая версия также может быть написана на Python, но она будет уже гораздо лучше первой». \n",
      "\r\n",
      "Неослабевающая привлекательность Python отчасти связана с оптимизмом этого языка и с тем, как легко на нем начать все сначала. «Если вы уделили массу времени написанию и отладке кода, то вам совсем не захочется отбросить его и начать все сначала». Сооснователь и генеральный директор Drpopbox Дрю Хьюстон написал на Python первый прототип этого сервиса во время пятичасового автобусного рейса из Бостона в Нью-Йорк. «Первые прототипы Dropbox в основном были отбракованы, неоднократно», — говорит ван Россум.\n",
      "\r\n",
      "Чему можно научиться у Python по поводу того, как создавать качественные инструменты для умственной работы? Тим Питерс, один из крупнейших контрибьюторов Python, дает несколько подсказок на эту тему в афористично написанном «Дзен Python», где поясняет принципы, которыми руководствовался ван Россум. В контексте этой статьи наиболее важны следующие два: «Простое лучше сложного. Сложное лучше переусложненного». Это практически рецепт, по которому мозг приоритезирует собственные функции ради эффективного расхода энергии.\n",
      "\r\n",
      "С точки зрения ван Россума не менее важен социальный аспект мышления и создания инструментов. Что он извлек из своей тридцатилетней одиссеи с Python? «Я узнал, что в одиночку с таким делом не справиться, и это был сложный урок для меня. Узнал, что не всегда достигаешь того результата, к которому стремился, но, возможно, результат не хуже или даже лучше того, что ты ожидал».    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Специально к старту нового потока курса «Python для веб-разработки» представляем подборку из 57 репозиториев, которые будут полезны как начинающему, так и опытному разработчику: это репозитории с ответами на вопросы собеседований, репозитории с книгами, небольшие, но полезные консольные инструменты и проекты, которые вдохновят вас написать красивый, работающий и полезный код.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Эксперт вы или начинающий разработчик — не важно. В любом случае вы задаете себе такие вопросы:\n",
      "\n",
      "\n",
      "Как выглядит хороший, чистый код?\n",
      "Где я могу научиться писать код лучше?\n",
      "Как стать лучше в профессии?\n",
      "\r\n",
      "И в этот момент на помощь приходит GitHub. Мы посмотрим на топовые репозитории Github, которые не только вдохновят вас и чему-то научат, но и дадут почувствовать вкус того, что творческий ум способен сделать с помощью Python.\n",
      "\n",
      "Почему Python?\r\n",
      "Разработчики со всего мира выбрали Python основным языком многих проектов. Python — это простой в использовании язык. Он универсален, у него большая коллекция пакетов, которая удерживает и привлекает новых разработчиков. Немного фактов:\n",
      "\n",
      "\n",
      "Python считается одним из лучших инструментов для Data Science.\n",
      "Это второй по популярности язык на GitHub.\n",
      "Python — самый популярный язык в ML.\n",
      "Тренды поиска в Google 2019 года ставят Python на второе место по популярности среди языков в сети.\n",
      "В репозитории пакетов Python содержится 147 000 пакетов.\n",
      "Согласно StackOverflow именно Python — наиболее предпочтительный язык.\n",
      "\n",
      "Итак, список\r\n",
      "С этими репозиториями я сталкивалась лично. Они помогают мне в работе с кодом: я у них учусь, восхищаюсь ими, вдохновляюсь ими, нахожу их полезными в чём-то ином. Этот список даст вам преимущество в смысле кода и повысит вашу ценность как специалиста. Я разобью подборку на такие категории:\n",
      "\n",
      "1. Книги.\n",
      "2. Собеседования.\n",
      "3. Обучение.\n",
      "4. Крутые проекты.\n",
      "5. Фреймворки, модули, инструменты.\n",
      "\n",
      "1. Книги.\n",
      "Hitchhiker’s Guide to Python от Real Python (21.9к ★)\r\n",
      "Эта книга — руководство по установке, настройке и применению Python.\n",
      "\n",
      "Python Machine Learning от Sebastian Raschka и Vahid Mirjalili (1.6к ★)\r\n",
      "Блокноты с кодом из классических учебников ML.\n",
      "\n",
      "Cosmic Python (1.3к ★)\r\n",
      "Книга об архитектуре приложений на Python в смысле управления сложностью.\n",
      "\n",
      "Byte of Python от Swaroop C H (1.3к ★)\r\n",
      "Книга для начинающих. Прочитайте ее, если программирования для вас неизвестная область.\n",
      "\n",
      "2. Собеседования.\n",
      "Cracking the Coding Interview от Bogdan (140 ★)\r\n",
      "Решения вопросов с собеседований, представленных в шестом издании «Cracking the Coding Interview» (CTCI).\n",
      "\n",
      "Interactive Coding Challenges от Donne Martin (21.2к ★)\r\n",
      "Более 120 интерактивных задач по кодированию Python (алгоритмы и структуры данных) — поставляется с картами программы для запоминания Anki.\n",
      "\n",
      "Python Interview Questions от Ian Stapleton Cordasco (108 ★)\r\n",
      "Список вопросов, которые могут быть заданы работодателями Python\n",
      "\n",
      "300 Python Interview Questions от Learning Zone (64 ★ы)\r\n",
      "Более 300 вопросов на собеседовании по Python.\n",
      "\n",
      "3. Обучение.\n",
      "The Algorithms/Python от The Algorithms (91.5к ★)\r\n",
      "Все алгоритмы из компьютерной науки на Python. Отлично подходит для технических собеседований.\n",
      "\n",
      "Awesome Python от Vinta Chen (88.7к ★)\r\n",
      "Отобранный список потрясающих фреймворков, библиотек, программного обеспечения и ресурсов с кодом, охватывающим практически все, для чего используется Python.\n",
      "\n",
      "Full Speed Python от João Ventura (2.9к ★)\r\n",
      "Это книга для самообразования. Она призвана научить Python через практику.\n",
      "\n",
      "Python Robotics от Atsushi Sakai (10.6к ★)\r\n",
      "Примеры кода на Python для робототехники.\n",
      "\n",
      "Learn Python 3 от Jerry Pussinen (2.8к ★)\r\n",
      "Блокноты Jupyter Notebook для преподавания и обучения Python 3.\n",
      "\n",
      "Learn Python от Oleksii Trekhleb (5.3к ★)\r\n",
      "Песочница и список трюков Python. Коллекция скриптов Python, разделенная по темам и содержащая примеры кода с пояснениями.\n",
      "\n",
      "Python Reference от Sebastian Raschka (2.5к ★)\r\n",
      "Полезные функции, учебники и другие связанные с Python вещи.\n",
      "\n",
      "Manim от 3b1b (27.8к ★)\r\n",
      "Анимационный движок для создания объяснительных видеоматериалов по математике. В основном он используется при программном создании анимации.\n",
      "\n",
      "NLTK от NLTK (9.4к ★]\r\n",
      "Коллекция библиотек и инструментов с открытым исходным кодом для обработки естественного языка.\n",
      "\n",
      "Free programming books от Free Ebook Foundation (164к ★)\r\n",
      "Бесплатные книги по программированию. Есть раздел Python с большим количеством бесплатных электронных книг.\n",
      "\n",
      "100 Days of ML Code от Avik Jain (30.5к ★)\r\n",
      "Рабочие листы с пошаговыми описаниями, которые знакомят пользователей с основами машинного обучения. Содержит ссылки на примеры кода, наборы данных и полезные видео, объясняющие ключевые математические понятия.\n",
      "\n",
      "D2L от Dive Into Deep Learning (8к ★)\r\n",
      "Интерактивный углубленный учебник с кодом, математикой и дискуссиями. Работа идет с несколькими фреймворками. Принято в 140 университетах и 35 странах.\n",
      "\n",
      "Models от TensorFlow (67.1к ★)\r\n",
      "Репозиторий с открытым исходным кодом, где вы найдете множество связанных с глубоким обучением библиотек и моделей.\n",
      "\n",
      "TensorFlow examples от Aymeric Damien (39.2к ★)\r\n",
      "Справочник для всех, кто начинает работать с фреймворком машинного обучения Google TensorFlow. Содержит множество примеров кода, демонстрирующих все, начиная от базовых операций TensorFlow и заканчивая построением нейронных сетей.\n",
      "\n",
      "Project Based Learning от Tu V. Tran (40к ★)\r\n",
      "Список ориентированных на проекты учебников по программированию, в том числе по созданию веб-скреперов, приложений, ботов и т.д.\n",
      "\n",
      "Coding Problems от Meto Trajkovski (1.4к ★)\r\n",
      "Решения различных проблем кодирования/алгоритмики и множество полезных ресурсов для изучения алгоритмов и структур данных.\n",
      "\n",
      "Крутые проекты\n",
      "Airflow от Apache (19.1к ★)\r\n",
      "Платформа для разработки программ, планирования и мониторинга рабочих процессов.\n",
      "\n",
      "Hug от Hug API (6.4к ★)\r\n",
      "Цель Hug — сделать разработку API на Python как можно проще.\n",
      "\n",
      "\n",
      "\n",
      "Rebound от Jonathan Shobrook (3.4к ★)\r\n",
      "Инструмент командной строки, при возникновении исключения немедленно показывающий результат со Stack Overflow — [прим. перев. — сайта с ответами на вопросы по программированию].\n",
      "\n",
      "You Get от Mort Yao (36.9к ★)\r\n",
      "Крошечная утилита командной строки для загрузки медиаконтента (видео, аудио, изображений) из интернета.\n",
      "\n",
      "Snallygaster от Hanno Böck (1.7к ★)\r\n",
      "Инструмент сканирования на предмет скрытых файлов на серверах HTTP.\n",
      "\n",
      "DeepFaceLab от iperov (21.1к ★)\r\n",
      "Инструмент, который может создавать изображения и видео DeepFake, позволяя вам делать много забавных вещей, например, удаление и замена лиц.\n",
      "\n",
      "\n",
      "Пример обмена лицами.\n",
      "\n",
      "Photon от Somdev Sangwan (7.2к ★)\r\n",
      "Мощный и простой в использовании веб-скрепер. Он следует рекомендациям из OSINT — методологии, которая делают возможным сбор и анализ информации, полученной из открытых или общедоступных источников.\n",
      "\n",
      "ZeroNet от ZeroNet (16.2к ★)\r\n",
      "Децентрализованные веб-сайты с помощью криптовалюты Bitcoin в сети BitTorrent.\n",
      "\n",
      "\n",
      "\n",
      "Gym от OpenAI (22.6к ★)\r\n",
      "Инструментарий для разработки и сравнения алгоритмов обучения с подкреплением.\n",
      "\n",
      "Detectron от Facebook Research (23.8к ★)\r\n",
      "Исследовательская платформа Facebook AI Research (исследования искусственного интеллекта Facebook) для исследования обнаружения объектов, реализующая популярные алгоритмы, такие как Mask R-CNN и RetinaNet.\n",
      "\n",
      "\n",
      "Пример вывода Mask R-CNN через Detectron\n",
      "\n",
      "Magenta от Magenta (15.9к ★)\r\n",
      "Исследовательский проект, исследующий роль машинного обучения в создании произведений искусства и музыки. В первую очередь это связано с разработкой новых алгоритмов глубокого обучения и обучением с подкреплением для написания песен, создания изображений, рисунков и других материалов.\n",
      "\n",
      "Mopidy от Mopidy (6.7к ★)\r\n",
      "Расширяемый музыкальный сервер — он воспроизводит музыку с локального диска, Spotify, SoundCloud, Google Play Music и других сервисов.\n",
      "\n",
      "Face Recognition от Adam Geitgey (37.1к ★)\r\n",
      "Распознавайте и управляйте лицами на Python или из командной строки с помощью самой простой в мире библиотеки распознавания лиц.\n",
      "\n",
      "\n",
      "Определение изображения Джо Байдена с помощью инструмента распознавания лиц.\n",
      "\n",
      "Wagtail от Wagtail (9.6к ★)\r\n",
      "Система управления контентом на Django, ориентированная на гибкость и впечатления пользователей.\n",
      "\n",
      "YAPF от Google (11.2к ★)\r\n",
      "Берет код и переформатирует его в лучший формат в соответствии с вашим руководством по стилю, даже если первоначально код не нарушал руководство.\n",
      "\n",
      "Zulip (12.8к ★)\r\n",
      "Мощное приложение для группового чата с открытым исходным кодом, которое сочетает в себе незамедлительность чата в реальном времени с преимуществами продуктивности тредовых бесед.\n",
      "\n",
      "4. Фреймворки, модули, инструменты.\n",
      "Dash от Plotly (13.3к ★)\r\n",
      "Фреймворк Python для создания аналитических веб-приложений без JavaScript.\n",
      "\n",
      "\n",
      "Приложение-карта с указанием дат открытия магазинов Walmart. Изображение на Plotly.\n",
      "\n",
      "Django (53.5к ★)\r\n",
      "Django — это веб-фреймворк Python высокого уровня, который способствует быстрой разработке и чистому, прагматичному дизайну.\n",
      "\n",
      "scikit-learn от scikit-learn (42.9к ★)\r\n",
      "Модуль Python для машинного обучения, созданный на основе SciPy.\n",
      "\n",
      "Falcon от Falconry (8.1к ★)\r\n",
      "Серьезный, минималистичный фреймворк REST-сервисов и серверной части приложений для разработчиков Python с акцентом на надежность, корректность и производительность в масштабе.\n",
      "\n",
      "Flask от Pallets (52.8к ★)\r\n",
      "Flask — это легкий WSGI фреймворк для веб-приложений. Он разработан, чтобы сделать начало работы быстрым и легким, есть возможность масштабирования до сложного приложения.\n",
      "\n",
      "Keras от Keras Team (2.1к ★)\r\n",
      "Высокоуровневый API нейронных сетей, написанный на Python и способный работать поверх TensorFlow, CNTK или Theano. Он разработан с упором на возможность быстро экспериментировать.\n",
      "\n",
      "Kivy от Kivy (12.1к ★)\r\n",
      "Кросс-платформенная среда Python с открытым исходным кодом для разработки приложений, использующих инновационные пользовательские интерфейсы с поддержкой мультитач.\n",
      "\n",
      "NumPy от NumPy (15.4к ★)\r\n",
      "Библиотека Python с открытым исходным кодом для работы с n-мерными массивами, она предлагает обширный набор инструментов для численных операций для достижения повышения производительности и сокращения времени выполнения.\n",
      "\n",
      "pandas от pandas (27.2к ★)\r\n",
      "Гибкая и мощная библиотека для анализа и обработки данных для Python, предоставляющая структуры маркированных данных.\n",
      "\n",
      "Requests от Python Software Foundation (43.9к ★)\r\n",
      "Библиотека Python, которая позволяет отправлять запросы HTTP/1.1, добавлять заголовки, данные форм, составные файлы и устанавливать параметры с помощью простых словарей Python.\n",
      "\n",
      "SciPy от SciPy (7.7к ★)\r\n",
      "Мультидоменная библиотека Python с открытым исходным кодом для Data Science, которая охватывает естественные науки, математику и инженерию.\n",
      "\n",
      "Seaborn от Michael Waskom (7.8к ★)\r\n",
      "Библиотека для визуализаций, основанная на Matplotlib и предлагающая дополнительный уровень настройки графиков и диаграмм, созданных Matplotlib.\n",
      "\n",
      "Statsmodels от Statsmodels (5.7к ★)\r\n",
      "Статистический модуль, предлагающий различные классы и функции для множества статистических моделей, делающий возможным статистический анализ и исследование данных.\n",
      "\n",
      "Theano от Theano (9.3к ★)\r\n",
      "Theano — это библиотека, которая позволяет эффективно определять, оптимизировать и оценивать математические выражения, включающие многомерные массивы.\n",
      "\n",
      "Tornado от Tornado Web (19.6к ★)\r\n",
      "Веб-фреймворк Python и библиотека асинхронных сетей, первоначально разработанные FriendFeed.\n",
      "\n",
      "Visdom от Facebook Research (7.7к ★)\r\n",
      "Гибкий инструмент для создания, организации и совместного использования визуализаций меняющихся, насыщенных данных. Поддерживает Torch и Numpy.\n",
      "\n",
      "\n",
      "\n",
      "Matplotlib от Matplotlib (12.6к ★)\r\n",
      "Библиотека для создания двухмерных графиков, позволяющая получать пригодные к публикации изображения в различных форматах твердых копий и в интерактивных средах на разных платформах.\n",
      "\n",
      "Заключение\r\n",
      "Я надеюсь, что вы найдете эти репозитории такими же полезными и вдохновляющими, как и я, и воспользуетесь ими для расширения своих навыков и знаний. Приятного программирования!\n",
      "\n",
      "\n",
      "\n",
      "Курс «Python для веб-разработки»\n",
      "Обучение профессии Data Science\n",
      "Обучение профессии Data Analyst\n",
      "Онлайн-буткемп по Data Analytics\n",
      "\n",
      "\n",
      "Eще курсы\n",
      "\n",
      "Курс по Machine Learning\n",
      "Продвинутый курс «Machine Learning Pro + Deep Learning»\n",
      "Курс «Математика и Machine Learning для Data Science»\n",
      "Разработчик игр на Unity\n",
      "Профессия Веб-разработчик\n",
      "Профессия Java-разработчик\n",
      "Курс по JavaScript\n",
      "C++ разработчик\n",
      "Обучение профессии C#-разработчик\n",
      "Курс по аналитике данных\n",
      "Курс по DevOps\n",
      "Профессия iOS-разработчик с нуля\n",
      "Профессия Android-разработчик с нуля\n",
      "\n",
      "\n",
      "\n",
      "Рекомендуемые статьи\n",
      "\n",
      "Как стать Data Scientist без онлайн-курсов\n",
      "450 бесплатных курсов от Лиги Плюща\n",
      "Как изучать Machine Learning 5 дней в неделю 9 месяцев подряд\n",
      "Сколько зарабатывает аналитик данных: обзор зарплат и вакансий в России и за рубежом в 2020\n",
      "Machine Learning и Computer Vision в добывающей промышленности\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Привет, Хаброжители! «Простой Python» познакомит вас с одним из самых популярных языков программирования. Книга идеально подойдет как начинающим, так и опытным программистам, желающим добавить Python к списку освоенных языков. Любому программисту нужно знать не только язык, но и его возможности.\n",
      "\r\n",
      "Вы начнете с основ Python и его стандартной библиотеки. Узнаете, как находить, загружать, устанавливать и использовать сторонние пакеты. Изучите лучшие практики тестирования, отладки, повторного использования кода и получите полезные советы по разработке. Примеры кода и упражнения помогут в создании приложений для различных целей. \n",
      "\r\n",
      "Что изменилось? — Абсолютно новые 100 страниц (там есть котики). — Глав стало большое, но они стали короче. — Появилась глава, посвященная типам данных, переменным и именам. — Добавился рассказ о новых возможностях Python, таких как f-строки. — Обновилась информация о сторонних библиотеках. — Новые примеры кода. — Дан обзор библиотеки asyncio. — Рассмотрен новый стек технологий: контейнеры, облачные технологии, data science и машинное обучение. Что осталось неизменным? — Примеры c утками и плохими стихотворениями. Они навечно с нами.\n",
      "\n",
      "\n",
      "Структура книги\n",
      "В первой части излагаются основы языка программирования Python: главы 1–11 следует читать по порядку. Я оперирую простейшими структурами данных и кода, постепенно составляя из них более сложные и реалистичные программы. Во второй части (главы 12–22) показывается, каким образом язык программирования Python используется в определенных прикладных областях, таких как Интернет, базы данных, сети и т. д.: эти главы можно читать в любом порядке.\n",
      "\r\n",
      "Вот краткое содержание всех глав и приложений и обзор новых терминов, с которыми вы там встретитесь.\n",
      "\r\n",
      "Глава 1. «Python: с чем его едят». Компьютерные программы не так уж и отличаются от других инструкций, с которыми вы сталкиваетесь каждый день. Мы рассмотрим небольшие программы, написанные на Python. Они продемонстрируют синтаксис языка, его возможности и способы применения в реальном мире. Вы узнаете, как запустить программу внутри интерактивного интерпретатора (оболочки), а также из текстового файла, сохраненного на вашем компьютере.\n",
      "\r\n",
      "Глава 2. «Данные: типы, значения, переменные и имена». В компьютерных языках используются данные и инструкции. Компьютер по-разному хранит и обрабатывает разные типы данных. Их значения или можно изменять (такие типы называются изменяемыми), или нельзя (неизменяемые типы). В программе, написанной на Python, данные могут быть представлены как литералами (числами вроде 78 или текстовыми строками вроде «waffle»), так и именованными переменными. В отличие от многих других языков программирования Python относится к переменным как к именам, и это влечет за собой некоторые важные последствия.\n",
      "\r\n",
      "Глава 3. «Числа». В этой главе показываются простейшие типы данных, применяемые в языке программирования Python: булевы переменные, целые числа и числа с плавающей точкой. Вы также изучите простейшую математику. В примерах этой главы интерактивный интерпретатор Python используется как калькулятор.\n",
      "\r\n",
      "Глава 4. «Выбираем с помощью оператора if». С существительными (типами данных) и с глаголами (программными структурами) мы поработаем в нескольких главах. Код, написанный на Python, обычно выполняется по одной строке за раз: от начала программы до ее конца. Структура if позволяет запускать разные строки кода исходя из результата сравнения определенных данных.\n",
      "\r\n",
      "Глава 5. «Текстовые строки». Здесь мы обратимся к существительным и миру текстовых строк. Вы научитесь создавать, объединять, изменять и получать строки, а также выводить их на экран.\n",
      "\r\n",
      "Глава 6. «Создаем циклы с помощью ключевых слов while и for». Снова глаголы. Вы научитесь создавать цикл двумя способами — с помощью for и с помощью while, а также узнаете, что такое итераторы — одно из основных понятий Python.\n",
      "\r\n",
      "Глава 7. «Кортежи и списки». Пришло время рассмотреть первые структуры данных более высокого уровня: списки и кортежи. Они представляют собой последовательности значений, которыми вы будете пользоваться как конструктором Lego для того, чтобы создавать более сложные структуры. Вы научитесь проходить по ним с помощью итераторов, а также быстро создавать списки с помощью списковых включений.\n",
      "\r\n",
      "Глава 8. «Словари и множества». Словари и множества позволяют сохранять данные не по позиции, а по их значению. Вы увидите, насколько это удобно, — данная особенность Python станет одной из ваших любимых.\n",
      "\r\n",
      "Глава 9. «Функции». Соединяйте структуры данных из предыдущих глав со структурами кода, чтобы выполнять сравнение, выборку или повторение операций. Упаковывайте код в функции и обрабатывайте ошибки с помощью исключений.\n",
      "\r\n",
      "Глава 10. «Ой-ой-ой: объекты и классы». Слово «объект» недостаточно конкретно, но имеет большое значение во многих компьютерных языках, в том числе и в Python. Если вы уже занимались объектно-ориентированным программированием на других языках, то в сравнении с ними Python покажется вам более простым. В этой главе объясняется, когда следует использовать объекты и классы, а когда лучше выбрать другой путь.\n",
      "\r\n",
      "Глава 11. «Модули, пакеты и программы». Вы узнаете, как перейти к более крупным структурам кода — модулям, пакетам и программам, а также где можно разместить код и данные, как ввести и вывести данные, обработать различные параметры, просмотреть стандартную библиотеку Python и то, что находится вне ее.\n",
      "\r\n",
      "Глава 12. «Обрабатываем данные». Вы научитесь профессионально обрабатывать данные и управлять ими. Эта глава полностью посвящена текстовым и двоичным данным, особенностям использования символов стандарта Unicode, а также поиску текста с помощью регулярных выражений. Вы познакомитесь с типами данных byte и bytearray — соперниками типа string, в которых содержатся необработанные бинарные значения вместо текстовых символов.\n",
      "\r\n",
      "Глава 13. «Календари и часы». С датой и временем работать бывает непросто. Здесь мы рассмотрим распространенные проблемы и способы их решения.\n",
      "\r\n",
      "Глава 14. «Файлы и каталоги». Простые хранилища данных используют файлы и каталоги. В этой главе речь пойдет о создании и использовании файлов и каталогов.\n",
      "\r\n",
      "Глава 15. «Данные во времени: процессы и конкурентность». Это первая глава, в которой мы приступаем к изучению системы. Начнем с данных во времени — вы научитесь использовать программы, процессы и потоки для того, чтобы выполнять больше работы за один промежуток времени (конкурентность). Среди прочего будут упомянуты последние добавления в async (более подробно они рассматриваются в приложении В).\n",
      "\r\n",
      "Глава 16. «Данные в коробке: надежные хранилища». Данные могут храниться в простых файлах и каталогах внутри файловых систем и структурироваться с помощью распространенных форматов, таких как CSV, JSON и XML. Однако по мере того, как объем и сложность данных будут расти, вам, возможно, придется использовать базы данных — как традиционные реляционные, так и современные базы данных NoSQL.\n",
      "\r\n",
      "Глава 17. «Данные в пространстве: сети». Отправляйте ваш код и данные через пространство по сетям с помощью служб, протоколов и API. В качестве примеров рассматриваются как низкоуровневые сокеты, библиотеки обмена сообщениями и системы массового обслуживания, так и развертывание в облачных системах.\n",
      "\r\n",
      "Глава 18. «Распутываем Всемирную паутину». Всемирной сети посвящена отдельная глава, в которой рассматриваются клиенты, серверы, извлечение данных, API и фреймворки. Вы научитесь искать сайты и извлекать из них данные, а затем разработаете реальный сайт, используя параметры запросов и шаблоны.\n",
      "\r\n",
      "Глава 19. «Быть питонщиком». В этой главе содержатся советы для программистов, пишущих на Python: вы получите рекомендации по установке (с помощью pip и virtualenv), использованию IDE, тестированию, отладке, журналированию, контролю исходного кода и документации. Узнаете также, как найти и установить полезные пакеты сторонних разработчиков, как упаковать свой код для повторного использования и где получить более подробную информацию.\n",
      "\r\n",
      "Глава 20. «Пи-Арт». При помощи языка программирования Python можно создавать произведения искусства: в графике, музыке, анимации и играх.\n",
      "\r\n",
      "Глава 21. «За работой». У Python есть специальные приложения для бизнеса: визуализация данных (графики, графы и карты), безопасность и регулирование.\n",
      "\r\n",
      "Глава 22. «Python в науке». За последние несколько лет Python стал главным языком науки, он используется в математике, статистике, физике, биологии и медицине. Его сильные стороны — наука о данных и машинное обучение. В этой главе демонстрируются возможности таких инструментов, как NumPy, SciPy и Pandas.\n",
      "\r\n",
      "Приложение А. «Аппаратное и программное обеспечение для начинающих программистов». Если вы новичок в мире программирования, из этого приложения вы можете узнать, как на самом деле работает аппаратное и программное обеспечение и что означают некоторые термины, с которыми в дальнейшем вам придется сталкиваться.\n",
      "\r\n",
      "Приложение Б. «Установка Python 3». Если вы еще не установили Python 3 на свой компьютер, в этом приложении вы найдете информацию о том, как это сделать независимо от того, какая операционная система у вас установлена: Windows, Mac OS/X, Linux или другой вариант Unix.\n",
      "\r\n",
      "Приложение В. «Нечто совершенно иное: async». В разных релизах Python добавляется функциональность для работы с асинхронностью — разобраться с ней может быть сложно. Я упоминаю о ней в тех главах, в которых заходит речь об асинхронности, но в этом приложении рассматриваю тему более подробно.\n",
      "\r\n",
      "Приложение Г. «Ответы к упражнениям». Здесь содержатся ответы на упражнения, приведенные в конце каждой главы. Не подглядывайте туда, пока не попробуете решить задачи самостоятельно, в противном случае вы рискуете превратиться в козленочка.\n",
      "\r\n",
      "Приложение Д. «Вспомогательные таблицы». В этом приложении содержатся справочные данные.\n",
      "\n",
      "\n",
      "\n",
      "Версии Python\r\n",
      "Языки программирования со временем изменяются — разработчики добавляют в них новые возможности и исправляют ошибки. Примеры этой книги написаны и протестированы для версии Python 3.7. Версия 3.7 является наиболее современной на момент выхода этой книги, и о самых значимых нововведениях я расскажу. Версия 3.8 вышла в конце 2019 года — я рассмотрю самую ожидаемую функциональность(оригинальное издание выпущено до выхода версии 3.8; текущая версия — 3.8.2) Узнать, что и когда было добавлено в язык программирования Python, можно, посетив страницу docs.python.org/3/whatsnew: там представлена техническая информация. Она, скорее всего, покажется трудной для понимания, если вы только начинаете изучать Python, но может пригодиться в будущем, если вам нужно будет писать программы для компьютеров, на которых установлены другие версии Python.\n",
      "\n",
      "Изменчивость\n",
      "Изменчивость одна лишь неизменна.\r\n",
      " Перси Шелли\r\n",
      "Тип также определяет, можно ли значение, которое хранится в ящике, изменить — тогда это будет изменяемое значение, или оно константно — неизменяемое значение. Неизменяемый объект как будто находится в закрытом ящике с прозрачными стенками (см. рис. 2.1): увидеть значение вы можете, но не в силах его изменить. По той же аналогии изменяемый объект похож на коробку с крышкой: вы можете не только увидеть хранящееся там значение, но и изменить его, не изменив его тип.\n",
      "\n",
      "\r\n",
      "Python является строго типизированным языком, а это означает, что тип объекта не изменяется, даже если его значение изменяемо (рис. 2.2).\n",
      "\n",
      "\n",
      "Значения-литералы\r\n",
      "Существует два вида определения данных в Python:\n",
      "\n",
      "\n",
      "как литералы;\n",
      "\n",
      "как переменные.\n",
      "\r\n",
      "В следующих главах вы увидите, как указываются значения-литералы для разных типов данных — целые числа представляют собой последовательность цифр, дробные числа содержат десятичную точку, текстовые строки заключаются в кавычки и т. д. Но в примерах этой главы — чтобы избежать излишней сложности — мы будем использовать лишь короткие целые числа из десятичной системы счисления и один-два списка. Десятичные целые числа такие же, как числа в математике: они представляют собой последовательность цифр от 0 до 9. В главе 3 мы рассмотрим дополнительные детали работы с целыми числами (например, знаки и недесятичные системы счисления).\n",
      "\n",
      "Переменные\r\n",
      "Вот мы и добрались до ключевого понятия языков программирования. \n",
      "\r\n",
      "Python, как и большинство других компьютерных языков, позволяет вам определять переменные — имена для значений в памяти вашего компьютера, которые вы далее будете использовать в программе.\n",
      "\r\n",
      "Имена переменных в Python отвечают определенным правилам.\n",
      "\n",
      "\n",
      "Они могут содержать только следующие символы:\n",
      "\n",
      "\n",
      "буквы в нижнем регистре (от a до z);\n",
      "\n",
      "буквы в верхнем регистре (от A до Z);\n",
      "\n",
      "цифры (от 0 до 9);\n",
      "\n",
      "нижнее подчеркивание (_).\n",
      "\n",
      "Они чувствительны к регистру: thing, Thing и THING — это разные имена.\n",
      "Они должны начинаться с буквы или нижнего подчеркивания, но не с цифры.\n",
      "Python особо обрабатывает имена, которые начинаются с нижнего подчеркивания (об этом вы сможете прочитать в главе 9).\n",
      "Они не могут совпадать с зарезервированными словами Python (их также называют ключевыми).\n",
      "\r\n",
      "Перед вами список зарезервированных слов:\n",
      "\n",
      "False    await    else    import    pass\n",
      "None     break    except     in     raise\n",
      "True     class    finally    is     return\n",
      "and      continue for      lambda   try\n",
      "as       def      from     nonlocal while\n",
      "assert   del      global   not      with\n",
      "async    elif     if       or       yield\r\n",
      "Внутри программы Python увидеть список зарезервированных слов можно с помощью команд:\n",
      "\n",
      ">>> help(\"keywords\")\r\n",
      "или:\n",
      "\n",
      ">>> import keyword\n",
      ">>> keyword.kwlist\r\n",
      "Корректными являются такие имена:\n",
      "\n",
      "\n",
      "a;\n",
      " a1;\n",
      " a_b_c___95;\n",
      " _abc;\n",
      " _1a.\n",
      "\r\n",
      "А следующие имена некорректны:\n",
      "\n",
      "\n",
      "1;\n",
      "1a;\n",
      " 1_;\n",
      " name!;\n",
      " another-name.\n",
      "\n",
      "Присваивание\r\n",
      "В Python символ = применяется для присваивания значения переменной.\n",
      "\n",
      "В школе нас учили, что символ = означает «равно». Почему же во многих языках программирования, включая Python, этот символ используется для обозначения присваивания? Одна из причин — на стандартной клавиатуре отсутствуют логические альтернативы вроде стрелки влево, а символ = не слишком сбивает с толку. Кроме того, в компьютерных программах присваивание используется чаще, чем проверка на равенство.\r\n",
      "Программы непохожи на алгебру. В школе мы имели дело с подобными уравнениями:\r\n",
      "y = x + 12\n",
      "\r\n",
      "Решить уравнение можно, подставив значение для x. Если вы зададите для x значение 5, то, поскольку 5 + 12 равно 17, значение y будет равно 17. Подставьте значение 6, и y будет равен 18. И так далее.\n",
      "\r\n",
      "Строки компьютерной программы могут выглядеть как уравнения, но означают они при этом нечто иное. В Python и других компьютерных языках x и y являются переменными. Python знает, что цифра или простая последовательность цифр вроде 12 или 5 является числовым литералом. Рассмотрим небольшую программу на Python, которая схожа с этим уравнением, — она выводит на экран значение y:\n",
      "\n",
      ">>> x = 5\n",
      ">>> y = x + 12\n",
      ">>> y\n",
      "17\r\n",
      "Здесь мы видим большое различие между математикой и программами: в математике знак = означает равенство обеих сторон, а в программировании он означает присваивание: переменной слева мы присваиваем значение с правой стороны.\n",
      "\r\n",
      "В программировании также принято, что все находящееся справа от знака = должно иметь значение (это называется инициализацией). Справа вы можете увидеть значение-литерал, переменную, которой было присвоено значение, или их комбинацию. Python знает, что 5 и 12 — это числовые литералы. В первой строке целочисленное значение 5 присваивается переменной х. Теперь мы можем использовать переменную х в следующей строке. Когда Python читает выражение y = x + 12, он делает следующее:\n",
      "\n",
      "\n",
      "видит знак = в середине;\n",
      "понимает, что это оператор присваивания;\n",
      "вычисляет значение с правой стороны (получает значение объекта, на который ссылается переменная х, и добавляет его к 12);\n",
      "присваивает этот результат переменной слева — y.\n",
      "\r\n",
      "Теперь, введя имя у в интерактивном интерпретаторе, можно увидеть его новое значение.\n",
      "\r\n",
      "Если вы начнете программу со строки y = x + 12, Python сгенерирует исключение (ошибку), поскольку переменная х еще не имеет значения:\n",
      "\n",
      ">>> y = x + 12\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "NameError: name 'x' is not defined\r\n",
      "Более подробно об исключениях можно прочитать в главе 9. На компьютерном языке мы скажем, что переменная х не была инициализирована.\n",
      "\r\n",
      "В алгебре вы могли бы сделать все наоборот — присвоить значение у, чтобы подсчитать значение х. Для того, чтобы сделать это в Python, вам нужно получить значения-литералы и инициализированные переменные с правой стороны оператора присваивания до того, как присвоить значение переменной х:\n",
      "\n",
      ">>> y = 5\n",
      ">>> x = 12 - y\n",
      ">>> x\n",
      "7\n",
      "Переменные — это имена, а не локации\r\n",
      "Пришло время сделать важное утверждение о переменных в Python: переменные — всего лишь имена, и в этом заключается отличие Python от других языков программирования. Об этом важно помнить, особенно при работе с такими изменяемыми объектами, как списки. Операция присваивания не копирует значение, а только лишь прикрепляет имя к объекту, содержащему нужные данные. Имя — это ссылка на объект, а не сам объект. Можно представить, что имя — это этикетка, приклеенная на коробку с объектом, которая размещается где-то в памяти компьютера (рис. 2.3).\n",
      "\r\n",
      "В других языках программирования переменные сами по себе имеют тип и привязываются к локации в памяти. Вы можете изменить значение в этой локации, но оно должно быть того же типа. Именно поэтому в статических языках нужно объявлять тип переменных. В Python этого делать не требуется, поскольку имя может ссылаться на все что угодно: значение и тип мы получаем, идя по цепочке к самому объекту с данными. Такой подход экономит время, но при этом имеет свои недостатки.\n",
      "\n",
      "\n",
      "Вы можете неверно написать имя переменной и получить исключение, поскольку она ни на что не ссылается, Python не выполняет такую проверку автоматически в отличие от статических языков. В главе 19 показывается способ предварительной проверки переменных.\n",
      "В сравнении с такими языками, как С, у Python скорость работы ниже. Ведь он заставляет компьютер выполнять больше работы, для того чтобы вам не пришлось выполнять ее самостоятельно.\n",
      "\r\n",
      "Попробуйте сделать следующее с помощью интерактивного интерпретатора (рис. 2.4).\n",
      "\n",
      "\n",
      "Как и раньше, присвойте значение 7 имени a. Это создаст объект-«ящик», содержащий целочисленное значение 7.\n",
      " Выведите на экран а.\n",
      " Присвойте имя а переменной b, заставив b прикрепиться к объекту-«ящику», содержащему значение 7.\n",
      "Выведите b.\n",
      "\n",
      ">>> a = 7\n",
      ">>> print(a)\n",
      "7\n",
      ">>> b = a\n",
      ">>> print(b)\n",
      "7\n",
      "\r\n",
      "В Python, если нужно узнать тип какого-либо объекта (переменной или значения), можно использовать конструкцию type(объект). type() — одна из встроенных в Python функций. Чтобы проверить, указывает ли переменная на объект определенного типа, используйте конструкцию isinstance(type):\n",
      "\n",
      ">>> type(7)\n",
      "<class 'int'>\n",
      ">>> type(7) == int\n",
      "True\n",
      ">>> isinstance(7, int)\n",
      "True\n",
      "Когда я упоминаю функцию, то после ее имени размещаю круглые скобки (()) и таким образом подчеркиваю, что это именно функция, а не имя переменной или что-либо еще.\r\n",
      "Попробуем проделать это с разными значениями (58, 99.9, 'abc') и переменными (a, b):\n",
      "\n",
      ">>> a = 7\n",
      ">>> b = a\n",
      ">>> type(a)\n",
      "<class 'int'>\n",
      ">>> type(b)\n",
      "<class 'int'>\n",
      ">>> type(58)\n",
      "<class 'int'>\n",
      ">>> type(99.9)\n",
      "<class 'float'>\n",
      ">>> type('abc')\n",
      "<class 'str'>\r\n",
      "Класс — это определение объекта (классы детально рассматриваются в главе 10). В Python значения терминов «класс» и «тип» примерно одинаковы.\n",
      "\r\n",
      "Как вы могли заметить, при упоминании имени переменной Python ищет объект, на который она ссылается. Неявно Python выполняет большое количество действий и часто создает временные объекты, которые будут удалены спустя одну-две строки. \n",
      "\r\n",
      "Снова рассмотрим пример, показанный ранее:\n",
      "\n",
      ">>> y = 5\n",
      ">>> x = 12 - y\n",
      ">>> x\n",
      "7\r\n",
      "В этом фрагменте кода Python сделал следующее:\n",
      "\n",
      "\n",
      "создал целочисленный объект со значением 5;\n",
      "создал переменную у, которая указывает на этот объект;\n",
      "нарастил счетчик ссылок для объекта, содержащего значение 5;\n",
      "создал еще один целочисленный объект со значением 12;\n",
      "вычел значение объекта, на который указывает переменная у (5), из значения 12, содержащегося в анонимном объекте;\n",
      "присвоил результат (7) новому (пока еще безымянному) целочисленному объекту;\n",
      "заставил переменную х указывать на этот новый объект;\n",
      "нарастил счетчик ссылок для объекта, на который указывает переменная х;\n",
      "нашел значение объекта, на который ссылается переменная х (7), и вывел его на экран.\n",
      "\r\n",
      "Когда количество ссылок на объект становится равным нулю, это означает, что ни одно имя на него больше не ссылается, поэтому хранить такой объект нет необходимости. В Python имеется сборщик мусора, который позволяет повторно использовать память, занятую уже ненужными на данный момент объектами: представьте себе, будто кто-то следит за этими полками с памятью и забирает ненужные коробки на переработку.\n",
      "\r\n",
      "В нашем случае объекты со значениями 5, 12 и 7, а также переменные x и y больше не нужны. Сборщик мусора Python может или отправить их в небесный рай для объектов, или сохранить, исходя из соображений производительности, так как небольшие целые числа используются довольно часто.\n",
      "\n",
      "Присваивание нескольким именам\r\n",
      "Вы можете присвоить значение сразу нескольким переменным одновременно:\n",
      "\n",
      ">>> two = deux = zwei = 2\n",
      ">>> two\n",
      "2\n",
      ">>> deux\n",
      "2\n",
      ">>> zwei\n",
      "2\n",
      "Переназначение имени\r\n",
      "Поскольку имена указывают на объекты, если изменить значение, присвоенное имени, оно начнет указывать на другой объект. Счетчик ссылок старого объекта уменьшится на 1, а счетчик ссылок нового увеличится на ту же величину.\n",
      "\n",
      "Копирование\r\n",
      "Как вы видели на рис. 2.4, присваивание существующей переменной а новой переменной b заставит b указывать на тот же объект, что и a. Если вы выберете этикетку a или b и обратитесь к объекту, на который они указывают, вы получите одинаковый результат.\n",
      "\r\n",
      "Если объект неизменяем (например, целое число), его значение нельзя изменить, поэтому по умолчанию оба имени являются доступными только для чтения. Попробуйте выполнить следующий код:\n",
      "\n",
      ">>> x = 5\n",
      ">>> x\n",
      "5\n",
      ">>> y = x\n",
      ">>> y\n",
      "5\n",
      ">>> x = 29\n",
      ">>> x  \n",
      "29\n",
      ">>> y\n",
      "5\r\n",
      "Когда мы присваиваем переменную x переменной y, переменная y начинает указывать на целочисленный объект со значением 5, на который также указывает и переменная x. Далее мы изменяем переменную x так, чтобы она указывала на целочисленный объект со значением 29. Объект со значением 5, на который все еще указывает переменная y, не изменился. \n",
      "\r\n",
      "В случае, когда оба имени указывают на изменяемый объект, вы можете изменить значение объекта с помощью любого имени. Если вы этого еще не знали, такая особенность может вас удивить.\n",
      "\r\n",
      "Список представляет собой изменяемый массив значений (в главе 7 этот тип данных описывается более подробно). В нашем примере a и b указывают на список, содержащий три целочисленных объекта:\n",
      "\n",
      ">>> a = [2, 4, 6]\n",
      ">>> b = a\n",
      ">>> a\n",
      "[2, 4, 6]\n",
      ">>> b\n",
      "[2, 4, 6]\r\n",
      "Эти элементы списка (a[0], a[1] и a[2]) сами по себе являются именами, указывающими на целочисленные объекты со значениями 2, 4 и 6. Список хранит элементы в заданном порядке.\n",
      "\r\n",
      "Теперь давайте изменим первый элемент списка с помощью имени а и убедимся, что список b также изменился:\n",
      "\n",
      ">>> a[0] = 99\n",
      ">>> a\n",
      "[99, 4, 6]\n",
      ">>> b\n",
      "[99, 4, 6]\r\n",
      "Когда первый элемент списка изменяется, он больше не указывает на объект со значением 2. Теперь он указывает на объект со значением 99. Список все еще имеет тип list, но его значения (элементы списка и их порядок) можно изменить.\n",
      "\n",
      "Выбираем хорошее имя переменной\n",
      "Он говорил правильные вещи, но называл их неверными именами.\r\n",
      "Элизабет Барретт Браунинг\r\n",
      "Удивительно, но выбор соответствующих имен для переменных очень важен. Во многих примерах кода, которые мы успели рассмотреть, я использовал простейшие имена вроде a и x. В реальных программах вам будет нужно отслеживать гораздо больше переменных одновременно и придется балансировать между краткостью и понятностью. Например, имя num_loons можно напечатать быстрее, чем number_of_loons или gaviidae_inventory, однако они более понятны, чем имя n.\n",
      "\r\n",
      "» Более подробно с книгой можно ознакомиться на сайте издательства\r\n",
      "» Оглавление\r\n",
      "» Отрывок\n",
      "\r\n",
      "Для Хаброжителей скидка 25% по купону — Python\n",
      "\r\n",
      "По факту оплаты бумажной версии книги на e-mail высылается электронная книга.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет, Хабр! Я Слава, руководитель команды «Профессиональные инструменты» в Циане, член ПК Moscow Python Conf и член core-команды авторов в Яндекс.Практикуме.\n",
      "\r\n",
      "Вместе с ребятами мы развиваем курс «Мидл python-разработчик», и сегодня я хочу поделиться моим личным списком книг, которые помогут вам структурировать и углубить свои знания о разработке на языке Python.\n",
      "\n",
      " \n",
      "\n",
      "«Изучаем Python», Марк Лутц \n",
      "\n",
      "\r\n",
      "Открывает топ книга Марка Лутца «Изучаем Python». Она занимает особое место в моей карьере — моё изучение Python проходило именно по ней. В студенчестве она казалась мне фундаментальным трудом — в книге почти полторы тысячи страниц!\n",
      "\r\n",
      "Книга позволит войти в Python, что называется, с нуля. Почему люди программируют на Python? Кто использует Python сегодня? Что можно делать при помощи Python? Каковы сильные стороны Python? Ответы на эти вопросы позволят получить общее представление о языке.\n",
      "\r\n",
      "Книга знакомит читателя с интерактивным режимом — командной строкой Python. Вы узнаете о том, как выводить текст в стандартный поток вывода, о модулях и об их импорте, об особенностях использования Python в Windows.\n",
      "\r\n",
      "Дальше следует описание типов данных и особенностей работы с ними. Отдельными главами раскрывается работа со строками и словарями. Вы узнаете о ветвлении и циклах, итераторах, генераторах, функциях и их аргументах, о модулях и пакетах как способах организации кода. Затем следует введение в ООП: классы, наследование, перегрузка операторов, полиморфизм и метаклассы. Рассказывается об исключениях, о их роли в языке.\n",
      "\r\n",
      "На мой взгляд, «Изучаем Python» очень плавно и всеобъемлюще знакомит читателя с миром Python, с одной стороны, не углубляясь в детали, с другой — давая всю полноту картины.\n",
      "\n",
      "«Высокопроизводительный Python: практическое пособие для людей», Миша Горелик, Ян Освальд\n",
      "\n",
      "\r\n",
      "Второе место топа занимает книга М. Горелика и Я. Освальда «Высокопроизводительный Python: практическое пособие для людей».\n",
      "\r\n",
      "Книга позволит вам немного заглянуть под капот языка. Что вообще такое высокая производительность применительно к Python? Книга знакомит с виртуальной машиной Python, учит пользоваться инструментами профилирования для поиска узких мест в использовании процессорного времени, рассказывает о типах данных с точки зрения их внутреннего устройства.\n",
      "\r\n",
      "Вы узнаете о многопоточности, асинхронном и мультипроцессорном исполнении, о том, как организовать кластерные вычисления и как потреблять меньше оперативной памяти. Для особо искушённых есть отдельная глава о компиляции кода на Python в C.\n",
      "\r\n",
      "Ваш код может быть абсолютно верным, но работать недостаточно быстро. Чтобы ускорить его работу, вам нужно понять, как устроен язык, на котором вы пишете. Это практическое руководство поможет вам получить глубокое понимание реализации Python.\n",
      "\n",
      "«Python. Разработка на основе тестирования», Гарри Персиваль\n",
      "\n",
      "\r\n",
      "Следующая книга топа — «Python. Разработка на основе тестирования» Гарри Персиваля. Мне посчастливилось пообщаться с Гарри в рамках прошедшей Russian Python Week 2020. Говорят, британцы чопорные, со странным чувством юмора. Я думаю, что это стереотипы. Гарри много и смешно шутил, улыбался, рассказывал истории из жизни, был очень открыт. Лучше всего о своей книге Гарри расскажет сам.\n",
      "\n",
      "\n",
      "Меня иногда спрашивают, как я пришёл к тому, чтобы написать эту книгу. Впервые я изучил Python по книге «Dive Into Python» — это было примерно в 2008-м. Я изучал словари, строки, узнавал различные фичи языка — все вот эти вещи. И где-то к 14-й главе впервые появились слова о юнит-тестировании. Там нужно было написать калькулятор римских чисел. И было написано, что гораздо легче это сделать, если вы пишете юнит-тесты. Я подумал: «О, это звучит круто!» Но это звучало как что-то, что вам следует делать. Как чистить зубы зубной нитью — хорошо бы, чтобы вы это делали каждый день. Разумеется, с таким посылом я проигнорировал этот совет полностью и сразу же о нём забыл. К тому моменту я программировал на Python уже три недели — зачем мне какое-то юнит-тестирование? У меня и так дела идут хорошо! Если я напишу небольшое Django-приложение, несколько веб-страниц — без проблем, я могу всё это проверить. И знаете что? У меня действительно было всё отлично. Поначалу.\n",
      "\r\n",
      "Когда моё приложение было небольшим, было очень просто проверить руками, работает оно или нет. Где-то на второй неделе разработки у меня уже было восемь различных модулей, 12 веб-страниц, появилось множественное наследование веб-форм, и перед очередным изменением я стал рассуждать так.\n",
      "\r\n",
      "– Я внесу изменения сюда. О, нет, это поломает мне вон тот класс. Лучше изменю здесь. Хотя… есть риск, что перестанет работать веб-форма. Лучше вот сюда. Да, точно, сюда! Ах, черт, всё равно сломалось!\n",
      "\r\n",
      "И довольно быстро даже крошечное изменение стало для меня очень болезненным. Я не представлял, что на самом деле от чего зависит, страх вносить изменения сковал меня, я не мог ничего отрефакторить — мой код превратился в лапшу.\n",
      "\r\n",
      "Когда я пришёл на свою первую работу, там практиковали разработку через тестирование и экстремальное программирование. И я был уже готов усвоить свой урок, но всё ещё много жаловался: «Юнит-тесты и функциональные тесты? Зачем так много тестов?» Разработка была примерно такой: ты делаешь микроизменение — и запускаешь тест. Делаешь ещё микроизменение — снова запускаешь тест. И так всё время. Я говорил: «Вы серьёзно? Неужели мы не можем пропустить этот шаг, мы сэкономим время!» На что мне спокойно говорили: «Нет-нет, Гарри, продолжай делать так». Я жаловался, я ныл, это было тяжело, это занимало много времени. Но! Через какое-то время мне открылась вся прелесть такого подхода! Я понял, что до этого делал что-то не так, и что вот он — правильный путь. Спустя пару лет я решил написать книгу об этом.\n",
      "\r\n",
      "Разработка на основе тестирования — не естественная вещь; чтобы проникнуться, нужна практика. В книге я как раз и пытаюсь дать немного практики.\r\n",
      "Книга позволит вам переосмыслить вашу работу и подход к ней, буквально перевернёт сознание. Многие рутинные вещи обретут новый смысл, к ним появится интерес. Качество тестов заметно улучшится, и это неизбежно повлияет на качество кода и архитектуру приложения.\n",
      "\n",
      "«Architecture Patterns with Python: Enabling Test-Driven Development, Domain-Driven Design, and Event-Driven Microservices», Гарри Персиваль, Боб Грегори\n",
      "\n",
      "\r\n",
      "Я не мог не поговорить с Гарри Персивалем о новой книге, над которой он работает в соавторстве с Бобом Грегори, — «Архитектурные паттерны в Python». Книгу можно почитать бесплатно на сайте www.cosmicpython.com. Гарри продолжает.\n",
      "В конце первой книги я стал говорить о разных типах тестов. В проекте могут быть низкоуровневые юнит-тесты, интеграционные тесты, где мы проверяем интеграцию с API, или с файловой системой, или с базой данных, а также высокоуровневые тесты — e2e-тесты, функциональные тесты; возможно, тесты на всю систему целиком. Я начал рассуждать о правильном балансе, как много каких тестов должно быть, — об идее пирамиды тестирования. Я также рассуждал о цене, которую нужно за тесты платить: некоторые тесты проходят очень быстро, некоторые медленно, некоторые более надёжные, некоторые менее. Оказалось, что это тяжело — найти правильный баланс скорости тестов и уверенности в том, что они надёжные.\n",
      "\r\n",
      "Новая книга пытается дать ответ на этот вопрос. Она о выборе архитектурных шаблонов, о способах структурировать ваш код, которые позволят получить максимальное покрытие юнит-тестами и минимальное количество тяжёлых e2e- и интеграционных тестов. И достигается это как раз хорошим дизайном вашего приложения. Это непросто. \n",
      "\r\n",
      "Мне невероятно повезло: я встретил своего коллегу Боба, который оказался экспертом в вопросе. Он стал соавтором книги и рассказал мне о том, как он решает эту задачу. Мы транслировали несколько хороших идей из мира Java и C# (представителей энтерпрайз-мира) в мир Python. Разумеется, мы не просто вставляли код Java в Python (это было бы ужасно), мы пытались ответить на вопрос, как это лучше всего делать в стиле pythonic way.\n",
      "\r\n",
      "Основной вопрос, на который я пытаюсь дать ответ в этой книге, — каким образом архитектура приложения может помочь нам получить тесты наивысшего качества.\r\n",
      "Книгу стоит читать как продолжение «Python. Разработка на основе тестирования». Если вам понравилась первая книга, а идеи, рассказанные там, нашли у вас отклик, то вторая книга ответит на все ваши вопросы и поможет воплотить подход в жизнь — на работе или в вашем личном проекте.\n",
      "\n",
      "«Python. К вершинам мастерства», Лучано Рамальо\n",
      "\n",
      "\r\n",
      "Замыкает сегодняшний топ книга «Python. К вершинам мастерства» великолепного Лучано Рамальо. Книга стала очень успешной и была переведена на восемь языков.\n",
      "\r\n",
      "Если попытаться одной строкой раскрыть, о чём эта книга, то я бы сказал так: она о том, как использовать Python 3 наилучшим возможным способом.\n",
      "\r\n",
      "Лучано 66 лет, он живёт и работает в Сан-Паулу в Бразилии. Мне посчастливилось пообщаться с ним в рамках прошедшей Russian Python Week 2020. Несмотря на возраст, Лучано показался мне очень живым, открытым и жизнерадостным собеседником.\n",
      "\n",
      "\n",
      "\r\n",
      "Сейчас Лучано работает над вторым изданием книги. Он поделился, что основным изменением в новой книге будет глава «Введение в Type Hints» — она ещё не закончена, но уже сейчас в ней 60 страниц, и это будет самая большая глава книги!\n",
      "\r\n",
      "Однажды Лучано позвали играть в волейбол.\n",
      "\r\n",
      "— Я не умею играть в волейбол, — сказал Лучано.\r\n",
      "— Да никто понятия не имеет, как играть в волейбол, нам просто нужен ещё один человек, чтобы сформировать команду, — ответили ребята.\n",
      "\r\n",
      "В команде только капитан был профессионалом. Он объяснил: игрок с дальней части поля передаёт мяч игроку в центре, он делает передачу игроку под сеткой, который в свою очередь отправляет мяч на сторону противника. Это Best Practice, так играют Pro.\n",
      "\r\n",
      "Игра началась, команда действовала по инструкции и со временем начала проигрывать. \n",
      "\r\n",
      "Реальность была такова, что никто за исключением капитана не умел играть достаточно хорошо, и часто при передачах команда теряла мяч — он падал на землю, и ход переходил к противнику.\n",
      "\r\n",
      "Лучано собрал команду.\n",
      "\r\n",
      "— Давайте перестанем притворяться, что мы Pro — на самом деле мы не умеем профессионально играть! Давайте сразу закидывать мяч на сторону противника без передач внутри — так, как мы делали когда-то давно в школе!\n",
      "\r\n",
      "И это сработало! Это не было Best Practice, но эта тактика лучше всего подходила к ситуации и для команды, которая у них была. И в конце концов это привело к победе.\n",
      "\r\n",
      "Эта небольшая история показательна и для мира Python. Нужно понимать, что Python используют очень много людей, которые не являются профессиональными разработчиками — это физики, биологи, социологи и другие. Нужно, чтобы инструмент не требовал от них выполнения Best Practice, а позволял решать насущную задачу. И Python это позволяет.\n",
      "\r\n",
      "Лучано очень любит программировать. Он начал изучать Python 20 лет назад и с тех пор обучил ему многих других людей. Лучано обнаружил, что поскольку Python очень лёгок для обучения, иногда люди даже не читают tutorial полностью. Из-за этого иногда, читая чужой код, можно обнаружить, что человек пишет Java-код на Python или Ruby-код на Python, то есть пытается повторить на другом языке то, к чему он привык. Лучано создал курс «Python для тех, кто знает Python». После этого он много выступал на тему того, как делать те или иные вещи в стиле pythonic way. Так и родилась книга.\n",
      "\r\n",
      "Книга стала очень успешной и в мире, и для самого Лучано: он получил работу в ThoughtWorks. Он очень хотел там работать, но не решался отправлять к ним своё резюме из-за высоких требований к кандидатам. После того как книга вышла в свет, они сами к нему пришли. Сегодня ThoughtWorks поддерживают Лучано как публичного спикера точно так же, как поддерживают Мартина Фаулера.\n",
      "\r\n",
      "Русское название для книги предложил Python core developer Юрий Селиванов. Лучано считает, что это хороший перевод его книги. \n",
      "\r\n",
      "Если вы серьёзно настроены построить карьеру Python-разработчика, то эта книга для вас.\n",
      "\n",
      "Зачем читать книги\r\n",
      "Я глубоко убеждён, что читать книги необходимо людям всех возрастов. Не только литературу профессиональную, но и художественную. Книги действуют на человека, как точильный камень действует на саблю, — они придают знаниям остроту.\n",
      "\r\n",
      "Стивен Р. Кови в своей книге «7 навыков высокоэффективных людей» называет это «затачиванием пилы». Этот навык — ваши личные ресурсы и средства. Он поддерживает и развивает самый ценный ваш ресурс — вас самих.\n",
      "\r\n",
      "«Как ты узнаёшь новое?» — вопрос, который обычно я задаю кандидатам на собеседованиях. Ответ на него для меня действительно важен, и тут мне недостаточно услышать про чтение статей, просмотр видео на ютубе или прослушивание подкастов. Только книги придают знаниям структуру, без разрывов и со всей полнотой контекста проводя читателя к просветлению.\n",
      "\r\n",
      "Перефразируя Антона Птушкина, скажу: «Читайте, оно того стоит!»\n",
      "\r\n",
      "P.S. Список книг, который мы обсудили сегодня, не претендует на полноту. Если вы считаете, что есть книга, которая не менее других достойна занять своё место в рейтинге, то я приглашаю вас присоединиться к доске Trello, на которой мы — русское комьюнити Python-разработчиков — формируем собственный рейтинг книг о разработке на Python. Чувствуйте себя как дома — голосуйте за существующие, добавляйте новые книги или целые категории!\n",
      "\r\n",
      "P.P.S. Остались вопросы? Ищите меня в сети: slavabezborodov.com    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Когда дело доходит до автоматизации тестирования, первый и самый сложный вопрос, который встает перед вами это какой язык выбрать, чтобы он имел хорошую поддержку автоматизации тестирования. Python, Java, C#, Ruby и т.д. – лишь некоторые из популярных языков, которые можно использовать с фреймворком для автоматизации Selenium. Несмотря на это, во всем мире QA-команды активно используют именно Python, особенно в связке с Selenium WebDriver.    У разработчиков и тестировщиков появляется несколько вариантов, когда дело доходит до использования языков программирования с фреймворком Selenium. Как у инженера по автоматизации, у меня была возможность пользоваться разными языками, начиная с С и заканчивая Java, Python, C++ в зависимости от проекта. Несмотря на то, что мне удобно использовать Java и C# с Selenium, Python все равно остается лучшим языком для написания скриптов для автоматизации тестирования с этим фреймворком.    В этой статье я расскажу о 12 основных причинах, по которым автоматизированное тестирование на Python в связке с Selenium WebDriver – это отличный вариант в моем случае.     Итак, начнем. Немногословный и хорошо читаемый Большинство из вас согласится, что Python – один из лучших языков программирования, когда речь идет о простоте написания кода и удобстве чтения. По сравнению с другими языками программирования, которые можно использовать для автоматизации тестирования с Selenium, Python не такой многословный и относительно прост в использовании.    Поскольку Python – скриптовый язык, тестировщику не нужно беспокоиться о компиляции для преобразования кода в исполняемый файл. Дзен Python, который является отличным руководством по автоматизации тестирования на Python, напоминает нам о том, что реализация, которую легко объяснить – это всегда хорошая идея! По сути, это значит, что тесты, реализованные как часть набора тестов должны быть удобочитаемыми, наглядными и принцип их работы должно быть легко объяснить.    API, реализующие автоматизацию тестирования на Python, используются для подключения к браузеру через фреймворк автоматизации Selenium. Если использовать Selenium WebDriver по назначению, то тесты пишутся предельно легко. Ниже я приложил скриншот с основополагающими принципами проектирования на Python (если вы напишете import this в командной строке, то получите Дзен Python): Исчерпывающий список фреймворков для автоматизации тестирования на Python    PyUnit (или Unittest) – это дефолтный фреймворк для тестирования на Python. Помимо PyUnit, в Python есть ряд фреймворков тестирования для Selenium. Вот несколько широко используемых - PyTest, Behavior, Robot, Lettuce, Node 2 и Testify. Такие фреймворки, как PyTest, Nose 2 и Testify, можно использовать для автоматизированного тестирования юнитов, интеграционного тестирования и тестирования систем. Фреймворк Robot – это наиболее предпочтительный фреймворк автоматизации тестирования Python для автоматизации роботизированных процессов (RPA), тестирования с Selenium и ATDD (разработки на основе приемочных тестов). Lettuce и Behavior - это фреймворки для тестирования на Python, которые лучше всего подходят для BDD (Behavior Driven Development). Весь широкий спектр фреймворков можно установить с минимальными усилиями (например, с помощью команды pip install) и их очень удобно использовать в связке с Selenium WebDriver.    Очень немногие языки программирования предлагают такое разнообразие для автоматизации тестирования, и это еще одно преимущество Python, которое делает его лучшим языком для автоматизации тестирования. PyTest – лучший фреймворк для автоматизации тестирования на Python    Ранее мы говорили о популярных фреймворках для автоматизированного тестирования на Python, но этот список возглавляет PyTest, как лучший фреймворк. PyTest также может использоваться для модульного, интеграционного и сквозного тестирования, как и другие фреймворки Python.    Несмотря на то, что PyUnit (unittest) – фреймворк по умолчанию, разработчики и тестировщики активно использую PyTest, поскольку он функциональнее PyUnit. Тесты могут состоять из простых функций, или же они могут принимать входные параметры для поддержки параметризированного тестирования. Фикстуры PyTest позволяют с легкостью выполнять тесты на различных сочетаниях браузеров и платформ. Параметризированные фикстуры PyTest можно выполнять с различными входными значениями.     Перенос существующих реализаций, использующих PyTest и Selenium WebDriver, на параметризованные фикстуры требует минимальных усилий. Фикстуры лучше всего подходят для выполнения операций настройки и очистки. Плагины, которыми можно расширить PyTest, могут помочь в покрытии кода, параллельном тестировании и многом другом.   Очень простое параллельное тестирование Параллельное тестирование в Selenium широко используется для ускорения выполнения автоматизированных тестов на различных сочетаниях браузеров и платформ (т.е. операционных систем). Все языки программирования поддерживают параллельное тестирование (или параллельное выполнение) с помощью фреймворка Selenium, но его чертовски просто использовать в Python.    При использовании фреймворка PyTest плагин pytest-xdist помогает выполнять тесты параллельно. Этот вариант предпочтителен в тех случаях, когда тесты не являются потокобезопасными и нуждаются в изоляции состояния. Установить плагин можно выполнив pip install <имя плагина> в терминале. Плагин pytest-parallel следует использовать для параллельного выполнения тестов Selenium. Он является потокобезопасным и не управляет состоянием в среде Python.    Следовательно, pytest-xdist реализует параллелизм, тогда как pytest-parallel – конкурентность и параллелизм. В них есть способ выделить максимальное количество воркеров (или процессов) для запуска максимального количества одновременных тестов на одного воркера.    Включение параллельного выполнения с помощью PyTest не требует каких-либо изменений в существующей реализации, поскольку соответствующие опции для параллельного тестирования предоставляются в терминале при запуске кода.    Тот же принцип параллельного тестирования применим и к другим фреймворкам тестирования на Python, таким как Node 2, Behavior, Lettuce и другим. С другой стороны, включение параллельного тестирования с помощью Selenium Java или Selenium C# или других комбинаций требует изменений в коде и подразумевает больше шагов, чем в Python. Простота установки пакетов PIP – это стандартная система менеджмента пакетов в Python. Установка нового пакета с помощью pip не требует загрузки пакета. Команда pip install ищет необходимый пакет в PyPI, подтягивает зависимости и устанавливает необходимые пакеты, чтобы обеспечить выполнение запроса на установку без каких-либо проблем.    Несмотря на то, что такие языки, как C# и Java, предлагают относительно схожие функции (например, консоль диспетчера пакетов в C#), установка пакетов в Python через pip – совершенно другой опыт! Мультипарадигмальный язык программирования Python – это мультипарадигмальный язык программирования. Следовательно, в нем есть полная поддержка ООП и структурного программирования. Большинство функций Python поддерживают функциональное программирование и аспектно-ориентированное программирование.    Требования аспектно-ориентированного программирования реализуются путем включения метапрограммирования, которое дает возможность программам рассматривать другие программы в качестве данных. Читабельность и написание тестовых функций на Python реализованы лучше, поскольку фреймворк не навязывает добавление функций в классы.    В отличие от других языков программирования, Python позволяет тестировщику решать, что следует использовать для автоматизации тестирования на Selenium – классы или функции. Широкий выбор IDE Для автоматизированного тестирования на Python я предпочитаю использовать PyCharm (Community Edition), поскольку его можно легко использовать вне зависимости от фреймворков, которые обеспечивают автоматизацию с Selenium. Я бы рекомендовал переходить на PyCharm (Professional Edition) только в том случае, если вы используете BDD -фреймворки в Python.    Можно использовать GVim, Notepad++ и Visual Studio Code для быстрого редактирования, особенно если сценарии автоматизации тестирования не настолько сложны. Visual Studio Code я выбираю в тех случаях, когда дело доходит до редактирования кода, поскольку он предоставляет некоторые плагины, которые в значительной степени облегчают задачи редактирования.  Динамическая типизация    Python использует динамическую типизацию и позднее связывание (или динамическое разрешение имен), которое связывает методы и имена переменных в ходе выполнения. Эта механика очень удобна для автоматизации тестирования на Python.    Python также предлагает такие опции, как Pyre и Mypy, которые используются для статической проверки типов. С их помощью Python позволяет сочетать возможности динамической и статической типизации. Мощная и беспроблемная отчетность Отчеты – одна из основных составляющих автоматизации тестирования с Selenium, ведь именно отчеты обеспечивают наибольшую наглядность процесса автоматизированного тестирования. Отчеты, которые выдают корректную информацию в сжатой и понятной форме, можно отправить заинтересованным сторонам, чтобы они были в курсе прогресса на фронте тестирования.    Интегрировать отчеты с автоматизацией тестирования на Selenium с помощью PyTest можно с помощью пакета pytest-html. Отчеты о тестах, сгенерированные с pytest-html, предоставляют визуальный контент для легкого сравнения результатов. Сгенерировать отчеты для тестов Selenium можно без особых усилий в PyTest.    С помощью API, предлагаемых cloud Selenium Grid, таких как Lambda Test, можно построить более внушительную стратегию ведения отчетности для автоматизированного тестирования.  Командная строка в помощь Ранее мы упомянули ряд IDE, которые можно использовать для разработки и выполнения автоматизированного тестирования на Python. Большинство фреймворков для тестирования оснащены тест-раннерами, которые позволяют выполнять тесты из терминала (командной строки).    Если вы хорошо знакомы с Python, то комбинация IDE (например, Visual Studio Code) и тест-раннера поможет сделать эту работу за вас! Просто поставьте в IDE необходимые плагины для популярных фреймворков тестирования (например, PyTest, Behavior и т.д.). Теперь вы полностью готовы к насыщенному событиями путешествию в мир автоматизированного тестирования на Python! Подходит как для больших, так и для маленьких проектов В Python есть большая стандартная библиотека. Конструкции языка и объектно-ориентированный подход помогают создавать легко читаемый код, который выполняет необходимую работу.    Простая номенклатура имен, которой следуют фреймворки (например, тестовые функции в PyTest должны начинаться с test_), облегчает работу по опознаванию тестовых функций.    Начать автоматизировать тестирование на Python несложно, поскольку порог входа очень низкий. Язык подходит как для больших, так и для маленьких проектов, что делает его лучшим скриптовым языком для автоматизации тестирования.    Простая интеграция с инструментами CI/CD Jenkins – один из самых популярных открытых инструментов для непрерывной интеграции (CI) и непрерывной доставки (CD). Travis CI, Circle CI, TeamCity, Bamboo, Azure Pipeline и т.д. – другие варианты, которые можно использовать вместо Jenkins, однако я все равно предпочитаю использовать Jenkins с «открытым исходным кодом».    Freestyle и пайплайн Jenkins – два варианта оркестрации работ в проекте. В отличие от других языков, Python не требует «сборки» как таковой, но сильная экосистема делает Python, Selenium и Jenkins убийственной комбинацией.    Экосистема Python содержит инструменты, которые можно легко интегрировать в Jenkins для генерации отчетов и тестирования, а также для отчетов по метрикам кода, такие как Pylint. Поскольку генерация отчетов PyTest может запускаться из самой командной строки, нет необходимости добавлять последовательность действий после сборки при использовании пайплайна Jenkins с PyTest.     Использовать Jenkins в связке с PyTest очень просто, так как переход от локальной среды выполнения к Jenkins подразумевает минимальное количество шагов (например, создание проекта, добавление параметров командной строки для выполнения и т. д.), что существенно упрощает переход! Jenkins предлагает прямой вариант создания проекта Maven, но помните про кривую обучения, если вы хотите использовать отчетность, что непосредственно к Jenkins не относится.    Вот и все! Несмотря на то, что выбор лучшего языка программирования для автоматизации тестирования с Selenium весьма субъективен, Python на сегодняшний день является лучшим скриптовым языком для автоматизации тестирования. Легко начать автоматизировать тесты с Python, поскольку широкий спектр его фреймворков можно использовать для модульного, кросс-браузерного тестирования и других его видов. Я уверен, что вам понравится автоматизированное тестирование на Python, как только вы начнете им пользоваться.    Расскажите в комментариях о том, в связке с каким языком вы предпочитаете использовать Selenium, и как вы оцениваете его по сравнению с Python, бесспорным королем автоматизированного тестирования.    Счастливого тестирования!В преддверии старта курса \"Python QA Engineer\" мы приглашаем всех желающих записаться на бесплатный двухдневный интенсив в рамках которого вы сможете изучить детальное планирование авто-тестирования на Python. Вместе с преподавателем сделаете разбор функциональности приложения, формализацию и описание тестовых сценариев, подготовите и настроите инфраструктуру и напишете авто-тесты.Интенсив: \"Планирование и реализация автоматизированного тестирования\". День 1Интенсив: \"Планирование и реализация автоматизированного тестирования\". День 2    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Многие из языков, на которых я пишу, считаются мертвыми. Эта статья - по мотивам моего выступления на прошлогоднем Highload, где я рассказывал про специфику Ruby и Python при высоких нагрузках, про их мейнстрим, который вы можете встретить в выживших проектах. Это рассказ не про спортивное программирование, где делают миллион запросов в секунду на одной ноде, выжигая из Python или Ruby всё и оставляя голый С. Python и Ruby действительно медленные, у них есть GIL, но при правильном использовании это не проблема, а статья расходов. Что разработчики Highload решений могут получить за эти деньги?Если вы предпочитаете слушать или смотреть (у меня там забавные крылья!) — видео моего выступления на конференции HighLoad++ Весна 2021.Мы в Evrone занимаемся заказной разработкой. Делаем бэкенд, веб, ERP, CRM с использованием широкого стека технологий: Python, Ruby, Go, Rust, фронтенд, даже Elixir иногда. Среди наших многочисленных клиентов мы обнаружили мнение, что для каждой задачи есть какой-то лучший инструмент. И если мы делаем highload, то, наверное, обязательно его писать на Си, или на худой конец на Rust. Или на Go, если у нас очень много микросервисов.А потом люди неожиданно понимают, что мир не черно-белый. К примеру, приходят к руководству питонисты, все 20 человек, и говорят: «Знаете, мы уже давно делаем machine learning, вам всё нравится, но вообще мы же питонисты! Мы хотим делать не только machine learning! У нас есть Django, есть FastAPI — давайте мы вам немножко бэкендика забабахаем!» Руководство сидит и думает: «Но мы же highload. Как можно использовать Python в highload? Это же противоестественно» — и не знает, что ответить.А бывают еще более страшные истории, когда молодые амбициозные разработчики говорят: «Мы посмотрели, как Дэвид Хейнемейер Ханссон с помощью фулстека Ruby сделал Hey.com вчера, и этот Hey.com держит сотни тысяч подключений, и Ruby прямо такой фулстечный! Давайте мы тоже зафигачим — будет быстро, качественно, недорого!»Сидит руководство и думает: «Хм, быстро, качественно и недорого — ведь не бывает такого! А если на HeadHunter зайти, там питонистов 5 тысяч, а рубистов — всего жалкая тысяча. Как же можно писать большие проекты на Ruby? Непонятно».Этим кто-нибудь пользуется?Я не последний некромант. Если посмотреть на такие большие проекты, как Uber, Instagram, Reddit — все они написаны на Python. И они не просто написаны на Python. Разработчики этих компаний также, как и я, рассказывают, что они писали на Python, пишут на Python, будут писать на Python, что Python — это круто. Что им нравится, они реализуют highload, решают амбициозные задачи и быстро фигачат фичи. А Shopify, Netflix или GitHub? Они — сюрприз! — написаны на Ruby. И разработчики из этих компаний также радостно докладывают о своих результатах. Например, в GitHub рассказывают, как они портировали GitHub со старой версии Rails на новые версии Rails, как они всё это превозмогали, но как им понравилось. И как они быстро пилят фичи и все отлично работает под большими нагрузками.Посмотрим на топ-50 стартапов Y Combinator: Что бы ни говорили про Y Combinator, но в их топе-50 стартапов за несколько лет, которые прибыльны и приносят миллиарды долларов, мы видим: Python, Ruby, Python, Ruby, Ruby, Ruby, Python.МейнстримNginxPython и Ruby проекты начинаются с Nginx. Это исторически сложившаяся защита мягкого подбрюшья «application servers».Много лет назад, когда Nginx только создавался, сервера приложений Perl и PHP не могли быстро обрабатывать большое количество медленных запросов. Если тысяча клиентов набегали на бэкенд инфраструктуру и начинали по одному байтику что-то запрашивать, то бэкенд-инфраструктура на этом заканчивалась. Тогда был создан Nginx, который способен через себя проксировать все эти запросы, отсюда название — reverse proxy.Потом Nginx начал решать CPU intensive задачи, брать на себя сертификаты, HTTP/2 (внутри современных Python и Ruby проектов вы часто можете встретить HTTP/1.1) и кэширование. Вовремя включенное кэширование способно ускорить бэкенд в 10, а если повезет, то и в 20 раз.Nginx может общаться с application server по бинарному протоколу или по HTTP-протоколу. Как показал сервер uWSGI для Python — разницы особой нет. А вот если говорить о самом application сервер, пастухе стада питонов — там разница есть: он заботится о запущенных процессах Python. Application серверApplication серверов много. В 2020 году разработчиков Ruby спросили, какой application сервер они используют. Оказалось, что application сервер Puma стал лидером практически единогласно.Application server: пастух стада питоновУ питонистов почему-то такое не спрашивают. Я несколько месяцев выяснял это в тусовке питонистов, и многие говорят, что используют традиционный исторический uWSGI, но у кого-то в ходу и современный хипстерский Gunicorn, и асинхронный Waitress. Согласия в мире питонистов нет, но в целом мы видим, что подавляющее большинство питонистов и рубистов application сервера используют.Application сервер в мире Python и Ruby разработки играет фундаментальную роль. Он загружает Python, указывает ему запустить некоторое количество потоков, в каждый поток загрузить приложение и по входящим запросам вызывает код этих приложений.Именно Application сервер контролирует количество процессов и потоков, может их автоматически масштабировать и ограничивать ресурсы (процессор, память, диск). Он следит за запущенными процессами Python, его не просто так называют пастухом. Если какой-то из процессов начал слишком медленно отвечать на запросы или использовать слишком много CPU, то именно application сервер убьет и вновь подымет его из мертвых.Раньше Application сервера использовались и для того, чтобы перезапускать веб-приложения без обрыва соединений и перезапуска всего. Но сейчас Docker и Kubernetes подходят для этого гораздо лучше, да и решается такая задача там проще.Еще Application сервера собирают метрики и разнообразные логи в бэкенды, предоставляют API для ваших плагинов. Например, написав плагин к Puma или к uWSGI, можно указать, что точно является критерием того, что приложение работает плохо. А потом покопаться у него внутри и узнать подробности.Application сервера обеспечивают выполнение фоновых задач, потому что в популярных протоколах общения application серверов и бэкендов в принципе не было такой возможности. Например, самый популярный протокол общения с Ruby — Rack, а с Python — WSGI, и там просто не предусмотрено функциональности фоновых задач. Конечно, сейчас всё это уже есть, но пока новьё можно встретить только на очень новых проектах.Application сервера реализуют очереди, таймеры, локи, RPC, WebSockets, у них свой собственный уровень роутинга и кэширования, как в Nginx. И это на самом деле не просто так.Application сервера для Python и Ruby разработчиков позволяют распределить сложность: часть роутинга и кэширования отдать Nginx, другую часть поместить в application сервер, а что-то реализовать на уровне приложения. Распределение сложности по проекту позволяет делать большие проекты более читаемыми и писать читаемый код.Современные application сервера делают много всего, но главное — они запускают процессы, потоки и в этих потоках выполняют код бэкенд-приложения. Процессы, потоки, GIL и GCПриведу аналогию трёх стульев. Она технически некорректная, но позволяет очень просто, «на пальцах» объяснить разницу между языками и работает для большинства случаев. Авторы большинства известных мне современных мейнстрим-языков программирования хотят реализовать три штуки:Скорость — чтобы написанные на этом языке программы быстро выполнялись;Совместимость по памяти — чтобы из этого языка можно было вызвать OpenSSL или другую библиотеку, передать ей огромный буфер в несколько десятков или сотен мегабайт, и библиотека смогла бы с этим буфером работать.Высокоуровневый синтаксис, который не заставляет программиста заботиться о памяти, но позволяет использовать «резиновые» массивы и словари. Чтобы о памяти вместо программиста думал язык программирования. Когда любой язык программирования выбирает два любых стула, третий становится очень сложным. Если язык программирования хочет быть быстрым и совместимым по памяти, то код должен быть перемолот в очень мелкую кашицу, размазан по регистрам, по кэшам первого, второго и даже третьего уровня —  чтобы он максимально быстро раскладывался по памяти. Потому что у современных процессоров обращение к памяти примерно в 100 раз медленнее, чем другие операции. Получаем компилируемый код — C, Rust, Go, C++. Но синтаксис такого языка вынуждает разработчика самому заботиться о памяти.Если язык программирования хочет быть быстрым, и чтобы программист не заботился о памяти, чтобы он писал высокоуровневый код, то такой язык сам перемелет этот код в очень мелкую кашицу. Это быстрые высокоуровневые языки Java, C#, JavaScript, но сделать к ним нативное расширение будет болью. Потому что в любой момент за каждым куском памяти может прийти Compacting Garbage Collector и сказать: «Отдай мне эту память, мне ее надо переложить». Тяжело писать расширения к Java. К JavaScript полегче, но там отдельная история.Наконец, если язык хочет быть совместим со всем огромным количеством нативного кода, который уже написан и он хочет предложить программистам высокоуровневый, удобный, приятный синтаксис, чтобы программистам не надо было заботиться о памяти, о слайсах, о времени жизни объектов — то такой язык будет медленным.Поэтому, размножаясь процессами и потоками, Python, Ruby и PHP используют GIL (Global Interpreter Lock) для простого управления памятью. Его боятся все питонисты (так же как рубисты боятся GVL, Global Virtual Machine Lock). Это страшная штука не позволяет больше, чем одному потоку работать одновременно в рамках запущенного процесса Python или Ruby. И GIL сделан не просто так. Он нужен, чтобы языки реализовывали «резиновые» контейнеры, списки и ассоциативные массивы, которые могут расти невозбранно в любую сторону. А также чтобы в этих языках были  быстрые сборщики мусора — цена, которую Python и Ruby платят за крутой, удобный, высокоуровневый синтаксис и за расширяемость по памяти.«Поднять» Global Interpreter Lock для многопоточности помогают нативные расширения. Тогда Python или Ruby, которые запустили 16 потоков, в каждом из них смогут отправить данные по сети одновременно. Правда, первый же вернувшийся обратно в виртуальную машину высокоуровневый код заставит остальные потоки подождать.Garbage collector при этом можно отключить. Например, в Wargaming пишут всю бизнес-логику танков на Python и отключают garbage collector. Но не тот garbage collector, который reference counting, а mark-and-sweep — и стараются писать код, который не делает циклических зависимостей, то есть не течет по памяти.Если говорить про Ruby, то, к примеру, в очень больших монолитах, которые обслуживают десятки или сотни тысяч запросов в секунду, большой garbage collector приходит примерно на один запрос из ста  и тормозит весь этот балаган на 100 мс. А маленький garbage collector приходит раз в 10 запросов, но всего на 10 мс — и это не критично, если мы делаем бизнес-логику, а не числодробилки.Мейнстрим: процессы для параллелизма, потоки для асинхронности Мейнстрим в Python и Ruby использует процессы для размножения CPU intensive tasks, а потоки для асинхронности — база данных, диск, сетка.В данный момент я не знаю хорошего способа сесть сразу на три стула. Либо вручную заботимся о памяти, либо ее за нас превратят в несовместимую ни с чем кашицу. Поэтому на практике используются высокоуровневые, но неторопливые Python и Ruby вместе с нативными расширениями на Rust или C++. Конечно, это не серебряная пуля, и во многих highload-случаях такая интеграция не пройдет. Но для бизнес-логики этого достаточно.Процессы и потоки в PythonПроцессы и потоки выполняют код. Рассмотрим, как это организовано в Python. Запустился Python, ему подложили 5 Мб сорцов, и дальше компилятор Python скомпилирует эти сорцы в байткод.Компилятор => байткод => VM для памяти и семантикиДа, Python компилирует сорцы, как и Ruby. Подавляющее большинство мейнстримовых языков сейчас компилируются. Python и Ruby — компилируются в байткод, и дальше этот байткод выполняется виртуальной машиной. А, например, в Java, сначала компилируется байткод, который потом выполняется виртуальной машиной и перекомпилируется в машинный код.На Web Framework Benchmarks можно посмотреть, как себя ведет Python под нагрузкой. Очень крутое железо с повышенным количеством ксеонов, памяти, ядер, дисков и всего остального на голом Python выполняет чуть меньше миллиона запросов в секунду. Но если подключить Django, то количество запросов сразу падает в 10 раз и бэкенд начинает «тормозить».Bare Python / Django = 10 / 1Что же такое делает Django, что тормозит Python в 10 раз? Я очень внимательно посмотрел на сорцы, на документацию, на стек, на отладчик и обнаружил — вы не поверите! — Django выполняет код. Он выполняет много-много питоновского кода, который реализует фичи.Так как это фреймворк, то в самом Django реализуется много всего. Это и ORM, и Routing, и работа с шаблонами. Да, мы любим React, Vue и Server-Side Rendering, но очень много проектов не настолько сложны. Они используют шаблоны и формочки, чтобы за 20 минут сделать интерфейс ERP.Внутри Django есть Middleware API для плагинов, который на каждый запрос съедает чуть-чуть (несколько десятков) байткодов. И, конечно, в Django есть развесистые механизмы кэширования, логирования и тестирования. Есть еще миллион мелочей, которые используются в больших проектах: sessions, auth, forms, security, cfg, notify, email, files, i18n и CLI. В довесок в Django есть командный интерфейс для разработчиков, с помощью которого они способны наскаффолдить (от английского scaffold) себе приложение за 10-15 минут.То есть современный фреймворк делает много всего, позволяя Python-разработчикам быть продуктивными и очень быстро выкатывать бизнес-фичи. Но ценой скорости. Это опять же подходит для бизнес-логики, но не подходит для числодробилок.Процессы и потоки в RubyА как обстоят дела с Ruby? Собственно, точно так же. Ruby тоже берет исходники, компилирует их в байткод, а затем виртуальная машина байткод выполняет, чтобы обеспечить богатую семантику и заботу о памяти — чтобы разработчики могли писать высокоуровневый код и не париться.Компилятор => байткод => VM для памяти и семантикиА еще Ruby — сама неторопливость (на видео я в этот момент картинно развожу руки и еще глубже опускаю капюшон). Он в 4 раза медленнее Python на тех же бэкенд-задачах. Хотя можно найти некую закономерность. Голый Ruby дает 200 с небольшим тысяч запросов в секунду, а как только мы ставим его на рельсы — скорость падает в 10 раз:Bare Ruby / Rails = 10 / 1Ruby фреймворк тормозит ровно по тем же причинам. Я сравнил полторы тысячи страниц документации Django, и несколько сотен страниц документации Ruby on Rails. Количество фичей, которые фреймфорки предлагают разработчикам, очень похоже. Каждая из них съедает по чуть-чуть байткода — и в результате вся система позволяет очень быстро фигачить код, но работает в 10 раз медленнее.Хорошо, вот есть JavaScript — точно такой же высокоуровневый язык программирования, который точно также компилируется в байткод виртуальной машины, а потом перекомпилируется с помощью JIT в машинные коды, и все очень-очень быстро. Давайте добавим JIT в Ruby!Несколько версий назад это сделали, но оказалось, что ускорение возможно только на синтетических задачах. Например, на задаче трех тел, когда есть несколько планет и мы много раз в секунду считаем гравитационное взаимодействие между ними. Автор Юкихиро Мацумото назвал такие задачи «синтетическими бенчмарками». Они действительно ускорились в 10 раз, но Ruby on Rails немножко затормозил. То есть JIT не помогает.Почему? Потому что Ruby on Rails большой, а код начинает тормозить в современных процессорах, когда не укладывается в кэши. Движок V8 для JavaScript создан для того, чтобы компилировать код в мелкую кашицу, размазывая его по кэшам процессора. Чтобы код выполнялся быстро, они принесли в жертву совместимость по памяти (на самом деле нет, но там отдельная, сложная и печальная история. Тяжело сидеть на трех стульях). А Python и Ruby хотят использовать OpenSSL и сишные extension’ы и не хотят Compacting Garbage Collector. Поэтому компиляция кусочков кода с существующим рантаймом языка, с существующей семантикой работы памяти не приведет к ускорению огромного rails-приложения. Rails-приложение просто не влезает в кеши, использует слишком много динамического кода рантайма и нативные расширения. И отказывается работать быстрее.ВыводыКак один из организаторов сообщества разработчиков Moscow Python, на митапах и Python-завтраках я много рассказываю про асинхронность. Но если говорить про выжившие Python и Ruby проекты, что делались несколько лет назад, то async мы там, скорее всего, не встретим, как и FastAPI. Там будет Django, Flask, Ruby on Rails, Hanami. Стоять они будут за Nginx, на котором, к примеру, если включить кэш, можно их ускорить в 10-20-30 раз.Бизнес-логику в современных бэкендах можно писать на чем угодно.  Балансировка между нодами слабо зависит от того, на каком именно языке реализовано бэкенд-приложение, крутящееся на этих нодах. CPU-bound масштабировался и масштабируется процессами. А базу, память, сеть, диск, процессор уже оптимизировали за нас. Во все остальные места можно вставить Rust или С.Поэтому современный стек (Python или Ruby) позволяют разработчикам быстро выкатывать фичи, но ценой того, что код может «тормозить» в странных местах. К примеру, у нас может тормозить не наша бизнес-логика, не база, а — неожиданно — ORM, к которому обратились не тем заклинанием.  Современные ORM позволяют легко делать много разных крутых штук, но также легко можно выстрелить себе в ногу.Поэтому от Python и Ruby разработчиков требуется высокая квалификация, чтобы при той скорости, с которой они выкатывают фичи, их код не тормозил. Чтобы начать использовать SQLAlchemy, нужно прочитать несколько сотен страниц документации и несколько лет учиться. К сожалению, не все это делают.На предстоящей конференции Python Conf++ 2021 я расскажу почему \"простой\" Python скатывается в неподдерживаемый ужас, в котором уже через год не могут разобраться ни сам автор, ни его коллеги. 27-28 сентября впервые за два года мы встречаемся офлайн. Приходите, нам есть, что обсудить. Билеты, расписание и тезисы докладов.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Представляю вашему вниманию очередной выпуск обзора наиболее интересных материалов, посвященных теме анализа данных и машинного обучения.\n",
      "\n",
      "Общее\n",
      "\n",
      "  5 трендов в области анализа и обработки данных в 2015 году\n",
      " Примеры работы генетического алгоритма — встретил два очень наглядных примера работы генетических алгоритмов с достаточно большим количеством настраиваемых параметров.\n",
      " Отличная коллекция различных наборов данных — коллекция различных наборов данных от Sebastian Raschka.\n",
      " NASA будет прибегать к помощи машинного обучения при изучении звезд\n",
      " Развитие Deep Learning в Google Search — очень интересная статья под названием «Google Search Will Be Your Next Brain» из серии статей о развитии поисковой системы в компании Google. В этой статье речь пойдет о появлении и развитии в компании методик Deep Learning, покупке компании DeepMind, развитии проекта Google Brain и технологий искусственного интеллекта.\n",
      " Интервью с Demis Hassabis — продолжение предыдущей статьи, интервью с Demis Hassabis — основателем компании DeepMind, которую компания Google купила за 400 млн. долларов.\n",
      " Инструменты с открытым исходным кодом от Facebook для более эффективного использования методик Deep Learning\n",
      " В Baidu построили суперкомпьютер для Deep Learning\n",
      " Как проводить собеседование на позицию Data Scientist\n",
      " Talking Machines: Эпизод 2: Интервью с Ilya Sutskever — второй эпизод «Talking Machines», в данном случае это интервью с Ilya Sutskever — одним из членов команды Google Research.\n",
      " 8 тенденций Big Data в 2015 году по версии DataFloq\n",
      "  R не теряет своей актуальности — немного размышлений о популярности языка программирования R и о том, что он не теряет своей популярности, а даже наоборот.\n",
      "   Python против R: что изучать в первую очередь? — в продолжение темы обсуждения языков программирования для анализа данных — неплохое сравнение от автора блога Udacity двух популярных языков, которые используются для анализа данных в настоящее время и очевидный, как мне кажется, вывод в конце.\n",
      " 5 провалов 2014 года в области работы с данными\n",
      " 10 экспертов из области Big Data, о которых стоит знать\n",
      " 12 лучших историй прошлого года в области Big Data\n",
      "\n",
      "Теория и алгоритмы машинного обучения, примеры кода\n",
      "\n",
      " Свои Яндекс-Новости с преферансом и куртизанками\n",
      " Событийная аналитика\n",
      "    Введение в машинное обучение с помощью Python и Scikit-Learn\n",
      "   Искусство Feauture Engineering в машинном обучении\n",
      "   Метод главных компонент за 3 простых шага — очередная отличная статья от Sebastian Raschka. В данном случае он расскажет про основы метода главных компонент (Principal Component Analysis).\n",
      "  Что такое Deep Learning? — неплохая статья вводного уровня, объясняющая так быстро набирающий сейчас популярность метод машинного обучения Deep Learning.\n",
      " Краткий обзор Deep Learning\n",
      "  Геометрия классификаторов — в данной статье развивается тема достаточно популярного исследования “Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?” с примерами кода на языке программирования Python.\n",
      "  Примеры Deep Learning на Python\n",
      "  Балансировка нагрузки с помощью RStudio Server Pro\n",
      "  Использование библиотеки microbenchmark для сравнения времени выполнения различных выражений в R\n",
      "  Запуск R в параллельном режиме (простой способ)\n",
      "  О деревьях принятия решений простым языком\n",
      " Эффективность работы модели (часть 1) — автор блога Analytics Vydhya поможет разобраться с тем, насколько эффективна ваша предсказательная модель и расскажет о возможных способах измерения эффективности работы модели.\n",
      " Фундаментальные методы Data Science: Классификация, регрессия и сравнение схожести\n",
      "   Пример визуализации расширенного фильтра Калмана при помощи R — продолжение статьи «Пример визуализации фильтра Калмана при помощи R» из прошлого обзора, в данном случае представлен пример визуализации расширенного фильтра Калмана (EKF, Extended Kalman filter) при помощи языка программирования R.\n",
      "  Пример кода: R: тотальная векторизация\n",
      "\n",
      "Соревнования по машинному обучению\n",
      "\n",
      " Соревнование по машинному обучению «National Data Science Bowl» — не так давно на Kaggle началось новое соревнование по машинному обучению «National Data Science Bowl».\n",
      "  Результаты соревнования «Angry Birds AI Competiton»\n",
      " Соревнование по машинному обучению: ChaLearn Automatic Machine Learning Challenge (AutoML)\n",
      "\n",
      "Онлайн-курсы, обучающие материалы и литература\n",
      "\n",
      "  Big Data for Business — новый платный курс по теме Big Data на русском языке с возможностью обучения как оффлайн, так и онлайн. Длительность обучения 3 месяца. Занятия 3 раза в неделю по 3 часа. Сертификат в конце обучения.\n",
      "  Очередная сессия «Machine Learning» от Andrew Ng — 19 января начинается очередная сессия самого, пожалуй, популярного на данный момент онлайн-курса по машинному обучению. \n",
      "  Стартует курс «Statistical Learning» — 19 января на сайте Stanford Online стартует интересный курс по машинному обучению под названием «Statistical Learning».\n",
      "  Начало курса «Statistics and R for the Life Sciences» — 19 января начинается интересный курс под названием «Statistics and R for the Life Sciences» от Harvard University на edX.\n",
      "  Бесплатная электронная книга: «Rabbit. Introduction to R» — неплохая книга по основам R, которая является сопровождением к онлайн-курсу «Introduction to R».\n",
      "\n",
      "Data engineering\n",
      "\n",
      " Big Data на вашем компьютере: Установка Hadoop-кластера\n",
      " Повышение эффективности сортировки в Apache Spark\n",
      " Как развернуть кластер Hadoop\n",
      " Пример персонализации с использованием Apache Cassandra в компании Spotify\n",
      "\n",
      "\n",
      "Обзоры\n",
      "\n",
      " Интересное из мира R (12-18 января 2015 г.)\n",
      " Еженедельный дайджест от DataScienceCentral (19 января)\n",
      " Лучшие материалы за неделю от KDnuggets.com (4 — 10 января)\n",
      " Новости Data Science от MyDataMine.com (14 января)\n",
      " Новости Big Data от MyDataMine.com (16 января)\n",
      " 7 популярных статей от Vincent Granville\n",
      " Еженедельный сборник лучших материалов от R1Soft (16 января)\n",
      " Лучшие ресурсы за неделю от Data Elixir (№18)\n",
      " Наиболее интересные материалы от Freakonometrics №203\n",
      " Наиболее интересные материалы по High Scalability (16 января)\n",
      "\n",
      "Предыдущий выпуск: Обзор наиболее интересных материалов по анализу данных и машинному обучению №30 (5 — 11 января 2015)    \n",
      " 30-31 марта ребята из СИБУР и AI Community проводят Хакатон по анализу данных в Нижнем Новгороде. Призовой фонд — 200 000 рублей. Ну а подробности под катом!\n",
      "\n",
      "\n",
      "\n",
      "Кстати, ребята из СИБУР рассказали о задачах Хакатона в своей статье. \n",
      "\n",
      "Принять участие в хакатоне могут специалисты следующих профилей:\n",
      "\n",
      "Data Engineer\n",
      "Data Architect \n",
      "Data Scientist \n",
      "Архитектор решений \n",
      "Front-end разработчик \n",
      "Back-end разработчик \n",
      "UX/UI Designer \n",
      "Product owner \n",
      "Scrum master\n",
      "\n",
      "Хакатон делится на 4 этапа:\n",
      "\n",
      "Образовательная программа\n",
      "Митап\n",
      "Отбор команд на офлайн-хакатон\n",
      "Хакатон\n",
      "\n",
      "В рамках первого этапа, образовательной программы, зарегистрированные на хакатон участники имеют возможность смотреть видеоуроки и читать статьи, специально подготовленные для них специалистами компании. Уроки помогут улучшить свои навыки и быстро создать удобный функциональный прототип на хакатоне. Кроме того, за прохождение тестов после каждого из уроков участник может заработать баллы, которые затем обменять на хакатоне на различные призы. Баллы всех членов команды суммируются.\n",
      "Митап состоится 29 числа, за сутки до хакатона. На нем встретятся большинство участников, где они смогут объединиться в команды, познакомиться, задать вопросы экспертам и начать серьезную подготовку к работе.\n",
      "В тот же день будут отобраны финальные команды участников, которые будут приглашены на хакатон.\n",
      "30-31 марта пройдет сам хакатон. Кстати, о его программе. Сбор участников начинается в 9:00 30 марта. Уже в 10:00 мероприятие будет открыто и начнутся экспертные сессии, чек-поинты по проверке решений и другие подготовительные этапы. \n",
      "Последний чек-поинт в первый день состоится в 21:00. После этого и до утра команды будут иметь право работать всю ночь. Второй день начнется с зарядки в 8:30. В 11:00 состоится экспертная труба, после которой, в 14:30, буду объявлены финалисты – 10 команд, представившие лучшие прототипы по выбранной одной из двух задач. С 15:00 до 17:00 пройдут выступления команд и подведение итогов хакатона. В 17:00 гостей ждет закрытие, награждение победителей и фуршет.\n",
      "Экспертами на хакатоне станут ведущие специалисты по анализу данных, разработке, проектному менеджменту и маркетингу. Среди них: \n",
      "\n",
      "Глеб Ивашкевич, руководитель направления Data Science AI Today. Ментор программы Y-Data; \n",
      "Анастасия Макеенок, экс-глава направления по стартапам и академическим взаимодействиям в представительстве Microsoft в России и Восточной Европе; \n",
      "Сергей Мартынов, в интернет-бизнесе более 15 лет, в прошлом руководитель таких проектов, как Gosuslugi.ru и Почта Mail.Ru;\n",
      "Илья Королев, инвестиционный портфель — 850+ миллионов рублей, 18 компаний из сфер LegalTech, AR/VR и MarTech и Consumer Internet;\n",
      "Павел Доронин, основатель AI Community и лаборатории по цифровой трансформации AI Today;\n",
      "Алексей Павлюков, работает над созданием веб-сервисов и систем машинного обучения в областях анализа текстов, документов и изображений.\n",
      "Николай Кугаевский, независимый разработчик. Работал в компаниях Яндекс.Деньги и iFree. \n",
      "Александр Крот,  руководитель проектов по анализу данных компании СИБУР.\n",
      "\n",
      "Регистрация    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Представляю вашему вниманию очередной выпуск обзора наиболее интересных материалов, посвященных теме анализа данных и машинного обучения.\n",
      "\n",
      "Общее\n",
      "\n",
      "  Почему стоит изучать R, если вы решили заняться «наукой о данных»\n",
      " Постер Data Science\n",
      " Глубокие недостатки глубокого обучения — интересная статья с блога KDnuggets о потенциальных недостатках популярного метода машинного обучения Deep Learning.\n",
      "  Может ли Microsoft сделать R простым?\n",
      " Большой список ресурсов по теме искусственного интеллекта — здесь есть ссылки на курсы, книги, ресурсы по программированию, примеры кода и еще масса других интересных ссылок по теме AI.\n",
      "  О машинном обучении простым языком — краткая статья об основах машинного обучения от авторов блога Analytics Vidhya.\n",
      " Новичок в теме Deep Learning? Вот 4 простых урока от Google\n",
      " Big Data: 5 ведущих компаний и их планы на 2015 год\n",
      "\n",
      "Теория и алгоритмы машинного обучения, примеры кода\n",
      "\n",
      "  Линейная регрессия на R — в данной публикации описаны 4 способа использования линейной регрессии в языке программирования R.\n",
      "   Когда данных действительно много: Vowpal Wabbit\n",
      "  Deep learning и Caffe на новогодних праздниках\n",
      "  Word2Vec в примерах\n",
      " Проект Providence: Использование машинного обучения в работе Stack Exchange\n",
      " Создание предсказательной модели в Azure ML — пошаговый пример создания предсказательной модели в набирающем популярность облачном продукте Microsoft Azure Machine Learning.\n",
      "  Слежение за объектом в реальном времени с дрона с использованием Python и OpenCV\n",
      "  Eigenfaces: создание призрачных образов из набора лиц — достаточно интересная статья, посвященная одному из теоретических аспектов распознавания лиц.\n",
      "  Статистический взгляд Deep Learning (часть 1): рекурсивные обобщенные линейные модели (Recursive GLMs)\n",
      "   Пример реализации комитета алгоритмов с помощью scikit-learn — очередная хорошая статья от Sebastian Raschka, в данном случае это пример реализации комитета алгоритмов, а именно Weighted Majority Rule Ensemble Classifier с использованием библиотеки scikit-learn.\n",
      "  Пример предсказательной модели в R: кластеризация по сходству тэгов — небольшой пример кода на R по кластеризации данных от автора библиотеки Caret.\n",
      "  spaCy: современная библиотека для обработки естественного языка — интересная библиотека для NLP с использованием Python.\n",
      "  Параллельное программирование с использованием GPU и R\n",
      " Введение в потоковый метод k-средних в Apache Spark 1.2\n",
      "  Краткое введение в линейную регрессию\n",
      "  Использование Apache Hadoop для предсказания задержек авиарейсов (часть 3) — третья часть серии статей с блога компании Hortonworks о практическом использовании Apache Hadoop для предсказания задержек авиарейсов.\n",
      " Deep learning для обработки естественного языка: подборка недавних публикаций\n",
      "  Что такое обучение с учителем (Supervised learning) и обучение без учителя (Unsupervised learning)\n",
      "\n",
      "Соревнования по машинному обучению\n",
      "\n",
      " Интервью с победителями соренования «American Epilepsy Society Seizure Prediction Challenge» на Kaggle\n",
      " Обучаемый персептрон в соревнованиях по машинному обучению\n",
      "\n",
      "Онлайн-курсы, обучающие материалы и литература\n",
      "\n",
      "  Начало курса «Линейная алгебра» на Coursera — 2 февраля на Coursera начнется русскоязычный курс, посвященный линейной алгебре и представленный НИУ «Высшая школа экономики».\n",
      "   DataQuest: онлайн-обучение машинному обучению — не так давно запущенная англоязычная бесплатная платформа для изучения Data Science с использованием Python.\n",
      "   Стартовал онлайн-курс «Linear Algebra — Foundations to Frontiers» — 28 января на edX начался курс по основам линейной алгебры от «The University of Texas at Austin: Linear Algebra — Foundations to Frontiers».\n",
      "  Coding the Matrix на Coursera — и еще один курс, посвященный линейной алгебре, начнется в ближайшие дни на Coursera: «Coding the Matrix: Linear Algebra through Computer Science Applications».\n",
      "  Обзор книги: Getting started with data science in the cloud\n",
      "   Книга: Beginning Data Science with R\n",
      "\n",
      "Видеоматериалы, подкасты\n",
      "\n",
      "   Введение в Deep Learning от Nando de Freitas — интересное видео от Dr. Nando de Freitas (Adjunct Professor at UBC Computer Science, Full-time Professor at Oxford). В данном случае это лекция по основам Deep Learning.\n",
      " Talking Machines: Эпизод 3: Интервью с Kevin Murphy — третий эпизод «Talking Machines», в данном случае это интервью с Kevin Murphy (Research Scientist, Google), в данном эпизоде затронуты такие темы, как использование Torch, PyMC, Weka, Theano, Caffe и другие.\n",
      "  Введение в Microsoft Azure Machine Learning — видеолекция об основах Microsoft Azure Machine Learning с портала Microsoft Virtual Academy.\n",
      "\n",
      "Data engineering\n",
      "\n",
      " Интересные цифры из опроса по Apache Spark\n",
      "\n",
      "Обзоры\n",
      "\n",
      " Лучшие материалы за неделю от KDnuggets.com (18 — 24 января)\n",
      " Новости Data Science от MyDataMine.com (29 января)\n",
      " Новости Big Data от MyDataMine.com (26 января)\n",
      " Еженедельный дайджест от DataScienceCentral (5 февраля)\n",
      " Лучшие ресурсы за неделю от Data Elixir (№20)\n",
      " Еженедельный сборник лучших материалов от R1Soft (30 января)\n",
      " Наиболее интересные материалы по High Scalability (30 января)\n",
      "\n",
      "Предыдущий выпуск: Обзор наиболее интересных материалов по анализу данных и машинному обучению №32 (19 — 25 января 2015)    \n",
      " Первая часть статьи была опубликована тут.\n",
      "\n",
      "Как читать и редактировать Excel файлы при помощи openpyxl\n",
      "\n",
      "\n",
      "ПЕРЕВОД\n",
      "Оригинал статьи — www.datacamp.com/community/tutorials/python-excel-tutorial\n",
      "Автор — Karlijn Willems \n",
      "\n",
      "Эта библиотека пригодится, если вы хотите читать и редактировать файлы .xlsx, xlsm, xltx и xltm.\n",
      "\n",
      "Установите openpyxl using pip. Общие рекомендации по установке этой библиотеки — сделать это в виртуальной среде Python без системных библиотек. Вы можете использовать виртуальную среду для создания изолированных сред Python: она создает папку, содержащую все необходимые файлы, для использования библиотек, которые потребуются для Python.\n",
      "\n",
      "Перейдите в директорию, в которой находится ваш проект, и повторно активируйте виртуальную среду venv. Затем перейдите к установке openpyxl с помощью pip, чтобы убедиться, что вы можете читать и записывать с ним файлы:\n",
      "\n",
      "# Activate virtualenv\n",
      "$ source activate venv\n",
      "\n",
      "# Install `openpyxl` in `venv`\n",
      "$ pip install openpyxl\n",
      " \n",
      "Теперь, когда вы установили openpyxl, вы можете начать загрузку данных. Но что именно это за данные? Например, в книге с данными, которые вы пытаетесь получить на Python, есть следующие листы:\n",
      "\n",
      "\n",
      "\n",
      "Функция load_workbook () принимает имя файла в качестве аргумента и возвращает объект рабочей книги, который представляет файл. Это можно проверить запуском type (wb). Не забудьте убедиться, что вы находитесь в правильной директории, где расположена электронная таблица. В противном случае вы получите сообщение об ошибке при импорте.\n",
      "\n",
      "# Import `load_workbook` module from `openpyxl`\n",
      "from openpyxl import load_workbook\n",
      "\n",
      "# Load in the workbook\n",
      "wb = load_workbook('./test.xlsx')\n",
      "\n",
      "# Get sheet names\n",
      "print(wb.get_sheet_names()) \n",
      "Помните, вы можете изменить рабочий каталог с помощью os.chdir (). Фрагмент кода выше возвращает имена листов книги, загруженной в Python. Вы можете использовать эту информацию для получения отдельных листов книги. Также вы можете проверить, какой лист активен в настоящий момент с помощью wb.active. В приведенном ниже коде, вы также можете использовать его для загрузки данных на другом листе книги:\n",
      "\n",
      "# Get a sheet by name \n",
      "sheet = wb.get_sheet_by_name('Sheet3')\n",
      "\n",
      "# Print the sheet title \n",
      "sheet.title\n",
      "\n",
      "# Get currently active sheet\n",
      "anotherSheet = wb.active\n",
      "\n",
      "# Check `anotherSheet` \n",
      "anotherSheet\n",
      "На первый взгляд, с этими объектами Worksheet мало что можно сделать. Однако, можно извлекать значения из определенных ячеек на листе книги, используя квадратные скобки [], к которым нужно передавать точную ячейку, из которой вы хотите получить значение.\n",
      "\n",
      "Обратите внимание, это похоже на выбор, получение и индексирование массивов NumPy и Pandas DataFrames, но это еще не все, что нужно сделать, чтобы получить значение. Нужно еще добавить значение атрибута:\n",
      "\n",
      "# Retrieve the value of a certain cell\n",
      "sheet['A1'].value\n",
      "\n",
      "# Select element 'B2' of your sheet \n",
      "c = sheet['B2']\n",
      "\n",
      "# Retrieve the row number of your element\n",
      "c.row\n",
      "\n",
      "# Retrieve the column letter of your element\n",
      "c.column\n",
      "\n",
      "# Retrieve the coordinates of the cell \n",
      "c.coordinate\n",
      "Помимо value, есть и другие атрибуты, которые можно использовать для проверки ячейки, а именно row, column и coordinate:\n",
      "\n",
      "Атрибут row вернет 2;\n",
      "Добавление атрибута column к “С” даст вам «B»;\n",
      "coordinate вернет «B2».\n",
      "\n",
      "Вы также можете получить значения ячеек с помощью функции cell (). Передайте аргументы row и column, добавьте значения к этим аргументам, которые соответствуют значениям ячейки, которые вы хотите получить, и, конечно же, не забудьте добавить атрибут value:\n",
      "\n",
      "# Retrieve cell value \n",
      "sheet.cell(row=1, column=2).value\n",
      "\n",
      "# Print out values in column 2 \n",
      "for i in range(1, 4):\n",
      "     print(i, sheet.cell(row=i, column=2).value)\n",
      "Обратите внимание: если вы не укажете значение атрибута value, вы получите <Cell Sheet3.B1>, который ничего не говорит о значении, которое содержится в этой конкретной ячейке.\n",
      "\n",
      "Вы используете цикл с помощью функции range (), чтобы помочь вам вывести значения строк, которые имеют значения в столбце 2. Если эти конкретные ячейки пусты, вы получите None. \n",
      "Более того, существуют специальные функции, которые вы можете вызвать, чтобы получить другие значения, например get_column_letter () и column_index_from_string.\n",
      "\n",
      "В двух функциях уже более или менее указано, что вы можете получить, используя их. Но лучше всего сделать их явными: пока вы можете получить букву прежнего столбца, можно сделать обратное или получить индекс столбца, перебирая букву за буквой. Как это работает:\n",
      "\n",
      "# Import relevant modules from `openpyxl.utils`\n",
      "from openpyxl.utils import get_column_letter, column_index_from_string\n",
      "\n",
      "# Return 'A'\n",
      "get_column_letter(1)\n",
      "\n",
      "# Return '1'\n",
      "column_index_from_string('A')\n",
      "Вы уже получили значения для строк, которые имеют значения в определенном столбце, но что нужно сделать, если нужно вывести строки файла, не сосредотачиваясь только на одном столбце?\n",
      "\n",
      "Конечно, использовать другой цикл.\n",
      "\n",
      "Например, вы хотите сосредоточиться на области, находящейся между «A1» и «C3», где первый указывает левый верхний угол, а второй — правый нижний угол области, на которой вы хотите сфокусироваться. Эта область будет так называемой cellObj, которую вы видите в первой строке кода ниже. Затем вы указываете, что для каждой ячейки, которая находится в этой области, вы хотите вывести координату и значение, которое содержится в этой ячейке. После окончания каждой строки вы хотите выводить сообщение-сигнал о том, что строка этой области cellObj была выведена.\n",
      "\n",
      "# Print row per row\n",
      "for cellObj in sheet['A1':'C3']:\n",
      "      for cell in cellObj:\n",
      "              print(cells.coordinate, cells.value)\n",
      "      print('--- END ---')\n",
      "Обратите внимание, что выбор области очень похож на выбор, получение и индексирование списка и элементы NumPy, где вы также используете квадратные скобки и двоеточие чтобы указать область, из которой вы хотите получить значения. Кроме того, вышеприведенный цикл также хорошо использует атрибуты ячейки!\n",
      "\n",
      "Чтобы визуализировать описанное выше, возможно, вы захотите проверить результат, который вернет вам завершенный цикл:\n",
      "\n",
      "('A1', u'M')\n",
      "('B1', u'N')\n",
      "('C1', u'O')\n",
      "--- END ---\n",
      "('A2', 10L)\n",
      "('B2', 11L)\n",
      "('C2', 12L)\n",
      "--- END ---\n",
      "('A3', 14L)\n",
      "('B3', 15L)\n",
      "('C3', 16L)\n",
      "--- END ---\n",
      "Наконец, есть некоторые атрибуты, которые вы можете использовать для проверки результата импорта, а именно max_row и max_column. Эти атрибуты, конечно, являются общими способами обеспечения правильной загрузки данных, но тем не менее в данном случае они могут и будут полезны.\n",
      "\n",
      "# Retrieve the maximum amount of rows \n",
      "sheet.max_row\n",
      "\n",
      "# Retrieve the maximum amount of columns\n",
      "sheet.max_column\n",
      "Это все очень классно, но мы почти слышим, что вы сейчас думаете, что это ужасно трудный способ работать с файлами, особенно если нужно еще и управлять данными.\n",
      "Должно быть что-то проще, не так ли? Всё так!\n",
      "\n",
      "Openpyxl имеет поддержку Pandas DataFrames. И можно использовать функцию DataFrame () из пакета Pandas, чтобы поместить значения листа в DataFrame:\n",
      "\n",
      "# Import `pandas` \n",
      "import pandas as pd\n",
      "\n",
      "# Convert Sheet to DataFrame\n",
      "df = pd.DataFrame(sheet.values)\n",
      "Если вы хотите указать заголовки и индексы, вам нужно добавить немного больше кода:\n",
      "# Put the sheet values in `data`\n",
      "data = sheet.values\n",
      "\n",
      "# Indicate the columns in the sheet values\n",
      "cols = next(data)[1:]\n",
      "\n",
      "# Convert your data to a list\n",
      "data = list(data)\n",
      "\n",
      "# Read in the data at index 0 for the indices\n",
      "idx = [r[0] for r in data]\n",
      "\n",
      "# Slice the data at index 1 \n",
      "data = (islice(r, 1, None) for r in data)\n",
      "\n",
      "# Make your DataFrame\n",
      "df = pd.DataFrame(data, index=idx, columns=cols)\n",
      "Затем вы можете начать управлять данными при помощи всех функций, которые есть в Pandas. Но помните, что вы находитесь в виртуальной среде, поэтому, если библиотека еще не подключена, вам нужно будет установить ее снова через pip.\n",
      "\n",
      "Чтобы записать Pandas DataFrames обратно в файл Excel, можно использовать функцию dataframe_to_rows () из модуля utils:\n",
      "\n",
      "# Import `dataframe_to_rows`\n",
      "from openpyxl.utils.dataframe import dataframe_to_rows\n",
      "\n",
      "# Initialize a workbook \n",
      "wb = Workbook()\n",
      "\n",
      "# Get the worksheet in the active workbook\n",
      "ws = wb.active\n",
      "\n",
      "# Append the rows of the DataFrame to your worksheet\n",
      "for r in dataframe_to_rows(df, index=True, header=True):\n",
      "    ws.append(r)\n",
      "Но это определенно не все! Библиотека openpyxl предлагает вам высокую гибкость в отношении того, как вы записываете свои данные в файлы Excel, изменяете стили ячеек или используете режим только для записи. Это делает ее одной из тех библиотек, которую вам точно необходимо знать, если вы часто работаете с электронными таблицами.\n",
      "\n",
      "И не забудьте деактивировать виртуальную среду, когда закончите работу с данными!\n",
      "\n",
      "Теперь давайте рассмотрим некоторые другие библиотеки, которые вы можете использовать для получения данных в электронной таблице на Python.\n",
      "\n",
      "Готовы узнать больше?\n",
      "\n",
      "Чтение и форматирование Excel файлов xlrd\n",
      "Эта библиотека идеальна, если вы хотите читать данные и форматировать данные в файлах с расширением .xls или .xlsx.\n",
      "\n",
      "# Import `xlrd`\n",
      "import xlrd\n",
      "\n",
      "# Open a workbook \n",
      "workbook = xlrd.open_workbook('example.xls')\n",
      "\n",
      "# Loads only current sheets to memory\n",
      "workbook = xlrd.open_workbook('example.xls', on_demand = True)\n",
      "Если вы не хотите рассматривать всю книгу, можно использовать такие функции, как sheet_by_name () или sheet_by_index (), чтобы извлекать листы, которые необходимо использовать в анализе.\n",
      "\n",
      "# Load a specific sheet by name\n",
      "worksheet = workbook.sheet_by_name('Sheet1')\n",
      "\n",
      "# Load a specific sheet by index \n",
      "worksheet = workbook.sheet_by_index(0)\n",
      "\n",
      "# Retrieve the value from cell at indices (0,0) \n",
      "sheet.cell(0, 0).value\n",
      "Наконец, можно получить значения по определенным координатам, обозначенным индексами.\n",
      "О том, как xlwt и xlutils, соотносятся с xlrd расскажем дальше.\n",
      "\n",
      "Запись данных в Excel файл при помощи xlrd\n",
      "\n",
      "Если нужно создать электронные таблицы, в которых есть данные, кроме библиотеки XlsxWriter можно использовать библиотеки xlwt. Xlwt идеально подходит для записи и форматирования данных в файлы с расширением .xls.\n",
      "\n",
      "Когда вы вручную хотите записать в файл, это будет выглядеть так:\n",
      "\n",
      "# Import `xlwt` \n",
      "import xlwt\n",
      "\n",
      "# Initialize a workbook \n",
      "book = xlwt.Workbook(encoding=\"utf-8\")\n",
      "\n",
      "# Add a sheet to the workbook \n",
      "sheet1 = book.add_sheet(\"Python Sheet 1\") \n",
      "\n",
      "# Write to the sheet of the workbook \n",
      "sheet1.write(0, 0, \"This is the First Cell of the First Sheet\") \n",
      "\n",
      "# Save the workbook \n",
      "book.save(\"spreadsheet.xls\")\n",
      "Если нужно записать данные в файл, то для минимизации ручного труда можно прибегнуть к циклу for. Это позволит немного автоматизировать процесс. Делаем скрипт, в котором создается книга, в которую добавляется лист. Далее указываем список со столбцами и со значениями, которые будут перенесены на рабочий лист.\n",
      "\n",
      "Цикл for будет следить за тем, чтобы все значения попадали в файл: задаем, что с каждым элементом в диапазоне от 0 до 4 (5 не включено) мы собираемся производить действия. Будем заполнять значения строка за строкой. Для этого указываем row элемент, который будет “прыгать” в каждом цикле. А далее у нас следующий for цикл, который пройдется по столбцам листа. Задаем условие, что для каждой строки на листе смотрим на столбец и заполняем значение для каждого столбца в строке. Когда заполнили все столбцы строки значениями, переходим к следующей строке, пока не заполним все имеющиеся строки. \n",
      "\n",
      "# Initialize a workbook\n",
      "book = xlwt.Workbook()\n",
      "\n",
      "# Add a sheet to the workbook\n",
      "sheet1 = book.add_sheet(\"Sheet1\")\n",
      "\n",
      "# The data\n",
      "cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
      "txt = [0,1,2,3,4]\n",
      "\n",
      "# Loop over the rows and columns and fill in the values\n",
      "for num in range(5):\n",
      "      row = sheet1.row(num)\n",
      "      for index, col in enumerate(cols):\n",
      "          value = txt[index] + num\n",
      "          row.write(index, value)\n",
      "\n",
      "# Save the result\n",
      "book.save(\"test.xls\")\n",
      "В качестве примера скриншот результирующего файла:\n",
      "\n",
      "\n",
      "\n",
      "Теперь, когда вы видели, как xlrd и xlwt взаимодействуют вместе, пришло время посмотреть на библиотеку, которая тесно связана с этими двумя: xlutils.\n",
      "\n",
      "Коллекция утилит xlutils\n",
      "\n",
      "Эта библиотека в основном представляет собой набор утилит, для которых требуются как xlrd, так и xlwt. Включает в себя возможность копировать и изменять/фильтровать существующие файлы. Вообще говоря, оба этих случая подпадают теперь под openpyxl.\n",
      "\n",
      "Использование pyexcel для чтения файлов .xls или .xlsx\n",
      "\n",
      "Еще одна библиотека, которую можно использовать для чтения данных таблиц в Python — pyexcel. Это Python Wrapper, который предоставляет один API для чтения, обработки и записи данных в файлах .csv, .ods, .xls, .xlsx и .xlsm. \n",
      "\n",
      "Чтобы получить данные в массиве, можно использовать функцию get_array (), которая содержится в пакете pyexcel:\n",
      "\n",
      "# Import `pyexcel`\n",
      "import pyexcel\n",
      "\n",
      "# Get an array from the data\n",
      "my_array = pyexcel.get_array(file_name=\"test.xls\")\n",
      " \n",
      "Также можно получить данные в упорядоченном словаре списков, используя функцию get_dict ():\n",
      "# Import `OrderedDict` module \n",
      "from pyexcel._compact import OrderedDict\n",
      "\n",
      "# Get your data in an ordered dictionary of lists\n",
      "my_dict = pyexcel.get_dict(file_name=\"test.xls\", name_columns_by_row=0)\n",
      "\n",
      "# Get your data in a dictionary of 2D arrays\n",
      "book_dict = pyexcel.get_book_dict(file_name=\"test.xls\")\n",
      "Однако, если вы хотите вернуть в словарь двумерные массивы или, иными словами, получить все листы книги в одном словаре, стоит использовать функцию get_book_dict ().\n",
      "\n",
      "Имейте в виду, что обе упомянутые структуры данных, массивы и словари вашей электронной таблицы, позволяют создавать DataFrames ваших данных с помощью pd.DataFrame (). Это упростит обработку ваших данных!\n",
      "\n",
      "Наконец, вы можете просто получить записи с pyexcel благодаря функции get_records (). Просто передайте аргумент file_name функции и обратно получите список словарей:\n",
      "\n",
      "# Retrieve the records of the file\n",
      "records = pyexcel.get_records(file_name=\"test.xls\")\n",
      "Записи файлов при помощи pyexcel\n",
      "\n",
      "Так же, как загрузить данные в массивы с помощью этого пакета, можно также легко экспортировать массивы обратно в электронную таблицу. Для этого используется функция save_as () с передачей массива и имени целевого файла в аргумент dest_file_name:\n",
      "\n",
      "# Get the data\n",
      "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "\n",
      "# Save the array to a file\n",
      "pyexcel.save_as(array=data, dest_file_name=\"array_data.xls\")\n",
      "Обратите внимание: если указать разделитель, то можно добавить аргумент dest_delimiter и передать символ, который хотите использовать, в качестве разделителя между “”.\n",
      "\n",
      "Однако, если у вас есть словарь, нужно будет использовать функцию save_book_as (). Передайте двумерный словарь в bookdict и укажите имя файла, и все ОК:\n",
      "\n",
      "# The data\n",
      "2d_array_dictionary = {'Sheet 1': [\n",
      "                                   ['ID', 'AGE', 'SCORE']\n",
      "                                   [1, 22, 5],\n",
      "                                   [2, 15, 6],\n",
      "                                   [3, 28, 9]\n",
      "                                  ],\n",
      "                       'Sheet 2': [\n",
      "                                    ['X', 'Y', 'Z'],\n",
      "                                    [1, 2, 3],\n",
      "                                    [4, 5, 6]\n",
      "                                    [7, 8, 9]\n",
      "                                  ],\n",
      "                       'Sheet 3': [\n",
      "                                    ['M', 'N', 'O', 'P'],\n",
      "                                    [10, 11, 12, 13],\n",
      "                                    [14, 15, 16, 17]\n",
      "                                    [18, 19, 20, 21]\n",
      "                                   ]}\n",
      "\n",
      "# Save the data to a file                        \n",
      "pyexcel.save_book_as(bookdict=2d_array_dictionary, dest_file_name=\"2d_array_data.xls\")\n",
      "Помните, что когда используете код, который напечатан в фрагменте кода выше, порядок данных в словаре не будет сохранен!\n",
      "\n",
      "Чтение и запись .csv файлов\n",
      "\n",
      "Если вы все еще ищете библиотеки, которые позволяют загружать и записывать данные в CSV-файлы, кроме Pandas, рекомендуем библиотеку csv:\n",
      "\n",
      "# import `csv`\n",
      "import csv\n",
      "\n",
      "# Read in csv file \n",
      "for row in csv.reader(open('data.csv'), delimiter=','):\n",
      "      print(row)\n",
      "      \n",
      "# Write csv file\n",
      "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "outfile = open('data.csv', 'w')\n",
      "writer = csv.writer(outfile, delimiter=';', quotechar='\"')\n",
      "writer.writerows(data)\n",
      "outfile.close()\n",
      "Обратите внимание, что NumPy имеет функцию genfromtxt (), которая позволяет загружать данные, содержащиеся в CSV-файлах в массивах, которые затем можно помещать в DataFrames.\n",
      "\n",
      "Финальная проверка данных\n",
      "\n",
      "Когда данные подготовлены, не забудьте последний шаг: проверьте правильность загрузки данных. Если вы поместили свои данные в DataFrame, вы можете легко и быстро проверить, был ли импорт успешным, выполнив следующие команды:\n",
      "\n",
      "# Check the first entries of the DataFrame\n",
      "df1.head()\n",
      "\n",
      "# Check the last entries of the DataFrame\n",
      "df1.tail()\n",
      "Note: Используйте DataCamp Pandas Cheat Sheet, когда вы планируете загружать файлы в виде Pandas DataFrames.\n",
      "\n",
      "Если данные в массиве, вы можете проверить его, используя следующие атрибуты массива: shape, ndim, dtype и т.д.:\n",
      "\n",
      "# Inspect the shape \n",
      "data.shape\n",
      "\n",
      "# Inspect the number of dimensions\n",
      "data.ndim\n",
      "\n",
      "# Inspect the data type\n",
      "data.dtype\n",
      "Что дальше?\n",
      "\n",
      "Поздравляем, теперь вы знаете, как читать файлы Excel в Python :) Но импорт данных — это только начало рабочего процесса в области данных. Когда у вас есть данные из электронных таблиц в вашей среде, вы можете сосредоточиться на том, что действительно важно: на анализе данных. \n",
      "\n",
      "Если вы хотите глубже погрузиться в тему — знакомьтесь с PyXll, которая позволяет записывать функции в Python и вызывать их в Excel.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Международные научные конференции помогают следить за трендами в индустрии, узнавать о передовых разработках ведущих компаний, университетов и рассказывать о себе. Конечно, это относится только ко времени, когда мир не погружён в пучину пандемии. \n",
      "\r\n",
      "До того, как все страны перешли на режим самоизоляции, мы командой Яндекс.Толоки успели съездить на конференцию WSDM (произносится как wisdom), чтобы провести туториал по краудсорсингу, презентовать нашу статью и пообщаться с коллегами по цеху.\n",
      "\r\n",
      "Меня зовут Алексей Друца, я руководитель отдела эффективности и развития управления краудсорсинга и платформизации в Яндексе. В компании занимаюсь теоретическими и прикладными исследованиями в областях, связанных с дискретными алгоритмами, теорией аукционов, машинным обучением, анализом данных и вычислительной математикой. За время работы я опубликовал более 20 научных статей, в том числе в рамках конференций NIPS, KDD, WWW, WSDM, SIGIR и CIKM. В этом посте расскажу о своих впечатлениях после посещения WSDM, а также сделаю небольшой обзор самых интересных докладов. \n",
      "\n",
      "\n",
      "Плакат конференции\n",
      "\n",
      "Что за конференция?\n",
      "WSDM — одна из главных конференций по научным исследованиям, связанным с поиском и анализом данных. В этом году она стала тринадцатой по счёту и проходила с 3 по 7 февраля в Хьюстоне, штат Техас. \n",
      "\r\n",
      "Немного статистики. В конференции участвовали около 700 человек. Авторы 615 научных работ подали заявки, чтобы получить возможность презентовать свои статьи на конференции. Организаторы выбрали 91 статью, в том числе нашу работу про сбор краудсорсинговых данных. Из 20 заявок на проведение туториалов организаторы WSDM приняли 9, включая заявку Яндекса. \n",
      "\r\n",
      "Главной частью конференции стала стендовая сессия. На всех подобных научных мероприятиях это основной способ представить работы: авторы принятых статей готовят плакаты с исчерпывающей информацией об исследовании и отвечают на вопросы заинтересованных коллег (подробнее о формате). Помимо стендовой сессии участники могли рассказать о своих достижениях в трёх форматах:\n",
      "\n",
      "\n",
      "5-минутный доклад о работе (эту возможность получили 46 участников);\n",
      "lightning-talk на 60 секунд с кратким рассказом основной сути доклада (такой формат был предложен 45 участникам);\n",
      "демо с демонстрацией работы того или иного инструмента. \n",
      "\n",
      "Среди опубликованных на конференции работ была статья и от нашей команды. Она также про краудсорсинг, но в ней идёт речь о другом источнике краудсорсинговых данных — собранных через капчу.\n",
      "\n",
      "\n",
      "Постер нашей статьи\n",
      "\r\n",
      "Метод сбора разметки с помощью капчи давно известен и используется многими компаниями. Работает это так: подозрительным пользователям предлагается ввести текст с двух картинок. Первое изображение — контрольное, у нас уже есть правильный ответ для него. Второе изображение содержит неизвестный нам текст, его мы как раз хотим расшифровать с помощью пользователя. Если человек вводит правильный текст с первой — контрольной — картинки, то мы считаем его достаточно надёжным и записываем себе его второй ответ.\n",
      "\r\n",
      "Это очень удобный, масштабируемый и бесплатный способ разметки. Но есть проблема: капча обычно предлагается подозрительным пользователям, часть которых является ботами. При расшифровке картинок такими роботами мы часто получаем похожие, согласованные ошибки. Люди, в отличие от ботов, редко ошибаются в одной и той же букве. \n",
      "\r\n",
      "Обычно компании, использующие этот метод разметки, считают корректным тот ответ, который дали большинство пользователей. Но с учётом высокой вероятности допущения похожих ошибок ботами такая схема приводит к получению неверных данных. \n",
      "\r\n",
      "Мы же обучили ML-модель, которая по факторам ввода капчи предсказывает, какой ответ будет наиболее корректным. С полным содержанием статьи можно познакомиться здесь.\n",
      "\n",
      "А что про туториал?\r\n",
      "В самый первый день конференции мы провели практический туториал на основе Яндекс.Толоки. Мои коллеги уже рассказывали про наш сервис на Хабре, его подробное описание вот здесь. Если коротко, Толока — это краудсорсинговая платформа, которая помогает выполнять множество задач. С помощью Толоки можно расшифровывать аудиозаписи, проводить фокус-группы, модерировать комментарии или распознавать картинки, используя полученные данные для машинного обучения.\n",
      "\r\n",
      "Среди туториалов на WSDM только наш проходил в течение всего дня. \n",
      "\n",
      "\n",
      "Перед туториалом\n",
      "\r\n",
      "Мы рассказали, как правильно решать задачи с помощью краудсорсинга. Чтобы эффективно размечать данные с помощью этого способа организации рабочего процесса, нужно не просто дать людям задание, а правильно декомпозировать, верно сформулировать задачу и настроить процессы, например, контроль качества. Часть информации, которой мы делились с участниками конференции, можно найти в нашем опубликованном видео-курсе. В нём базовая теория краудсорсинга показана на примере решения задачи о сегментации объектов на изображении.\n",
      "\n",
      "\n",
      "Программа туториала\n",
      "\r\n",
      "Для конференции мы специально придумали пайплайн, который включал в себя классификацию, сбор данных в интернете, пост-приемку и сравнения бок-о-бок. Он состоял из четырёх этапов. Участники туториала представляли себя в роли владельцев онлайн-магазина одежды. Они брали картинку, выбирали на ней какой-то элемент одежды (например, ботинки) и давали толокерам задание найти в базе магазина самые похожие товары. Затем эти товары ранжировались по похожести другими толокерами. \n",
      "\n",
      "\n",
      "Этапы пайплайна\n",
      "\r\n",
      "В конце дня после появления результатов все участники получили обратную связь и практические советы, призванные помочь сделать каждый проект более эффективным.\n",
      "\r\n",
      "Например, в реальном мире некоторые шаги нашего пайплайна можно было бы, ориентируясь на имеющиеся данные, автоматизировать с помощью API. Но на конференции нам важно было показать, как каждый из этапов можно обработать именно с помощью краудсорсинга — эффективно и масштабируемо.\n",
      "\n",
      "\n",
      "Что ещё можно сделать, чтобы получать лучшие результаты и тратить меньше денег\n",
      "\r\n",
      "Практически все участники туториала прошли его полностью, дойдя до самых последних шагов. Они научились собирать датасет из похожих товаров онлайн-магазина с помощью краудсорсинга. Разобранный нами на туториале пайплайн достаточно универсальный, он может применяться не только в интернет-торговле, но и в любой индустрии, где нужно предлагать похожие объекты.\n",
      "\n",
      "О чём рассказывали другие компании?\n",
      "\r\n",
      "Полный список опубликованных работ можно найти на сайте конференции. \n",
      "\r\n",
      "Мы отметили большое количество работ, связанных с рекомендательными системами поиска и сферой e-commerce. На наш взгляд, большинство команд не предлагали новые научные теории, а представляли результаты внедрения в продукт тех или иных технологий. Было много докладов про решения на основе нейросетей — авторы рассказывали, какие именно библиотеки для этого применялись.\n",
      "\r\n",
      "Вот несколько постеров, которые привлекли наше внимание, с комментариями:\n",
      "\r\n",
      "• CrowdWorker Strategies in Relevance Judgment Tasks\n",
      "\n",
      "\n",
      "Постер работы CrowdWorker Strategies in Relevance Judgment Tasks\n",
      "\r\n",
      "Эта работа заинтересовала нас своей темой. Авторы рассказывают о том, как опыт исполнителей в краудсорсинге влияет на их поведение: клики по заданиям, использование горячих клавиш, время выполнения. \n",
      "\n",
      "\n",
      "Разница во времени выполнения задач между более и менее опытными исполнителями\n",
      "\r\n",
      "После проведённого эксперимента авторы работы выяснили, что уже после двух выполненных на краудсорсинговой платформе заданий менее опытные работники достигают сравнимой с опытными скорости выполнения.\n",
      "\r\n",
      "Общий вывод: при наличии способов контроля качества выполнения задач опыт исполнителей сильно не сказывается на итоговом качестве данных. \n",
      "\r\n",
      "• Predicting Human Mobility via Attentive Convolutional Network\n",
      "\n",
      "\n",
      "Постер работы Predicting Human Mobility via Attentive Convolutional Network\n",
      "\r\n",
      "Эта статья про предсказание маршрута пользователя — точки, в которой он окажется в будущем. Большинство таких методов предсказания работает с GPS-координатами, а авторы этой работы сконцентрировались на геотегах в социальных сетях. \n",
      "\r\n",
      "Авторы работы рассматривают траектории пользователей как картинки и используют для них фильтры. У каждой картинки в качестве показателей получаются последовательные паттерны. Также к этой нейросети добавляется механизм внимания, чтобы учитывать долговременные предпочтения. \n",
      "\r\n",
      "Авторы провели эксперименты на трёх датасетах и заключили, что их модель работает лучше, чем существующие модели с GPS-координатами. \n",
      "\r\n",
      "• Metrics, User Models, and Satisfaction\n",
      "\r\n",
      "Авторы работы изучили, как связаны метрики, описывающие поведение пользователей поисковой системы, с их удовлетворенностью.\n",
      "\n",
      "\n",
      "Постер работы Metrics, User Models, and Satisfaction\n",
      "\r\n",
      "Они подтвердили, что метрики с пользовательскими моделями, которые отражают типичное поведение, также имеют тенденцию быть метриками, которые хорошо коррелируют с оценками удовлетворенности пользователей.\n",
      "\r\n",
      "• Hierarchical User Profiling for E-commerce Recommender Systems\n",
      "\n",
      "\n",
      "Постер работы Hierarchical User Profiling for E-commerce Recommender Systems\n",
      "\r\n",
      "Авторы работы решают задачу рекомендаций для разных уровней детализации. \n",
      "\r\n",
      "Предложенная ими иерархическая структура профилирования пользователей моделирует многоуровневые интересы пользователей с помощью Pyramid Recurrent Neural Networks, которые обычно состоят из микрослоя, слоя элементов и нескольких слоев рекуррентных нейронных сетей категорий.\n",
      "\n",
      "Что в итоге?\r\n",
      "Эта конференция будет полезна специалистам, которые занимаются улучшением поиска. \n",
      "\r\n",
      "Перед посещением WSDM и любой другой конференции мы советуем внимательно изучать программу и принятые работы — это поможет не просто растерянно бродить между постерами, воркшопами и выступлениями, а пообщаться с авторами заинтересовавших проектов. \n",
      "\r\n",
      "А ещё не забывайте, что все работы есть в сети, и вы можете изучить их самостоятельно. Это, кстати, отличный способ с пользой провести свободное время.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Доброго времени суток! Меня зовут Алексей. Сейчас я обучаюсь на аналитика данных в \"Яндекс Практикум\". Дело для меня непривычное, совершенно не связанное с моей предыдущей деятельностью (пока что работаю врачом, иногда пишу рассказы и повести), так что порой некоторые темы даются с большим трудом.Начинающий аналитик данных с первых дней учёбы сталкивается с необходимостью освоить одну из наиболее важных в его будущей работе библиотек python - pandas. По себе знаю: порой здесь возникает такая путаница в голове, что первые простые задания вызывают ступор. Пройдя множество учебных заданий и успешно сдав несколько проектов, хочу поделиться с такими же новичками, как я сам, парой советов, которые, надеюсь, смогут упростить учебный процесс и первые шаги в новой профессии. И мой главный совет: \"сделайте себе шаблон\"!Не важно, в чём вы пишете код: \"Google colaboratory\", \"Jupiter notebook\" или в какой-то иной среде. Не важно, сколько вы пока знаете: если осваиваете профессию с нуля, вносите в шаблон всё, что уже умеете - позже всегда можно удалить лишнее. Шаблон поможет вам быстро сориентироваться в любой новой задаче, напомнит о необходимых манипуляциях. Постарайтесь найти баланс между общими правилами оформления работы, которые от вас требуют (преподаватели, ревью, заказчики), логикой программирования и вашими личными предпочтениями в ведении документации. Лично я большую часть учебных проектов выполнил в \"Google colaboratory\" (далее по тексту просто \"колаб\"), где предпочитаю следующую структуру шаблона.ШапкаНазвание шапки напишем в отдельной текстовой ячейке с ### - там мы создаём раздел, который в колаб можно свернуть, спрятав ячейки текста и кода. Плюс: если кто-то будет просматривать вашу работу в \"Jupiter notebook\", текст сформирует заголовок и пункт оглавления, что будет удобно и вам, и просматривающим вашу работу. Ниже вставьте хотя бы одну пустую ячейку под описание, вводной части или какого-то предварительного пояснения. Подготовка к работеНовый раздел. Также стоит название раздела записать в пустой ячейки, начав с ###.В этом разделе стоит загрузить библиотеки и добавить некоторые предварительные настройки. Отдельно по колаб. Если вы, как и я, предпочитаете работу в колаб, в первую очередь необходимо подключить возможность использовать доступный вам виртуальный диск \"Google drive\". Чтобы избежать конфликта при ревью в другой среде, заключите код в конструкцию \"try - except\". Если вы планируете скачивать из вашей работы какие-либо файлы, добавьте в этот же блок функцию колаб для загрузки файлов.try:  from google.colab import drive  drive.mount('/content/drive')  # загрузка файлов из колаб  from google.colab import filesexcept:  print('Подключение к google-drive не выполнено. Вы просматриваете проект в другой среде. Код запущен далее.')Здесь же можно добавить скрытие предупреждений, которые колаб щедро раздаёт при работе:# colab часто выдаёт предупреждения - скроем ихimport warningswarnings.filterwarnings(\"ignore\")Далее поставим ячейку кода загрузки библиотек. Конечно, для анализа данных основной вашей библиотекой будет pandas (запишите её сразу со стандартным псевдонимом pd):import pandas as pdВ зависимости от целей вашей работы на этом этапе может возникнуть необходимость загрузки и других библиотек. При изготовлении шаблона оставьте себе максимальное количество полезных библиотек, добавьте комментарии, какие библиотеки и для чего вам необходимы. К примеру, мой шаблон в этом разделе пестрит обилием следующих библиотек:# для работы с показателями времениfrom datetime import datetime # для \"красоты\" (графики и оформление)import matplotlib.pyplot as pltfrom pandas.plotting import register_matplotlib_converters# конвертеры, которые позволяют использовать типы pandas в matplotlib  register_matplotlib_converters()import seaborn as snsimport plotly.express as pxfrom plotly import graph_objects as go # для математических и статистических операцийimport numpy as npimport math as mthimport scipy.statsfrom scipy import stats as st# библиотеки для работы с дашбордамиimport sysimport getoptfrom sqlalchemy import create_engine # для создания двойного пути к базе данных - об этом чуть позжеimport osДалее заранее организуйте для себя наиболее удобный и приемлемый в рамках вашей работы вид выводимых пандами данных. Для этого добавьте код редактирования опций. Подробнее об опциях можно почитать в официальной документации по ссылке.Лично я предпочитаю такие скромные вводные:# количество строк в таблице не больше 5, чтобы не писать head() за каждым фреймомpd.set_option('max_rows', 5)# при необходимости раскрыть из комментария полный вывод данных в колонке# pd.set_option('display.max_colwidth', None)# оставим всего три знака после запятой у чисел с плавающей точкойpd.set_option('display.float_format', '{:.3f}'.format)Даже, если вы ещё только учитесь, очень рано вам понадобится использовать графики. Вам, вашим преподавателям или заказчикам будет комфортно просматривать графики и диаграммы, если они будут выполнены в общей стилистике. Редактируя параметры matplotlib.pyplot можно заранее задать размер шрифта заголовков и осей, цветовую гамму и стиль. Подробнее о параметрах можно почитать в официальной документации по ссылке.Вот пример параметров моего основного шаблона:large = 16; med = 12; small = 10params = {'axes.titlesize': large,          'legend.fontsize': med,          'figure.figsize': (12, 8),          'axes.labelsize': med,          'axes.titlesize': med,          'xtick.labelsize': med,          'ytick.labelsize': med,          'figure.titlesize': large}plt.rcParams.update(params)plt.style.use('seaborn-whitegrid')# на мой взгляд лучше всего подходит палитра deep# она не пёстрая, относительно контрастная# однако в реальном заказе я предпочёл бы принятые у заказчика цвета для подобной работыsns.set_palette('deep') sns.set_style(\"whitegrid\")Ознакомление с даннымиКогда всё готово для работы, можно приступить к загрузке данных и ознакомлению. Как я уже говорил, я прохожу обучение в “Яндекс Практикум”. При работе над учебными проектами студентам даётся выбор: работать данными в “Jupiter notebook” на платформе или скачать файл и работать локально в удобной для обучающегося среде. Если работать локально, то перед отправкой на ревью потребуется исправить часть кода с загрузкой данных, чтобы они подгружались из того же ресурса, что и у преподавателя… Или вы можете задать двойной путь к файлу данных: один ваш локальный, второй - ссылка преподавателя. Для этого воспользуемся техникой создания двойного пути с помощью библиотеки os:pth1='первый_путь_к_файлу/../файл.csv'pth2='второй_путь_к_файлу/../файл.csv'if os.path.exists(pth1):  df = pd.read_csv(pth1, delimiter='\\t')elif os.path.exists(pth2):  df = pd.read_csv(pth2, delimiter='\\t')else:  print('Проверьте правильность пути к датасету')Этот приём пригодится вам также при ситуации, когда один и тот же код вы используете на разных машинах, средах и так далее (к примеру, часть работы делаете на стареньком домашнем компьютере без интернета, часть - непосредственно на рабочем месте, может быть, даже в гостях у любимой бабушки). При работе с несколькими файлами данных прописывайте им соответствующие пути (1 файл - pth1, pth2; 2 файл - pth3, pth4 и т.д.) с последующей конструкцией if-else.Когда данные загружены, можно посмотреть сформированные датафреймы. Если вы, как и я, указали в опциях ('max_rows', 5), то ставить head() уже не потребуется - датафрейм автоматически выведет только 5 строк (две первых, пустую - обозначает прочие строки, две последних). Если вам нужно больше строк, используйте head() или отредактируйте опции и перезапустите код. Для большого числа строк можно добавить функцию подсветки строк при наведении курсора мыши:# добавим подсветку строки при наведении - просто для красотыdf.style.set_sticky(axis=\"index\")Далее, если необходимо, пройдите по датафрейму стандартными функциями оценки. Некоторые педагоги “Практикума” требуют применения максимального числа функций на этапе ознакомления с данными. Возможно, в вашей учёбе или работе вам потребуется проделать аналогичную работу. Так что не поленитесь добавить себе в шаблон:-оценку общих данных о датафремеdf.info()-поиск пропущенных значенийdf.isna().sum()-поиск абсолютных дубликатовdf.duplicated().sum()ПредобработкаПредобработка данных зависит от качества первоначальных данных и стоящих перед вами задач. Тем не менее, стоит внести себе в шаблон хотя бы в качестве напоминания того, что вы можете сделать с датафреймом прежде, чем приступите к анализу. Приведите имена колонок в стилистически правильные или удобные вам, если это необходимо. К примеру, таким способом:df.columns = ['имена колонок через запятую']Обработайте дубликаты. В зависимости от данных и поставленных задач, с абсолютными дубликатами возможны разные варианты работы. Если задача допускает удаление дубликатов, то сделайте это здесь всего одной строкой, не забыв при этом обновить индекс фрейма:df = df.drop_duplicates().reset_index(drop=True)Если ваш датафрейм содержит колонку исключительно уникальных значений (к примеру, id пользователей), а поставленная задача требует максимального разнообразия данных, вы также можете провести очистку датафрейма от дубликатов без учёта колонки уникальных значений:df = df.drop_duplicates(subset=['колонки, по которым идёт очистка'], keep=False)df = df.reset_index(drop=True)Используйте этот метод аккуратно: без учёта уникальных значений вы можете потерять очень много данных, хотя и добьетесь максимального разнообразия. Вот пожалуй и всё, что стоит по минимуму иметь в пригодным для учёбы и работы шаблоне. Всё остальное вы можете внести сами по вашим желаниям и необходимости. Удачи на пути Панды!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Публикуем обзор первого дня Data Science Week 2017, в течение которого наши спикеры говорили о применении анализа данных в сфере недвижимости.\n",
      "\n",
      "\n",
      "\n",
      "ЦИАН\r\n",
      "Касательно конкретных кейсов применения, освещать тему всего дня начал Павел Тарасов — руководитель отдела машинного обучения в ЦИАН  — крупнейшем сервисе по аренде и продаже недвижимости, где публикуется более 65 000 новых объявлений в день, среди которых от 500 до 1000 являются мошенническими. Главная цель злоумышленников — собрать как можно больше звонков для того, чтобы заставить клиента перевести им деньги или, в случае недобросовестных риэлторов, продать какой-то другой продукт.\n",
      "\r\n",
      "Для решения данной задачи компанией активно применяется машинное обучение с использованием большого количества факторов: от описания объявления и до цены, при этом наиболее важной фичей являются фотографии. Яркий пример:\n",
      "\n",
      "\n",
      "\r\n",
      "Следовательно, необходимо применять алгоритмы поиска по фотографии для того, чтобы выявлять объявления с украденными фото и несуществующими квартирами. Существуют 3 основных подхода:\n",
      "\n",
      "\n",
      "Locality-sensitive hashing (aHash, pHash, dHash ...)\n",
      "Дескрипторы ORB/ SIRF/ SURF\n",
      "Нейросетевые дескрипторы\n",
      "\r\n",
      "Perceptive Hash — самый распространенный алгоритм решения подобных задач, суть которого заключается в том, что мы сжимаем картинку до размера 32х32, для каждого пикселя считаем, ярче он среднего значения или нет, и затем сравниваем картинки по расстоянию Хэмминга. Стоит отметить, что помимо сжатия также необходимо убрать цвет, яркость и контраст, чтобы в случае, если, к примеру, немного изменилась яркость фотографии, пиксели не меняли своего значения относительно среднего. Алгоритм хорошо работает в случае изменения цветов и яркости, обрезанных фотографий, но хуже — с поворотами, что очевидно: ведь тогда меняется расположение пикселей.\n",
      "\n",
      "\n",
      "\r\n",
      "ORB-дескрипторы — алгоритм, в основе которого лежит идея расчета дескрипторов, которые позволяют найти на картинках некоторые ключевые точки, посчитать для них хеши и таким образом по этим точкам сравнивать фотографии между собой:\n",
      "\n",
      "\n",
      "\r\n",
      "Такой подход также хорошо работает с «обрезками», лучше работает с поворотами, но вычислительно более сложен. Основная проблема алгоритма заключается в том, что он очень сильно опирается на геометрию объекта: все дома для него будут одинаковыми, ведь у них треугольная крыша, несколько окон и т.д., что выливается в большое количество ложных срабатываний.\n",
      "\r\n",
      "Следующий алгоритм основан на Deep Learning: нейросетевые дескрипторы — берется многослойная, обученная на размеченном датасете нейросеть и через нее «прогоняется» каждое изображение, после чего на каждом слое сети у нас получается по набору чисел для каждой картинки. Эти наборы чисел и будут являться дескрипторами (как правило в качестве дескрипторов берутся последние несколько слоев).\n",
      "\r\n",
      "Проблема нейросетевых дескрипторов в том, что для обучения глубокой сети необходимо иметь сотни тысяч размеченных изображений, по несколько тысяч на каждый класс, дома должна различаться между собой и т.д., но даже выполнение этих условий не гарантирует, что нейросеть не будет классифицировать многие дома как одинаковые и на последних слоях не окажутся одинаковые числа.\n",
      "\r\n",
      "Таким образом, получив новое объявление мы можем, используя один из вышеописанных подходов, выяснить, была ли опубликована эта фотография на нашем сервисе ранее, однако тут возникает следующая проблема: если объявление с этой фотографией уже имеется в нашей базе, то это не всегда значит, что оно мошенническое. Например, строители типовых новостроек по всему городу могут использовать одну фотографию для всех своих объявлений. Как тут быть?\n",
      "\r\n",
      "Здесь нам на помощь снова приходят нейронные сети и Transfer Learning: берем уже обученную нейронную сеть (например, GoogleNet) и фиксируем веса слоев, кроме нескольких последних (зависит от нашей стратегии обучения). Ввиду того, что тот же GoogleNet обучен распознавать «котиков» и «собачек» и не может отличить дом от сарая, мы собираем выборку из домов и квартир, размечаем данные и прогоняем их через эту обученную нейросеть. В результате она сможет распознавать, что действительно находится на фото и отличит повторяющуюся планировку квартир от действительно украденных фотографий.\n",
      "\r\n",
      "На очереди следующий вопрос: у нас есть 2 объявления, 2 одинаковых фотографии, какая из них фейковая? Самый простой вариант — правило «первой ночи»: кто первый разместил фотографию, тот и прав. Понятно, что не всегда это верно, поскольку, например, арендодатели при смене арендатора могут заново использовать те же фотографии в новом объявлении, которое может быть выложено уже после того, как мошенник украл и выложил свое. Другой подход — использовать машинное обучение, собрав выборку из пар объявлений с одинаковыми фотографиями, но отличающимися параметрами (ценой, описанием, временем размещения и т.д.) и, обучившись на этой выборке, выявлять мошенников по всем факторам сразу.\n",
      "\r\n",
      "В итоге у нас получился готовый пайплайн по распознаванию мошеннических объявлений, используя фотографии.\n",
      "\n",
      "ДомКлик\r\n",
      "Продолжил тему применения Data Science в недвижимости Алексей Гречишкин — директор python-разработки ДомКлик — сервиса для поиска и покупки недвижимости в ипотеку, дочерней компании Сбербанка. Алексей рассказал о 3 главных направлениях деятельности компании, где применяется машинное обучение:\n",
      "\n",
      "\n",
      "Оптимизация расписания работы сотрудников в соответствии с потоком клиентов\n",
      "Прогнозирование конверсии сделки\n",
      "Выбор объявлений для витрины и модерация\n",
      "\r\n",
      "Во-первых, компании необходимо ускорять время обработки заявок клиентов. Норматив — 30 минут, в то время как на деле среднее время ожидания — 4 часа. Происходит это из-за того, что поток клиентов неравномерен, поэтому на первый план выходит задача планирования расписания менеджеров, чтобы на местах их было больше в периоды пиковой нагрузки и меньше, когда клиентов почти нет. Сразу к результатам:\n",
      "\n",
      "\n",
      "Зеленый временной ряд — действительное количество заявок на единицу времени. Синий — предсказание по обучающей выборке (использовать нельзя). Красный — предсказание по тестовой выборке.\n",
      "\r\n",
      "Для того, чтобы получить такую точность были предприняты следующие шаги. Берем данные за полгода и сдвигаем их на неделю назад, чтобы текущая неделя, за которую данные у нас только что появились, оказалась тестовой, затем повторяем эту процедуру 8 раз. В результате средний коэффициент детерминации по тестовой выборке равен 98%, за исключением последней недели, где часто попадаются неполные и недообработанные данные, поэтому R-квадрат получается ниже — 92%.\n",
      "\r\n",
      "Если говорить о моделях, которые используются в процессе, то это в первую очередь SARIMAX, поскольку она позволяет учесть как сезонную компоненту, так и экзогенные переменные (отпуска, болезни и т.д.):\n",
      "\n",
      "model = sm.tsa.statespace.SARIMAX(table_name[:], exog = Cal[:],\n",
      "order=(1,1,0), seasonal_order=(2,1,0,7), enforce_stationarity =\n",
      "False).fit()\n",
      "model2 = sm.tsa.statespace.SARIMAX(table_name[:], exog = Cal[:],\n",
      "order=(1,0,0), seasonal_order=(2,0,0,7), enforce_stationarity =\n",
      "False).fit()\n",
      "forecast['forecast'] = model.forecast(b, exog =\n",
      "Cal_Pred[:b])*2/3+model2.forecast(b, exog = Cal_Pred[:b])*1/3\r\n",
      "Использование двух моделей для предсказания одного временного ряда обуславливается тем, что первая работает с рядом разности («Насколько количество заявок за текущую неделю изменилось по сравнению с предыдущей?»), в то время как вторая работает непосредственно с прошлыми значениями ряда. Затем предсказания обоих моделей взвешиваются в соотношении 2:1 (подобрано вручную, как чаще всего и делают в продакшене), получаем конечный результат.\n",
      "\r\n",
      "Прогнозирование конверсии сделки начинается с момента подачи заявки на ипотеку клиентом. Мы сразу же начинаем предсказывать, дойдет ли человек до непосредственно покупки или «отвалится» в середине. На первом этапе разработки моделей мы использовали лишь динамические факторы: качество работы менеджера, история его сделок, регион сделки, возраст клиента и т.д. Количество факторов было ограничено, а модели неустойчивы, поэтому было решено добавить более информативные параметры: звонки, приходил ли к нам в офис, присылал ли какие-то документы, благодаря чему удалось увеличить точность прогнозов на 30-40% и теперь мы можем с 80% вероятностью предсказать совершите ли вы покупку в первый же день подачи вашей заявки. При этом с каждым последующим днем точность растет, в частности, на последних этапах подачи документов точность уже 95-99% (отличный результат, учитывая, что и на последнем этапе нередко случаются отказы). Модель была сделана на xgboost (хотя пробовали на CatBoost — «не взлетело»).\n",
      "\r\n",
      "Наконец, работа с витриной и модерация объявлений также являются приоритетными задачами компании. На сервисе ДомКлик публикуются объявления только от проверенных агентов, тем не менее и от них могут приходить дубли фотографий, нецензурная лекция в описании и т.д., поэтому важно выявлять и устранять такие явления. Также мы активно используем технологию speech-to-text, расшифровывая разговор продавца и клиента, анализируем определенные маркеры: договорились ли о встрече, не ругались ли.\n",
      "\r\n",
      "Вдобавок, компания старается облегчить клиентам поиск квартиры: благодаря алгоритмам распознавания типов изображений пользователь может фильтровать объявления по фото. Например, искать квартиры только с фотографиями планировки или двора.\n",
      "\n",
      "Airbnb\r\n",
      "Завершал разговор о применении машинного обучения в области недвижимости Евгений Шапиро — специалист компании Airbnb, базирующейся в Сан-Франциско и выпускник нашей программы «Специалист по большим данным». Евгений рассказал о схеме выявления и предотвращения мошенничества на платформе.\n",
      "\n",
      "\n",
      "\r\n",
      "Существует множество видов мошеннических операций, совершаемых на платформе: кража аккаунтов, фишинг, фейковые страницы и листинги, оплата украденными кредитными картами самим себе, спам. Следовательно для выявления мошенников нам сначала необходимо понять, какое именно действие совершает пользователь, потому что интерфейс сервиса позволяет сделать одно и то же действие разными способами. Мы начинаем собирать различную информацию (действия клиента, «следы», которые он оставляет, разные cookies и т.д.) и, классифицировав тип выполняемого действия, подключаем модели машинного обучения, которые оценивают вероятность того, что мы можем позволить это действие (in-flow evaluation). Если эта вероятность недостаточно высока, то мы предлагаем пользователю предоставить какие-то дополнительные данные, чтобы удостовериться, что у него нет никаких плохих намерений (верификация по телефону или email). К примеру, если в Ваш аккаунт был совершен вход из Гайаны, то скорее всего, это не Вы (хотя на 100% мы не уверены, для этого и верификация).\n",
      "\r\n",
      "На случай если по какой-то причине системой было пропущено нежелательное действие, то применяется out-of-flow evaluation — ML модели проверяют хранилища данных на предмет уже совершенных мошеннических действий, которые затем необходимо как можно скорее «зачистить». Например, если кто-то создал 1000 аккаунтов с одной и той же картинкой мы можем их выявить и массово устранить. Сюда же относится и account suspension: если мы уверены, что какой-то аккаунт совершал странные действия против других пользователей, то мы его блокируем.\n",
      "\r\n",
      "Если подробнее разбирать in-flow evaluation, то все действия клиентов оцениваются rules engine с именем Kyoo (так звали судью в StarTrek), который, собирая данные из различных источников, оценивает события с точки зрения набора простых правил (например, если залогинился с нескольких ID сразу, то что-то не так) и присваивает каждому один из лейблов: аккаунт украден, оплата ворованной картой и т.д. Kyoo был написан на Scala наподобие Facebook Haxl.\n",
      "\r\n",
      "Говоря об источниках данных, стоит отметить, что, просто обращаясь к разным API, данные будут не очень интересными (состояние аккаунта, его листинга), хотя как правило для оценки риска важны более агрегированные метрики: с какой частотой совершаются действия на сайте, с какой скоростью загружаются страницы, сколько платежей мы видели с кредитных карт клиента. Эти параметры вычисляются постоянно, агрегировать их из баз данных невозможно, поэтому оптимальным вариантом будет предвычислить некоторое количество таких сигналов и поместить в хранилище по типу «ключ — значение», чтобы к тому времени, как осуществляется какое-то действие, у нас уже был большой объем информации о пользователе.\n",
      "\r\n",
      "Однако здесь у нас возникает проблема архитектуры: часто для принятия решения нам нужен сигнал не только на исторических данных, но и на текущий момент (точнее по завершении текущего дня). Таким образом, в Airbnb следующая имплементация лямбда-архитектуры с 2 частями: первая отвечает за оффлайн-сигналы, которые можно вычислить в Hive, там можно делать сколь угодно сложные вычисления, пока они укладываются в 24 часа, а вторая — это real-time события, которые через Kafka уходят в real-time aggregation.\n",
      "\r\n",
      "В результате получаем стабильный пайплайн обработки данных, максимально эффективно реагирующий на запросы. К примеру, сколько платежей за последние 7 дней по этой кредитной карте произошло? Фактически этот сигнал является комбинацией того, что мы знаем: количество платежей за последние 30 дней (из 1 части архитектуры) и за текущий день (из 2 части). Такой подход позволяет нам одновременно и тренировать модели, и скорить их на основании реальных данных.\n",
      "\r\n",
      "Партнером Data Science Week 2017 выступает компания МегаФон, а инфо-партнером — компания Pressfeed.\n",
      "\r\n",
      "Pressfeed — Способ бесплатно получать публикации о своей компании. Сервис подписки на запросы журналистов для представителей бизнеса и PR-специалистов. Журналист оставляет запрос, вы отвечаете. Регистрируйтесь. Удачной работы.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " На kaggle сейчас проходит конкурс USA Census по поиску интересных фактов в American Community Survey данных за 2013 год. Данные этого анкетирования выложены в свободный доступ, подробности можно найти здесь. \n",
      "Kaggle выбрал для анализа два направления — персональные сведения (пол, возраст, семейное положение и т.д.) и сведения о домохозяйствах (различные характеристики жилья, доход домохозяйства, налоговые платежи и прочее). Хочу поделиться своими результатами, которые сфокусированы на различиях домохозяйств в зависимости от вида права собственности на их жилье — владение с ограничением (ипотека или заем), владение без ограничений и не владеют (аренда).\n",
      "\n",
      "\n",
      "infographics: American Housing Survey Factsheets\n",
      "\n",
      "Данные American Community Survey (ACS) являются взвешенными, дизайн исследования задан повторными весами. Поэтому все статистики, где это имеет смысл, являются взвешенными. Все домохозяйства страны разделены на 2351 кластер по географическому принципу с населением около 100 тысяч человек в каждом кластере. Эти кластеры называются PUMAs (public use microdata areas). Далее всюду рассматриваем целевую аудиторию, в которой домохозяйства либо являются собственниками жилья с ограничениями на имущество, либо являются собственника своего жилья, либо арендуют жилье. Это целевая аудитория составляет около 86 % от общего числа домохозяйств в стране. \n",
      "\n",
      "Сравнение расходов на ипотеку и аренду\n",
      "Следующие два графика показывают средние расходы домохозяйств на ипотеку и аренду в этих кластерах. Единицы измерений первого графика — доли затрат на ипотеку/аренду от дохода домохозяйства, второй график показывает затраты домохозяйств на ипотеку/аренду в долларах за месяц. Красная линия на обоих графиках показывает медианные величины по аренде относительно небольшого интервала по ипотеке.\n",
      "\n",
      "\n",
      "\n",
      "Можно видеть, что в среднем доля расходов на жилье у домохозяйств, которые его арендуют, выше, чем у домохозяйств, которые приобрели жилье в ипотеку. Но в абсолютных цифрах картина меняется на противоположную — ежемесячные платежи в среднем больше у второй группы. Эти наблюдения справедливы для почти всех регионов.\n",
      "\n",
      "Преобладающий вид права собственности на жилье в зависимости от уровня дохода домохозяйства\n",
      "Рассмотрим распределение долей домохозяйств с одним из трех видов права собственности по децилям их доходов за год. То есть делим целевую аудиторию на 10 равных частей (с учетом весов) согласно их уровню дохода. В первую группу попадают 10 % домохозяйств с наименьшим доходом и так далее по возрастанию уровня дохода домохозяйств. \n",
      "\n",
      "Получаем следующий результат (красным обозначена доля арендующих жилье, светло-серым — доля собственников без ограничений, синим — доля ипотечных собственников)\n",
      "\n",
      "\n",
      "\n",
      "Легко видеть тренд различия в долях арендующих жилье и собственников с незавершенными ипотечными выплатами, в зависимости от уровня дохода. Примерное равенство этих долей (37-38 %) достигается в 5 дециле. Доля собственников без обременения на жилье, начиная с 3 дециля, при росте уровня дохода падает ~ 1.5 % на дециль, за исключением последней группы с наибольшими доходами.\n",
      "\n",
      "Влияние социальных факторов на вид права собственности на жилье\n",
      "Рассмотрим три типа семей\n",
      "\n",
      "Оба супруга работают или являются военнослужащими\n",
      "Только муж работает или является военнослужащим, жена не работает и не ищет работу\n",
      "Только жена работает или является военнослужащей, муж не работает и не ищет работу\n",
      "\n",
      "Ранее мы разделили всю целевую аудиторию на децили по уровню дохода домохозяйств за год. Рассмотрим распределение уровней доходов каждого из этих классов семей по полученным границам найденных децилей.\n",
      "\n",
      "\n",
      "\n",
      "Как и ожидалось, доход семьей, в которых работают оба супруга, имеет значительное смещение к верхним децилям. На основании предыдущих сведений можно предположить, что доля семей этого класса, которые арендуют жильё, меньше средних цифр по всей стране. Так и есть, меньше почти в два раза\n",
      "\n",
      "\n",
      "\n",
      "Как видим, семьи, в которых работает только жена, а муж не работает и не ищет работу, менее состоятельны по сравнению с семьями, где работают оба супруга. Верно ли, что доля семей, которые арендуют жилье, в классе, где работает только жена, больше, чем доля арендаторов в семьях с обоими работающими супругами? Оказывается нет.\n",
      "\n",
      "\n",
      "\n",
      "Убедимся, что средняя величина доходов на человека за год, в семьях, в которых работают оба супруга превосходит таковую для остальных двух классов семей. График ниже показывает диапазон среднего дохода на человека в первых трёх квартилях.\n",
      "\n",
      "\n",
      "\n",
      "А теперь посмотрим на распределение долей видов права собственности в этих классах по квартилям доходов всех домохозяйств. \n",
      "\n",
      "\n",
      "\n",
      "В каждой группе одного уровня доходов семьи, в которых работает только жена, имеют самую низкую долю арендаторов жилья. Покажем, что эти различия статистически значимые. Для этого задаем дизайн исследования. После этого находим логистическую регрессию вида \n",
      "\n",
      " \n",
      "Получаем следующую таблицу коэффициентов моделей с величинами стандартных отклонений и p.value значениями\n",
      "\n",
      "\n",
      "\n",
      "То есть во всех четырех случаях коэффициент B_2 значимо отличается от 0. Для линейной комбинации коэффициентов B_2 — B_1, использую тест Вальда, можно показать, что коэффициент B_2 значимо меньше коэффициента B_1 во всех четырех случаях.\n",
      "\n",
      "Это доказывает наше предположение о том, что семьи, где работает лишь супруга, реже арендуют жилье, чем остальные два класса семей. Можно показать, что образовавшаяся разница уходит на долю семей, которые имеют свое собственное жилье без ограничения права собственности на него. \n",
      "\n",
      "Те, кто заинтересовался, могут пройти по этой ссылке на kaggle, где размещены полноценные Google Charts с элементами управления и тултипами, код всех вычислений на языке R и дополнительные сведения и графика.    \n",
      " \n",
      "\n",
      "Был как-то в лагере в Одессе парнишка-школьник, который сидел в углу и не участвовал в общих движухах. В анкете он указал, что имеет успехи по математике. Подхожу, спрашиваю, мол, чего сидим, кого ждем. «Скучно и бессмысленно», — отвечал отрок. «Самый умный, да? Хочешь задачу по геометрии, решишь?» — «А то!». \n",
      "\n",
      " — Есть задачи на построение, слышал про такие? Ну так вот, даны циркуль и линейка, надо угол разделить на 4 равные части, осилишь за 10 минут?\n",
      " — Вот ответ.\n",
      " — Хорошо, вот следующий левел, есть угол, надо циркулем и линейкой разделить его на 3 части, на ужине покажешь ответ, идет?\n",
      "\n",
      "Пацан на ужин не пришел. Зато через 10 месяцев он пришел играть в настолки в Яндекс, потому что поступил в МИФИ (а вообще он из Молдовы), потому что я дал ему контакты своего одноклассника-яндексоида, которому в кайф по скайпу помогать юнлингам по математике.\n",
      "\n",
      "Летние школы и лагеря — это круто и полезно. Под катом сказ о том, как мы ловим подростков над пропастью во ржи и устраиваем для них АД, кусаем питоном и прокачиваем в области социальной инженерии.\n",
      "\n",
      "\n",
      "Это Борис. Он — верблюд. Борис умеет плеваться и кусаться, но он не подозревает, что ждет его этим летом.\n",
      "\n",
      "Почему GoTo\n",
      "Во-первых. Когда я был школьником, мне посчастливилось практически случайно попасть в летнюю школу во Владивостоке. Это был один из значимых и ключевых опытов. Да, там была моя первая настоящая победа в Starcraft CTF и Quake 2. Еще в школе читали лекции преподы из физтеха и МГУ, что дало мне дофига спецприемов, которые я использовал на олимпиадах по физике и математике.\n",
      "\n",
      "Во-вторых. Мне всегда было любопытна радиотехника и программирование, но находясь в «дыре», мне было очень трудно найти людей, готовых что-то подсказать, поделиться опытом. Сейчас я вижу в лице проекта GoTo именно ту структуру, которой так не хватало, когда я учился в школе. Тут есть и единомышленники и преподы (точнее профессионалы, работающие в своей области), готовые в нерабочее время прокачивать молодых. \n",
      "\n",
      "В-третьих, в школе у нас была «шайка единомышленников» — и мы тащили друг друга по математике, физике, Quake2 и Starcraft. В универы мы поступили разные, меньше времени проводили бок о бок и как следствие — взаимоиндукция и взаимоусиление в области проганья, ИБ и Counter Strike была не так сильна, как в школе. На базе GoTo формируется сообщество единомышленников (ну не совсем душа в душу, всегда есть свои терки), которые смотрят в одном направлении и готовы ботать вместе над проектом.\n",
      "\n",
      "Поэтому уже третий год я ввязываюсь в движухи команды GoTo. И вам того же желаю.\n",
      "\n",
      "Как это уже было\n",
      "AYCamp (ABBYY+Yandex) 2014 года\n",
      "Отчетная статья на Хабре: «Свой образовательный лагерь с покером, 3d-принтером, роботами и посадкой на Марс»\n",
      "\n",
      "Так же мы вместе со школьниками написали несколько статей.\n",
      "\n",
      "Как Митник троллил ФБР. Статьи на Хабр из лагеря для школьников\n",
      "Интервью с Романом Удовиченко. Code Jam TOP 10 или как хорошо живется олимпиадникам\n",
      "Хакер/фрикер 1903 года: взлом «защищенного» беспроводного канала связи\n",
      "\n",
      "GoTo Camp 2015 год \n",
      "Отчетная статья на Хабре:  «Детский лагерь: биссектрально-пифагоровы треугольники, перепрограммирование мозга, радар-детектор и взлом наручников»\n",
      "Фотки\n",
      "«Мой друг ушел из математики в бизнес. Но так как в бизнесе душа умирает, а в математике душа воскрешает, друг по вечерам воскресал свою душу.»\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Дизайн скафандров\n",
      "\n",
      "\n",
      "Тестирование скафандров (кислотный дождь и падение метеоритов)\n",
      "\n",
      "\n",
      "он рэилс\n",
      "\n",
      "\n",
      "Теперь вы понимаете как работают 386 процессоры?\n",
      "\n",
      "\n",
      "Сканирование мозга на предмет обнаружения мозговых слизней\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Хакатон 2016\n",
      "Анонс на Хабре: «Хакатон по анализу открытых данных пользователей социальной сети ВКонтакте. Для школьников и первокурсников».\n",
      "Фотки\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Вы попали на финал Всероссийской инженерной олимпиады\n",
      "\n",
      "Примеры проектов-исследованийКоманда About Blank. Согласно недавно обновленному законодательству люди, профили которых имеют более 3000 подписчиков, приравниваются к СМИ. Тема влияния СМИ на массы очень интересна как с научной точки зрения, так и коммерческой. Ребята взялись за исследование этих пользователей-хабов: обнаружили интересную информацию о профилях, узнали примерное соотношение количества ботов и реальных людей, выделили преобладающие виды контента на их стенах, попытались обозначить признаки, обуславливающие их популярность. Показали, что эти пользователи действительно постят в большинстве случаев фото и аудиозаписи, а также имеют открытые профили и принимают сообщения от всех пользователей.\n",
      "\n",
      "Развивая эту идею, можно понять, чем интересуется аудитория, подписанная на этих людей, узнать их увлечения, а также повлиять на них. Полученная информация позволит заниматься эффективным продвижением рекламы, рассчитанной на определенные группы пользователей в сети. Стоит выделить этот проект, поскольку ребята одни из немногих, кто взялся за практическую задачу и довел её до стадии работающего прототипа с возможностью будущей монетизации сервиса как аналога Я.Директа и Google.Ads.\n",
      "\n",
      "Команда Map of connections. В данном проекте ребята взялись за довольно интересное исследование — изучение закономерностей общения между учениками разных школ Москвы. Исследовалась плотность связей между учениками разных школ в зависимости от локации и наличия совпадающего профиля сравниваемых школ. Были построены графы связей между учащимися и получены так называемые компоненты связности, показывающие, как разделены по кругам общения те или иные группы школьников, а также представлена визуализация связей на карте г. Москвы. Такие данные можно использовать для улучшения взаимодействия школ в различных сферах: начиная от олимпиадного движения и заканчивая спортивными и общественными активностями.\n",
      "\n",
      "Команда 10011001. Команда студентов разработала приложение, которое позволяет искать зависимости данных автоматически и не строить гипотезы вручную. С помощью данного функционала они построили корреляции различных социальных признаков существующей выборки школьников в соцсети ВКонтакте: связи их основных интересов, музыкальных и литературных предпочтений с гендерным различиями, с национальностью, образом жизни и многими другими характеристиками. Главной ценностью их работы является возможность задавать любые входные данные и получать самые значимые корреляции, которые можно использовать для построения среднестатистического отношения выделенной по признакам группы людей к интересующей нас теме.\n",
      "\n",
      "[Источник]\n",
      "\n",
      "Городские лагеря\n",
      "Мы провели несколько интенсивов (с суровым погружением в тему машинного обучения) в центре Москвы с размещением в хостле.\n",
      "Фотки\n",
      "Бобук кастует тренды здравомыслие \n",
      "\n",
      "\n",
      "Сыграем, нам разрешили\n",
      "\n",
      "Пабам\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Вроде брали адекватных вожатых\n",
      "\n",
      "\n",
      "Получаем знания из книг напрямую\n",
      "\n",
      "\n",
      "Инкапсуляция\n",
      "\n",
      "\n",
      "Малый, да удалый\n",
      "\n",
      "\n",
      "Я знаю кунг-фу\n",
      "\n",
      "\n",
      "Мысли уважаемых людей из области ИТ по поводу образования\n",
      "Питер Тиль\n",
      "«Учебная тревога» \n",
      "Питер Тиль советует работодателям нанимать больше людей без высшего образования, а выпускникам школ — десять раз подумать, прежде чем поступать в университет.\n",
      "\n",
      "Пол Грэм\n",
      "«Что мы хотели знать еще в школе»\n",
      "«Я написал эту речь для выступления перед выпускниками школы. Мне не удалось с ней выступить, так как школьная администрация запретила меня приглашать.»\n",
      "\n",
      "BackStage\n",
      " — Серега, про что рассказывать будем?\n",
      " — Могу рассказать про то как поджигать бездымный порох чтоб пальцы не оторвало.\n",
      " — Ха, я могу дополнить рассказом как мы в детстве пилили танковый снаряд, чтоб выковырять из него «семидыр».\n",
      " — Я смотрю на своих знакомых и фигею, каким чудом все выжили.\n",
      " — Может про ИИ в играх расскажешь?\n",
      " — Давай лучше я их социнжинирингом потроллю. \n",
      "\n",
      " — Алена, я никуда не поеду, пока не найду верблюда, чтобы сфотать его для статьи на Хабре. («Зачем в лагере для программистов верблюд Боря»)\n",
      " — Давай мы его в договор включим как обязательное условие.\n",
      " — Говорят он плюется. Давай сделаем проект интернета вещей для верблюда, ну там «умный верблюд» типа, прикрутим ему на горб ардуинку и сенсоры и будет он как шагающий танк в «Возвращении джедая».\n",
      "\n",
      "\n",
      " — Савватеев (профессор РЭШ и МФТИ) тему выбрать не может.\n",
      " — В смысле?\n",
      " — Ну, мечется, либо «Нерешенные задачи математики» брать или «Математику, которая появилась благодаря интернет».\n",
      "-…\n",
      " — В итоге он выбрал тему — «Математика интернет-торгов: аукционы в теории и на практике». И написал, что «а давай все же вот такую тему, не для лохов же».\n",
      "\n",
      " — Верблюда не будет, зато будут лохматые коровки (шетландские), с ними можно будет обниматься. И страусиные яйца.\n",
      "\n",
      " — Алён, а давай я у Федосеева спутник стрельну, мне на инженерной олимпиаде очень понравилось, как ребята за 3 дня из рассыпухи спутник сделали и запрогали.\n",
      " — А если они его сломают?\n",
      " — Тем лучше, значит мы провели аудит безопасности и надежности. Это даже интереснее, потому что попадет в хаб «информационная безопасность».\n",
      "\n",
      " — По ночам все нормально будет, за территорию лагеря не уйдут. Да и там егеря с ружьями.\n",
      "\n",
      " — Ален, а что ваш герой интернета, который для медузы бота написал как-то скомканно мне отвечает на вопросы в вк?\n",
      " — Леш, он в 8 классе.\n",
      "\n",
      " — А давай, тем кто в «хакерской олимпиаде» выиграет, тому дадим из дробовика пострелять?\n",
      " — Леша! Не вздумай про это писать. Родители детей не отпустят.\n",
      "\n",
      "Диалог с E-contenta.\n",
      " — Алена, я обожаю твоих детей! Чувак за 1 день без документации по API все запилил. Вот подумываю не пора ли мне уволиться. На мою зп можно тут детей двадцать таких содержать, ну может не двадцать, но точно больше десяти.\n",
      " — )))\n",
      " — Юра у нас по такой же схеме работает, но он так жжет, что я хочу ему скоро в штат предложить пойти. А то так больше выходить будет. Вчера нам сделал лекцию по метрикам качества иерархических классификаторов.\n",
      " — Круто так! Я прям как мамаша радуюсь!\n",
      " — Блин, они за маленькие деньги фигачат лучше взрослых.\n",
      "\n",
      "На собеседовании с вожатыми:\n",
      " — Ну в прошлом лагере я делала дискотеки и танцевальные конкурсы.\n",
      " — А паять вы умеете? А интегралы брать?\n",
      "\n",
      "P.S. \n",
      "\n",
      "\n",
      "Анонс мастер-класса «Всякая фигня про социальную инженерию».\n",
      "\n",
      " Что такое социальная инженерия и как с этим теперь жить.\n",
      " Базовые технологии манипуляции: как проходить мимо очереди, как зайти на любую стройку и почти в любую компанию. \n",
      " Примеры когнитивных искажений: из рекламы, из устоявшихся правил а-ля «Всё, что нас не убивает, детает сильнее».\n",
      " Основные триггерные механики, «лесенка» убеждения. \n",
      " Смещение фокуса, играем в переговоры.\n",
      " Практическое применение: учимся отвечать на комментарии в агрессивной среде.\n",
      "\n",
      "\n",
      "Ну, а я опять заставлю самых непослушных переводить Пола Грэма, писать статьи на Хабр о том, как мы хакали спутник и прогали анализатор когнитивных искажений.\n",
      "\n",
      "Кому интересно подробности вот тут — GoTo Camp 2016.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Пост-призер новогоднего NUCо-конкурса Intel.\n",
      "Возникла необходимость создания портативного приемопередатчика, предназначенного для цифровой обработки и формирования ВЧ сигналов в реальном времени, для обучения практикантов основам анализа спектра и цифровой обработке сигналов.\n",
      "Вот как мы решили эту задачу.\n",
      "\n",
      "Одни из основных требований, предъявляемые к приемопередатчику:\n",
      "\n",
      "программное обеспечение для работы с приемопередатчиком (пользовательский интерфейс) должен работать под управлением ОС Windows;\n",
      "удаленное управление по сети (следует из первого пункта);\n",
      "портативность (возможность взять устройство домой).\n",
      "\n",
      "За основу был взят SDR-трансивер USRP B200 от Ettus Research. По своим радиочастотным параметрам он нам полностью подошел, и, что самое главное, он у нас был и не один.\n",
      "\n",
      "Однако, с его применением возникли несколько неприятных проблем:\n",
      "\n",
      "Софт, который работает с этим трансивером под Windows, либо платный, либо неудобный, либо его невозможно доработать под собственные нужды.\n",
      "Софт, который нас полностью устраивает (конкретно, GNU Radio) АДЕКВАТНО работает только под LINUX.\n",
      "Сам трансивер имеет интерфейс управления USB 3.0, что лишает возможности удаленного управления. Конечно, можно приобрести и сетевую версию трансивера, но приемники-то уже есть.\n",
      "\n",
      "Итак, поразмыслив немного, было принято решение использовать мини-ПК с установленной Ubuntu и GNU Radio, на котором будет осуществляться непосредственно обработка данных с трансивера. Пользовательский интерфейс будет работать под Windows с удаленным управлением и получением обработанных данных по сети.\n",
      "\n",
      "Требования к мини-ПК:\n",
      "\n",
      "наличие USB 3.0 для связи с трансивером;\n",
      "сеть 1 Гб/с, на случай если возникнет необходимость передавать сырые данные с трансивера;\n",
      "габаритные размеры.\n",
      "\n",
      "Одноплатники на базе ARM отбросили сразу, не в каждом магазине их можно найти (необходимо заказывать), да и не хотелось связываться с ARM, т.к. уже имелись наработки с х86.Подумав, выбрали Intel NUC NUC6CAYH (цена на момент покупки ~ 10 т.р). Докупив 4 Gb RAM, SSD на 60 Gb, мы получили полноценный ПК.\n",
      "\n",
      "Чтобы не бояться убить приёмник решено сразу разместить всё в корпусе. Под рукой оказался кейс Explorer 2712. Да еще и ударопрочный!\n",
      "\n",
      "Сборка макетного образца:\n",
      "\n",
      "Intel NUC NUC6CAYH пришлось освободить от корпуса для удобства монтажа на несущую пластину, и чтобы занимал меньше места.\n",
      "Больше фото\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Кабельные сборки с SMA на N-типа, а также разъем питания, светодиод, кнопку и разъем RJ-45, выходящие наружу, пришлось докупать. Монтажная пластина выполнена методом лазерной резки. Блок питания, идущий в комплекте с Intel NUC пришлось немного доработать, чтобы разместить внутри кейса.\n",
      "И вот что получилось:\n",
      "\n",
      "\n",
      "Спектр решаемых задач полученного приемопередатчика достаточно большой.\n",
      "При работе по VNC доступно:\n",
      "\n",
      "GNU Radio (для разработки потоковых графов);\n",
      "gr-fosphor (RTSA-spectrum);\n",
      "gqrx (SDR-receiver);\n",
      "При работе по SSH доступно удаленное управление:\n",
      "YateBTS;\n",
      "GNU Radio (для запуска скомпилированных скриптов);\n",
      "наборами утилит для захвата сырых данных с приемника.\n",
      "\n",
      "Заключение\n",
      "На фото представлен макетный образец, разработанный еще в 2018 году. На данный момент успешно функционирует уже несколько таких приемопередатчиков (Фото остальных, к сожалению, нет). В будущем планируется разработка приемопередатчика с автономным питанием от АКБ, что не грозит большими проблемами, т.к. заявленная расчетная мощность Intel NUC NUC6CAYH составляет всего 10 Вт при напряжении питания 12-19 В.\n",
      "Разработчики: Дмитрий Сергеев aka dimserg92 и Александр Шкарлатов aka CrazyAlex25.    \n",
      " Привет, Хабр!\n",
      "\n",
      "Мы уже в третий раз запускаем чемпионат по Data Science совместно с сообществом экспертов и команд по искусственному интеллекту AI Community. В этом году соревнование пройдет полностью в онлайн, а призовой фонд составит 1 миллион рублей. \n",
      "\n",
      "Главное о чемпионате:\n",
      "\n",
      "\n",
      "Стартуем 21 ноября, собираем заявки до 13 декабря, победителей объявим 19 декабря\n",
      "Решать кейсы можно индивидуально или с командой\n",
      "Подать заявку могут все (вообще все, вне зависимости от опыта и места жительства), за исключением наших действующих сотрудников, увы\n",
      "Призовой фонд — 1 000 000 рублей, а лучшие участники могут получить стажировки и вакансии.\n",
      "\n",
      "\n",
      "\n",
      "Подробнее о задачах 2020 — под катом.\n",
      "\n",
      "Переходим к задачам\n",
      "Участники Sibur Challenge 2020 поработают не с игровыми заданиями, а реальными бизнес-кейсами и данными компании.\n",
      "\n",
      "Задача 1 (про производство): Сырье\n",
      "\n",
      "Широкая фракция легких углеводородов (или ШФЛУ) — это сырье, которое поступает по трубопроводу от газоперерабатывающих заводов на производства СИБУРа. На станциях подкачки, расположенных вдоль трубопровода, подается дополнительное сырье — то есть состав ШФЛУ, поступающий потребителю, меняется со временем.\n",
      "\n",
      "Задача — создать прогноз состава поступающей ШФЛУ. Это позволит СИБУРу оптимизировать технологические процессы и снизить издержки.\n",
      "\n",
      "Задача 2 (про бизнес): Продажи\n",
      "\n",
      "Менеджеры по продажам СИБУРа работают с огромным количеством потенциальных клиентов. Названия компаний при этом могут быть указаны по-разному, например, с сокращениями или на разных языках. Это усложняет работу по поиску новых клиентов: дубликаты приходится определять вручную.\n",
      "\n",
      "Участникам предстоит использовать машинное обучение и открытые источники, чтобы определить, принадлежат ли названия одной и той же компании.\n",
      "\n",
      "Ещё немного о задачах, а также вступительное слово нашего главного ментора — Алексея Винниченко, руководителя центра аналитики в СИБУР Диджитал, — в видеоприглашении.\n",
      "\n",
      "Как всё будет проходить\n",
      "Будем решать задачи с 21 ноября по 13 декабря. Все этапы соревнования пройдут онлайн — на специальной платформе, где ваши решения будут храниться в облаке. \n",
      "\n",
      "В прошлом году мы опробовали (и получилось неплохо) коммуникацию через telegram — в этом году у нас работают канал и закрытый чат. Все новости чемпионата, информация о начисленных баллах — будет там.\n",
      "\n",
      "И самое главное — зарегистрироваться можно по ссылке (уже можно). \n",
      "\n",
      "Удачи!    \n",
      " Группа исследователей из IBM, используя информацию о перемещениях 500 тыс. пользователей мобильной связи, разработали модель для улучшения маршрутов общественного транспорта. \n",
      "\n",
      "Модель успешно опробовали в городе Абиджан (население 3,8 млн человек, столица Кот-д’Ивуара). На иллюстрации показаны действующие маршруты автобусов (малиновым) и улучшения, предлагаемые алгоритмом (синим). Он предложил 65 возможных улучшений, в том числе три новых маршрута, с общей экономией времени 10% в пассажиро-минутах для всех пассажиров, учитывая время поездки и время ожидания.\n",
      "\n",
      "Новый алгоритм полезен не только для оптимизации, но и для прокладки новых маршрутов в только что построенных микрорайонах. Здесь он чётко показывает, какими путями, в какое время и в каком объёме передвигается людская масса. Можно составить оптимальные маршруты и расписание транспорта. Важен сам факт появления нового инструмента, который могут использовать городские власти для оптимизации инфраструктуры мегаполиса.\n",
      "\n",
      "\n",
      "85 маршрутов общественного транспорта SOTRA в Абиджане\n",
      "\n",
      "Каждый владелец сотового телефона выступает в роли индивидуального сенсора в сети. Информация с сенсоров собирается оператором сотовой связи и анализируется. В данном случае информацию о звонках в сети сотовой связи Абиджана с декабря 2011 года по апрель 2012 года предоставил оператор Orange. База включает 2,5 миллиарда записей и на сегодняшний день является крупнейшей базой подобного рода, доступной для научных исследований. Естественно, база очищена от любой персональной информации: вся статистика анонимна.\n",
      "\n",
      "\n",
      "Плотность пользователей, по месту проживания (слева) и месту работы (справа)\n",
      "\n",
      "В Абиджане транспортное сообщение состоит из 539 автобусов в сети SOTRA, 5000 микроавтобусов и 11000 общественных такси. Авторы научной работы использовали информацию о звонках и SMS с 500 тыс. мобильных телефонов. Во время каждого звонка оператор сохраняет информацию о базовой станции, которая обслуживает абонента, что позволяет с достаточной точностью определить его координаты. Перемещение телефона регистрируется, если впоследствии его начинает обслуживать другая сота.\n",
      "\n",
      "\n",
      "Сравнение времени до и после оптимизации маршрутов SOTRA, в сотнях тысяч пассажиро-минут для всего пассажиропотока\n",
      "\n",
      "Вообще, подобная «слежка» в реальном времени может быть очень полезна. Например, создаются даже алгоритмы предсказания преступлений на основе информации с сотовой сети. Если этот алгоритм IBM применить не на исторических данных, а на информации в реальном времени, то теоретически можно даже оперативно редактировать расписание общественного транспорта, мгновенно реагируя на увеличение людского трафика по определённым маршрутам.\n",
      "\n",
      "Алгоритм обработки данных под названием AllAboard составили специалисты из дублинской лаборатории IBM Research, участвующие в программе Data for Development. Итоговый доклад “AllAboard: a system for exploring urban mobility and optimizing public transport using cellphone data” представлен на конференции NetMob 2013, которая посвящена обработке баз данных из сотовых сетей. \n",
      "\n",
      "\n",
      "Среднее время ожидания транспорта на остановках (вверху) и плотность пассажиропотока по разным маршрутам (внизу)\n",
      "\n",
      "Ознакомиться с научной работой можно в сборнике материалов конференции (pdf), стр. 397–411.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " На мой взгляд, данная статья будет интересна тем, кто планирует организовать стартап и при этом нуждается в людях (то есть нет таких друзей, готовых принять участие в проекте). Еще, это полезный опыт для тех, кто занимается наймом в принципе, например, в качестве кейса.\n",
      "\n",
      "Описание кейса\n",
      "Предыстория (и все подводные камни)\r\n",
      "В нашей компании есть отдельное направление развития, которое можно назвать «Управление персоналом». Помимо повседневных HR обязанностей мы занимается разработкой программ и методов развития персонала. В один прекрасный момент появилась идея, что было бы круто заняться долгосрочным прогнозированием развития персонала. Этот сложный проект было решено реализовывать инновационным способом — применить великое и могучее Машинное обучение! И, конечно же, никто в нашей компании толком не знал, что это такое, кроме того, что именно машинное обучение – популярный и прогрессивный метод прогнозирования.\n",
      "\r\n",
      "Важно отметить, что у руководства нашей компании с большим багажом опыта и решающим голосом в решении брать или не брать человека есть одно жесткое требование – наличие оконченного высшего образования. Магистратуры или специалитета. Насколько я понял, это связано с печальным опытом, когда сотрудники компании пытались совмещать учебу и работу, что приводило к проблемам со сроками и качеством проектов.\n",
      "\n",
      "Цель: сформировать команду, которая будет реализовывать различные внутренние и внешние IT-проекты, в первую очередь, в области машинного обучения. \n",
      "\n",
      "Задачи: \n",
      "\r\n",
      "A. Первый год – найти двух математиков/ data scientist/ data analyst (обещаю, что разберусь, как точно можно определить, что именно делает у нас человек) для первого проекта по \r\n",
      "прогнозированию методами машинного обучения;\n",
      "\r\n",
      "B. Второй год – найти еще двух математиков для ускорения работы над проектами;\n",
      "\r\n",
      "C. Сформировать команду, которая продолжит свою работу над различными IT-проектами в компании;\n",
      "\r\n",
      "D. Понять, где можно находить нужных людей на постоянной основе (наладить взаимоотношения с выпускающими образовательными организациями, для начала);\n",
      "\n",
      "Проблемные зоны:\n",
      "\n",
      "\n",
      "Что такое машинное обучение и как именно с помощью него можно прогнозировать развитие персонала?;\n",
      "Кто наш будущий сотрудник? Кто именно занимается этим прогнозированием? Кто потенциально может им заниматься? – образование, знания, навыки и другие профессионально и личностно важные качества;\n",
      "Высокие формальные требования к кандидатам со стороны руководства: только окончившие в этом или прошлом году специалисты или магистры;\n",
      "Отсутствие бренда: о нашей компании пока никто толком не знает, а мы, ко всему прочему, на данном этапе развития – внутренний проект. То есть, нет даже сайта;\n",
      "Отсутствие гибкости в условиях по заработной плате: довольно низкий уровень оплаты за полный рабочий день в офисе, но, на мой взгляд, актуальный для вакансии уровня «стажер»;\n",
      "Никакого аутсорса: одна из целей – создать команду, которая реализует актуальные проекты и будет продолжать работать и развиваться в рамках проектов компании. Да, я понимаю, что можно заказать на стороне, будет быстро-качественно-дешевле, но…нет;\n",
      "Неведомый формат компании, который чувствуют все: думаю, многим знакома такая ситуация, когда в компании подбираются люди, которые каким-то образом можно назвать «формат компании». В нашем случае, это личностные качества, которые можно определить так: инициативность, заинтересованность в задаче, креативность, способность преодолеть сопротивление со стороны руководства. Вроде бы стандарт, но, в итоге, многих людей не взяли из-за этого.\n",
      "\n",
      "Положительные черты рабочего процесса (пряники, которые, насколько я знаю, уже не являются чем-то необычным):\n",
      "\n",
      "\n",
      "«Индивидуальный график работы»: условно, это стандартная система 5/2 по 8 часов в день/ 40 часов в неделю с тем отличием, что прийти и уйти можно когда удобно; Конечно, обязательно присутствовать на важных встречах и совещаниях разного жанра, плотно и качественно взаимодействовать с командой. Если заболел, не обязательно бежать за справкой – 2-3 дня в месяц на «легкий перерыв» все поймут;\n",
      "Коллектив: никогда не думал, что это действительно работает, но наш молодой (средний возраст около 26) и активный коллектив действительно притягивает людей;\n",
      "Интересная задача: еще одно открытие для меня, что наша задача многим кандидатам была интересна. Или они делали вид. Пока не решил;\n",
      "«Учиться, учиться и еще раз учиться…»: в нашей компании приветствует самообразование различного характера. Смотри курсы, посещай образовательные мероприятия – все поощряется и часто оплачивается (готовы оплатить курсы, выезды и все в этом духе);\n",
      "Печеньки/ кофе/ чай;\n",
      "Техника: купим то, что нужно под твою задачу, только придется подождать неделю.\n",
      "\n",
      "Что было применено/ не применено (точнее, что запомнилось)\r\n",
      "Посещение университетов и вузов (защиты квалификационных работ, общаги, сайты, группы): конечно, в первую очередь мы обратились в выпускающие учреждения, которые находятся в Санкт-Петербурге. Думаю, список таковых очевиден. Есть два варианта – «зайти» официально и «неофициально». В любом случае вся сложность заключается в том, чтобы найти того человека, который несет реальную ответственность за распространение информации о трудоустройстве и работающий канал распространения. Так, например, у некоторых университетов на момент мая-июнь 2018 года не работал практически ни один сайт университета, кроме ИТМО, в частности, careers.ifmo.ru. Только откликов с него я не увидел совсем. Может быть не понял, что они от туда… В целом, у официального пути ключевая сложность в том, что если у тебя нет известности и репутации, то работать с тобой никто не будет. Точнее так: с тобой не будут работать с точки зрения масштаба университета, но кафедры вполне благосклонны к взаимодействию.\n",
      "\r\n",
      "У СПбГУ неплохо работает группа в ВК, в частности группа Матмех СПбГУ (https://vk.com/mmspbu). \n",
      "\r\n",
      "В итоге, так или иначе, удалось посетить защиты в СПбГУ (Матмех и ПМПУ), ИТМО (многие направления), Политех. По результату получалось так, что выпускники чаще всего трудоустроены (особенно магистранты), в том числе, работают над собственными проектами. Я понимаю, что это логично, что посещать защиты – дедовский метод. Тем не менее, несколько собеседований, таким образом, было назначено. Кандидаты, которые приходили, были достаточно интересными, но, так или иначе появлялись сложности, которые не позволяли нам ударить по рукам: либо возможного работника не устраивал начальный уровень заработной платы, либо нас не устраивал уровень знаний или личностные особенности кандидата.\n",
      "\n",
      "Сайты для рекрутинга \n",
      "\n",
      "Hh – 2/4! Лучший результат. \n",
      "\r\n",
      "Очевидно, что это самый простой способ. Казалось бы – оплати базу данных и размещение вакансии и отпинывай кандидатов. Но если сузить рамки до требований руководства с учетом менталитета команды, то это выбор из пяти десятков. Опять же, остается проблема – одна зарплата для всех, кого мы берем на первом этапе. Ни в плюс, ни в минус она не пересматривается категорично. Могу лишь сказать, что для рынка это нормальная зарплата для стажера. Только стажеры обычно это 4- курс специалитета…\n",
      "\n",
      "Superjob\n",
      "\r\n",
      "Не скажу, сколько конкретно было собеседований, но они были. По итогу, получается, что плюс у этого поставщика услуг в том, что можно платить за каждую отдельную анкету, а не за всю базу. То есть, если удается найти резюме, которое подходит по формальным требованиям, то можно открыть именно его. \n",
      "\r\n",
      "Основная проблема, с которой я столкнулся – неактуальные резюме. То есть, в последний раз некоторые резюме обновляли очень давно. \n",
      "\n",
      "Сообщества ВК\n",
      "\n",
      "vk.com/progjob – не пробовал, но рекомендуют;\n",
      "vk.com/itmozg – не пробовал, но рекомендуют;\n",
      "vk.com/mmspbu – описал раньше;\n",
      "\n",
      "Сообщества Facebook\n",
      "\n",
      "www.facebook.com/groups/349336311780984\n",
      "www.facebook.com/groups/itrecruitergroup/?ref=group_browse_new\n",
      "www.facebook.com/groups/itrecruiting/?ref=group_browse_new\r\n",
      "Результата у меня не было, но вдруг вам поможет.\n",
      "\n",
      "«По знакомству»\n",
      "\r\n",
      "Самый эффективный метод. Без шуток. Конечно, нам за два года потребовалось найти всего 4 специалиста, тем не менее, каждый второй был приведен первым человеком.\n",
      "\n",
      "Что советовали знакомые, но использовано не было:\n",
      "\r\n",
      "«Мой круг»\n",
      "\r\n",
      "Думаю, это самый ценный ресурс, который я не использовал и который хочу попробовать в следующий раз. \n",
      "\n",
      "Посещение Хакатонов\n",
      "\r\n",
      "Также как и «Мой круг». Слышал, что очень часто хакатоны сами по себе используются крупными игроками, чтобы набирать работников.\n",
      "\n",
      "Ярмарки вакансии\n",
      "\r\n",
      "Игнорировались из-за мнения руководства, что это нерезонно и ресурсы на них выделяться не будут. Бесплатных, на от момент, я не нашел. Бегать в качестве участника и ловить людей за руку мне не хотелось.\n",
      "\n",
      "Что получилось?\r\n",
      "На данном этапе результат таков:\n",
      "\n",
      "\n",
      "В ближайшее время с интересующими нас кафедрами будет заключены договоры на проведение практик и стажировок;\n",
      "На начальном этапе, когда удается найти всеми правдами и неправдами человека – надо тащить его знакомых. Сетевой маркетинг в действии;\n",
      "Штат укомплектован;\n",
      "Проекты живы и развиваются;\n",
      "Костяк команды сформирован;\n",
      "Написана эта статья.\n",
      "\n",
      "Спасибо за внимание!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет, Хабр! Представляю вашему вниманию перевод статьи \"Pythonで0からディシジョンツリーを作って理解する　（3. データ分析ライブラリPandas編）\".\n",
      "\n",
      "Это третья статья из серии. Ссылки на предыдущие статьи: первая, вторая\n",
      "\n",
      "В данной статье я объясню, как работать с библиотекой Pandas, чтобы создавать Decision Tree.\n",
      "\n",
      "3.1 Импортируем библиотеку\n",
      "# импортируем pandas и прописываем, что далее мы будем ее указывать как pd\n",
      "import pandas as pd\n",
      "3.2 Data frame и Series\n",
      "В pandas используются такие структуры, как Data frame и Series. \n",
      "Рассмотрим их на примере следующей таблицы, напоминающей Excel.\n",
      "\n",
      "Одна строка данных называется Series, столбцы — атрибутами этих данных, а вся таблица целиком — Data frame-ом.\n",
      "\n",
      "\n",
      "3.3 Создаем Data frame\n",
      "Подключаем Excel-таблицу с помощью read_excel или ExcelWriter\n",
      "# Сохраняем Excel файл туда же, где находится и файл ipynb\n",
      "df0 = pd.read_excel(\"data_golf.xlsx\")\n",
      " \n",
      "# выводим DataFrame как HTML таблицу \n",
      "from IPython.display import HTML\n",
      "html = \"<div style='font-family:\\\"メイリオ\\\";'>\"+df0.to_html()+\"</div>\"\n",
      "HTML(html)\n",
      " \n",
      "# Сохраняем в Excel файл (with автоматически выполняет f.close)\n",
      "with pd.ExcelWriter(\"data_golf2.xlsx\") as f: \n",
      "       df0.to_excel(f)\n",
      "Создание Data Frame из словаря (ассоциативного массива): словарь собирает вместе данные столбцов DataFrame\n",
      "\n",
      "# создание из словаря: сбор данных из столбцов\n",
      " \n",
      "d = {\n",
      "    \"Погода\":[\"Ясно\",\"Ясно\",\"Облачно\",\"Дождь\",\"Дождь\",\"Дождь\",\"Облачно\",\"Ясно\",\"Ясно\",\"Дождь\",\"Ясно\",\"Облачно\",\"Облачно\",\"Дождь\"],\n",
      "    \"Температура\":[\"Жарко\",\"Жарко\",\"Жарко\",\"Тепло\",\"Холодно\",\"Холодно\",\"Холодно\",\"Тепло\",\"Холодно\",\"Тепло\",\"Тепло\",\"Тепло\",\"Жарко\",\"Тепло\"], \n",
      "    \"Влажность\":[\"Высокая\",\"Высокая\",\"Высокая\",\"Высокая\",\"Норм\",\"Норм\",\"Норм\",\"Высокая\",\"Норм\",\"Норм\",\"Норм\",\"Высокая\",\"Норм\",\"Высокая\"],\n",
      "    \"Ветер\":[\"Нет\",\"Есть\",\"Нет\",\"Нет\",\"Нет\",\"Есть\",\"Есть\",\"Нет\",\"Нет\",\"Нет\",\"Есть\",\"Есть\",\"Нет\",\"Есть\"],\n",
      " \n",
      "\"Гольф\":[\"×\",\"×\",\"○\",\"○\",\"○\",\"×\",\"○\",\"×\",\"○\",\"○\",\"○\",\"○\",\"○\",\"×\"],\n",
      "}\n",
      "df0 = pd.DataFrame(d)\n",
      "Создание Data Frame из массивов: сбор данных из строк DataFrame\n",
      "\n",
      "# создание из массивов: сбор данных из строк \n",
      "d = [[\"Ясно\",\"Жарко\",\"Высокая\",\"Нет\",\"×\"],\n",
      "     [\"Ясно\",\"Жарко\",\"Высокая\",\"Есть\",\"×\"],\n",
      "     [\"Облачно\",\"Жарко\",\"Высокая\",\"Нет\",\"○\"],\n",
      "     [\"Дождь\",\"Тепло\",\"Высокая\",\"Нет\",\"○\"],\n",
      "     [\"Дождь\",\"Холодно\",\"Норм\",\"Нет\",\"○\"],\n",
      "     [\"Дождь\",\"Холодно\",\"Норм\",\"Есть\",\"×\"],\n",
      "     [\"Облачно\",\"Холодно\",\"Норм\",\"Есть\",\"○\"],\n",
      "     [\"Ясно\",\"Тепло\",\"Высокая\",\"Нет\",\"×\"],\n",
      "     [\"Ясно\",\"Холодно\",\"Норм\",\"Нет\",\"○\"],\n",
      "     [\"Дождь\",\"Тепло\",\"Норм\",\"Нет\",\"○\"],\n",
      "     [\"ясно\",\"Тепло\",\"Норм\",\"Есть\",\"○\"],\n",
      "     [\"Облачно\",\"Тепло\",\"Высокая\",\"Есть\",\"○\"],\n",
      "     [\"Облачно\",\"Жарко\",\"Норм\",\"Нет\",\"○\"],\n",
      "     [\"Дождь\",\"Тепло\",\"Высокая\",\"Есть\",\"×\"],\n",
      "    ]\n",
      "# название столбцов и строк можно указать как columns и index соответственно. В случае, если их опускать, указывается соответствующий им номер.\n",
      "\n",
      "df0 = pd.DataFrame(d,columns=[\"Погода\",\"Температура\",\"Влажность\",\"Ветер\",\"Гольф\"],index=range(len(d)))\n",
      "3.4 Получаем информацию из таблицы\n",
      "# получение информации из таблицы\n",
      " \n",
      "# количество строк и столбцов\n",
      "print(df0.shape) # вывод (14, 5)\n",
      " \n",
      "#  получаем количество строк\n",
      "print(df0.shape[0]) # вывод 14\n",
      " \n",
      "# получаем названия столбцов\n",
      "print(df0.columns) # вывод Index(['Погода', 'Температура', 'Влажность', 'Ветер', 'Гольф'], dtype='object')\n",
      " \n",
      "# получаем названия строк (Название строки df0 - это автоматически присвоенный индекс）\n",
      "print(df0.index) # вывод RangeIndex(start=0, stop=14, step=1)\n",
      "3.5 Получаем значения loc iloc values\n",
      "# получение значений\n",
      " \n",
      "# получаем значение, указав строку и столбец\n",
      "# получаем значение Влажности в строке под №1 (вторая сверху)\n",
      "print(df0.loc[1,\"Влажность\"]) # вывод Высокая\n",
      "\n",
      "# получаем значение, указав массив из нескольких строк и столбцов\n",
      "# получаем значения Погоды и Гольфа из строк 1,2,4, и полученные данные тоже будут Data Frame-ом  \n",
      "df = df0.loc[[1,2,4],[\"погода\",\"Гольф\"]]\n",
      "print(df)\n",
      "# вывод\n",
      "#    Погода    Температура    Влажность    Ветер    Гольф\n",
      "# 1    Ясно        Жарко        Высокая     Есть      ×\n",
      "# 2    Облачно    Жарко        Высокая    Нет    ○\n",
      "# 3    Дождь        Тепло        Высокая    Нет    ○\n",
      "# 4    Дождь        Холодно    Норм        Нет    ○\n",
      "\n",
      "# iloc позволяет индексировать строки и столбцы. Индексы отсчитываются от 0.\n",
      "# получаем данные из строк с 1 по 3, не включая столбец Гольф. Так как iloc указывает индекс, если написать 1:4, то 4-ка включена не будет. \n",
      "df = df0.iloc[1:4,:-1]\n",
      "print(df)\n",
      "# вывод\n",
      "#    Погода    Температура    Влажность    Ветер\n",
      "# 1    Ясно        Жарко        Высокая     Есть     \n",
      "# 2    Облачно    Жарко        Высокая    Нет\n",
      "# 3    Дождь        Тепло        Высокая    Нет\n",
      "\n",
      "\n",
      "# получаем значение из одной строки (Series)\n",
      "# получаем данные из самой первой строки. s это Series\n",
      "s = df0.iloc[0,:]\n",
      "# так же, как и со словарем, значение можно получить с помощью s[\"название столбца\"]\n",
      "print(s[\"Погода\"]) # вывод Ясно\n",
      "\n",
      "# все значения получаем в виде массива (numpy.ndarray).\n",
      "print(df0.values)\n",
      "3.6 Цикл данных, пройдемся по данным с помощью iterrows iteritems\n",
      "# цикл данных, просматриваем данные\n",
      "# в цикле проходимся по строкам. Смотрим данные по каждой строчке.\n",
      "for i,row in df0.iterrows():\n",
      "    # i это название строки (индекс строки), row это Series\n",
      "    print(i,row)\n",
      "    pass\n",
      "\n",
      "# в цикле проходимся по столбцам. Смотрим данные по вертикали.\n",
      "for i,col in df0.iteritems():\n",
      "    # i это название столбца, col это Series\n",
      "    print(i,col)\n",
      "    pass\n",
      "3.7 Частота value_counts\n",
      "# частота вывода данных\n",
      "# получаем все данные из столбца Погода. s это Series\n",
      "s = df0.loc[:,\"Погода\"]\n",
      "\n",
      "# получаем необходимое количество нужных данных\n",
      "print(s.value_counts())\n",
      "# вывод\n",
      "# Ясно    5\n",
      "# Дождь    5\n",
      "# Облачно    4\n",
      "# Name: Погода, dtype: int64\n",
      "\n",
      "# Например, получили количество строк, когда встречается “Ясно”\n",
      "print(s.value_counts()[\"Ясно\"]) # Вывод 5\n",
      "3.8 Извлекаем конкретные данные query\n",
      "# извлечение конкретных данных\n",
      "# получаем данные, когда Погода - Ясно.\n",
      "print(df0.query(\"Погода=='Ясно'\"))\n",
      "# вывод\n",
      "#    Погода    Температура    Влажность    Ветер    Гольф\n",
      "# 0    Ясно        Жарко        Высокая    Нет    ×\n",
      "# 1    Ясно        Жарко        Высокая    Есть    ×\n",
      "# 7    Ясно        Тепло        Высокая    Нет    ×\n",
      "# 8    Ясно        Холодно    Норм        Нет    ○\n",
      "# 10    Ясно        Тепло        Норм        Есть    ○\n",
      "\n",
      "# получаем данные, когда Погода - ясно, и иду на гольф \n",
      "print(df0.query(\"Погода=='Ясно' and Гольф=='○'\"))\n",
      "# вывод\n",
      "#    Погода    Температура    Влажность    Ветер    Гольф\n",
      "# 8    Ясно        Холодно    Норм        Нет    ○\n",
      "# 10    Ясно        Тепло        Норм        Есть    ○\n",
      "\n",
      "# получаем данные, когда Погода - ясно, или иду на гольф \n",
      "print(df0.query(\"Погода=='Ясно' or Гольф=='○'\"))\n",
      "# вывод\n",
      "#    Погода    Температура    Влажность    Ветер    Гольф\n",
      "# 0    Ясно        Жарко        Высокая    Нет    ×\n",
      "# 1    Ясно        Жарко        Высокая    Есть    ×\n",
      "# 2    Облачно    Жарко        Высокая    Нет    ○\n",
      "# 3    Дождь        Тепло        Высокая    Нет    ○\n",
      "# 4    Дождь        Холодно    Норм        Нет    ○\n",
      "# 6    Облачно    Холодно    Норм        Есть    ○\n",
      "# 7    Ясно        Тепло        Высокая    Нет    ×\n",
      "# 8    Ясно        Холодно    Норм        Нет    ○\n",
      "# 9    Дождь        Тепло        Норм        Нет    ○\n",
      "# 10    Ясно        Тепло        Норм        Есть    ○\n",
      "# 11    Дождь        Тепло        Высокая    Есть    ○\n",
      "# 12    Дождь        Жарко        Норм        Нет    ○\n",
      "Спасибо за прочтение!\n",
      "\n",
      "Мы будем очень рады, если вы расскажете нам, понравилась ли вам данная статья, понятен ли перевод, была ли она вам полезна?    \n",
      " \n",
      "\n",
      "По разным оценкам сейчас в мире существует от трех до семи тысяч языков. Между языками могут быть очень значительные различия в графематике, фонетике, грамматике, лексике. Но если посмотреть шире, станет ясно, что все языки очень похожи и подвержены описанию при помощи универсальных категорий.\n",
      "\n",
      "В этой лекции Елена Грунтова раскрывает студентам Малого ШАДа понятие грамматической категории и подробно рассказывает падежах, их природе, типах, а также способах выражения.\n",
      "\n",
      "Лекция рассчитана на старшеклассников, но выходит за рамки школьной программы, поэтому может быть интересна и взрослым.\n",
      "\n",
      "    \n",
      " \n",
      "\n",
      "Большинство статей и выступлений про качественные исследования посвящены методам сбора информации. Но очарование качественных методов в маркетинговых и UX-исследованиях исчезает на этапе анализа и представления результатов. Эту непростую задачу исследователи решают по-разному. Иногда ради быстрого результата, или по незнанию они нарушают методологию качественных исследований, отчего работа выглядит непрофессионально и не вызывает доверия.\n",
      "\n",
      "В статье рассмотрим один из методов качественного анализа – Метод тематических сетей: \n",
      " — обсудим частые ошибки качественных исследований и вспомним «кальсонных гномов»;\n",
      " — разберем пошаговый алгоритм метода и его возможные ловушки;\n",
      " — применим метод к анализу ИТ-продукта и нарисуем тематическую карту.Статья будет интересна: — исследователям;\n",
      " — менеджерам продуктов и маркетологам;\n",
      " — тем, кто использует в работе результаты исследований;\n",
      " — тем, кто давно искал удобный метод структурирования нецифровой информации. Статья продолжает тему исследований пользователей. В этот раз отступим от анкетного исследования (статья 1 и статья 2), вернемся к анализу потребностей пользователей (статья 3) и обсудим качественные методы обработки результатов исследования. \n",
      "\n",
      "За что исследователи любят качественные методы? За необязательную репрезентативность выборки, отсутствие статистики и прочую свободу действий. Свобода на деле оказывается мнимой, но заметить это можно, только если относиться к методологии качественных исследований всерьез. А без такого отношения и под влиянием устоявшейся ориентации на позитивизм качественные методы в бизнес-исследованиях становятся «ленивыми» количественными, которым будто разрешили «не быть такими строгими». \n",
      "\n",
      "Чаще всего ошибки появляются на этапе выбора методологии и на этапе обработки и представления результатов. Выбору методологии исследований я планирую посвятить отдельные статьи. А вот две основные ошибки обработки результатов качественных исследований:\n",
      "\n",
      "1. Результаты качественных исследований представляются, как в презентации «кальсонных гномов» из «Южного парка». Подробно обосновывается первый этап – качественный метод сбора данных, а потом, минуя обсуждение результатов, даются выводы и рекомендации. Такие отчеты не содержат промежуточные обобщенные данные, и вычисления исследователей остаются для заказчика в «черном ящике». Заказчики экономят время и, при большом доверии к исследователям, рады получить только итоговые выводы. Но если посмотреть, сколько информации теряется и искажается при такой интуитивной обработке, и заказчики, и исследователи не будут рады результатам.\n",
      "\n",
      "2. Исследователи ссылаются на высказывания и действия участников исследования, распространяют результаты на всех пользователей и пытаются прогнозировать их поведение (это допустимо только для количественных методов). Иногда используется статистика: описательная (подсчет частот и средних, таблицы сопряженности) и индуктивная (корреляционный, факторный анализ и др.). Действительно, эти методы допустимо использовать при обработке качественных данных в социальных науках, но в маркетинговых исследованиях они не могут уберечь исследователя и заказчика от смещения внимания с качественных данных на количественные результаты, с понимания на предсказание.\n",
      "\n",
      "Даже в отчетах опытных UX-исследователей встречается этот легкий переход от качественных методов сбора информации к их количественной интерпретации. При такой подаче результатов читатель интуитивно обобщает выводы, или подсчитывает проценты: «Из 6 участников UX-тестирования 3 не смогли найти кнопку заказа – Ясно. Половина наших пользователей до нас не доходит!», «60% пользователей (3 человека из 5) выбрали кнопку зеленого цвета. – Нужно менять палитру сайта, раз большинству нравится зеленый». В итоге получаются смехотворные результаты.\n",
      "\n",
      "«Не все, что важно, может быть измерено, не все, что может быть измерено, важно» (А. Эйнштейн).\n",
      "Основной задачей качественных исследований остается описание феноменологии принятия решений пользователями и понимание их эмоций, мотивов и путей рассуждения (когниций, предубеждений, стереотипов, и т.д.), а не подсчет действий отдельных участников эксперимента. Значит, и метод анализа результатов качественного исследования принесет больше пользы, если будет сосредоточен на понимании, передаче смыслов и описании вариантов оценки ситуации пользователем, а не на подсчете проделанных им операций. \n",
      "\n",
      "Метод тематических сетей (Thematic networks) – это метод упорядочивания результатов качественного исследования. Основная цель тематического анализа – выделение основных тем, содержащихся в качественных данных, их группировка и отображение, удобное для понимания исследователю и читателю (заказчику исследования). Результатом таких группировок становится паутиноподобная сеть, в которой отражаются все выделенные темы и связи между ними.\n",
      "\n",
      "Так выглядит классическая структура тематической сети.\n",
      "\n",
      "Нельзя однозначно отследить отделение тематического анализа, как самостоятельного метода, так как этапы, принципы и общая структура угадываются и в других качественных методах (например, методе обоснованной теории, качественного анализа и других). Так что, метод тематических сетей – не принципиально новый. Но его применение обосновано, удобно и легко автоматизируется в программах для качественных исследований, так называемых Computer Assisted/Aided Qualitative Data AnalysiS (CAQDAS). Например, в программе ATLAS.ti.\n",
      "\n",
      "Важно, что в цели тематического анализа не входит определение причин аргументации. Анализ тематических сетей не подразумевает также подсчет частоты встречаемости разных тем, или их комбинаторику. Хотя частая встречаемость темы в текстах считается положительным обстоятельством, даже при одном упоминании темы в одном тексте, она включается в анализ на общих правах. Основная задача тематического анализа – разбиение текстов на осмысленные отрезки и выделение в них «рационализаций» (когнитивных схем, интерпретаций, обоснований, возражений и др.). Другими словами, все найденные темы, независимо от частоты их появления в текстах и авторства, одинаково важны для анализа.\n",
      "\n",
      "Метод тематических сетей подходит для работы с самыми разными данными:\n",
      "\n",
      "Метод тематических сетей хорошо проявляет себя, когда исследователь встречается с неожиданными, или скрытыми паттернами мышления (неосознаваемыми предубеждениями, тайными желаниями, и др.). В таком анализе удается не только выделить основных «когнитивных игроков», но и дать им интерпретацию и даже описать их типичные маски в обыденной речи, или поведении пользователей.\n",
      "\n",
      "При обычном тестировании продукта, когда реакции пользователей, в основном, предсказуемы и ожидаемы, метод тематических сетей не приносит новых открытий, но помогает доказать и качественно структурировать основные темы, чтобы потом на их основе наметить конкретные действия по доработке, или продвижению продукта.\n",
      "\n",
      "Надежность метода тематических сетей находится под постоянным вниманием критиков, так как во многом зависит от опыта и выбранной стратегии кодировщика, проводящего анализ. Чтобы повысить надежность метода, рекомендуется параллельно обрабатывать данные нескольким кодировщикам. Если этого не удается достичь, исследователю рекомендуется регулярно возвращаться к оригинальным текстам и сравнивать их с выделенными кодами и темами.\n",
      "\n",
      "Для удобства рассмотрим использование метода на примере анализа текста. Аналогично его можно применять к другим видам данных.\n",
      "\n",
      "Этапы применения метода тематических сетей\n",
      "\n",
      "Этап 1: Кодирование.\n",
      "На этом этапе в тексте выделяются осмысленные текстовые фрагменты. Каждый из них кодируется.\n",
      "\n",
      "Что нужно делать\n",
      "Шаг 1. Исследователь просматривает тексты и записывает в отдельном документе названия кодов и их расшифровки. Позже эта система кодирования будет применена для обработки текстов.\n",
      "\n",
      "Как подбираются коды\n",
      "Способы кодирования, в общем, сводятся к следующим:\n",
      "⎯ система кодов выбирается на основе заранее заданных теоретических предпосылок (дедуктивный путь, «theory-driven» исследование);\n",
      "⎯ система кодов выбирается на основе значимых тем, появляющихся в тексте (индуктивный путь, «data-driven» исследование);\n",
      "⎯ сочетаются оба подхода.Выбранные коды могут быть заданными понятиями, взятыми из гипотез исследования, фрагментами обыденной речи участников исследования, неологизмами, или сленгом.\n",
      "\n",
      "Шаг 2. Исследователь разбивает текст на осмысленные участки (абзацы, предложения, фразы, слова) и присваивает им коды.\n",
      "\n",
      "В итоге такой работы все тексты разбиваются на коды. При применении сервисов автоматизации на этом этапе каждый выделенный фрагмент текста заменяется кодом. Этот этап позволяет проверить, все ли темы в оригинальных текстах охвачены кодами, и все ли коды точно подходят к заменяемым отрезкам текста.\n",
      "\n",
      "Где можно ошибиться\n",
      "На этом этапе важен принцип «необходимости и достаточности». С одной стороны, коды должны быть четко определены, чтобы избежать смешения понятий и пересечения области значений нескольких кодов. С другой стороны, количество кодов должно быть ограниченным и обоснованным, чтобы исследователь не поддался соблазну присвоить код каждому новому предложению.\n",
      "\n",
      "Также не забывайте записывать для каждого кода расшифровку. Вначале кажется, что все коды можно удержать в памяти, но позже могут возникнуть курьезные ситуации, когда придется переделывать работу. \n",
      "\n",
      "Этап 2. Определение тем.\n",
      "На этом этапе в текстах выделяются базовые темы.\n",
      "\n",
      "Что нужно делать\n",
      "Шаг 3. Исследователь просматривает коды и выделяет среди них наиболее значимые, объединяющие, или выдающиеся темы. Близкие по смыслу коды группируются в темы.\n",
      "\n",
      "Шаг 4. Исследователь просматривает темы и «очищает» их, чтобы избежать повторов и сделать темы достаточно широкими, способными включить несколько кодов. В итоге, каждая тема должна быть довольно узкой, чтобы точно передавать определенную идею и не пересекаться с другими темами, но и достаточно широкой, чтобы обобщать разные высказывания и формулировки в этой смысловой области.\n",
      "\n",
      "Удобно работать, если тем начального уровня анализа получается не больше 16. В то же время, хорошо, если их не очень мало: с 2-3 темами тематическая карта не получится интересной и содержательной.\n",
      "\n",
      "Где можно ошибиться\n",
      "Это этап обобщения информации, но это не значит, что на нем необходимо группировать все коды. Если в тексте выделились коды, которые нельзя сгруппировать, и которые представляют значимость для исследования, таким кодам присваиваются собственные темы. Однако, на практике обычно удается сгруппировать и обобщить в темы несколько кодов.\n",
      "\n",
      "Этап 3. Конструирование тематической сети.\n",
      "На этом этапе рисуется тематическая сеть – графическая основа метода.\n",
      "\n",
      "Что нужно делать\n",
      "Шаг 5. Исследователь группирует и упорядочивает по смыслу базовые темы. Порядок задается контентом, или предварительными теоретическими основаниями исследования. Темы можно переименовать, чтобы внутри одной группы они представляли однородные и легко считываемые элементы.\n",
      "\n",
      "Шаг 6. Исследователь организует близкие по смыслу базовые темы и обобщает их под одной общей «организующей» темой. Название этой темы содержит обобщение для всей группы входящих в нее тем. Оно должно быть достаточно говорящим и считываемым с первого взгляда.\n",
      "\n",
      "Шаг 7. Исследователь объединяет близкие организующие темы под общими «глобальными» темами. Глобальные темы обычно включают по 2-3 организующие темы и становится финальной обобщающей частью тематической сети. Поэтому их названия обычно метафоричны, они передают общий смысл объединенных ими тем.\n",
      "\n",
      "Количество кодов, базовых, организующих и глобальных тем не предписано правилами. Оно задается разнообразием собранного материала и аналитической работой исследователя. Но обычно в работах встречается до 3-х глобальных тем (чаще всего 1-2), по 2-3 организующие темы для каждой глобальной и по 2-3 базовые темы для каждой организующей темы.\n",
      "\n",
      "Шаг 8. Как только произошли все обобщения и темы получили «говорящие» названия, настает время визуализации. Она делается просто, с соблюдением 2 важных принципов: 1) в центре сети размещается глобальная тема, от которой отходят организующие и – далее – базовые темы; 2) расположение тем одной группы не должно быть иерархичным, они все располагаются, как однородные равноценные элементы сети.\n",
      "\n",
      "Шаг 9. Когда сеть нарисована, исследователь возвращается к ее проверке и обновлению. На этом этапе исследователь открывает сырые тексты и удостоверяется, что каждый затронутый в тексте значимый аспект нашел адекватное отображение в сети и не исказился в процессе обобщения. Также проверяется, есть ли для каждой темы в сети эквивалент в сырых данных, цитатой из которого можно при необходимости проиллюстрировать тему.\n",
      "\n",
      "Где можно ошибиться\n",
      "Выбор названий для организующих и глобальных тем – довольно сложная задача. Названия должны быть говорящими, не искажать смысл объединенных тем и находить соответствия в сырых данных. На этапе обобщения велика опасность смещения акцентов и добавления личных смыслов от исследователя. Поэтому, рекомендуется выполнить эту работу нескольким исследователям независимо и потом сравнить результаты, или одному исследователю несколько раз перепроверить выделенные темы.\n",
      "\n",
      "Этап 4. Описание и объяснение тематической сети.\n",
      "Создание тематических сетей – это только инструмент анализа, но не анализ по сути. Для того, чтобы совершить качественный анализ и продвинуться в понимании текстов, исследователь объясняет выделенные темы и определяет паттерны мышления участников исследования, которые скрываются за этими темами.\n",
      "\n",
      "Что нужно делать\n",
      "Шаг 10. Описание сети. Как только сеть построена, исследователь возвращается к сырым материалам и описывает содержание сети с использованием цитат из оригинальных текстов.\n",
      "\n",
      "Шаг 11. Объяснение сети. Когда для частей сети предложены примеры из текстов, исследователь приступает к их обсуждению и интерпретации.\n",
      "\n",
      "Он возвращается к прочтению оригинальных текстов, но на этот раз читает их не линейно, а использует каркас тематической сети. Теперь он может давать объяснения текстам с точки зрения выделенных базовых, организующих и глобальных тем. Таким образом, у исследователя и его читателя появляется инструмент для понимания, обобщения и иллюстрации содержания оригинальных материалов. \n",
      "\n",
      "В аналитическом отчете результаты анализа появляются в виде текстового блока, где обсуждаются темы, приводятся участки оригинальных текстов и дается их объяснение.\n",
      "\n",
      "Где можно ошибиться\n",
      "При описании и объяснении сети рекомендуется использовать дословные цитаты из оригинальных текстов, наиболее ярко характеризующие тему, с краткой справкой об участнике исследования, или источнике, в котором была взята цитата.\n",
      "Если исследователь уверен, что в данных скрыты темы, но он не может подобрать для них примеры, такой материал нельзя включать в отчет. Нужно либо провести дополнительное исследование, либо изменить методологию интерпретации данных.\n",
      "Используйте правило качественного анализа: «Не все, что есть в данных, нужно включать в анализ. Но все, что включено в анализ, должно иметь подтверждение в оригинальных данных». \n",
      "\n",
      "Этап 5. Резюмирование тематической сети.\n",
      "На этом этапе обобщаются результаты, выделяются и обсуждаются основные темы. \n",
      "\n",
      "Что нужно делать\n",
      "Шаг 12. Исследователь описывает и анализирует выделенные темы. В итоговом отчете анализ представляется в форме свободного описания.\n",
      "\n",
      "Где можно ошибиться\n",
      "Ценность этой части отчета – в рассуждениях исследователя и глубине погружения в анализ. Стиль этих рассуждений задается принятой коммуникацией в компании. Анализ не должен быть громоздким, но должен учитывать основное требование качественных исследований – глубокое, «трехмерное» погружение в проблему. \n",
      "Этап 6. Интерпретация паттернов.\n",
      "Заключительный аналитический этап.\n",
      "\n",
      "Что нужно делать\n",
      "Шаг 13. Исследователь возвращается к первоначальным задачам, сопоставляет гипотезы с результатами, дает объяснения, предлагает интерпретации, дополняет первоначальные теоретические конструкты.\n",
      "В итоговом отчете анализ представляется в форме заключения.\n",
      "\n",
      "Где можно ошибиться\n",
      "Выводы качественных исследований нередко оставляют у исследователей и заказчиков ощущение: «Я и так это знал!». После исследования могут опуститься руки, работа покажется бесполезной тратой времени. Однако, деятельность исследователя заключается в доказательстве гипотез. Очевидное знание (которое только кажется очевидным) не может быть взято в работу без обоснованных доказательств, если в вашей компании принят научный подход и используются исследования. Не забывайте также об известном когнитивном искажении «эффект знания задним числом», при котором информация кажется людям очевидной только после того, как они ее узнают.\n",
      "Наконец, за кажущейся очевидной информацией могут скрываться новые открытия, или неподтвердившиеся частные гипотезы, которые ошибочно считались очевидными.\n",
      "\n",
      "Вот, и все. 13 шагов, и построение тематических сетей завершено. Теперь обсудим пример использования метода в исследовании пользователей ИТ-продукта.\n",
      "\n",
      "Пример использования тематических сетей в исследовании отзывов пользователей ИТ-продукта\n",
      "\n",
      "Используем метод тематических сетей для иллюстрации отношения пользователей к облачной системе автоматизации рекрутинга «HRP». Наша компания – совладелец этой системы, мы с самого начала участвовали в ее разработке. Cвои продукты исследовать сложнее, но попытаемся представить сокращенный пример анализа, чтобы проиллюстрировать применение метода тематических сетей, не нарушая договор о неразглашении.\n",
      "\n",
      "Качественное исследование пользовательского опыта не предполагало анализ глубинных побуждений и неосознаваемых предубеждений (что может встречаться, например, при изучении опыта игроков компьютерных игр), поэтому анализ получился простым и очевидным. Но вполне соответствующим задаче демонстрации метода.\n",
      "\n",
      "Дано: Система существует на рынке около 2-х лет. Она хорошо известна в области сервисов автоматизации рекрутинга, заявки на тестовый доступ поступают регулярно и в желаемом количестве. Однако, много пользователей теряется при переходе от бесплатного тестирования к покупке платного доступа. Цена ежемесячного использования невысокая (ниже, чем у большинства конкурентов), поэтому гипотеза о ценовых возражениях клиентов для этого исследования отбрасывается.\n",
      "Оставляют заявки релевантные пользователи: руководители компаний, рекрутеры, руководители рекрутинговых служб и руководители служб автоматизации. Гипотеза о том, что нерелевантный поток заявок влияет на плохую конверсию, также отбрасывается.\n",
      "\n",
      "Задача: узнать, какие когниции (мысли, рационализации) возникают у пользователей при тестировании системы автоматизации рекрутинга.\n",
      "\n",
      "Качественный материал исследования: Отзывы пользователей собирались неструктурированно, преимущественно из писем обратной связи и телефонных разговоров. В базу исследования вошли: отзывы, вопросы и возражения, написанные (сказанные) пользователями после знакомства с системой. Регион сбора отзывов: Россия, Беларусь, Украина, Казахстан, Литва. Количество текстов, участвующих в анализе: 40 записей.\n",
      "\n",
      "Напомним, что при применении качественных методов исследования на первый план выходит не количество респондентов, не репрезентативность выборки и не соотношение охваченных и откликнувшихся пользователей. В таких исследованиях важны только темы (паттерны мышления, рационализации, идеи, возражения и т.д.), возникающие у пользователя при знакомстве с продуктом. Поэтому, мнения, собранные от 8 пользователей и обработанные качественными методами анализа, так же важны, как мнения, собранные от 200 пользователей. Особенно, при условии достижения «насыщения» результатов на ранних этапах сбора информации. (Термин «насыщение» в исследованиях подразумевает, что исследование можно остановить, когда темы начинают повторяться, и каждый последующий участник не приносит новых данных).\n",
      "\n",
      "Этап 1. Определение кодов.\n",
      "На этом этапе мы просмотрели все записи и выделили коды, емко передающие затронутые в текстах вопросы. У нас получилось 42 кода (см. таблицу ниже). Обычно для кодов подбираются схематичные, односложные названия, понятные кодировщику. Но в целях наглядности оставим в примере развернутые названия, чтобы дополнительно не давать их подробные расшифровки.\n",
      "\n",
      "Этап 2. Определение тем.\n",
      "Коды были объединены в 11 базовых тем, а те – в 6 организационных тем. В итоге финального обобщения получилось 2 глобальные темы: «Драйверы «ЗА» покупку системы» и «Сомнения и пожелания доработки».\n",
      "\n",
      "Эти глобальные темы соответствовали задачам нашего исследования. При других задачах могли быть выделены другие глобальные темы. Например, «Действия для маркетинга» и «Действия для разработки». В этом заключается удобство метода тематических сетей: он достаточно гибкий и позволяет структурировать результаты под нужным углом, чтобы потом с ними было удобно работать на следующих стадиях доработки продукта.\n",
      "\n",
      "Получились такие результаты:\n",
      "\n",
      "\n",
      "Этап 3. Конструирование тематической сети.\n",
      "Для каждой глобальной темы должна быть построена отдельная тематическая сеть. Для примера, построим сеть для первой темы: «Драйверы «ЗА» покупку системы».\n",
      "\n",
      "Пример тематической сети отзывов пользователей ATS.\n",
      "\n",
      "Этап 4. Описание и объяснение тематической сети.\n",
      "(На этом этапе обсуждаются выделенные темы и приводятся дословные цитаты из оригинальных текстов. Для примера ограничимся разбором нескольких тем).\n",
      "\n",
      "(…)\n",
      "Тема: «Интерфейс системы современный, красивый, приятный в использовании».\n",
      "\n",
      "Тема «приятности» и современности интерфейса оказалась одной из лидирующих в отзывах пользователей. Часто выделяли интерфейс системы пользователи, проводившие сравнительный анализ нескольких систем автоматизации рекрутинга. Они упоминали, что система выглядит современнее и приятнее, чем другие программы, с которыми им приходилось работать. Интерфейс подкупает «простотой» и «чистотой», отсутствием лишних деталей и украшений. \n",
      "\n",
      "Евгения, руководитель направления по подбору персонала, медиа-холдинг:\n",
      " «Ваша программа действительно хороша, а именно:\n",
      " — простотой функционала \n",
      " — нет награмаждения ненужных фич.\n",
      " — к интерфейсу просто нужно привыкнуть».\n",
      "В то же время, когда интерфейс рассматривался пользователями с позиции не общего восприятия, а быстрого выполнения задачи (составить заявку, назначить исполнителя, прикрепить кандидата и т.д.), упоминались, напротив, сложности, «лишние движения» и неудобства. Эти отзывы касались заложенного в систему процесса подбора и составили основу следующей темы.\n",
      "\n",
      "Тема: «К заданному в системе процессу подбора сложно адаптироваться, он не соответствует процессу подбора в компании клиента».\n",
      "\n",
      "Пользователи отмечали сложность адаптации к заложенному в системе процессу подбора (согласование заявки, назначение исполнителей, обязательные этапы рассмотрения кандидатов и подачи обратной связи). Часто пользователи хотели адаптировать процесс к практике компании, или выступали за гибкую систему, позволяющую пропускать некоторые этапы.\n",
      "\n",
      "Олег, технический директор, ИТ-компания: «Причина – избыточность действий и информации, которая не участвует в наших процессах рекрутинга, плюс уникальность наших внутренних действий, которые не предусматриваются системой».\n",
      "\n",
      "Марина, HR-менеджер, производственный холдинг: «Постоянно поступают жалобы от Заказчиков в подразделениях о том, что процедура заполнения заявки на подбор персонала в системе чрезмерно сложная».\n",
      "Тема: «Опасения по поводу кражи, или потери внесенной в систему информации».\n",
      "\n",
      "Вопросы и сомнения по поводу кражи, или потери данных встречались как в письменных отзывах о системе, так и в первичных вопросах пользователей при знакомстве с системой. Обычно угроза воспринималась в пяти возможных сценариях: 1) кража базы кандидатов провайдером сервиса (то есть, нами), 2) взлом системы и кража базы кандидатов конкурентами, 3) потеря информации из-за ошибки сотрудников клиента, 4) потеря информации из-за технических неполадок и 5) потеря информации из-за прекращения оплаченного доступа к системе.\n",
      "\n",
      "Евгения, руководитель направления по подбору персонала, медиа-холдинг:\n",
      "«В нашем случае нам важно иметь всю информацию у нас в компании и максимально снизить риск потери данных. \n",
      "Мы желаем Вам процветать и развиваться, но будем очень не рады, если доступ к Вашему ресурсу по той или иной причине будет прекращен и мы имеем риск потерять всю нашу базу.\n",
      "Когда выбирали программу для работы не продумали эту историю».\n",
      "И хотя мы сразу предоставляли гарантии сохранности информации (цитировали фрагменты договора, гарантировали ежедневное сохранение информации, ссылались на мировой опыт использования облачных сервисов и пр.), многие пользователи все равно высказывали опасения по поводу возможной потери данных.\n",
      "(…)\n",
      "\n",
      "Этап 5. Резюмирование тематической сети.\n",
      "(На этом этапе обобщаются все темы, дается общая интерпретация результатам анализа. Пропустим этот этап, так как он задается логикой исследования и принятым стилем коммуникации в исследовательской группе и не нуждается в наглядности).\n",
      "\n",
      "Этап 6. Интерпретация паттернов.\n",
      "(На этом этапе темы сравниваются с гипотезами исследования, вносятся дополнения в теоретические конструкты, обсуждается, насколько мнения пользователей соответствуют представлениям разработчиков и владельцев продукта. Приведем фрагмент такого анализа).\n",
      "\n",
      "(…)\n",
      "Анализ отзывов выявил три особенно интересных обстоятельства:\n",
      "\n",
      "1. Аналитика.\n",
      "В систему заложен сервис аналитики, позволяющий получать отчет в виде графиков и диаграмм по разным показателям подбора. В отдельной вкладке автоматически формируются отчеты: показатели выполнения плана и следования нормативам закрытия вакансий, воронка подбора (конверсия соискателей на каждом этапе подбора), анализ эффективности каналов привлечения кандидатов и – в перспективе – анализ взаимосвязи показателей кандидатов на собеседовании с показателями их успешности на рабочем месте (пролонгированная аналитика для проведения глобальных исследований о компании).\n",
      "\n",
      "Для рекрутера такой сервис означает получение отчета о работе по «одному клику», а для руководителя – возможность быстро просмотреть аналитику и принять решение. Не говоря о набирающей популярность теме HR аналитики, HR Big Data и др., с сервисом аналитики можно проводить самые разные исследования в компании.\n",
      "\n",
      "Но наше исследование показало, что тема аналитики ни разу не появилась в отзывах пользователей. Люди при тестировании системы не проявляли потребности в проведении исследований, или в быстром контроле за процессом рекрутинга. Мы считали эту функцию очень важной и потратили на нее много ресурсов разработки и дизайна, упоминали во всех рекламных материалах, делали акцент при продажах. Но оказалось, что пользователи интересовались ей по остаточному принципу. По крайней мере, в их отзывах аналитика не фигурировала. \n",
      "\n",
      "Получив результаты, мы приняли решение уделить больше внимания презентации аналитики в тестовом доступе к системе и провести дополнительное исследование потребностей пользователей в аналитике.\n",
      "\n",
      "2. Жесткий процесс.\n",
      "Задумывая концепцию системы, мы сделали ставку на автоматизацию лучших практик подбора. Наша компания много лет формировала службы рекрутинга в разных компаниях, и именно этот опыт мы решили оцифровать. Процесс в системе обучал и вел за собой, содержал «защиты от дурака», не позволял совершить стандартные ошибки в процессе согласования заявок и рассмотрения кандидатов, защищал рекрутеров от несправедливых обвинений. Это мы считали важнейшей функцией системы. В сущности, первые продажи мы строили как раз на основе продажи лучшего опыта, автоматизированного процесса.\n",
      "\n",
      "Но общение с клиентами в процессе этих продаж зародило гипотезу, которую позже подтвердило исследование: клиенты не готовы покупать жесткий процесс и не готовы меняться под него.\n",
      "\n",
      "Оказалось, что та функция, которую мы считали сильнейшим козырем системы, стала ее основным недостатком. Почти в каждом отзыве мы встречали просьбы сделать процесс более гибким, убрать «ненужные» этапы, «облегчить» систему. Эта находка теперь требует от нас новых решений в области разработки и маркетинга.\n",
      "\n",
      "3. Оценка компетенций и сравнение кандидатов.\n",
      "Еще один интересный факт проявился в отношении третьего козыря системы – сервиса оценки и сравнения кандидатов.\n",
      "\n",
      "В систему заложена библиотека с 500 вопросами для оценки личностных качеств и профессиональных компетенций кандидатов. К каждому вопросу предлагается по 6 вариантов ответа. После проведения собеседования система предлагает табличное сравнение кандидатов на соответствие профилю должности с выставлением баллов.\n",
      "\n",
      "Эта функция системы – ее уникальное торговое предложение. Проведенный нами анализ рынка показал, что ни в одной другой ATS нет оценочных вопросов с вариантами ответов. Мы потратили много усилий на добавление этой функции: над созданием уникальных вопросов трудились профессиональные психологи и рекрутеры. Но исследование показало, что и в этом случае «козырь» системы воспринимается пользователями неоднозначно.\n",
      "\n",
      "Система оценки компетенций выборочно игнорировалась пользователями. Тем, кому была нужна только система управления рекрутингом, база в 500 оценочных вопросов была не нужна. Такие пользователи сравнивали условия сервиса с простейшими конкурентами и ожидали более низкую цену. Наша цена (включающая сервис оценки компетенций) казалась им завышенной.\n",
      "\n",
      "А пользователи, которые понимали ценность базы оценочных вопросов и уже знакомились с другими ИТ-продуктами по оценке компетенций, напротив, сомневались насчет подозрительно низкой цены. И эти сомнения в низкой цене парализовали покупку так же, как недовольство высокой ценой у первой группы пользователей.\n",
      "\n",
      "Получив такие данные, мы приняли решение по доработке системы и адаптации модели продаж. \n",
      "\n",
      "Краткое резюме статьи\n",
      "1. Методология качественных исследований включает не только качественные методы сбора информации, но и качественные методы анализа.\n",
      "2. В маркетинговых и UX-исследованиях часто нарушается этап качественного анализа: данные обрабатываются интуитивно, без использования специальных методов, или применяются количественные методы.\n",
      "3. Использование качественной методологии предполагает не подсчет отдельных действий пользователей, а поиск причин этих действий и сопровождающих их эмоций и размышлений. Цель качественных методов – не предсказание, а объяснение.\n",
      "4. Важное требование качественного анализа: «Не все, что есть в данных, нужно включать в анализ. Но все, что включено в анализ, должно иметь подтверждение в оригинальных данных».\n",
      "5. Результаты анализа качественных данных редко содержат неожиданные выводы. Но даже информация, кажущаяся очевидной, нуждается в доказательстве. \n",
      "6. Метод тематических сетей – удобный метод качественного анализа. Он позволяет анализировать разные виды данных, с трудом поддающиеся другим методам анализа.\n",
      "7. Метод тематических сетей состоит из 13 шагов анализа: исследовательские данные разбиваются на тематические фрагменты и последовательно группируются в темы более высокого порядка. Итогом анализа становится визуальная тематическая сеть.\n",
      "8. С помощью тематической сети легко визуализировать результаты исследования в презентациях и отчетах. Сеть удобно использовать, как основание для дерева решений, или планов разработки продукта.\n",
      "\n",
      "Алгоритм метода\n",
      "\n",
      "\n",
      "Можно дополнительно почитать (полные тексты статей есть в открытом доступе):\n",
      "\n",
      "1. Attride-Stirling, J. Thematic networks: an analytic tool for qualitative research / J. Attride-Stirling // Qualitative Research. – 2001. – vol. 1 no. 3. – P. 385-405.\n",
      "История возникновения метода тематических сетей, описание этапов проведения, а также подробный разбор применения на примере исследования культурных репрезентаций о сексуальности. \n",
      "\n",
      "2. Войскунский, А.Е. Качественный анализ данных / А.Е. Войскунский, C.В. Скрипкин // Вестник Московского университета. Сер. 14. Психология. – №2. –2001. – С. 93–109.\n",
      "Рассуждения о разделении качественных методов сбора информации и качественных методов анализа, описание еще одного метода анализа качественных данных – Качественного контент-анализа.\n",
      "\n",
      "3. A Survey and Thematic Analysis Approach as Input to the Design of Mobile Music GUIs / A. Tanaka [et al.]. – 2012. \n",
      "Пример тематического анализа (упрощенного) в исследовании пользователей музыкальных мобильных приложений.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Когда-то давно владелец магазина, он же продавец, мог легко запомнить все товары своего ассортимента. Рассказать об особенностях каждого, историю, насколько товар эффективен, знал точно как он продается, когда заказать еще… \n",
      "\r\n",
      "С развитием ритейла управление движением товаров требует других подходов. Системы учета и аналитики продаж, управления ассортиментом дополняют опыт работников магазина или торговой сети.\n",
      "\r\n",
      "Серьезные решения, например, о выведении товара из ассортимента, принимаются не так просто. И категорийному менеджеру, и управляющему магазином нужны обоснования для таких действий. \n",
      "\r\n",
      "Поэтому одного вида анализа недостаточно. Применяют совмещение нескольких видов (по-другому, кросс-анализ).\n",
      "\r\n",
      "В статье мы на примере товарной группы “Кондитерских изделий” рассмотрим основные подходы к организации кросс-анализа. А еще узнаем, кто виноват в том, что Рафаэлло — товар с нестабильными продажами.\n",
      "\n",
      "\r\n",
      "При работе с ассортиментом сети или магазина используют кросс-подход, куда включены АВС и XYZ анализы.\n",
      "\n",
      "В чем суть и почему именно их мы выбираем?\n",
      "АВС-анализ – это распространённый метод изучения ассортимента, с помощью которого можно определить вклад каждого товара в оборот и прибыль магазина, распределить товары по категориям для эффективного управления ассортиментом. \n",
      "\r\n",
      "Основные плюсы АВС анализа:\n",
      "\n",
      "рационализация управления ассортиментом — дает понять важность товаров, распределить усилия управляющих магазинами, категорийщиков;\n",
      "быстрый результат и быстрое применение управленческих решений;\n",
      "если проводить регулярно, сравнение с предыдущими периодами дает возможность отследить стадии жизненного цикла товаров.\n",
      "\n",
      "Минусы:\n",
      "\n",
      "нужно учитывать многие дополнительные факторы, такие как товары-новинки, элитные и т.п.\n",
      "нужен массив данных для аналитики за определенный период: база по чекам, продажам или другим ритейл-данным;\n",
      "важна стабильность на рынке товаров, если происходят какие-то непредвиденные ситуации (резкая инфляция, политические волнения, и т.п.), данные АВС-анализа могут быть неточны.\n",
      "\r\n",
      "Больше про методику проведения АВС-анализа мы писали тут.\n",
      "\n",
      "XYZ–анализ определяет стабильность продаж товара за определенный период. Результаты позволяют разделить товары по категориям и выделить для них место на складе, уровень запасов и организацию доставки.\n",
      "\n",
      "Плюсы XYZ анализа:\n",
      "\n",
      "данные для управления ассортиментом товаров и складскими запасами, организации работы с поставщиками;\n",
      "настройка разных вариантов доставки для разных категорий товаров;\n",
      "использование анализа для прогнозирования стабильности спроса;\n",
      "определение проблемных магазинов с нестабильными продажами;\n",
      "определение товарных дыр, коррекция системы поставок товаров.\n",
      "\n",
      "Минусы:\n",
      "\n",
      "нужна, также как и для АВС, стабильность показателей, без сотрясений рынка;\n",
      "необходимы данные за несколько лет для полноценного анализа;\n",
      "сложно работать с сезонными товарами, а их достаточно много в ритейле;\n",
      "невозможно использовать на товарах с коротким жизненным циклом.\n",
      "\r\n",
      "Больше про проведение XYZ-анализа рассказали тут.\n",
      "\r\n",
      "Объединение результатов АВС и XYZ-анализов — популярный подход к управлению товарным ассортиментом. Оба метода хорошо дополняют друг друга. Если АВС-анализ позволяет оценить вклад каждого продукта в структуру сбыта, то XYZ–анализ позволяет оценить скачки сбыта и его нестабильность. \n",
      "\r\n",
      "Совмещая и используя кросс-подход, мы получаем статус или место каждого товара в ассортименте товарной группы, магазина или всей торговой сети. \n",
      "\r\n",
      "Использование совмещенного анализа дает ряд дополнительных достоинств:\n",
      "\n",
      "выявление товаров со стабильными продажами, значительных для оборота магазина или сети, и убыточных товаров;\n",
      "повышение доли прибыльных товаров без нарушения принципов ассортиментной политики;\n",
      "определение причин, влияющих на количество и место товаров, хранящихся на складе;\n",
      "перераспределение усилий персонала по управлению ассортиментом и его складскими запасами.\n",
      "\r\n",
      "Стоит помнить что эти виды анализа, как и их совмещение, возможны только при наличие четкого учета товарооборота и статистики продаж.\n",
      "\n",
      "Способы проведения совмещенного анализа\r\n",
      "Есть 2 способа проведения кросс-анализа: последовательный и параллельный.\n",
      "\r\n",
      "Выбор одного из них зависит от цели и нужных результатов. Объясним поподробнее.\n",
      "\n",
      "Последовательный способ предполагает, что сначала анализ проводится по одному из видов, по отдельному критерию. Далее для каждой из полученных категорий применяется анализ по 2 критерию, или виду и т.д.\n",
      "\r\n",
      "Визуально это выглядит так.\n",
      "\n",
      "\n",
      "\r\n",
      "Такой подход применяется для больших массивов данных. Например, если кросс-анализ проводится по всему ассортименту сети, по большой товарной группе. \n",
      "\r\n",
      "Второй аспект — такой анализ предполагает значительные аналитические усилия его организатора. Необходимо определить важность каждого критерия для будущего анализа и выстроить структуру анализа в нужном порядке.\n",
      "\r\n",
      "Например, цель кросс-анализа — оптимизация пространства на складе магазина, в таком случае первым критерием будет стабильность продаж, т.е. XYZ анализ, вторым критерием — количество продаж каждого товара по АВС, третьим — оборот, опять таки по АВС.\n",
      "\r\n",
      "При выборе другой цели — скажем, определение элитных товаров — первым критерием анализа будет Оборот магазина, потом уже Количество продаж и Вхождение в чеки. А вот стабильность продаж товара с помощью XYZ анализа, тут не так уж и важна.\n",
      "\r\n",
      "Т.е. последовательный анализ может проводится, если количество товаров в товарной группе достаточно большое, если нужно определить политику работы с группой, выработать стратегию управления и продвижения нужных товаров.\n",
      "\n",
      "Параллельный подход предполагает построение категорийной матрицы по заданному количеству критериев, анализ и работу с каждой категорией в ячейке матрицы.\n",
      "\r\n",
      "Так выглядит матрица для кросс анализа по АВС и XYZ. Обычно, аналитик, в любом случае, упрощает ее еще больше и определяет общий формат работы для нескольких категорий.\n",
      "\n",
      "\n",
      "\r\n",
      "Для АВС-анализа матрица может строится по нескольким критериям. Например, на картинке ниже А по обороту + А по количеству продаж + Х по стабильности продаж.\n",
      "\n",
      "\r\n",
      "Возможен вариант, что не все клетки матрицы АВС-XYZ будут заполнены. Потому что достаточно часто продажи товаров не стабильны, а значит категорию X выделить невозможно. Или аналитик торговой сети ставит такие коэффициенты анализа, что в некоторые категории товары не попадают.\n",
      "\r\n",
      "Параллельный подход стоит использовать когда необходимо выработать четкие рекомендации для работы с товарами каждой категории. Например, от аналитиков магазина список общих рекомендаций по сети для управляющих магазинами, для отдела закупок. \n",
      "\r\n",
      "Этот подход используется если товаров в ассортименте не так уж много.\n",
      "\n",
      "Пример и описание вариантов работы с каждой категорией\r\n",
      "Используем проведенный с помощью сервиса BI Datawiz.io кросс-анализ по товарной группе “Кондитерские изделия” в сети супермаркетов.\n",
      "\r\n",
      "Анализ будет проводится по 4 критериям:\n",
      "\n",
      "оборот (АВС)\n",
      "количество продаж (АВС)\n",
      "количество чеков с определенным товаром (АВС)\n",
      "стабильность продаж (XYZ)\n",
      "\n",
      "Период для анализа — текущий год, интервал — неделя.\n",
      "\n",
      "Основной ассортимент — товары продающиеся чаще, чем 2 раза в неделю.\n",
      "\r\n",
      "Количество товаров и процентное соотношение выбранных категорий видно на картинке ниже.\n",
      "\n",
      "\n",
      "\r\n",
      "Проанализируем полученные результаты, пройдемся только по ключевым (граничным) категориям:\n",
      "\r\n",
      "АААХ — для товаров-лидеров;\r\n",
      "АААZ — товары, важные для оборота сети, но с нестабильными продажами, на которые обязательно надо обратить внимание;\r\n",
      "BBBY — для товаров-среднячков;\r\n",
      "CCCZ — для товаров-аутсайдеров.\n",
      "\r\n",
      "Категория АААХ — 28 товаров из 1260.\n",
      "\n",
      "\r\n",
      "Самые “сладкие” товары. Обеспечивают основной товарооборот сети, поэтому нужно постоянное их наличие. \n",
      "\r\n",
      "Если проанализировать товары, входящие в эту категорию — это, в основном, жевательные резинки, печенье и популярные детские сладости.\n",
      "\r\n",
      "Как видно на картинке ниже, наибольшее количество продаж у жевательных резинок, продающихся у касс, так же они довольно ощутимо влияют на оборот товарной группы.\n",
      "\n",
      "\n",
      "\r\n",
      "Возможны два варианта работы с категорией — создание избыточного “страхового” запаса или организация доставки “точно в срок”, поскольку товары продаются стабильно и возможен точный прогноз продаж. \n",
      "\r\n",
      "Но стоит помнить, что товары категории АААX ключевые для работы магазина, поэтому если доставка на любом этапе провалится, это может привести к значительным проблемам в работе магазина.\n",
      "\n",
      "АААZ — категория с большим знаком вопроса.\n",
      "\r\n",
      "Почему при высоком количестве продаж, значительном влиянии на оборот магазина и частом вхождении в чеки, продажи товаров этой категории так тяжело предсказать?\n",
      "\r\n",
      "В категории АААZ 23 товара из 1260.\n",
      "\n",
      "\n",
      "\r\n",
      "Визуализация по ключевым показателям — обороту и количеству продаж показала, что конфеты Рафаэлло больше всего влияют на оборот сети и своей товарной группы.\n",
      "\n",
      "\n",
      "\r\n",
      "Судя по нашему жизненному опыту они достаточно популярны. Почему же их продажи настолько нестабильны, коэффициент вариации — 0,73?\n",
      "\r\n",
      "Открыв продажи конкретного товара, мы получили ответ на вопрос. \n",
      "\r\n",
      "Продажи конфет этой марки просто взлетают два раза в году — на 14 февраля и на 8 марта. Отсюда и нестабильность.\n",
      "\n",
      "\n",
      "\r\n",
      "Товары категории АААZ при высоких показателях продаж отличаются низкой прогнозируемостью, поэтому стоит проработать систему поставок:\n",
      "\n",
      "\n",
      "перевести часть товаров на систему заказов с постоянной суммой (объемом) заказа;\n",
      "обеспечить по части товаров более частые поставки;\n",
      "выбрать поставщиков, расположенных близко к магазину, тем самым снизив количество складского запаса;\n",
      "повысить периодичность контроля.\n",
      "\r\n",
      "В примере с Рафаэлло — просто обеспечить больший объем поставок конфет к праздникам.\n",
      "\r\n",
      "Категория BBBY одна из категорий товаров “среднячков”, они составляют основу ассортимента товарной группы. В эту категорию входит 63 товара из 1260.\n",
      "\n",
      "\n",
      "\r\n",
      "Продажи товаров недостаточно стабильны для того, чтобы обеспечить постоянное наличие, нужно поддерживать складской запас для страховки.\n",
      "\r\n",
      "Можно перейти на систему с постоянным временем или объемом заказа.\n",
      "\n",
      "CCCZ, как нижняя категория матрицы, объективно самые проблемные товары — 85 записей. \r\n",
      "Важно помнить, сюда могут попасть новые товары, элитные товары, товары под заказ и т. п. Поэтому категория требует вдумчивого анализа каждой позиции, и только потом радикальных действий. \n",
      "\n",
      "\n",
      "\r\n",
      "Но, работа с этой и смежными категориями дает возможность выделить товары, которые можно и нужно из ассортимента выводить, распродавая остатки. \n",
      "\r\n",
      "На скрине ниже видно эти товары, находящиеся в области графика около 0 и практически не влияющие на оборот торговой сети.\n",
      "\n",
      "\n",
      "\r\n",
      "Таким образом, выделяются подходы к работе с каждой полученной категорией. \n",
      "\r\n",
      "Главными условиями организации такой достаточно сложной аналитической деятельности является учет всех нужных показателей, скорость обработки результатов, экономия времени и усилий персонала сети. \n",
      "\n",
      "BI Datawiz.io дает возможность совмещать до 4 критериев при анализе товарного ассортимента. \n",
      "\r\n",
      "Совмещение нескольких вариантов анализа, или нескольких критериев дает возможность четче увидеть все нюансы ассортимента. А значит применять комплексные решения и рекомендации по управлению ассортиментом товарной группы, магазина или всей сети, складскими запасами и организацией поставок.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Бизнесу важно не только привлекать новых клиентов, но и удерживать их, возвращать на сайт и мотивировать делать повторные покупки. Мой четырехлетний опыт работы веб-аналитиком показал, что многие владельцы бизнеса не взаимодействуют с текущими клиентами просто потому, что не знают как это делать. Сегодня я расскажу, как с помощью RFM-анализа возвращать клиентов снова и снова. \n",
      " \r\n",
      "Сегментация и таргетинг — альфа и омега маркетинга. Можно не соглашаться с этим утверждением и бесконечно долго стрелять из пушки по воробьям. Разумнее и эффективнее анализировать поведение пользователей, разбивать аудиторию на сегменты и предлагать каждой группе персональное решение. Давайте рассмотрим конкретную методику и научимся применять RFM-анализ для сегментации клиентской базы.\n",
      "\r\n",
      "Итак, RFM — это анализ клиента по трем показателям: давность, частота и ценность покупок. В ходе этого анализа данные сегментируются в соответствии с показателями: \n",
      "\n",
      "\n",
      "Давность — показывает, как давно пользователь покупал что-либо у вас на сайте.\n",
      "Частота — как часто пользователь покупает что-то на сайте. \n",
      "Суммарная стоимость покупок — прибыль, которую вам приносит клиент. \n",
      "\r\n",
      "По этим показателям база клиентов сегментируется, и далее с каждой из этих групп можно вести индивидуальную коммуникацию. Такой подход приводит к увеличению общего числа покупок, так как клиенты возвращаются. \n",
      "\n",
      "Кому и зачем нужен RFM-анализ? \r\n",
      "В первую очередь это необходимо В2С-компаниям с клиентской базой от 10 000 контактов. Это условное ограничение, база может быть и меньше, в этом случае просто будет сокращается количество кластеров, на которые делится аудитория. В В2В-компаниях RFM-анализ не очень популярен, но тоже может использоваться маркетологами и владельцами бизнеса. \n",
      "\n",
      "\n",
      "\r\n",
      "RFM-анализ дает готовую схему, которая позволяет применять к каждой группе клиентов индивидуальный подход. Вы группируете клиентов и прогнозируете их поведение на основе прошлых действий. К примеру, тем, кто покупает часто и много, — спецпредложения, а кто давно ничего не покупал, получают бонус или скидку + таргетинг, чтобы напомнить о себе.\n",
      "\n",
      "\n",
      "\r\n",
      "Наиболее часто результаты RFM-анализа используют в работе с email-рассылками. Также он пригодится при подготовке скриптов телефонных звонков (по скрипту менеджер может обрабатывать клиента из определенного кластера) и, в принципе, для любых узкотаргетированных маркетинговых кампаний: например, ретаргетинг или ремаркетинг. \n",
      "\n",
      "Как провести RFM-анализ?\r\n",
      "Весь RFM-анализ разбивается по трехбалльной системе: давность заказа, частота и сумма покупок. В свою очередь давность заказа разделяют на давние, «спящие» и недавние заказы. Покупки по частоте делятся на разовые, редкие и частые. Сумма покупок делится на низко-, средне- и высокочековые. \n",
      "\n",
      "\n",
      "\r\n",
      "Сопоставляем эти параметры и получаем сегменты пользователей сайта. Их может быть до 27. \n",
      "\n",
      "\n",
      "\r\n",
      "На практике их может быть меньше. Количество сегментов зависит от базы клиентов, насколько она разносортная, насколько разные группы пользователей. \n",
      "\n",
      "\n",
      "\r\n",
      "Проводить анализ вручную трудоемко, лучше использовать сводные таблицы Excel. Я научу вас, как быстро и просто сделать RFM-анализ в Excel за 5-7 минут. \n",
      "\n",
      "Алгоритм RFM-анализа \r\n",
      "Для начала надо выгрузить из CRM или другой базы данных:\n",
      "\n",
      "\n",
      "уникальные данные клиента (это может быть почта, номер телефона, то, что идентифицирует клиента);\n",
      "даты покупок клиента;\n",
      "суммы покупок клиента. \n",
      "\n",
      "\n",
      "\r\n",
      "Этих трех параметров достаточно, чтобы сделать простой, быстрый и, главное, бесплатный RFM-анализ. Далее давайте создадим и настроим сводную таблицу. С помощью сводных таблиц (кликаете на Вставка — Сводная таблица), переносим на новый экран все три этих параметра. \n",
      "\n",
      "\n",
      "\r\n",
      "В полях Сводной таблицы три поля — email, Дата покупки и Сумма покупки — их нужно разбить на Строки и Значения. В Строки мы выносим один единственный показатель, в данном случае — это email-адреса (это могут быть и номера телефонов, любые контакты). Важно отметить, что email в этом столбце уже уникальны, они не повторяются. \n",
      "\n",
      "\n",
      "\r\n",
      "В значении Далее считаем по каждому пользователю такие показатели, как: количество покупок и сумма всех покупок. Важный параметр — Максимум по полю дата покупки. Сюда выводится дата последней покупки пользователя. Он нужен, чтобы высчитывать, как давно пользователь что-то покупал, этот расчет будет определять клиентов в тот или иной кластер. \n",
      "\n",
      "\n",
      "\r\n",
      "Вот такая получается простая сводная таблица практически в три клика. Все, что нужно для расчета – вынести в отдельные поля:\n",
      "\n",
      "\n",
      " уникальный email пользователя (просто копируем из сводной предыдущей таблицы);\n",
      "число покупок уникального клиента;\n",
      "сумма покупок;\n",
      "дата последней покупки. \n",
      "Далее по формуле, которая уже есть Eхсel, рассчитываются показатели RFM. \n",
      "\n",
      "\n",
      "\r\n",
      "Теперь у нас есть сводная таблица с расчетом RFM. В зависимости от того, как много, как часто и на какую сумму клиент делал покупки, формула рассчитывает и присваивает от 1 до 3 значений каждому клиенту. Далее у нас определяется кластер RFM – формула, по которой объединяются три эти цифры, получается сегмент или группа, к которой относятся те или иные пользователи.\n",
      "\n",
      "\n",
      "\r\n",
      "Эти данные собираются в кластер RFM. Вот так все эти кластеры выглядят. \n",
      "\n",
      "\n",
      "\r\n",
      "Теперь можно выделить сегмент (например, клиенты, которые покупали очень давно всего один раз) и уже целенаправленно работать с этой базой. Cегменты могут быть неравномерными, т. е. один включает 74 человека, другой — всего 1, а самих сегментов 27. Иногда бывает так, что в отдельном кластере оказывается всего один пользователь. В таких случаях лучше присоединить его к ближайшему крупному кластеру, где клиенты с похожими признаками. \n",
      "\r\n",
      "Полное руководство по созданию RFM-анализа и техническое руководство можно посмотреть в нашем вебинаре “Как превратить трафик в продажи с помощью данных о пользователях сайта”:\n",
      "\n",
      "\r\n",
      "После того, как мы научились делить клиентов на кластеры, давайте разберемся, как с ними работать. \n",
      "\n",
      "Потерянные клиенты\r\n",
      "Клиенты, которые сделали один раз небольшую покупку и больше не возвращались. Я рекомендую не тратить на них много времени. Их можно единоразово попробовать вернуть. Например, напомнить о себе, рассказав о каких-то акциях, спецпредложениях, распродажах. Если после таких рассылок пользователи к вам все же не возвращаются, стоит успокоиться и отпустить их. Лучше переключиться на другие кластеры. \n",
      "\n",
      " \n",
      "Пример рассылки магазина Adidas\n",
      "\n",
      "Пользователи, которые находятся под угрозой оттока \r\n",
      "Пользователи, которые сделали один раз крупную покупку и пропали. Как правило такие клиенты более перспективные, чем потерянные. Можно приложить больше усилий, чтобы их заинтересовать и вернуть. В первую очередь это могут быть:\n",
      "\n",
      "\n",
      "хорошие скидки;\n",
      "купоны на покупку; \n",
      "информация о распродажах;\n",
      "персональная подборка в зависимости от того, что они у вас уже покупали;\n",
      "предложение с аналогичными товарами, похожими или сопутствующими.\n",
      "\n",
      "\n",
      "Пример рассылки с допродажами магазина Reima\n",
      "\r\n",
      "Кроме того, можно привлечь и заинтересовать подобную аудиторию рассылкой с полезным контентом. Было бы отлично связаться с клиентом и узнать, по каким причинам они перестали у вас покупать. Что произошло, что бы могло побудить их продолжить покупать у вас. \n",
      "\n",
      "Бывшие лояльные клиенты\r\n",
      "Для этой группы подойдут те же мероприятия, что и для предыдущего кластера клиентов. Кроме вышеописанного, можно предложить им какие-то более долгосрочные мотивации, например, программы лояльности. Не стесняйтесь в коммуникациях хвалить свой магазин, продукт, услугу, показывать, чем они лучше других. \n",
      "\n",
      "\n",
      "Пример рассылки сервиса Rookee\n",
      "\n",
      "«Спящие» клиенты \r\n",
      "Это интересный кластер клиентов, которые помнят о вас, но по каким-то причинам перестали покупать. Что поможет их разбудить? В первую очередь это:\n",
      "\n",
      "\n",
      "выгодные акции и предложения;\n",
      "подборки к тематическому празднику;\n",
      "подарки и бонусы ко дню рождения клиента. \n",
      "\n",
      "\n",
      "Пример рассылки издательства МИФ\n",
      "\n",
      "Новички с низким и средним чеком \r\n",
      "Есть вероятность, что эту группу клиентов заинтересует какой-то обучающий контент, справочная информация. Важно, чтобы первое впечатление сложилось хорошее, чтобы эти пользователи перешли в лояльных. С ними можно поделиться статьей, обзором, руководством. Поздравить их с покупкой, поблагодарить за выбор вашей компании, пригласить в группы в соцсетях, на мероприятия, где можно будет с ними детально пообщаться и объяснить, почему ваш продукт им подходит. \n",
      "\n",
      "\n",
      "Пример рассылки сервиса Rookee\n",
      "\n",
      "Перспективные клиенты\r\n",
      "Перспективные – те, кто покупали на большие суммы, потенциальные VIP-клиенты. Нужно стараться удерживать их интерес. Например, можно с помощью опроса выяснить, довольны ли они услугами, что им интересно, какие у них потребности. Скидок этой группе клиентов предлагать не стоит, они и так лояльны и покупают. \n",
      "\n",
      "Идеальные клиенты\r\n",
      "Очень важно показывать, что вы их цените, вы их любите! Можно немного польстить какими-то интересными замечаниями, как, например, Яндекс.Музыка пишет, что музыкальному вкусу пользователя можно позавидовать. Я бы не рекомендовала утомлять клиента какими-то лишними ссылками, рассылками, СМС и звонками. Они с вами, они вас любят, и лишний раз надоедать не стоит. Когда им будет нужно, они сами обратятся за помощью.\n",
      "\r\n",
      "Со временем показатели RFM-анализа меняются, и клиенты переходят из одного сегмента в другой. Частота обновления данных зависит от того, насколько подвижная у вас база: какой жизненный цикл клиента, естественный период покупки, а также период, за который клиент успеет сделать повторную покупку. Для крупного успешного интернет-магазина — не чаще, чем раз в месяц. Если заказы происходят редко, достаточно пересматривать сегменты раз в квартал или полгода.\n",
      "\r\n",
      "RFM-анализ – это простой, но эффективный метод. Нужно всего 15-20 минут времени, чтобы разобраться, сделать сегментирование базы и начать работать с клиентами на новом уровне. Можно дальше развиваться и работать со специализированными сервисами. В Rookee мы используем Power BI, который позволяет отгружать email-адреса пользователей, количество оплаченных заказов, суммы заказов, даты, когда они были совершены в режиме онлайн. Это позволяет избегать обновления таблицы в ручном режиме, сегментирование всегда актуально. Полезно тем, кто работает с большим объемом данных. Однако, даже начав с обычной таблицы в Excel, вы делаете огромный шаг к тому, чтобы повысить срок жизни клиента, а значит и увеличить прибыль компании.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Фундаментальный анализ (FA) — это метод измерения внутренней стоимости ценной бумаги путем изучения взаимосвязанных экономических и финансовых факторов. При фундаментальном анализе изучается все, что может повлиять на стоимость ценных бумаг: макроэкономические факторы (состояние экономики и состояние конкретной отрасли бизнеса) микроэкономические факторы (эффективность управления компанией).\n",
      "\r\n",
      "Конечная цель инвестора в том, чтобы узнать стоимость, которую он сможет сравнить с текущей стоимостью ценной бумаги. Так инвестор увидит, является ли ценная бумага недооцененной или переоцененной. Впоследствии он принимает решение о ее покупке/продаже.\n",
      "\r\n",
      "Данный метод противоположен техническому анализу. В техническом анализе прогнозируется вектор цен, исходя из исторических рыночных данных (цена и объем). Недавно портал Investopedia опубликовал интересную статью о том, что такое фундаментальный анализ и как его применять. Мы подготовили для вас адаптированную версию данного материала. \n",
      "\n",
      "На чем основан фундаментальный анализ\r\n",
      "Главная цель фундаментального анализа — определить, правильно ли ценная бумага оценивается на рынке. Фундаментальный анализ проводится, учитывая макро и микро факторы. Так выявляются ценные бумаги, которые были неправильно (несправедливо) оценены на рынке.\n",
      "\r\n",
      "Чтобы понять справедливую рыночную стоимость акций, аналитики изучают общее состояние экономики, а затем и конкретную отрасль. Только после этого они переходят к показателям конкретных компаний.\n",
      "\r\n",
      "Фундаментальный анализ использует данные для оценки стоимости акций или других ценных бумаг. Например, инвестор может провести фундаментальный анализ стоимости облигаций, рассмотрев экономические факторы: процентные ставки и общее состояние экономики, а затем изучить информацию об эмитенте облигаций (например, о возможных изменениях его кредитного рейтинга).\n",
      "\r\n",
      "Для изучения акций в фундаментальном анализе учитываются: \n",
      "\n",
      "\n",
      "доходы;\n",
      "прибыль;\n",
      "потенциальный рост;\n",
      "собственный капитал;\n",
      "нормы прибыли.\n",
      "\r\n",
      "и другие данные для определения базовой стоимости компании и ее потенциала будущего роста. Все эти данные можно найти в финансовой отчетности компании.\n",
      "\r\n",
      "Чаще всего, фундаментальный анализ используется для акций. Но он полезен и для других финансовых инструментов: от облигаций до деривативов. \n",
      "\n",
      "Инвестирование и фундаментальный анализ\r\n",
      "Если внутренняя стоимость акции выше текущей рыночной цены, акция считается недооцененной. Ее рекомендуется покупать. Если внутренняя стоимость акции ниже рыночной цены, она считается переоцененной. Ее рекомендуется продавать.\n",
      "\r\n",
      "Инвесторы могут играть на повышение (покупка с ожиданием, что акции вырастут в цене у сильных компаний) и на понижение (продажа акций, которые упадут в цене с ожиданием выкупа их по более низкой цене у слабых компаний).\n",
      "\r\n",
      "Данный метод противопоставляется техническому анализу, который прогнозирует направление цен посредством анализа исторических рыночных данных (цена и объем).\n",
      "\n",
      "Количественный и качественный фундаментальный анализ\r\n",
      "Данные фундаментального анализа могут охватывать все, что связано с экономическим благополучием компании. Сюда могут относиться выручка, прибыль, а также доля компании на рынке и качество управления.\n",
      "\r\n",
      "Фундаментальные факторы делятся на две категории: количественные и качественные. Финансовое значение данных терминов не сильно отличается от их стандартных определений. \n",
      "\n",
      "\n",
      "Количественные основы являются жесткими числами. Это измеримые показатели бизнеса. Самый большой источник количественных данных — финансовая отчетность. В ней можно точно узнать о доходе, прибыли, активах и о многом другом.\n",
      "Качественные основы включают компетенцию руководителей компании, узнаваемость бренда, запатентованные технологии.\n",
      "\r\n",
      "Аналитики рассматривают все эти факторы в совокупности.\n",
      "\n",
      "Качественные характеристики\r\n",
      "При анализе компании всегда учитывается четыре основных показателя: \n",
      "\n",
      "\n",
      "Бизнес-модель: чем конкретно занимается компания? Пример: бизнес-модель компании основана на продаже курицы быстрого приготовления. Зарабатывает ли компания деньги именно на этом? Или основная часть дохода все же идет с роялти и франшиз?\n",
      "Конкурентоспособность: долгосрочный успех компании означает способность поддерживать и сохранять конкурентное преимущество. В этом случае акционеры компании могут получить приличные дивиденды в течение десятилетий.\n",
      "Менеджмент – важный критерий инвестирования. Даже самая лучшая бизнес-модель обречена, если руководители компании не смогут должным образом выполнить план. Розничным инвесторам сложно по-настоящему оценить менеджеров при личной встрече. Но всегда можно взглянуть на корпоративный сайт и проверить резюме высшего руководства и членов совета директоров. Насколько хорошо они справлялись с предыдущими задачами? \n",
      "Корпоративное управление – это политика организации; отношения и ответственность между руководством, директорами и заинтересованными сторонами. Политика определяется в уставе компании и ее внутренних актах, а также в корпоративном законодательстве и подзаконных актах. Инвесторы предпочитают иметь дела с компанией, которая управляется этично, справедливо, прозрачно и эффективно. Если такого нет — вероятно, руководство компании этого не хочет. \n",
      "Отраслевая принадлежность компании: клиентская база, доля рынка, общепромышленный рост, конкуренция, регулирование и бизнес-циклы. Изучение отрасли компании, даст инвестору более глубокое понимание о ее финансовом здоровье.\n",
      "\n",
      "Количественные основы для анализа\r\n",
      "Финансовая отчетность — это документ, в котором компания раскрывает информацию о своих финансовых результатах. Приверженцы фундаментального анализа используют количественную информацию из финансовой отчетности для принятия инвестиционных решений. Наиболее важные финансовые отчеты: отчет о прибыли и убытках, балансовый отчет, отчет о движении денежных средств.\n",
      "\n",
      "Балансовый отчет\r\n",
      "Балансовый отчет представляет собой отчет об активах, пассивах и собственном капитале компании на определенный момент времени. Балансовый отчет называется тем фактом, что финансовая структура предприятия сбалансирована следующим образом::\n",
      "\n",
      "Активы = Пассивы + Собственный Капитал\r\n",
      "Активы — это ресурсы, которыми компания владеет или которые контролирует в данный момент времени: наличные деньги, инвентарь, машины и здания. С другой стороны уравнения — общая стоимость финансирования для владения данными активами. Финансирование осуществляется из пассивов или из собственного капитала. Пассивы — это долг компании, собственный капитал — это общая стоимость всех активов, которые владельцы внесли в бизнес, включая нераспределенную прибыль (прибыль, полученную в предыдущие годы).\n",
      "\n",
      "Отчет о движении денежных средств\r\n",
      "Отчет о движении денежных средств — это отчет о движении денежных средств предприятия за определенный период времени. Как правило, отчет о движении денежных средств строится на следующих показателях:\n",
      "\r\n",
      "Денежные средства от инвестирования (CFI): денежные средства, используемые для инвестирования в активы, а также доходы от продажи оборудования или долгосрочных активов.\n",
      "\r\n",
      "Денежные средства от финансирования( CFF): денежные средства, уплаченные или полученные в результате эмиссии и заимствования средств.\n",
      "\r\n",
      "Операционный денежный поток (OCF): денежные средства, полученные от ежедневных деловых операций.\n",
      "\n",
      "Заключение\r\n",
      "Основная задача фундаментального анализа определить, отражает ли цена на фондовом рынке реальную (справедливую) стоимость акций.\n",
      "\r\n",
      "Предположим, акции компании торговались на уровне $20. После фундаментального анализа инвестор определил, что акция на самом деле стоит $25. \n",
      "\r\n",
      "В этом и заключается суть фундаментального анализа. Сосредоточившись на конкретном бизнесе, инвестор может оценить внутреннюю стоимость фирмы и найти возможности покупки со скидкой. Инвестиции окупятся, когда рынок догонит фундаментальные показатели. К слову, самым известным и успешным фундаментальным аналитиком является Уоррен Баффет, по прозвищу «Оракул из Омахи».\n",
      "\n",
      "Полезные ссылки по теме инвестиций и биржевой торговли:\n",
      "\n",
      "Открыть брокерский счет онлайн\n",
      "Тестовый счет с виртуальными деньгами\n",
      "Софт для торговли на бирже: торговый терминал, мобильные приложения\n",
      "Структурные продукты\n",
      "Модельные портфели\n",
      "\n",
      "Читайте обзоры, аналитику рынков и инвестидеи в Telegram-канале ITI Capital    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " О чем эта статья\n",
      "Здесь будет описан метод анализа логических цепочек. Изначально созданный как инструмент ведения дискуссий, этот подход может быть полезен в любой ситуации, где нужно найти ответ на вопрос истинно утверждение или ложно. В качестве интересного следствия приведенный анализ позволяет найти ключевые утверждения, лежащие в основе сформировавшейся картины мира. Также будет сформулирована аксиома волнового анализа о фундаментальном ограничении аналитического подхода. \n",
      "\n",
      "Есть ли смысл в спорах\n",
      "Какое-то время назад, я, как и многие другие обычные и до этого даже аполитичные люди, оказался добровольцем в информационной войне. Немалое количество часов провел я в спорах, доказывая свою правоту и удивляясь \"тупости\" противоположной стороны. Потихоньку нешуточные страсти поутихли, не причинив существенного ущерба. Не смотря на противоположность взглядов и подчас высокий накал дискуссии, я и мои оппоненты сумели не выйти за рамки разумности и сохранить хорошие отношения. В конце всего этого мы приобрели что-то вроде иммунитета к политическим спорам, и это, конечно, важно, но, как мне кажется, есть еще одно приобретение, и более ценное приобретение.\n",
      "Думаю, что многие это проходили. Когда количество споров на одну и ту же тему с разными людьми переваливает за десяток, спорить становится уже неинтересно. Вы понимаете, что знаете и предсказываете их логические цепочки, аргументы и контраргументы. Конечно, иногда встречаются новые ходы, но все же \"основные логические магистрали\" остаются приблизительно теми же, и их количество вполне ограничено. И это в принципе можно рассматривать как приобретение. Фактически таким образом разрабатывается некая карта логических связей. Правда, эта карта разбита на многие осколки, которые хранятся лишь в умах большого количества людей, не формализованная, не выраженная, с ложными или тупиковыми тропками… \n",
      "В какой-то момент для некоторых тем мне захотелось запротоколировать и проанализировать эту карту. Работая над этим, я получил то, что я назвал волновым анализом и несколько, на мой взгляд, интересных выводов, которыми и хочу поделиться.\n",
      "\n",
      "Терминология, используемая в статье\n",
      "Здесь краткое описание терминов, вводимых и поясняемых в статье.\n",
      "\n",
      "Волновой анализ — метод нахождения ответа на вопрос, является ли утверждение истинным или ложным с использованием логических цепочек, основанных на операциях опровержения и дополнения (см. ниже).\n",
      "Волновой граф. Логические цепочки волнового анализа могут быть представлены в виде графа. Вершины графа — это утверждения. Ребра графа — это связи между утверждениями. Существует 3 основных вида связи между вершинами волнового графа:\n",
      "\n",
      "прямое опровержение \n",
      "косвенное опровержение\n",
      "дополнение\r\n",
      "все остальные связи, необходимые в волновом анализе, могут быть представлены через эти три. \n",
      "\n",
      "Корневое утверждение (корневая вершина) — анализируемое основное утверждение, является корнем волнового графа. В рамках описанного подхода мы ожидаем, что это утверждение может быть только строго истинным или строго ложным. \n",
      "Прямое опровержение. Предположим у нас есть два утверждения A и B. Если истинность утверждения B приводит к тому, что утверждение A ложно, то это называется прямым опровержением и обозначается как A --п-> B. Символ --п-> значит \"ложно потому, что\". На графе прямое опровержение обозначается как сплошная линия со стрелкой от A к B.\n",
      "Косвенное опровержение. Отличается от прямого лишь силой связи. Если в случае прямого опровержения истинность B однозначно приводит к ложности A, то в случае косвенного опровержения истинность B лишь увеличивает вероятность истинности A. Обозначается как A --к-> B. Символ --к-> значит \"с некоторой вероятностью ложно потому, что\". На графе обозначается как пунктирная линия со стрелкой от A к B.\n",
      "Дополнение (или отрицание). Это обычное логическое отрицание, то есть если B истинно, то A ложно и, наоборот, если B ложно, то A истинно. Будем обозначать это как A<->B. Легко показать, что если A<->B, то B<->A. На графике будем обозначать это сплошной линией с двумя разнонаправленными стрелками. \n",
      "Прямое подтверждение. Если B истинно, то А тоже истинно. Легко увидеть, что это может быть представлено как A<->C--п->B. (C — просто дополнение А, и может быть выражено как \"не А\").\n",
      "Косвенное подтверждение. Если B истинно, то вероятность того, что А тоже истинно увеличивается. Легко увидеть, что это может быть представлено в виде A<->C--к->B.\n",
      "Волновые логические цепочки (волны). Если следовать стрелкам волнового графа, то мы получим \"волну\" утверждений, последовательно опровергающих предыдущее в цепочке утверждение и, соответственно, поочередно опровергая и подтверждая корневое утверждение. \n",
      "Сходимость (разрешимость) волнового графа. Считается, что волновой граф сходится (или является разрешимым), если его анализ приводит к однозначному выводу о ложности или истинности корневого утверждения.\n",
      "Тупиковые вершины. Это утверждения, которые не опровергаются ни одним другим утверждением. На графе из таких вершин не исходит ни одной стрелки (в том числе и двунаправленных). \n",
      "Циклы. Это любые циклы в волновых логических цепочках. Самые простые и фундаментальные циклы — это циклы, состоящие из двух элементов: дополнение и взаимное опровержение. Наличие неразрешимых циклов является следствием скрытой информации.\n",
      "Опорные точки. Это минимальный набор самосогласованных утверждений (в идеале одно), в которые человек верит, что позволяет графу сойтись. В зависимости от того, с каким знаком эта вера (верит ли он, что эти/это утверждение истинно или ложно) граф сходится либо с положительным, либо с отрицательным результатом. На полном волновом графе (см. определение ниже) эти вершины являются частью неразрешимых циклов (скрытая информация). \n",
      "Причинный анализ. Анализ основанный на причинно-следственных связях.\n",
      "Причинный граф. Этот граф можно получить из волнового, если следовать по волновым логическим цепочкам от опорного утверждения в сторону корневого, но только по утверждениям, находящимся в той же фазе, что и опорное. \n",
      "Абсолютно полный волновой граф. Волновой граф, учитывающий все волновые зависимости. Этот граф скорее теоретический, потому что на практике нет необходимости рассматривать все \"волны\". \n",
      "Полный волновой граф. Волновой граф, включающий все волновые логические цепочки с \"разумной\" (соответствующей контексту дискуссии) степенью глубины анализа.\n",
      "Основная аксиома волнового анализа: \"всегда есть скрытая информация, не позволяющая однозначно разрешить полный волной граф\".\n",
      "\n",
      "\n",
      "Волновой анализ\n",
      "Связи между утверждениями\n",
      "Рассмотрим два утверждения: A и B. Будем считать, что каждое из этих утверждений может быть или истинным или ложным. Тогда возможны следующие зависимости утверждения A от B:\n",
      "\n",
      "Независимость (А не зависит от В). Истинность или ложность утверждения A никак не зависит от истинности или ложности утверждения B. \n",
      "Прямое опровержение. Если утверждение B истинно, то утверждение A ложно. Будем обозначать такую связь, как A --п-> B. Суть в том, что стрелка --п-> фактически заменяет выражение \"ложно потому, что\", то есть A --п-> B это краткая запись выражения \"A ложно потому, что B (истинно)\". На графе эту связь мы будем обозначать сплошной линией со стрелкой от А к B:\n",
      "\n",
      "Замечание\n",
      "\r\n",
      "При этом, если утверждение B ложно, то это может ничего или почти ничего не значить для А.\n",
      "\n",
      "Пример\n",
      "\r\n",
      "Утверждение А: \"Вчера в 7 часов вечера Алиса была дома\".\r\n",
      "Утверждение B: \"Вчера в 7 часов вечера Алиса была в театре\".\r\n",
      "A --п->B: \"Вчера в семь часов вечера Алиса не была дома, потому что она была в театре.\" \n",
      "\n",
      "Дополнение (логическое отрицание). Если утверждение B истинно, то утверждение A ложно, и если утверждение B ложно, то утверждение A истинно. Будем обозначать это как А <-> B. Легко показать, что если А <-> B, то B <-> A. На графе эту связь мы будем обозначать сплошной линией с двунаправленной стрелкой:\n",
      "\n",
      "Пример\n",
      "\r\n",
      "Утверждение А: \"Вчера в 7 часов вечера Алиса была дома\".\r\n",
      "Утверждение B: \"Вчера в 7 часов вечера Алисы НЕ было дома\".\n",
      "\n",
      "Прямое подтверждение. Если утверждение B истинно, то утверждение А тоже истинно. Будем называть это прямым подтверждением. Легко показать, что такая зависимость составляется из предыдущих зависимостей: A<->C--п-> B. В этом случае если B истинно, то C ложно, а значит A истинно. На графе это может быть представлено в виде\n",
      "\n",
      "Пример\n",
      "\r\n",
      "Утверждение А: \"Вчера в 7 часов вечера Алиса была дома\".\r\n",
      "Утверждение B: \"Вчера в 7 часов вечера Боб был в гостях у Алисы, и видел её дома\".\r\n",
      "A <->C--п->B: \"Вчера в семь часов вечера Алиса была дома, потому что Боб в это время был у нее в гостях и видел её там.\"\r\n",
      "Дословно же эту запись можно представить, как \"Вчера в семь часов вечера Алиса не была вне дома, потому что Боб был у нее в гостях и видел её дома\". \n",
      "\n",
      "Косвенное опровержение. Если утверждение B истинно, то вероятность того, что утверждение А ложно увеличивается. Будем называть это косвенным опровержением и обозначать как A --к-> B. На графе будем обозначать это пунктирной линией со стрелкой от A к B:\n",
      "\n",
      "Замечание\n",
      "\r\n",
      "При этом, если утверждение B ложно, то это может ничего не значить для А.\r\n",
      "Отличие от прямого опровержения — в силе связи. Если В истинно, то это еще не значит, что А ложно, но говорит нам о том, что вероятность того, что А ложно увеличивается.\n",
      "\n",
      "Пример\n",
      "\r\n",
      "Утверждение А: Вчера в 7 часов вечера Алиса была дома\".\r\n",
      "Утверждение B: \"Вчера в 7 вечера к Алисе приходил Боб, звонил в дверь, но никто не открыл\".\r\n",
      "A --п->B: \"Вчера в семь часов вечера Алисы скорее всего не было дома, потому что в это время приходил Боб и звонил в дверь, но никто не открыл\". Но в действительности Алиса могла быть дома, но по каким-либо причинам не могла или не хотела открывать. \n",
      "\n",
      "Косвенное подтверждение. Если утверждение B истинно, то вероятность того, что утверждение А тоже истинно увеличивается. Будем называть это косвенным подтверждением. Легко показать, что такая зависимость составляется из предыдущих зависимостей: A<->C--к-> B. В этом случае, если B истинно, то вероятность того, что C ложно увеличивается, а значит вероятность того, что A истинно тоже увеличивается. Отличие от прямого подтверждения опять-таки в силе связи (как и для косвенного опровержения). На графе это может быть представлено как\n",
      "\n",
      "Пример\n",
      "\r\n",
      "Утверждение А: \"Вчера в 7 часов вечера Алиса была дома\".\r\n",
      "Утверждение B: \"Вчера в 7 часов вечера соседи слышали звуки пианино из квартиры Алисы\".\r\n",
      "A <-> C --к->B: \"Вчера в семь часов вечера Алиса скорее всего была дома, потому что соседи слышали звуки пианино из ее квартиры\". Но, в действительности, это ведь могла быть просто запись.\n",
      "\n",
      "\n",
      "Таким образом мы ввели 3 основных вида зависимости утверждения A от утверждения B: \n",
      "\n",
      "прямое опровержение\n",
      "косвенное опровержение\n",
      "дополнение (логическое отрицание)\n",
      "\n",
      "Все остальные необходимые для анализа комбинации можно получить из этих 3х.\n",
      "\n",
      "в случае нескольких опровержений одного утверждения, они объединяются логическим \"или\". На данном графе показано, что N ложно потому что M1 или M2 или… Mk (истинно) \n",
      "\n",
      "Замечание\n",
      "\r\n",
      "Возможна комбинация прямых и косвенных опровержений.\n",
      "\n",
      "если мы хотим показать аргументы, поддерживающие наше утверждение, то, как уже было замечено выше, это можно сделать через опровержение дополнения. Так, например, если мы имеем утверждение N1 и аргументы M1, M2, .., Mk, где M1 — косвенное подтверждение, а M2 и Mk — прямые, то это будет выглядеть следующим образом\n",
      "\n",
      "\n",
      "\n",
      "Граф, построенный по этим правилам мы будем называть волновым графом.\n",
      "Волновой граф\n",
      "Предположим, что происходит обсуждение истинности/ложности утверждения A. Утверждение А должно быть сформулировано таким образом, что возможен один и только один вариант — А истинно или А ложно. \n",
      "Построим волновые логические цепочки в виде графа по следующему принципу: \n",
      "\n",
      "утверждение А будет корнем нашего графа \n",
      "вершинами графа являются утверждения, поддерживающие или опровергающие корневое утверждение. Для удобства мы будем нумеровать вершины\n",
      "Замечание \n",
      "\r\n",
      "В данной статье мы будем использовать положительные числа, если утверждение является подтверждающим наше корневое утверждение, и отрицательные числа — если опровергает, но это, конечно, необязательно (обязательным является однозначность нумерации). При этом номером +0 будет обозначена корневая вершина, а -0 — её дополнение. Конечно, это всего лишь обозначение, поэтому тот факт, что +0 = -0 ничему не мешает.\n",
      "\n",
      "ребра графа отражают перечисленные виды зависимости и, соответственно, у нас всего лишь 3 вида ребер\n",
      "\n",
      "Волновой логической цепочкой (волной) мы будем называть движение от вершины к вершине по стрелкам ребер. Очевидно, что в такой цепочке мы всегда будем иметь последовательность аргументов за и против, что лично мне напоминает волну. Приблизительно это мы наблюдаем и в \"правильном\" споре (что, к сожалению, бывает далеко не всегда).\n",
      "\n",
      "Пример волнового графа. Отелло\n",
      "В качестве иллюстрации волнового анализа попробуем помочь Отелло разобраться в том, изменяла ли ему Дездемона. Вот приблизительно так может выглядеть волновой граф:\n",
      "\n",
      "\n",
      "корнем этого графа является утверждение \"Дездемона изменила Отелло\" \n",
      "далее вниз мы можем наблюдать волновые логические цепочки (волны). Цвет ближе к голубому (мы будем называть его голубым) является подтверждением того, что \"Дездемона НЕ изменяла Отелло\", а цвет ближе к желтому (мы будем называть его желтым), является подтверждением противоположного, а именно, что \"Дездемона изменила Отелло\". Мы видим чередование голубых и желтых вершин\n",
      "мы знаем, что истинными являются все голубые утверждения, Отелло же на момент убийства был уверен в том, что истинными являются все \"желтые утверждения\"\n",
      "рассмотрим примеры всех возможных зависимостей:\n",
      "\n",
      "прямое опровержение\r\n",
      "+0 --п-> -2: утверждение, что \"Дездемона изменила Отелло\" ложно потому, что \"Дездемона говорит, что она не изменяла\". Если то, что говорит Дездемона правда, то, конечно же, это прямое опровержение того, что Дездемона изменила\n",
      "косвенное опровержение\r\n",
      "+0 --к-> -3: утверждение, что \"Дездемона изменяла Отелло\" ложно потому, что \"Дездемона любит Отелло\". Действительно, если она действительно любит Отелло, то это еще не значит, что она ему не изменяла, но этот факт увеличивает вероятность того, что Дездемона верна Отелло\n",
      "дополнение\r\n",
      "+0 <-> -0: \"Дездемона изменила Отелло\" и \"Дездемона не изменяла Отелло\". \n",
      "прямое подтверждение\r\n",
      "+0 <-> -0 --п-> +3: \"Дездемона изменила Отелло\" потому, что \"Кассио сам рассказал об этом\". Очевидно, что, если Кассио рассказал правду, то это прямое подтверждение\n",
      "косвенное подтверждение.\r\n",
      "+0 <-> -0 --к-> +5: \"Дездемона изменила Отелло\" потому, что \"Кассио говорил о связи с Дездемоной во сне\". Понятно, что даже если это и правда, и Кассио говорил об этом во сне, то это еще не значит, что Дездемона изменила. Однако, это все же увеличивает вероятность этого\n",
      "\n",
      "мы можем рассмотреть отдельные волновые логические цепочки (волны). Для этого нужно просто следовать стрелкам. Помним, что каждая однонаправленная стрелка от вершины N к вершине M значит, что \"N ложно потому, что M (истинно)\". Так же у нас может быть конструкция с подтверждением (описана выше). Например, рассмотрим логическую цепочку +0 <->-0 —п-> +3 —п-> -6 —п-> +13 —п-> -14 —к-> +20 —п-> -15 <-> + 21 --к-> -19 <->+25:\n",
      "\n",
      "+0 <->-0 —п-> +3: Дездемона изменила Отелло, и доказательством является то, что Отелло слышал разговор Кассио с Яго, где Кассио говорил о связи с Дездемоной\n",
      "+3 —п-> -6: но это не так, потому что Отелло слышал только часть разговора и неправильно интерпретировал его\n",
      "-6 —п-> +13: интерпретация верна, Яго подтвердил верность интерпретации\n",
      "+13 —п-> -14: Яго лжёт\n",
      "-14 —к-> +20: У Яго нет мотива обманывать Отелло\n",
      "+20 —п-> -15: Нет, у Яго есть мотив. Он обижен на то, что Отелло назначил не его заместителем, а Кассио\n",
      "-15 <-> + 21 --к->-19: Яго обижен на Отелло и держит на него зло потому, что он злой и порочный человек\n",
      "-19 <->+25: Это не так, потому что Яго хороший и честный человек\r\n",
      "Зачем мы закончили --19 <-> + 25? Потому, что мы не можем точно знать (Отелло не может), каким человеком является Яго, злым или хорошим. У нас нет аргументов. Такая же ситуация бывает и в споре, когда оппоненты просто упираются в \"да — нет\" без какой-либо аргументации. Это и есть одна из опорных точек.\n",
      "Замечание\n",
      "\r\n",
      "Оценочные утверждения хорошо/плохо в действительности сложно анализировать. Проще говорить о фактах. Но часто подобное восприятие может быть ключевым для принятия решения (см. опорные точки ниже)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Анализ волнового графа\n",
      "Сходимость графа\n",
      "Будем называть граф разрешимым (или сходящимся), если анализ данного графа однозначно приводит к выводу о том, истинно или ложно корневое утверждение.\n",
      "Будем называть вершину N тупиковой, если из нее не выходит ни одной стрелки, в том числе и двунаправленной (дополнение), то есть она не опровергается ни одним утверждением (в идеале ни прямым, ни косвенным, но существенными для анализа являются только прямые опровержения). \n",
      "Тогда утверждение M, для которого M --п-> N будем называть утверждением, опровергнутым N. \n",
      "Ниже приведен алгоритм разрешения волнового графа.\n",
      "\n",
      "найдем (если есть) тупиковую вершину N\n",
      "найдем все утверждения, опровергнутые N. Пометим эти вершины и тупиковую вершину N как \"удаленные\". Все стрелки, ведущие к ним и от них, также пометим как \"удаленные\". Мы не учитываем их в дальнейшем анализе\n",
      "все вершины, которые не входят в волновые логические цепочки с началом в корне (потеряна логическая связь с корневым утверждением) помечаем как \"удаленные\". Все стрелки, ведущие к ним и от них, также пометим как \"удаленные\". Мы не учитываем их в дальнейшем анализе.\n",
      "если для двух вершин N и M таких что N<->M вершина M помечена как удаленная, то вершина N становится тупиковой (доказанной), и все стрелки ведущие из нее (все опровержения) помечаются как удаленные \n",
      "повторяем этот процесс до тех пор, пока не кончатся тупиковые вершины\n",
      "когда тупиковые вершины закончатся у нас возможны 3 варианта:\n",
      "\n",
      "корневое утверждение будет помечено как \"удаленное\". Тогда данный граф считается решенным. Наше корневое утверждение оказалось ложным\n",
      "осталось только корневое утверждение. В этом случает данный граф считается решенным. Наше корневое утверждение оказалось истинным \n",
      "все остальные варианты. В этом случае мы можем утверждать, что существует скрытая информация, которая не позволяет нам определить, является утверждение ложным или истинным\n",
      "Замечание\n",
      "\r\n",
      "Теоретически, в зависимости от последовательности взятия тупиковых вершин, мы можем получить разный результат. Это говорит или об ошибке в логике, или о ложной информации, принятой за истину. Если наш алгоритм в зависимости от последовательности действий приводит к разным результатам, то мы должны сначала исправить ложную информацию или ошибку в логике.\n",
      "\n",
      "\n",
      "\n",
      "Вернемся к примеру с Отелло.\n",
      "\n",
      "Пример сходимости.  Дездемона виновна\n",
      "Мы видим, что в том виде, как представлен волновой граф, он неразрешим. У нас только 5 тупиковых вершин: +19, +18, +17, +11 и +8. Вершины +8, +18 и +19 отражают косвенные связи, поэтому не интересны. Но утверждение +17 (\"Дездемона умеет лгать, потому что она уже лгала отцу, когда убежала к Отелло\") является действительно тупиковым. Это приводит к тому, что вершины +17 и -11 в соответствии с нашим алгоритмом мы помечаем как удаленные. Также вершину +11 разумно признать тупиковой. Нельзя полагаться на показания Эмилии о том, изменяла ли Дездемона. Она принципиально не может знать правды. Может предполагать, догадываться, но она не знает. Поэтому вершины +11, -1 можно удалить.\n",
      "Но все это мало помогает нам продвинуться в разрешении данного графа.\n",
      "Так почему же восприятие Отелло сложилось таким образом, что он был уверен, что Дездемона виновна?\n",
      "Давайте посмотрим, что произойдет, если вершину +23 (\"Яго говорит правду\") или +25 (\"Яго хороший и честный человек\") считать истинной? Как только мы принимаем это, то\n",
      "\n",
      "вершина +23 (или +25) становится тупиковой (нет опровержений)\n",
      "это приводит к тому, что ее и вершину -14 (со всеми связями) мы помечаем как удаленные\n",
      "вся ветка с вершинами +20, -16, -15, +21, +25, -19 теряется связь с корнем и поэтому также удаляется со всеми связями\n",
      "далее, вершины +15, +14, +13 становятся тупиковыми \n",
      "таким образом, удаляются они (+15, +14, +13), а также те вершины, которые были опровергнуты ими, а именно, -8, -7, -6 и те вершины, которые потеряли \"связь\" c корневым утверждением: +22, +26\n",
      "теперь тупиковой вершиной является вершина +6. Важная вершина, потому что она говорит о том, что Дездемона подарила платок Кассио. Это приводит к тому, что эта вершина, а также вершины -7, +7, +8, +9 вместе со связями могут быть удалены из графа\n",
      "тогда вершина +2 становится тупиковой, что позволяет нам удалить вершины +2, -3, +1, -5, -26\n",
      "следующим шагом мы можем удалить ветку, которая потеряла связь с вершиной: +12, -13, +19, +18, и мы получаем следующий граф: \n",
      "\n",
      "\n",
      "\n",
      "И этот граф формально опять-таки является неразрешимым. Причина в циклах:\n",
      "\n",
      "-4 <-> +4: Кассио лжёт или говорит правду? Отелло не знает ответа. Да, он слышал разговор, да, \"честный Яго\" подтвердил, что он не ослышался и правильно понял суть, но, возможно, Кассио просто бахвалился \n",
      "-10 <-> +16: Дездемона порочна или добрая и чистая душе? Это та дилемма, которую Отелло и пытается решить. Но этот цикл не так уж важен, потому что вершина -10 является лишь косвенным опровержением вершины +10\n",
      "+10 <-> -12: Дездемона лжет или говорит правду? \n",
      "\n",
      "В данном месте анализа Отелло уже сделал вывод, что Дездемона любит Кассио, но факт измены пока не подтвержден. Возможно, Кассио говорит неправду. Но характер мавра не знает полутонов, поэтому ему проще принять факт измены. Он делает еще один шаг, поверив, что Дездемона обманывает его, и что Кассио действительно рассказывал о своей связи с Дездемоной. Теперь граф сходится.\n",
      "Этот анализ показывает, что, как только Отелло принимает, что Яго говорит правду, и это перевешивает его доверие Дездемоне (соответственно, перестает ей доверять), так сразу граф формально сходится.\n",
      "\n",
      "\n",
      "Пример сходимости.  Дездемона невинна\n",
      "Итак, Отелло убил Дездемону в полной уверенности в ее порочности. Но сразу после ее смерти его видение ситуации развернулось на противоположное. Что же произошло, и как это объясняет волновой анализ?\n",
      "Вспомним последний акт. Эмилия, под впечатлением от смерти Дездемоны разоблачает своего мужа Яго, объясняя мавру историю с платком. Таким образом Отелло получил опровержение того, что Яго говорит правду. Отелло верит служанке, и это сразу же приводит к разрешению графа, но уже с противоположным результатом. Давайте посмотрим почему.\n",
      "Теперь к нашему графу мы можем добавить вершины -20, +24, -21, -17, -18 (здесь представлена только правая часть графа): \n",
      "\n",
      "При этом -20<->+24 --п->-17 и -20<->+24 --п->-18 мы можем отнести к прямым доказательствам:\n",
      "\n",
      "-20<->+24 --п->-17: \"Эмилия рассказала правду про платок, что показывает, что Яго лгал\", и это правда потому, что \"Яго так напуган словами Эмилии, что убивает её\"\n",
      "-20<->+24 --п->-18: \"Эмилия рассказала правду про платок, что показывает, что Яго лгал\", и это правда потому, что \"Эмилия находится под сильным впечатление от смерти Дездемоны и не способна лгать\"\n",
      "\n",
      "В действительности это не выглядит такими уж сильными аргументами, и \"волну\" можно продолжить и дальше. Например, возможно, Яго убивает Эмилию не потому, что она говорит правду, а по другой причине, или Эмилия лишь притворяется, что находится под сильным впечатлением. И при холодном анализе все это должно быть учтено. Но в данном случае интересно отношение Отелло, интересно то, как именно он все это воспринимает, а мавр поверил Эмилии.\n",
      "Проанализируем наш граф теперь.\n",
      "\n",
      "вершины -17 и -18 на нашем графе тупиковые. Значит, эти вершины, а также вершины +24 и +25 и все их связи мы помечаем как удаленные. Также мы удаляем вершину -21 вместе со связями, т.к. она потеряла связь с корнем графа\n",
      "теперь вершины -19 и -20 становятся тупиковыми. Значит, эти вершины, а также вершины +6, +23 и +20 и все их связи можно удалить. Также удаляем и вершины, не имеющие пути к корню, то есть -16, -15, +21, -8, +15, + 22\n",
      "теперь тупиковые вершины -14, -7. Удаляем их и вершины, которые они прямо опровергли: +13, +5\n",
      "\n",
      "Получаем граф (только правое крыло):\n",
      " \n",
      "Даже после признания того, что Яго лжец, граф формально не сходится. Действительно, теперь Отелло знает, что Яго лжец, что история с платком была подстроена. Нет никаких доказательств того, что Дездемона изменила (также нет и никаких опровержений). Но Отелло лично слышал то, как Кассио рассказывал про свою связь с Дездемоной. При этом Отелло не знает, говорил ли Кассио правду, а также не знает, правильно ли он интерпретировал диалог, который услышал. Но у Отелло уже произошло \"переключение\" восприятие, поэтому он считает, что, так как все было подстроено, то и диалог был понят неверно. Поэтому вершина +23 удаляется, и вершина -6 становится тупиковой. \n",
      "На следующем шаге это приводит к тому, что вершина +3 и с ней вершины -4 и +4 со всеми связями также удаляются, и вершина -0 остается без прямых и косвенных опровержений. Это значит, что все аргументы, как прямые, так и косвенные, подтверждающие, что Дездемона изменила были опровергнуты.\n",
      "Что мы имеем в данной точке нашего анализа?\n",
      "\n",
      "у нас не осталось ни одного аргумента, подтверждающего прямо или косвенно измену\n",
      "у нас есть аргументы, опровергающие измену (недоказанные и неопровергнутые)\r\n",
      "Можно ли считать в данной точке граф разрешенным? Пока нет, потому что далее мы должны произвести анализ всех аргументов опровергающих измену, и может получиться следующая ситуация:\n",
      "\n",
      "\n",
      "Такой граф говорит о том, что недостаточно данных. Это та ситуация, когда спорящие стороны упираются в неразрешимый \"да-нет\" цикл. Никто не может привести убедительных аргументов за или против, но по каким-то иррациональным причинам мнение сторон по этому вопросу является диаметрально противоположным. \n",
      "Почему же граф сошелся для Отелло? Для этого давайте взглянем на его левую часть. Теперь все просто, вершины -13 (\"Эмилия говорит правду\") и -12 ( \"Дездемона говорит правду\") становятся тупиковыми (доказанными) в восприятии Отелло, и после этого граф легко разрешается.\n",
      "Итак, что же произошло? Теперь, когда Отелло перестал верить Яго, он автоматически стал верить Дездемоне и Эмилии (что, в принципе, иррационально), и это привело к тому, что граф сошелся, но уже с противоположным результатом. Фаза восприятия мавра сдвинулась на 180 градусов.\n",
      "\n",
      "Иррациональные элементы и опорные точки\n",
      "В примере с Отелло мы видим, что изначальный граф не сходится. Чтобы граф сошелся, нужно было допустить иррациональность — поверить. Как только Отелло поверил Яго и перестал верить Дездемоне, граф сошелся. Отелло видит эту ситуацию в \"желтых тонах\" (он воспринимает все вершины с положительной нумерацией и желтым цветом, как истинные).\n",
      "Потом, когда Отелло поверил Эмилии и соответственно осознал, что Дездемона говорила правду, а Яго лгал, фаза восприятия Отелло скачком сдвигается на 180 градусов, и для Отелло становятся истинными утверждения всех голубых вершин (отрицательная нумерация, голубой цвет). \n",
      "Что же произошло? Отелло получил от Эмилии некоторую дополнительную информацию, но он даже не проверил, правдива ли она, а ведь Эмилия могла тоже обманывать, и холодный анализ требует построения \"волны\" доказательств/опровержений и для фактов, предоставленных Эмилией. Но мавр опять поверил. Да, в соответствии с той реальностью, которую предоставил нам Шекспир, в этот раз он не ошибся. Но, в жизни могло быть все не так, и на самом деле именно Яго мог бы быть истинным положительным героем, преданным и честным, попавшем в паутину интриги.\n",
      "Итак, мы видим важную вещь: в основе принятия решения в обоих случаях лежал иррациональный элемент. Мавр должен был сделать выбор — кому верить, а кому нет (взаимоисключающий выбор). И это также отражено на волновом графе. Существуют особые вершины, которые мы будем называть опорными утверждениями (опорными вершинами или просто опорными точками), и эти вершины действительно являются опорой для восприятия ситуации, и вера в то истинны или ложны эти утверждения способны перевернуть картину мира. \n",
      "Опорные утверждения — это минимальный набор самосогласованных утверждений (в идеале одно), в которые человек верит, что позволяет графу сойтись. В зависимости от того, с каким знаком эта вера (верит ли он, что эти/это утверждение истинно или ложно) граф сходится либо с положительным, либо с отрицательным результатом. На полном волновом графе (см. определение ниже) эти вершины являются частью неразрешимых циклов (скрытая информация).\n",
      "Например, как мы видели в случае Отелло, было несколько опорных утверждений, связанных с доверием Яго, доверием Дездемоне и доверием служанке Эмилии. Согласованность заключалась в том, что, чтобы граф сошелся недостаточно было верить Яго, нужно было также не верить Дездемоне и Эмилии и, наоборот.\n",
      "Пример 1\n",
      "\r\n",
      "Опорные утверждения явно постулируются в некоторых мировоззрениях. Так, например, в религии вера является явно декламируемым требованием. Это может быть вера авторитетам, священным текстам и в некоторые концепции (столпы веры). Конечно, в каждой религии свой набор опорных элементов. Мировоззрение, близкое к религиозному, встречается не только во всевозможных сектах, но также и в некоторых формах государственного управления и даже в некоторых вполне коммерческих компаниях.\n",
      "\n",
      "Замечание\n",
      "\r\n",
      "Мы никак не обсуждаем вопрос истинности или ложности этих концепций здесь. Удивительно, но иногда человек настолько ревностно оберегает свои опорные утверждения, что, защищая их, в попытке устранения явных противоречий с действительностью, к которым приводит ложная картина мира, основанная на ложных постулатах, может свято верить даже в самые невероятные вещи и не видеть или отрицать очевидное. \n",
      "Пример 2\n",
      "\r\n",
      "Вторым примером, который я бы хотел привести, является наука. Здесь тоже есть явно формулируемые опорные утверждения, например, математические аксиомы или фундаментальные законы физики. Принципиальное отличие от предыдущего примера заключается в том, что эти опорные утверждения постоянно проверяются на истинность, и, если наблюдается противоречие с экспериментом, то эти законы должны быть изменены таким образом, чтобы это противоречие устранить. Но все же эти утверждения продолжают быть опорными точками, потому что в действительности у них нет никакого логического обоснования (доказательства). Можно сказать, что ученые таким образом формулируют опорные точки самой природы.\n",
      "\n",
      "Причинный анализ\n",
      "На основе волнового графа мы можем построить \"обратный\" граф — с началом в опорной точке (или опорных точках) и по направлению к корневому утверждению. При этом мы могли бы использовать только одну связь — \"поэтому правда то, что\". Этот граф можно получить из волнового, если следовать по волновым логическим цепочкам от опорного утверждения в сторону корневого, но только по утверждениям, находящимся в той же фазе, что и опорное. Таким образом, мы также получаем последовательность утверждений, которые мы будем называть причинными логическими цепочками. В зависимости от того, верим ли мы в истинность или ложность опорного утверждения мы получаем два разных причинных графа (разные фазы волны).\n",
      "Пример\n",
      "\r\n",
      "Вернемся к нашему примеру с Отелло.\n",
      "\n",
      "\r\n",
      "Рассмотрим два утверждения -14 (\"Яго лжет\") и его дополнение +23 (\"Яго говорит правду\"). \n",
      "\r\n",
      "Если утверждение +23 становится опорным (Отелло верит, что Яго говорит правду), то это опорное утверждение становится корнем для причинных логических цепочек, приводящих к тому, что Отелло начинает считать, что \"Дездемона изменила Отелло\". Например, цепочка +23 -> +13 ->+3 ->+0 (обратите внимание, что все вершины желтого цвета), выделенная желтыми стрелками на графе выше, означает, что\n",
      "\r\n",
      "\"Яго говорит правду, поэтому правда то, что Кассио сам лично рассказывал о связи с Дездемоной, поэтому это правда, что Дездемона изменила Отелло\".\n",
      "\r\n",
      "В случае же если -14 является опорным (\"Яго лжет\"), то фаза меняется на противоположную. Рассмотрим цепочку -14 -> -6 ->0 (все вершины голубые):\n",
      "\r\n",
      "\"Яго лжет, поэтому правда то, что мавр слышал лишь часть диалога и неверно интерпретировал этот диалог, поэтому Дездемона не изменяла Отелло\".Из вышеприведенного примера видно, что логическое обоснование здесь слабое, скорее напоминающее косвенные (а не прямые) связи волнового анализа. Так, например, то, что Кассио рассказывал о связи с Дездемоной (даже, если бы это было правдой) не следует, что Дездемона действительно изменила с ним Отелло. Это является следствием того, что в данном примере мы движемся вверх лишь по одной волне. Нужно учитывать все волны, что делает логику довольно сложной и, похоже, без критического восприятия и анализа наш мозг избирает более легкий путь движения вверх лишь по одной (или нескольким из многих) волновой цепочке, что делает логику уязвимой.\n",
      "\n",
      "Аксиома волнового анализа\n",
      "Введем понятия полного и абсолютно полного волновых графов. \n",
      "Абсолютно полный волновой граф — это граф, на котором учтены все возможные волны. \n",
      "Это значит, что волны этого графа опускаются в том числе и до фундаментальных вопросов, связанных с мирозданием, нашим восприятием, вопросам бытия… Например, простое утверждение \"Я вижу стол\" порождает волны о реальности/иллюзорности нашего мира, объективности/субъективности восприятия, о моем психическом состоянии, ...\n",
      "Кажется довольно очевидным, что в силу погружения в подобные фундаментальные проблемы абсолютно полный волновой граф всегда содержит скрытую информацию, и подобные графы принципиально неразрешимы. Мы всегда упираемся в некоторые вещи, которые мы должны принять на веру (а чаще всего мы о них просто не думаем, воспринимая, как что-то очевидное).\n",
      "Абсолютно полный граф — это скорее теоретическая концепция. На практике нет необходимости рассматривать все подобные \"волны\". Нужно следовать контексту беседы и не вносить излишней сложности и детализированности. Например, нет смысла в политических спорах обсуждать философско-теологические вопросы. Разумно принять некоторые вещи как данность в контексте обсуждаемой темы.\n",
      "Если мы исключим не укладывающиеся в контекст беседы волны, то таким образом получим полный волновой граф.\n",
      "Замечание\n",
      "\r\n",
      "Возникает вопрос, где же проходит эта граница контекста беседы? На каком уровне глубины анализа разумно остановиться? Что мы готовы принять за аксиомы (и, следовательно, на веру) в нашем анализе? Предположим, что нам удалось найти разумный уровень глубины анализа, соответствующий обсуждаемой проблеме, что значит, что мы имеем консенсус по поводу истинности/ложности неких атомарных утверждений. Но существует еще много точек сокрытия информации:\n",
      "\n",
      "мы не можем точно сказать, что думает другой человек, и говорит ли он правду\n",
      "мы не можем доверять восприятию человека: часто оно подводит\n",
      "мы физически воспринимаем мир только из определенной точки, и, даже если мы свидетели чего-то, то видим/слышим лишь часть события\n",
      "у людей плохая память\n",
      "иногда люди намеренно лгут\n",
      "иногда группы людей намеренно лгут\n",
      "если мы говорим про электронику, то, проходя через руки людей, данные могут быть искажены\n",
      "если мы говорим про математические алгоритмы, то они могут быть взломаны, или их применение может быть некорректным\n",
      "...\n",
      "\n",
      "Как вывод сформулируем в качестве гипотезы следующее утверждение, которое мы будем называть основной аксиомой волнового анализа:\n",
      "\n",
      "всегда есть скрытая информация, не позволяющая однозначно разрешить полный волной граф\n",
      "\n",
      "Следствие:\n",
      "\n",
      "если произошло разрешение волнового графа, то это говорит о наличии иррационального элемента, ошибке логики или о неполноте волнового графа\n",
      "\n",
      "Это не математическое утверждение, скорее психологическое.\r\n",
      "Идея данной аксиомы заключается в том, что у нас всегда есть волны, которые \"доходят\" до скрытой информации. Не всегда уместно их исследовать, но всегда можно найти тот уровень, где появятся скрытые данные. Зачем это делать? А зачем формулировать вопрос, с которым все согласны? Темы поднимаются лишь тогда, когда требуется анализ, что предполагает искать точки сомнения, и такие точки в принципе всегда есть. \n",
      "\n",
      "Пояснение на примерах\n",
      "Хотя мы и пользуемся словом аксиома, но здесь нет математической строгости, прежде всего в силу неопределенности понятия полноты, которое мы ввели выше. В основе этого понятия лежит соответствие контексту, которое мы никак не определили. Давайте рассмотрим 2 примера, которые возможно прояснят идею.\n",
      "Пример 1. Заседание суда. Алиса утверждает, что была 1-го апреля 2020 года в 6 вечера дома. И суду надо понять не обманывает ли Алиса. В контексте данного анализа нас интересует информация свидетелей. Например, Боб и Ева были в этот день у нее гостях и подтверждают это. Здесь полный граф предполагает, как минимум, следующие волны \n",
      "\n",
      "возможно они обманывают (нужно рассмотреть все возможные причины)\n",
      "возможно им лишь казалось, что это было 6 часов (Алиса перевела стрелки часов, например)\n",
      "возможно, как раз ровно в 6 Алиса незаметно для гостей отлучилась \n",
      "\n",
      "Судья в конце концов должен или поверить, или нет в то, что свидетели говорят правду, а также в то, что они адекватно восприняли ситуацию, в которой находились. Без этой (или подобной) веры граф не получится разрешить. То есть обычно, в хоть сколько-нибудь сложных ситуациях, мы должны доверять или не доверять источникам информации, иначе граф не разрешится. Но, в данном случае, рассуждения о природе нашего восприятия и иллюзорности/реальности мира, а также о других вечных темах явно являются излишними.\n",
      "Пример 2. Теперь возьмем что-то простое, например, утверждение \"Я вижу стол\". Итак, раз я сформулировал это утверждение и пытаюсь произвести анализ его истинности, то в силу \"элементарности\" (как известно нет ничего сложнее, чем элементарное) я должен изменить уровень глубины нашего исследования. Теперь общефилософские вопросы как раз уместны, и можно порассуждать о том, насколько этот мир реален, насколько я сам адекватен, что значит \"вижу\", что такое \"настоящее\"… То есть когда мы говорим о чем-то элементарном, то в действительности наш граф становится (или приближается) к абсолютно полному графу, и является неразрешимым (пока мы что-то не примем на веру, как данность, как аксиому).\n",
      "\n",
      "Говорит ли введенная аксиома о том, что нельзя докопаться до истины? \n",
      "Нет, не говорит, но говорит о том, что всегда есть утверждения, которые невозможно обосновать логически, и если нам нужно разрешить граф, то мы должны сделать иррациональный выбор. Но это не значит, что это обязательно ошибка. \n",
      "Просто к логике, к знанию, к информации, нужно добавить что-то еще. \n",
      "Я бы назвал это что-то адекватностью. О сложности и неоднозначности этого понятия мы уже подробно говорили в этой статье, но в любом случае важным компонентом адекватности является также эмоциональная зрелость и развитое чувство гармонии. Можно прекрасно владеть информацией, данными, логическим мышлением и быть при этом безумцем с искаженным восприятием. \n",
      "Замечание\n",
      "\r\n",
      "Мы думаем, мы верим, что мы рассуждаем объективно и рассудочно и приходим к определенным выводам часто в полной уверенности в своей правоте, и иногда испытываем настоящий шок (как было со мной) от того, что другие люди, близкие, умные, эрудированные, приходят к абсолютно противоположному выводу, на основе тех же данных, той же информации. Почему? Если допустить, что основная аксиома волнового анализа верна, то становится понятно почему.\n",
      "\r\n",
      "Тот факт, что мы пришли к выводу и особенно то, что мы уверены в своей правоте говорит о том, что мы во что-то поверили. Тоже самое и с нашими оппонентами, но их \"вера\" другая. Мы имеем разную веру по поводу опорных точек нашего графа. \n",
      "Типичные ошибки при ведении дискуссии\n",
      "Волновой анализ также позволяет увидеть причины, почему спорить по старинке не эффективно.\n",
      "Приведем список типичных ошибок ведения споров:\n",
      "\n",
      "неполный граф. Спорящие не пытаются создать полный граф, например, принимают информацию без критического анализа, без попытки понять правда или нет, кто ее источник, какие намерения у автора...\n",
      "логические ошибки. Например, косвенные аргументы принимаются за прямые\n",
      "локальность анализа. Граф может быть довольно большим, со сложной топологией. Невозможно без специального инструмента держать в уме всю картину, поэтому спор сосредотачивается только в каком-то очень локальном месте. Обычно в споре у вас нет возможности сделать более чем несколько логических шагов\n",
      "обсуждение несущественного. Вместо концентрации внимания на опорных точках спор ведется на периферии\n",
      "закрытость. Спорящие часто бессознательно \"охраняют\" свои опорные точки и избегают их обсуждения \n",
      "желание доказать правоту. Вместо попытки найти истину, понять, где ты сам можешь ошибаться, обычно спор сводится к желанию победить, применяется психологическое давление, что лишь усиливает наше нежелание действительно анализировать и искать ответы\n",
      "\n",
      "Применение волнового анализа на практике\n",
      "Пример с Отелло содержит около 50 вершин — и это довольно простой анализ. Когда я пытался применить данный подход к реальной ситуации, я имел сотни вершин со множеством связей. Я пользовался и пользуюсь yEd для этого, но все-же это не удобно. Надо также понимать, что каждая вершина — это не просто утверждение, это может быть небольшой статьей с описанием фактов и доказательной базой.\n",
      "Нужно приложение. Вопросы удобства и автоматизации, конечно же, важны, но, что действительно необходимо, так это то, что это приложение должно давать возможность коллективного участия и контроля по принципу википедии. Это позволит максимально приблизить граф к полному волновому графу и избежать логических ошибок.\n",
      "Так что же может дать нам волновой анализ на практике?\n",
      "\n",
      "волновой граф позволяет найти опорные точки. Нет смысла обсуждать все волны, нужно сфокусироваться на опорных точках. До тех пор, пока ваш оппонент будет продолжать верить в то, во что он верит, его картина мира не изменится (также, как и ваша)\n",
      "граф позволит проследить зависимость событий от опорных точек, даже если они расположены достаточно далеко друг от друга \n",
      "Замечание\n",
      "\r\n",
      "Я часто наблюдаю, как люди не могут увязать события между собой, просто потому, что между ними несколько логических шагов. Граф решает эту проблему\n",
      "\n",
      "волновой граф позволит избавиться от логических ошибок\n",
      "это просто экономия времени. После построения волнового графа можно будет не выслушивать и не повторять одно и то же. Если вы видите, что ход вашего рассуждения представляет новую волну, вы можете добавить ее\n",
      "по нескольким утверждениям вашего оппонента вы можете достаточно точно сложить представление о его картине мира и его опорных точка\n",
      "этот подход позволит каждому самому наглядно увидеть, что лежит в основе его картины мира и, возможно, попытаться дополнить свой граф, приближая его к полному\n",
      "\n",
      "Принципы волнового анализа\n",
      "В отличии от абстракций науки в реальной жизни мы часто имеем дело с сильно эмоционально заряженными событиями. Сознательно или бессознательно мы избегаем некоторые темы и придерживаем некоторые аргументы, хотя, возможно, они и вполне логичны. Порой наши споры и обсуждения — это минное поле со множеством табу, и если кто-то по неосторожности инициирует логическую цепочку с \"запрещенным\" оттенком, то сразу становится уязвим. Вас быстро могут причислить к фашистам, врагам, извращенцам, либерастам, расистам… названий для таких изгоев множество. \n",
      "Если мы строим полный волновой граф, а ведь в этом и смысл, то нужно понимать, что все подобные табуированные темы тоже должны быть подняты. Если мы стремимся к адекватному восприятию, то мы не должны бояться подвергать анализу то, во что мы верим. \n",
      "Поэтому в основе анализа несомненно должна лежать свобода в создании любых логических цепочек и… смелость, не только потому, что все мы боимся быть неверно истолкованными и потерять хорошие отношения (или даже приобрести врагов), но и потому, что в принципе никто не застрахован от того, что, подвергнув свои убеждения глубокому и честному анализу вы останетесь тем же человеком!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " От переводчика: данный текст даётся с незначительными сокращениями по причине местами излишней «разжёванности» материала. Автор абсолютно справедливо предупреждает, что отдельные темы покажутся чересчур простыми или общеизвестными. Тем не менее, лично мне этот текст помог упорядочить имеющиеся знания по анализу сложности алгоритмов. Надеюсь, что он будет полезен и кому-то ещё.\r\n",
      "Из-за большого объёма оригинальной статьи я разбила её на части, которых в общей сложности будет четыре.\r\n",
      "Я (как всегда) буду крайне признательна за любые замечания в личку по улучшению качества перевода.\n",
      "\n",
      "Введение\r\n",
      "Многие современные программисты, пишущие классные и широко распространённые программы, имеют крайне смутное представление о теоретической информатике. Это не мешает им оставаться прекрасными творческими специалистами, и мы благодарны за то, что они создают.\n",
      "\r\n",
      "Тем не менее, знание теории тоже имеет свои преимущества и может оказаться весьма полезным. В этой статье, предназначенной для программистов, которые являются хорошими практиками, но имеют слабое представление о теории, я представлю один из наиболее прагматичных программистских инструментов: нотацию «большое О» и анализ сложности алгоритмов. Как человек, который работал как в области академической науки, так и над созданием коммерческого ПО, я считаю эти инструменты по-настоящему полезными на практике. Надеюсь, что после прочтения этой статьи вы сможете применить их к собственному коду, чтобы сделать его ещё лучше. Также этот пост принесёт с собой понимание таких общих терминов, используемых теоретиками информатики, как «большое О», «асимптотическое поведение», «анализ наиболее неблагоприятного случая» и т.п.\n",
      "\r\n",
      "Этот текст также ориентирован на учеников средней школы из Греции или любой другой страны, участвующих в Международной олимпиаде по информатике, соревнованиях по алгоритмам для учащихся и тому подобном. Как таковой, он не требует предварительного знания каких-либо сложных математических моментов и просто даст вам основу для дальнейшего исследования алгоритмов с твёрдым пониманием стоящей за ними теории. Как тот, кто в своё время много участвовал в различных состязаниях, я настоятельно рекомендую вам прочитать и понять весь вводный материал. Эти знания будут просто необходимы, когда вы продолжите изучать алгоритмы и различные передовые технологии.\n",
      "\r\n",
      "Я надеюсь, что это текст будет полезен для тех программистов-практиков, у которых нет большого опыта в теоретической информатике (то, что самые вдохновлённые инженеры-программисты никогда не ходили в колледж, уже давно свершившийся факт). Но поскольку статья предназначена и для студентов тоже, то временами она будет звучать, как учебник. Более того, некоторые темы могут показаться вам чересчур простыми (например, вы могли сталкиваться с ними во время своего обучения). Так что, если вы чувствуете, что понимаете их — просто пропускайте эти моменты. Другие секции идут несколько глубже и являются более теоретическими, потому что студенты, участвующие в соревнованиях, должны разбираться в теории алгоритмов лучше, чем средний практик. Но знать эти вещи не менее полезно, а следить за ходом повествования не так уж сложно, так что они, скорее всего, заслуживают вашего внимания. Оригинальный текст был направлен учащимся старшей школы, никаких особых математических знаний он не требует, поэтому каждый, имеющий опыт программирования (например, знающий, что такое рекурсия) способен понять его без особых проблем.\n",
      "\r\n",
      "В этой статье вы найдёте много интересных ссылок на материалы, выходящие за рамки нашего обсуждения. Если вы — работающий программист, то вполне возможно, что знакомы с большинством из этих концепций. Если же вы просто студент-новичок, участвующий в соревнованиях, переход по этим ссылкам даст вам информацию о других областях информатики и разработки программного обеспечения, которые вы ещё не успели изучить. Просмотрите их ради увеличения собственного багажа знаний.\n",
      "\r\n",
      "Нотация «большого О» и анализ сложности алгоритмов — это те вещи, которые и программисты-практики, и студенты-новички часто считают трудными для понимания, боятся или вообще избегают, как бесполезные. Но они не так уж сложны и заумны, как может показаться на первый взгляд. Сложность алгоритма — это всего лишь способ формально измерить, насколько быстро программа или алгоритм работают, что является весьма прагматичной целью. Давайте начнём с небольшой мотивации по этой теме.\n",
      "\n",
      "Мотивация\r\n",
      "Мы уже знаем, что существуют инструменты, измеряющие, насколько быстро работает код. Это программы, называемые профайлерами (profilers), которые определяют время выполнения в миллисекундах, помогая нам выявлять узкие места и оптимизировать их. Но, хотя это и полезный инструмент, он не имеет отношения к сложности алгоритмов. Сложность алгоритма — это то, что основывается на сравнении двух алгоритмов на идеальном уровне, игнорируя низкоуровневые детали вроде реализации языка программирования, «железа», на котором запущена программа, или набора команд в данном CPU. Мы хотим сравнивать алгоритмы с точки зрения того, чем они, собственно, являются: идеи, как происходит вычисление. Подсчёт миллисекунд тут мало поможет. Вполне может оказаться, что плохой алгоритм, написанный на низкоуровневом языке (например, ассемблере) будет намного быстрее, чем хороший алгоритм, написанный языке программирования высокого уровня (например, Python или Ruby). Так что пришло время определиться, что же такое «лучший алгоритм» на самом деле.\n",
      "\r\n",
      "Алгоритм — это программа, которая представляет из себя исключительно вычисление, без других часто выполняемых компьютером вещей — сетевых задач или пользовательского ввода-вывода. Анализ сложности позволяет нам узнать, насколько быстра эта программа, когда она совершает вычисления. Примерами чисто вычислительных операций могут послужить операции над числами с плавающей запятой (сложение и умножение), поиск заданного значения из находящейся в ОЗУ базы данных, определение игровым искусственным интеллектом (ИИ) движения своего персонажа таким образом, чтобы он передвигался только на короткое расстояние внутри игрового мира, или запуск шаблона регулярного выражения на соответствие строке. Очевидно, что вычисления встречаются в компьютерных программах повсеместно. \n",
      "\r\n",
      "Анализ сложности также позволяет нам объяснить, как будет вести себя алгоритм при возрастании входного потока данных. Если наш алгоритм выполняется одну секунду при 1000 элементах на входе, то как он себя поведёт, если мы удвоим это значение? Будет работать также быстро, в полтора раза быстрее или в четыре раза медленнее? В практике программирования такие предсказания крайне важны. Например, если мы создали алгоритм для web-приложения, работающего с тысячей пользователей, и измерили его время выполнения, то используя анализ сложности, мы получим весьма неплохое представление о том, что случится, когда число пользователей возрастёт до двух тысяч. Для соревнований по построению алгоритмов анализ сложности также даст нам понимание того, как долго будет выполняться наш код на наибольшем из тестов для проверки его правильности. Так что, если мы определим общее поведение нашей программы на небольшом объёме входных данных, то сможем получить хорошее представление и о том, что будет с ней при больших потоках данных. Давайте начнём с простого примера: поиска максимального элемента в массиве.\n",
      "\n",
      "Подсчёт инструкций\r\n",
      "В этой статье для реализации примеров я буду использовать различные языки программирования. Не переживайте, если вы не знакомы с каким-то из них — любой, кто умеет программировать, способен прочитать этот код без особых проблем, поскольку он прост, и я не собираюсь использовать какие-либо экзотические фенечки языка реализации. Если вы — студент-олимпиадник, то, скорее всего, пишите на С++, поэтому у вас тоже не должно возникать проблем. В этом случае я также рекомендую прорабатывать упражнения, используя С++ для большей практики.\n",
      "\r\n",
      "Максимальный элемент массива можно найти с помощью простейшего отрывка кода. Например, такого, написанного на Javascript. Дан входной массив А размера n:\n",
      "\n",
      "var M = A[ 0 ];\n",
      " \n",
      "for ( var i = 0; i < n; ++i ) {\n",
      "    if ( A[ i ] >= M ) {\n",
      "        M = A[ i ];\n",
      "    }\n",
      "}\n",
      "\r\n",
      "Сначала подсчитаем, сколько здесь вычисляется фундаментальных инструкций. Мы сделаем это только один раз — по мере углубления в теорию такая необходимость отпадёт. Но пока наберитесь терпения на то время, которое мы потратим на это. В процессе анализа данного кода, имеет смысл разбить его на простые инструкции — задания, которые могут быть выполнены процессором тотчас же или близко к этому. Предположим, что наш процессор способен выполнять как единые инструкции следующие операции:\n",
      "\n",
      "Присваивать значение переменной\n",
      "Находить значение конкретного элемента в массиве\n",
      "Сравнивать два значения\n",
      "Инкрементировать значение\n",
      "Основные арифметические операции (например, сложение и умножение)\n",
      "\r\n",
      "Мы будем полагать, что ветвление (выбор между if и else частями кода после вычисления if-условия) происходит мгновенно, и не будем учитывать эту инструкцию при подсчёте. Для первой строки в коде выше:\n",
      "\n",
      "var M = A[ 0 ]; \n",
      "\r\n",
      "требуются две инструкции: для поиска A[0] и для присвоения значения M (мы предполагаем, что n всегда как минимум 1). Эти две инструкции будут требоваться алгоритму, вне зависимости от величины n. Инициализация цикла for тоже будет происходить постоянно, что даёт нам ещё две команды: присвоение и сравнение.\n",
      "\n",
      "i = 0;\n",
      "i < n;\n",
      "\r\n",
      "Всё это происходит до первого запуска for. После каждой новой итерации мы будем иметь на две инструкции больше: инкремент i и сравнение для проверки, не пора ли нам останавливать цикл.\n",
      "\n",
      "++i;\n",
      "i < n;\n",
      "\r\n",
      "Таким образом, если мы проигнорируем содержимое тела цикла, то количество инструкций у этого алгоритма 4 + 2n — четыре на начало цикла for и по две на каждую итерацию, которых мы имеем n штук. Теперь мы можем определить математическую функцию f(n) такую, что зная n, мы будем знать и необходимое алгоритму количество инструкций. Для цикла for с пустым телом f( n ) = 4 + 2n. \n",
      "\n",
      "Анализ наиболее неблагоприятного случая\r\n",
      "В теле цикла мы имеем операции поиска в массиве и сравнения, которые происходят всегда:\n",
      "\n",
      "if ( A[ i ] >= M ) { ...\n",
      "\r\n",
      "Но тело if может запускаться, а может и нет, в зависимости от актуального значения из массива. Если произойдёт так, что A[ i ] >= M, то у нас запустятся две дополнительные команды: поиск в массиве и присваивание:\n",
      "\n",
      "M = A[ i ]\n",
      "\r\n",
      "Мы уже не можем определить f(n) так легко, потому что теперь количество инструкций зависит не только от n, но и от конкретных входных значений. Например, для A = [ 1, 2, 3, 4 ] программе потребуется больше команд, чем для A = [ 4, 3, 2, 1 ]. Когда мы анализируем алгоритмы, мы чаще всего рассматриваем наихудший сценарий. Каким он будет в нашем случае? Когда алгоритму потребуется больше всего инструкций до завершения? Ответ: когда массив упорядочен по возрастанию, как, например,  A = [ 1, 2, 3, 4 ]. Тогда M будет переприсваиваться каждый раз, что даст наибольшее количество команд. Теоретики имеют для этого причудливое название — анализ наиболее неблагоприятного случая, который является ни чем иным, как просто рассмотрением максимально неудачного варианта. Таким образом, в наихудшем случае в теле цикла из нашего кода запускается четыре инструкции, и мы имеем f( n ) = 4 + 2n + 4n = 6n + 4. \n",
      "\n",
      "Асимптотическое поведение\r\n",
      "С полученной выше функцией мы имеем весьма хорошее представление о том, насколько быстр наш алгоритм. Однако, как я и обещал, нам нет нужды постоянно заниматься таким утомительным занятием, как подсчёт команд в программе. Более того, количество инструкций у конкретного процессора, необходимое для реализации каждого положения из используемого языка программирования, зависит от компилятора этого языка и доступного процессору (AMD или Intel Pentium на персональном компьютере, MIPS на Playstation 2 и т.п. ) набора команд. Ранее же мы говорили, что собираемся игнорировать условия такого рода. Поэтому сейчас мы пропустим нашу функцию f через «фильтр» для очищения её от незначительных деталей, на которые теоретики предпочитают не обращать внимания.\n",
      "\r\n",
      "Наша функция 6n + 4 состоит из двух элементов: 6n и 4. При анализе сложности важность имеет только то, что происходит с функцией подсчёта инструкций при значительном возрастании n. Это совпадает с предыдущей идеей «наихудшего сценария»: нам интересно поведение алгоритма, находящегося в «плохих условиях», когда он вынужден выполнять что-то трудное. Заметьте, что именно это по-настоящему полезно при сравнении алгоритмов. Если один из них побивает другой при большом входном потоке данных, то велика вероятность, что он останется быстрее и на лёгких, маленьких потоках. Вот почему мы отбрасываем те элементы функции, которые при росте n возрастают медленно, и оставляем только те, что растут сильно. Очевидно, что 4 останется 4 вне зависимости от значения n, а 6n наоборот будет расти. Поэтому первое, что мы сделаем, — это отбросим 4 и оставим только f( n ) = 6n.\n",
      "\r\n",
      "Имеет смысл думать о 4 просто как о «константе инициализации». Разным языкам программирования для настройки может потребоваться разное время. Например, Java сначала необходимо инициализировать свою виртуальную машину. А поскольку мы договорились игнорировать различия языков программирования, то попросту отбросим это значение.\n",
      "\r\n",
      "Второй вещью, на которую можно не обращать внимания, является множитель перед n. Так что наша функция превращается в f( n ) = n. Как вы можете заметить, это многое упрощает. Ещё раз, константный множитель имеет смысл отбрасывать, если мы думаем о различиях во времени компиляции разных языков программирования (ЯП). «Поиск в массиве» для одного ЯП может компилироваться совершенно иначе, чем для другого. Например, в Си выполнение A[ i ] не включает проверку того, что i не выходит за пределы объявленного размера массива, в то время как для Паскаля она существует. Таким образом, данный паскалевский код:\n",
      "\n",
      "M := A[ i ]\n",
      "\r\n",
      "эквивалентен следующему на Си:\n",
      "\n",
      "if ( i >= 0 && i < n ) {\n",
      "    M = A[ i ];\n",
      "}\n",
      "\r\n",
      "Так что имеет смысл ожидать, что различные языки программирования будут подвержены влиянию различных факторов, которые отразятся на подсчёте инструкций. В нашем примере, где мы используем «немой» паскалевский компилятор, игнорирующий возможности оптимизации, требуется по три инструкции на Паскале для каждого доступа к элементу массива вместо одной на Си. Пренебрежение этим фактором идёт в русле игнорирования различий между конкретными языками программирования с сосредоточением на анализе самой идеи алгоритма как таковой.\n",
      "\r\n",
      "Описанные выше фильтры — «отбрось все факторы» и «оставляй только наибольший элемент» — в совокупности дают то, что мы называем асимптотическим поведением. Для f( n ) = 2n + 8 оно будет описываться функцией f( n ) = n. Говоря языком математики, нас интересует предел функции f при n, стремящемся к бесконечности. Если вам не совсем понятно значение этой формальной фразы, то не переживайте — всё, что нужно, вы уже знаете. (В сторону: строго говоря, в математической постановке мы не могли бы отбрасывать константы в пределе, но для целей теоретической информатики мы поступаем таким образом по причинам, описанным выше). Давайте проработаем пару задач, чтобы до конца вникнуть в эту концепцию.\n",
      "\r\n",
      "Найдём асимптотики для следующих примеров, используя принципы отбрасывания константных факторов и оставления только максимально быстро растущего элемента:\n",
      "\n",
      "f( n ) = 5n + 12 даст f( n ) = n. \r\n",
      "Основания — те же, что были описаны выше\n",
      "f( n ) = 109 даст f( n ) = 1.\r\n",
      "Мы отбрасываем множитель в 109 * 1 , но 1 по-прежнему нужен, чтобы показать, что функция не равна нулю\n",
      "f( n ) = n2 + 3n + 112 даст f( n ) = n2\r\n",
      "Здесь n2 возрастает быстрее, чем 3n, который, в свою очередь, растёт быстрее 112 \n",
      "f( n ) = n3 + 1999n + 1337 даст f( n ) = n3\r\n",
      "Несмотря на большую величину множителя перед n, мы по прежнему полагаем, что можем найти ещё больший n, поэтому f( n ) = n3 всё ещё больше 1999n (см. рисунок выше)\n",
      "f( n ) = n + sqrt( n ) даст f( n ) = n \r\n",
      "Потому что n при увеличении аргумента растёт быстрее, чем sqrt( n )\n",
      "\n",
      "\n",
      "Упражнение 1\n",
      "f( n ) = n6 + 3n \n",
      "f( n ) = 2n + 12\n",
      "f( n ) = 3n + 2n\n",
      "f( n ) = nn + n\n",
      "\r\n",
      "Если у вас возникли проблемы с выполнением этого задания, то просто подставьте в выражение достаточно большое n и посмотрите, какой из его членов имеет большую величину. Всё очень просто, не так ли?\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Связанные переменные – одна из главных проблем статического анализа. Данная статья посвящена разбору этой темы и рассказу о том, как разработчики PVS-Studio сражаются с ложными срабатываниями, появившимися из-за различных связей.О чём статья?Команда разработки PVS-Studio постоянно работает над повышением качества анализа. Но мало просто добавить в продукт улучшение – о нём, конечно же, надо рассказать! И сегодня мы решили поведать о том, как связи между переменными мешают статическому анализатору и как C# анализатор PVS-Studio старается их учитывать. Приятного чтения!Немного об анализе потока данныхНачнём немного издалека. Одним из важнейших механизмов, используемых C# анализатором PVS-Studio, является анализ потока данных (data flow analysis). Если не вдаваться в детали, это технология, позволяющая анализатору отслеживать возможные значения переменных. В PVS-Studio анализ потока данных тесно взаимодействует с другими технологиями, о которых можно прочитать здесь.Числовые и логический типыВозможности анализа потока данных проще всего продемонстрировать на примере числовых и логических переменных:int a = 5;\n",
      "int b = 3;\n",
      "bool flag = a > b;\n",
      "\n",
      "if (flag) // always true\n",
      "{\n",
      "  ....\n",
      "}\n",
      "Анализ потока данных позволяет PVS-Studio рассчитать точное значение flag и сообщить о том, что проверка не имеет смысла, ведь a всегда больше b.Во многих случаях выражения и, в частности, переменные могут иметь любое значение из некоторого множества. Например:void MyFunc(bool flag)\n",
      "{\n",
      "  int a = flag ? 1 : 10;\n",
      "  bool greater = a > 5;\n",
      "\n",
      "  if (greater)\n",
      "    Console.WriteLine(\"a > 5\");\n",
      "\n",
      "  if (a == 5) \n",
      "    Console.WriteLine(\"a = 5\");\n",
      "}\n",
      "В зависимости от значения, которое будет передано в параметр flag, переменная a будет равна 1 или 10. Следовательно, значение переменной greater может быть как true, так и false. Поэтому анализатор не будет считать проверку значения greater бессмысленной.С другой стороны, PVS-Studio точно знает, что a никогда не равно 5. Именно поэтому анализатор выдаст предупреждение:V3022 Expression 'a == 5' is always false.В некоторых случаях \"лишние\" проверки появляются из-за опечаток или логических ошибок. К примеру, иногда программист проверяет значение не той переменной, которой нужно.Null-state анализС переменными ссылочных типов дела обстоят несколько иначе. Здесь анализатор отслеживает возможность равенства null, то есть производит null-state анализ. Каждая переменная ссылочного типа с точки зрения PVS-Studio может быть в одном из 4 состояний:Unknown – когда нет информации о том, может переменная быть равна null или нет. Данное состояние все переменные ссылочных типов имеют по умолчанию;Null – когда переменная точно равна null;NotNull – когда переменная точно не равна null;PotentialNull – когда в некоторых случаях переменная точно равна null.Пример:void TestReferences(bool flag)\n",
      "{\n",
      "  string potentialNullStr = flag ? \"not null\" : null;\n",
      "    \n",
      "  _ = potentialNullStr.GetHashCode();\n",
      "}\n",
      "На момент вызова GetHashCode переменная potentialNullStr может быть равна или не равна null. Разыменование ссылки, потенциально имеющей значение null, может привести к выбрасыванию исключения, поэтому анализатор генерирует соответствующее предупреждение:V3080 Possible null dereference. Consider inspecting 'potentialNullStr'.Что же делать? Самое простое – всё-таки проверить, что переменная не равна null:void TestReferences(bool flag)\n",
      "{\n",
      "  string potentialNullStr = flag ? \"not null\" : null;\n",
      "    \n",
      "  if (potentialNullStr != null)\n",
      "    _ = potentialNullStr.GetHashCode();\n",
      "}\n",
      "Анализатор сможет легко вычислить, что в теле оператора if переменная potentialNullStr точно не равна null, а значит, вызов GetHashCode не приведёт к выбрасыванию исключения.Связанные переменныеВсё бы хорошо, но иногда в реальном коде проверки на null производятся более изощрённым способом. И нет, речь не про null-условный оператор – его поддержка является более-менее тривиальной задачей. В самом простом случае нам достаточно просто не ругаться, если доступ к члену осуществляется через \"?.\". Реальной же сложностью является именно проверка на null с помощью связанной переменной.Чтобы лучше понять тему, давайте вернёмся к ранее приведённому примеру:public void TestReferences(bool flag)\n",
      "{\n",
      "  string potentialNull = flag ? \"not null\" : null;\n",
      "\n",
      "  if (potentialNull != null)\n",
      "    _ = potentialNull.GetHashCode();\n",
      "}\n",
      "В переменную potentialNull может быть записан null. Однако перед разыменованием есть проверка, и анализ потока данных это учитывает. Но что если проверка на null производится неявно?public void TestReferences(bool flag)\n",
      "{\n",
      "  string potentialNull = flag ? \"not null\" : null;\n",
      "\n",
      "  if (flag)\n",
      "    _ = potentialNull.GetHashCode();\n",
      "}\n",
      "С точки зрения статического анализатора значение flag неизвестно, а значит, в potentialNull может быть записан null. Дальнейшая проверка не даёт никакой информации о potentialNull, ведь эта переменная в условии даже не используется. Соответственно, анализатор предупредит о потенциальном разыменовании нулевой ссылки.На самом же деле если flag = true, то и в potentialNull записана строка. Проверки на равенство null как бы нет, но никакого разыменования нулевой ссылки тут быть не может.Связи могут быть построены множеством способов. Ранее мы рассмотрели пример с переменными логического и ссылочного типов. Однако в общем случае что угодно может зависеть от чего угодно. К примеру, ниже представлена связь двух переменных ссылочных типов:public void RelatedVariables2(string param)\n",
      "{\n",
      "  string? potentialNull = param != null ? \"not null\" : null;\n",
      "\n",
      "  if (param != null)\n",
      "  {\n",
      "    _ = potentialNull.GetHashCode();\n",
      "  }\n",
      "}\n",
      "Переменная potentialNull равна null только в случае, если param равен null. Иначе говоря, либо обе переменные равны null, либо обе не равны. Соответственно, вызов GetHashCode здесь никогда не приведёт к выбросу исключения.Но что-то я зациклился на переменных ссылочных типов. Давайте рассмотрим другой пример:public void RelatedVariables3(int a, int[] array)\n",
      "{\n",
      "  int b = 0;\n",
      "  int index = -1;\n",
      "\n",
      "  if (a == 0)\n",
      "  {\n",
      "    b = 10;\n",
      "    index = 1;\n",
      "  }\n",
      "\n",
      "  if (b > 0)\n",
      "  {\n",
      "    _ = array[index];\n",
      "  }\n",
      "}\n",
      "Взгляните на этот код и скажите – может ли здесь произойти попытка обращения к элементу с индексом -1?Пожалуй, в таком примере даже человек не сразу разберётся. Переменная index не может быть равна -1, если b > 0. Ведь b > 0, только если a = 0, а если a = 0, то и index = 1. Надеюсь, вы не запутались :).Приведённые примеры являются синтетическими, и на реальном коде такое встречается не так часто. Тем не менее, пользователи анализатора время от времени сообщают о false positive срабатываниях, причиной которых является связанность переменных. К примеру, недавно нам написали о проблеме обработки кода следующего вида:public void Test()\n",
      "{\n",
      "  var a = GetPotentialNull();\n",
      "  bool z = a != null;\n",
      "\n",
      "  if (z)\n",
      "  {\n",
      "    _ = a.GetHashCode(); // <=\n",
      "  }\n",
      "}\n",
      "Увы и ах, раньше анализатор совершенно бессовестно лгал о возможном разыменовании нулевой ссылки!Это, конечно, не катастрофа. Ложные срабатывания в общем случае неизбежны, но анализатор предоставляет различные возможности для работы с ними. Самое простое, что можно сделать – отметить предупреждение как ложное, чтобы оно не мозолило глаза. Подробнее об этом можно прочитать здесь.Тем не менее, мы ведём беспрестанную борьбу с ложными срабатываниями, чтобы пользователи не тратили своё время на их изучение. Эта тема, кстати, хорошо раскрыта в статье \"Как и почему статические анализаторы борются с ложными срабатываниями\". Рекомендую к прочтению :).Ты не туда воюешь!Возможно, вам покажется, что мне не стоило всё это рассказывать. Ведь я говорю о слабостях статического анализа! Выглядит так, будто я играю не за ту команду :).Вот только на самом деле это совсем не так. Подобные статьи в первую очередь посвящены развитию анализатора, рассказу о том, как мы смогли сделать продукт лучше. А любое развитие начинается именно с признания проблем. Анализатор работает неидеально? Да. Иной раз не ругается там, где нужно, а иной раз выдаёт ложные срабатывания? Бывает. Но мы действительно работаем над этим. Клиенты пишут нам о проблемах, которые их волнуют больше всего, и мы делаем всё, чтобы сделать PVS-Studio лучше.Статьи же подобного плана позволяют нам рассказать миру о достигнутых успехах :). Кстати, о них...PVS-Studio и связанные переменныеРазнообразие возможных связей переменных поражает, и их поддержка – задача весьма нетривиальная. Однако бороться с ложными срабатываниями нужно, поэтому мы решили постепенно покрывать наиболее распространённые способы связей.Перед тем как начнём, небольшое отступление.Многие фрагменты, представленные в этой статье, являются синтетическими. Они могут казаться странными, и вообще \"непонятно, кто такое пишет\", но, поверьте, все примеры основаны на реальном коде. Примеры являются максимально простыми, но при этом позволяют воспроизвести то или иное поведение анализатора.Как разработчики PVS-Studio, мы очень благодарны пользователям за то, что они пишут нам о проблемах, с которыми столкнулись (в том числе и о ложных срабатываниях). Но ещё больше мы рады, когда нам присылают именно такие простые примеры кода, на которых некорректное поведение легко воспроизводится. Это невероятно ускоряет процесс внесения нужных исправлений :).Эвристический алгоритмПервым решением, позволившим избавиться от большого количества ложных срабатываний, стал специальным образом подобранный алгоритм. Он частично исключает предупреждения, появляющиеся из-за неявных связей различных значений с переменными ссылочных типов.Изучая ложные срабатывания, мы заметили интересную закономерность. Если разыменование производится в теле условной конструкции, то null-state соответствующей переменной, скорее всего, связан с выражением в условии. Иначе говоря, разыменование, производимое под условием, обычно было безопасным, так как соответствующая ссылка неявно проверялась с помощью связанной переменной.Приведём пример:void Test(bool condition)\n",
      "{\n",
      "  object a;\n",
      "  if (condition)\n",
      "    a = new object();\n",
      "  else\n",
      "    a = null;\n",
      "\n",
      "  ....\n",
      "\n",
      "  if (condition)\n",
      "    _ = a.ToString();\n",
      "}\n",
      "Так как разыменование переменной a производится в теле условной конструкции, анализатор как бы предполагает наличие связи между a и условием. За счёт этого PVS-Studio не будет выдавать предупреждение. В данном случае срабатывание на вызов ToString действительно было бы ложным, так как если condition = true, то a не равно null.В таком виде алгоритм отсекал слишком много хороших срабатываний, поэтому мы стали думать над доработками. Наилучших результатов удалось добиться, добавив дополнительное условие исключения: установка null должна быть в том же методе, в котором производится разыменование. Именно в этих случаях null-state обычно связан с условием.Приведём пример, в котором null может быть получен из другого метода:bool _flag;\n",
      "\n",
      "object GetPotentialNull() => _flag ? \"not null\" : null;\n",
      "\n",
      "void Test(bool condition)\n",
      "{\n",
      "  object a = GetPotentialNull();\n",
      "\n",
      "  if (condition)\n",
      "    _ = a.ToString();\n",
      "}\n",
      "Переменная a действительно разыменовывается под условием, однако никакой связи между ней и condition нет. Данная эвристика позволила \"спасти\" многие хорошие срабатывания, хотя и добавила немного ложных.Долгое время данный алгоритм был основным средством против связанных переменных. С его помощью и сейчас получается убирать значительную часть ложных срабатываний на коде реальных проектов. И всё же результаты работы такого исключения неидеальны: иной раз отсекаются хорошие срабатывания, а иногда напротив – \"пропускаются\" ложные. И если потеря пары хороших срабатываний является не столь критичной проблемой, то с ложными нужно что-то делать.Не такое уж и бессмысленное присваиваниеВообще клиенты обычно не пишут о том, что нам стоит \"поддержать связанные переменные\". Это даже звучит очень абстрактно! Как пользователи, они заинтересованы именно в качественном выводе PVS-Studio, а как и что там внутри работает – это не так важно. Поэтому и пишут нам о конкретных ложных срабатываниях, выданных анализатором. Мы же уже разбираемся, в чём именно проблема, и стараемся её решить.И вот в один прекрасный день пишет нам пользователь о предупреждении, выданном на код следующего вида:static void Foo()\n",
      "{\n",
      "  Holder h = new Holder();\n",
      "  Parameter p = h.GetParam();\n",
      "\n",
      "  p.Text = \"ABC\"; // <=\n",
      "  h.f();\n",
      "  p.Text = \"XYZ\"; // <=\n",
      "  h.f();\n",
      "}\n",
      "V3008 The 'p.Text' variable is assigned values twice successively. Perhaps this is a mistake. Check lines: 35, 33.Предупреждение говорит о бессмысленности первого присваивания – значение \"ABC\" никак не используется. Что-то тут не так, код нужно изучить и поправить...А вот и нет! Присваивание вовсе не бессмысленное. Но как же так? Первая мысль, которая может возникнуть, – нужно посмотреть, что это за свойство такое – Text. Может быть, присваивание этому свойству на что-то влияет? Ничего подобного:class Parameter\n",
      "{\n",
      "  internal string Text { get; set; }\n",
      "}\n",
      "Обычное автосвойство. Присваивание ему значения не приводит к выполнению каких-то особых действий. Соответственно, присваивать ему значение 2 раза подряд... Несколько странно. Однако срабатывание всё же ложное.Чтобы наконец стало понятно, в чём же тут дело, следует взглянуть на класс Holder:class Holder\n",
      "{\n",
      "  private Parameter param;\n",
      "  internal Parameter GetParam() \n",
      "  {\n",
      "    return param;\n",
      "  }\n",
      "  \n",
      "  internal Holder() \n",
      "  {\n",
      "    param = new Parameter();\n",
      "    param.Text = \"\";\n",
      "  }\n",
      "  \n",
      "  internal void f()\n",
      "  {\n",
      "    Console.WriteLine(\"Holder: {0}\", param.Text);\n",
      "  }\n",
      "}\n",
      "Оказывается, метод f использует значение свойства param.Text. Зная это, давайте вернёмся к исходному примеру:static void Foo()\n",
      "{\n",
      "  Holder h = new Holder();\n",
      "  Parameter p = h.GetParam();\n",
      "\n",
      "  p.Text = \"ABC\";\n",
      "  h.f();\n",
      "  p.Text = \"XYZ\";\n",
      "  h.f();\n",
      "}\n",
      "Фактически в переменную p записывается ссылка на поле param объекта h. При вызове метода f это поле используется – точнее, используется его свойство Text. При первом вызове f в Text записано \"ABC\", а при втором – \"XYZ\". Таким образом, каждое присваивание сыграло свою роль и никакой ошибки тут нет.В данном случае ложное срабатывание вызвано наличием достаточно необычной связи между свойством p.Text и переменной h. Вызов h.f() использует значение, записанное в p.Text, и диагностике нужно это как-то учитывать.Для решения данной проблемы мы решили доработать одно из исключений, которые уже были в диагностике. Суть исключения состоит в предотвращении срабатывания для случаев, когда между двумя присваиваниями объект был использован. Например:void Test()\n",
      "{\n",
      "  int a, x;\n",
      "  a = 10;\n",
      "  x = a; // a is used\n",
      "  a = 20;\n",
      "}\n",
      "Анализатор не срабатывает на таком коде, так как переменная a была использована между присваиваниями ей значений. В отличие от предыдущего случая, здесь переменная a используется явно, поэтому и исключить срабатывание здесь легко. Но что делать, когда использование присвоенного значения производится неявно, при вызове метода? Давайте разбираться:static void Foo()\n",
      "{\n",
      "  Holder h = new Holder();\n",
      "  Parameter p = h.GetParam();\n",
      "\n",
      "  p.Text = \"ABC\";\n",
      "  h.f();        // p.Text is used here\n",
      "  p.Text = \"XYZ\";\n",
      "  h.f();        // and here\n",
      "}\n",
      "Чтобы решить проблему, мы решили доработать правило V3008. Теперь при исследовании кода эта диагностика сохраняет пары потенциально связанных переменных. Если одна из них каким-либо способом используется, то использованной считается и другая. p считается потенциально связанной с h, так как её значение получено при вызове h.GetParam(). В то же время вызов h.f() означает не только использование h, но и потенциальное использование связанной с ней p и её свойств. Поэтому и срабатывание на \"лишнее присваивание\" p.Text больше не генерируется.Реальный пример связиСинтетика – это, конечно, хорошо, но кому она интересна? Да, круто, что анализатор стал лучше работать на выдуманных примерах. Правда толку от этого немного, если люди попросту не пишут код, на котором улучшение заметно. Про оценку анализаторов на синтетических примерах, кстати, есть достаточно яркая заметка. Там про C++, но суть не меняется.У нас же совсем другой случай. Во-первых, правка вносилась в первую очередь по просьбе клиента, то есть как минимум на коде в его проекте от ложных срабатываний мы избавились. Во-вторых, улучшения работы анализатора хорошо заметны и на других реальных проектах. К примеру, давайте взглянем на код из RavenDB, который мы используем для тестирования PVS-Studio:[Fact]\n",
      "public void CRUD_Operations_With_Array_In_Object_2()\n",
      "{\n",
      "  ....\n",
      "  var family = new Family()\n",
      "  {\n",
      "    Names = new[] { \"Hibernating Rhinos\", \"RavenDB\" }\n",
      "  };\n",
      "  newSession.Store(family, \"family/1\");\n",
      "  newSession.SaveChanges();\n",
      "\n",
      "  var newFamily = newSession.Load<Family>(\"family/1\");\n",
      "\n",
      "  newFamily.Names = new[] {\"Hibernating Rhinos\", \"RavenDB\"};   // <=\n",
      "  Assert.Equal(newSession.Advanced.WhatChanged().Count, 0);\n",
      "\n",
      "  newFamily.Names = new[] { \"RavenDB\", \"Hibernating Rhinos\" }; // <=\n",
      "  Assert.Equal(newSession.Advanced.WhatChanged().Count, 1);\n",
      "\n",
      "  newSession.SaveChanges();\n",
      "  ....\n",
      "}\n",
      "V3008 The 'newFamily.Names' variable is assigned values twice successively. Perhaps this is a mistake.Итак, анализатор сообщал, что в newFamily.Names дважды присваивается значение, при этом первое значение никак не используется. И действительно, по коду явного использования не видно. Но что если взглянуть получше?Объект класса Family сохраняется в сессию. На этот момент он содержит имена \"Hibernating Rhinos\" и \"RavenDB\". Затем этот же объект (или, как минимум, объект, содержащий те же значения) из сессии загружается. После этого в него записываются те же самые имена. А далее производится вызов:Assert.Equal(newSession.Advanced.WhatChanged().Count, 0);\n",
      "Очевидно, что эта проверка учитывает записанное ранее значение. Данный тест проверяет, что изменений нет – ведь имена записаны те же самые. Чуть ниже по коду имена меняются местами и опять производится подобная проверка. Там уже ожидается именно наличие изменений. Очевидна связь между вызовами newSession.Advanced.WhatChanged() и newFamily.Names.Получается, выдавать тут предупреждения про \"ненужное\" присваивание не стоит. И знаете что? Теперь PVS-Studio не будет делать этого :). Следовательно, разработчики не будут тратить время на исследование лишних срабатываний.Мы находили и другие примеры исчезновения ложных срабатываний. Однако все они сильно напоминают то, что мы разобрали ранее, поэтому предлагаю перейти к следующему разделу.Связи через оператор asНе успели мы порадоваться победе над ложными срабатываниями, сообщавшими о \"лишних\" присваиваниях, как другой клиент прислал нам новый пример:void Test(object obj)\n",
      "{\n",
      "  if (obj != null)\n",
      "    Console.WriteLine(\"obj is not null\");\n",
      "\n",
      "  string str = obj as string;\n",
      "\n",
      "  if (str != null)\n",
      "    Console.WriteLine(obj.GetHashCode()); // <=\n",
      "}\n",
      "V3125 The 'obj' object was used after it was verified against null.Ну, давайте разбираться.Вначале параметр obj проверяется на равенство null. Получается, что метод предполагает: в obj может быть передана нулевая ссылка. Затем c помощью оператора as производится преобразование obj к типу String, а результат записывается в переменную str.Самое интересное происходит дальше. Если str не равна null, то производится обращение к методу GetHashCode. Однако вызывается он не у str, а у obj! Получается, что тут проверили не ту переменную. Даже если str не равна null, то obj всё ещё остаётся потенциальным null-значением.По крайней мере так может показаться. На самом деле, если str != null, то и obj != null. Почему?Допустим, obj действительно равен null. Тогда первая проверка даст false – ну и ладно. Далее происходит вычисление значения для str. Так как переменная obj равна null, то и в str точно будет null. Из этого следует обратный вывод: если в str не null, то и в obj тоже не null.Круто, конечно, что мы это поняли, но надо бы и анализатор тоже научить. В этом нам помогает существующая реализация Data Flow. Для подходящих выражений из анализируемого кода PVS-Studio создаёт специальные объекты, которые хранят информацию о возможных значениях. Такие объекты мы называем виртуальными значениями. В них также содержатся и вспомогательные данные, широко используемые диагностиками. К примеру, Data Flow отслеживает, является ли значение переменной:результатом вызова FirstOrDefault;потенциально заражённым (подробнее тут);результатом приведения через оператор as;и так далее.Чтобы стало понятно, как именно анализатор начал учитывать связи через оператор as, вернёмся к примеру:void Test(object obj)\n",
      "{\n",
      "  if (obj != null)\n",
      "    Console.WriteLine(\"obj is not null\");\n",
      "\n",
      "  string str = obj as string;\n",
      "\n",
      "  if (str != null)\n",
      "    Console.WriteLine(obj.GetHashCode());\n",
      "}\n",
      "В переменную str записывается результат приведения obj через оператор as. Data Flow запишет в соответствующее виртуальное значение информацию об этом. Данный функционал уже был реализован и активно использовался некоторыми правилами. Одно из них — V3149.В момент обработки условия str != null анализатор вычисляет, что если это выражение истинно, то str точно не равна null. При этом анализатору уже известно, что значение str получено в результате приведения obj через оператор as. Получается, что и значение obj также вполне справедливо может считаться не равным null.Реальные примеры связей через оператор asЧестно говоря, мы даже сами не ожидали такого результата, но пропала целая куча ложных срабатываний. Кто бы мог подумать, что такая вот проверка на null через оператор as встречается столь часто?Issue 1В качестве первого примера приведу фрагмент кода из проекта SpaceEngineers:void Toolbar_ItemChanged(MyToolbar self, MyToolbar.IndexArgs index)\n",
      "{\n",
      "  Debug.Assert(self == Toolbar);\n",
      "    \n",
      "  var tItem = ToolbarItem.FromItem(self.GetItemAtIndex(index.ItemIndex));\n",
      "  ....\n",
      "}\n",
      "V3080 Possible null dereference of method return value when it is passed to method as its 1st argument.Итак, срабатывание говорило о том, что в метод ToolbalItem.FromItem может передаваться null, что приведёт к выбрасыванию исключения. Так ли это?В первую очередь стоит взглянуть на метод GetItemAtIndex:public MyToolbarItem GetItemAtIndex(int index)\n",
      "{\n",
      "  if (!IsValidIndex(index)) \n",
      "    return null;\n",
      "\n",
      "  return this[index];\n",
      "}\n",
      "Анализ потока данных позволил анализатору выяснить, что вызов этого метода в некоторых случаях возвращает null. Но приведёт ли это к проблемам? Давайте теперь перейдём к определению метода FromItem:public static ToolbarItem FromItem(MyToolbarItem item)\n",
      "{\n",
      "  var tItem = new ToolbarItem();\n",
      "  tItem.EntityID = 0;\n",
      "  var terminalItem = item as MyToolbarItemTerminalBlock;\n",
      "  if (terminalItem != null)\n",
      "  {\n",
      "    var block = item.GetObjectBuilder() as ....; // <=\n",
      "    ....\n",
      "  }\n",
      "  ....\n",
      "  return tItem;\n",
      "}\n",
      "Ранее мы видели, что в параметре item может быть записан null. Здесь же производится разыменование, а сам item перед этим не проверяется. Зато проверяется terminalItem! А если terminalItem не равен null, то и item точно не равен null.Issue 2Похожий пример мы обнаружили в проекте SharpDevelop:DocumentScript GetScript(string fileName)\n",
      "{\n",
      "  ....\n",
      "  var formattingOptions\n",
      "       = CSharpFormattingPolicies.Instance\n",
      "                                 .GetProjectOptions(compilation.GetProject());\n",
      "  ....\n",
      "}\n",
      "V3080 Possible null dereference of 'compilation.GetProject()' method return value at 'project.FileName' when it is passed to method as its 1st argument.Итак, анализатор предупреждал о возможном разыменовании нулевой ссылки внутри метода GetProjectOptions. Причиной этого является передача compilation.GetProject() в качестве первого аргумента. Давайте разбираться.Межпроцедурный анализ позволил выяснить, что GetProject действительно иногда возвращает null. Что же насчёт GetProjectOptions? Давайте взглянем:public CSharpFormattingPolicy GetProjectOptions(IProject project)\n",
      "{\n",
      "  if (!initialized)\n",
      "    return GlobalOptions;\n",
      "\n",
      "  var csproject = project as CSharpProject;\n",
      "  if (csproject != null) {\n",
      "    string key = project.FileName;            // <=\n",
      "    ....\n",
      "  }\n",
      "\n",
      "  return SolutionOptions ?? GlobalOptions;\n",
      "}\n",
      "Да, здесь производится обращение к свойству первого аргумента. Однако лишь в том случае, если он не равен null! Просто проверяется не сам project, а результат его приведения через as.Issue 3Ещё одно пропавшее ложное срабатывание выдавалось на код проекта ILSpy:protected override Expression DoResolve (ResolveContext ec)\n",
      "{\n",
      "  var res = expr.Resolve(ec);\n",
      "  var constant = res as Constant;\n",
      "\n",
      "  if (constant != null && constant.IsLiteral)\n",
      "  {\n",
      "    return Constant.CreateConstantFromValue(res.Type,           // <=\n",
      "                                            constant.GetValue(),\n",
      "                                            expr.Location);\n",
      "  }\n",
      "\n",
      "  return res;\n",
      "}\n",
      "V3080 Possible null dereference. Consider inspecting 'res'.res получает своё значение из вызова expr.Resolve(ec). В некоторых случаях он действительно возвращает null. Но в момент обращения к свойству Type переменная уже точно не равна null. Как и ранее, проверка произведена неявно, и если constant != null, то и res тоже.Были и многие другие ложные срабатывания, которые \"ушли\" благодаря поддержке связей через оператор as. Но все они так или иначе напоминают те, что мы здесь уже привели. Если у вас есть желание самостоятельно проверить, как PVS-Studio обрабатывает случаи такого вида, то вы можете бесплатно сделать это, загрузив анализатор по ссылке. Have fun!Типичные связанные переменныеРанее мы разобрали пару видов связей, которые встречались нам не так уж часто. Конечно, прогнав тесты, мы увидели, что новые правки действительно дали ощутимый результат. Однако куда чаще мы находили случаи связей переменных логического и ссылочных типов.Ранее я уже приводил пример, демонстрирующий такую связь:public void Test()\n",
      "{\n",
      "  var a = GetPotentialNull();\n",
      "  bool flag = a != null;\n",
      "\n",
      "  if (flag)\n",
      "  {\n",
      "    _ = a.GetHashCode(); // <=\n",
      "  }\n",
      "}\n",
      "V3080 Possible null dereference. Consider inspecting 'a'.Если flag = true, то переменная a не может быть равна null. Таким образом неявная проверка защищает от проблем.Чтобы научить анализатор учитывать такие связи, мы вновь решили внести доработки в наш Data Flow. Однако данный случай был несколько сложнее.В отличие от случая с оператором as, здесь понадобилось добавление нового типа информации о переменной. В частности – данных о связи с другой переменной. Обрабатывая объявление flag, анализатор вычисляет возможные значения переменных из выражения в случаях:если выражение (а следовательно, и flag) равно true;если выражение равно false.Обработав объявление flag, анализатор добавляет в соответствующее виртуальное значение 2 правила:если flag == true, то a != null;если flag == false, то a == null.Теперь, когда для flag имеются нужные данные, осталось лишь воспользоваться ими при обработке условия if (flag). Здесь Data Flow вычисляет возможные значения переменных в then-ветке. Соответственно, flag в ней всегда равна true, а связанная с этой переменной a точно не равна null.Мы довольно долго страдали от таких ложных срабатываний, поэтому и приняли решение разобраться с ними. И, похоже, преуспели :). Теперь анализатор отслеживает связи такого вида и учитывает их в работе.На синтетике всё это выглядит здорово, но давайте теперь посмотрим, что там в коде реальных проектов.Типичные связи в реальном кодеТут результат даже круче, чем со связями через оператор as. Что любопытно, правка позволила не только избавиться от ложных срабатываний, но и добавить несколько \"истинных\".Issue 1Для начала рассмотрим довольно простое ложное срабатывание на коде проекта BouncyCastle.public static Stream ReplaceSigners(....)\n",
      "{\n",
      "  ....\n",
      "\n",
      "  CmsTypedStream signedContent = parser.GetSignedContent();\n",
      "  bool encapsulate = (signedContent != null);\n",
      "  Stream contentOut = gen.Open(outStr,\n",
      "                               parser.SignedContentType.Id,\n",
      "                               encapsulate);\n",
      "  if (encapsulate)\n",
      "  {\n",
      "    Streams.PipeAll(signedContent.ContentStream, contentOut); // <=\n",
      "  }\n",
      "\n",
      "  ....\n",
      "}\n",
      "V3080 Possible null dereference. Consider inspecting 'signedContent'.Пропавшее ложное срабатывание говорило о возможном разыменовании нулевой ссылки. Если signedContent будет равно null, то обращение к свойству ContentStream приведёт к выбрасыванию исключения.Но обратите внимание на проверку значения encapsulate. Именно она неявно защищает от разыменования нулевой ссылки, ведь encapsulate = true только тогда, когда signedContent != null. Последние правки \"научили\" PVS-Studio учитывать такие связи, поэтому ложное срабатывание и пропало.Issue 2Следующий пример взят из проекта ccnet:public bool Authenticate(LoginRequest credentials)\n",
      "{\n",
      "  // Check that both the user name and the password match\n",
      "  string userName = GetUserName(credentials);\n",
      "  string password = NameValuePair.FindNamedValue(....);\n",
      "  \n",
      "  bool isValid =    !string.IsNullOrEmpty(userName)\n",
      "                 && !string.IsNullOrEmpty(password);\n",
      "\n",
      "  if (isValid)\n",
      "  {\n",
      "    isValid =    SecurityHelpers.IsWildCardMatch(userName,     // <=\n",
      "                                                 this.userName)\n",
      "              && ....;\n",
      "  }\n",
      "\n",
      "  return isValid;\n",
      "}\n",
      "V3080 Possible null dereference inside method at 'wildCard.Replace'. Consider inspecting the 1st argument: userName.Это предупреждение говорило о том, что в метод IsWildCardMatch в качестве первого аргумента потенциально передаётся нулевая ссылка, а также о том, что внутри может произойти её разыменование. Соответственно, возможна ситуация, когда будет выброшено исключение типа NullReferenceException. Но так ли это на самом деле?Значение первого аргумента – userName — приходит из вызова GetUserName. И оттуда действительно может приходить null, что и обнаружил анализатор. Да и в методе IsWildCardMatch действительно производится разыменование первого аргумента:public static bool IsWildCardMatch(string wildCard, string value)\n",
      "{\n",
      "  Regex wildCardRegex = new Regex(wildCard.Replace(\"*\",\n",
      "                                                   \"[a-zA-Z0-9_.@-]*\"),\n",
      "                                  RegexOptions.IgnoreCase);\n",
      "\n",
      "  return wildCardRegex.IsMatch(value);\n",
      "}\n",
      "Вот только null туда не передаётся! Наверняка вы уже заметили проверку значения isValid здесь:bool isValid =    !string.IsNullOrEmpty(userName)\n",
      "               && !string.IsNullOrEmpty(password);\n",
      "\n",
      "if (isValid)\n",
      "{\n",
      "  isValid =    SecurityHelpers.IsWildCardMatch(userName,\n",
      "                                               this.userName)\n",
      "            && ....;\n",
      "}\n",
      "Если isValid = true, то и userName никак не может быть null. Благодаря поддержке связей теперь и анализатор знает об этом.Issue 3Ещё одно интересное ложное срабатывание выдавалось на код из проекта FlashDevelop:public void HandleEvent(Object sender, NotifyEvent e, HandlingPriority priority)\n",
      "{\n",
      "  ....\n",
      "  features = enabledLanguages.ContainsKey(ext) ? enabledLanguages[ext] : null;\n",
      "  \n",
      "  if (completion == null)\n",
      "    completion = new Completion(config, settingObject);\n",
      "\n",
      "  completion.OnFileChanged(features);                      // <=\n",
      "\n",
      "  if (features != null && features.Syntax != null)\n",
      "    ....\n",
      "  ....\n",
      "}\n",
      "V3080 Possible null dereference inside method at 'features.Mode'. Consider inspecting the 1st argument: features.Срабатывание говорило о том, что переменная features, потенциально имеющая значение null, передаётся в метод OnFileChanged, что может привести к разыменованию нулевой ссылки.По коду хорошо видно, что в features в некоторых случаях записывается null, а ниже присутствует и соответствующее условие. Однако перед передачей в метод OnFIleChanged переменная никак не проверяется – нет даже какой-нибудь неявной проверки через связь.Так почему же поддержка связей привела к исчезновению этого срабатывания? Ответ находится в коде метода OnFileChanged:internal void OnFileChanged(CssFeatures features)\n",
      "{\n",
      "  if (features == this.features) return;\n",
      "  this.features = features;\n",
      "  enabled = features != null;               // <=\n",
      "\n",
      "  if (enabled)\n",
      "  {\n",
      "    wordChars = lang.characterclass.Characters;\n",
      "    if (features.Mode != \"CSS\") wordChars += features.Trigger;\n",
      "    InitBlockLevel();\n",
      "  }\n",
      "}\n",
      "А вот и связанные переменные! Разыменование features производится только в случае, если enabled = true, а это возможно лишь в случае, когда features != null. Таким образом, срабатывание действительно было ложным.Issue 4Как я упоминал ранее, отслеживание таких связей позволяет не только избавляться от ложных срабатываний, но и генерировать корректные предупреждения.Например, можно рассмотреть следующий фрагмент кода из Roslyn:public override object GetFunctionExtender(string name,\n",
      "                                           SyntaxNode node,\n",
      "                                           ISymbol symbol)\n",
      "{\n",
      "  ....\n",
      "  \n",
      "  var methodSymbol = (IMethodSymbol)symbol;\n",
      "  isDeclaration = methodSymbol.PartialDefinitionPart == null;\n",
      "  hasOtherPart = isDeclaration\n",
      "                    ? methodSymbol.PartialImplementationPart != null\n",
      "                    : methodSymbol.PartialDefinitionPart != null;    // <=\n",
      "    \n",
      "  ....\n",
      "}\n",
      "V3022 Expression 'methodSymbol.PartialDefinitionPart != null' is always true.Итак, научившись отслеживать связи соответствующего типа, PVS-Studio сформировал предупреждение о наличии в коде логического выражения, которое всегда возвращает true. Откуда взялось такое мнение?Как и раньше, логика здесь проста. isDeclaration будет иметь значение true только в том случае, если methodSymbol.PartialDefinitionPart будет равным null. С другой стороны, если isDeclaration равен false, то и methodSymbol.PartialDefinitionPart точно не равно null.Таким образом, последнее выражение тернарного оператора всегда будет иметь значение true. Иной раз always-true выражения представляют собой безобидный избыточный код, в других же случаях — свидетельствуют об ошибках. Иногда подобный код и правда пишут для улучшения читаемости. Какой именно случай здесь – сказать сложно.Если ошибки тут нет, то код, в принципе, можно было бы написать чуть проще:hasOtherPart =    !isDeclaration\n",
      "               || methodSymbol.PartialImplementationPart != null;\n",
      "С другой стороны, это лишь моё мнение и для кого-то проще будет именно тот код, который есть сейчас.ЗаключениеПеременные могут быть связаны огромным количеством способов, поэтому и поддержать их все достаточно проблематично, если вообще возможно. Такие связи встречаются не так уж часто, но всё-таки время от времени они приводят к появлению ложных срабатываний. Разработчики PVS-Studio постоянно работают над улучшением анализатора, и поддержка связанных переменных – одно из направлений нашей деятельности. Конечно, наиболее важны для нас именно пожелания наших клиентов. Тем не менее, нам ценна любая обратная связь. Поэтому предлагаю и вам, дорогие читатели, бесплатно попробовать статический анализатор на своих проектах. Уверен, вы не будете разочарованы :).А какие случаи связанности переменных встречали вы? Пишите в комментарии – посмотрим, сколько разных случаев мы сможем собрать.До новых встреч!Если хотите поделиться этой статьей с англоязычной аудиторией, то прошу использовать ссылку на перевод: Nikita Lipilin. PVS-Studio evolution: data flow analysis for related variables.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Материал будет полезен тем, кто осваивает язык R в качестве инструмента анализа табличных данных и хочет увидеть сквозной пример реализации основных шагов обработки.\r\n",
      "Ниже демонстрируется загрузка данных из csv-файлов, разбор текстовых строк с элементами очистки данных, агрегация данных по аналитическим измерениям и построение диаграмм.\r\n",
      "В примере активно используется функциональность пакетов data.table, reshape2, stringdist и ggplot2.\n",
      "\r\n",
      "В качестве «реальных данных» взята информация о выданных разрешениях на осуществление деятельности по перевозке пассажиров и багажа легковым такси в Москве. Данные предоставлены в общее пользование Департаментом транспорта и развития дорожно-транспортной инфраструктуры города Москвы. Страница набора данных data.mos.ru/datasets/655\r\n",
      "Исходные данные имеют следующий формат:\n",
      "ROWNUM;VEHICLE_NUM;FULL_NAME;BLANK_NUM;VEHICLE_BRAND_MODEL;INN;OGRN\n",
      "1;\"А248УЕ197\";\"ООО «ТАКСИ-АВТОЛАЙН»\";\"017263\";\"FORD FOCUS\";\"7734653292\";\"1117746207578\"\n",
      "2;\"А249УЕ197\";\"ООО «ТАКСИ-АВТОЛАЙН»\";\"017264\";\"FORD FOCUS\";\"7734653292\";\"1117746207578\"\n",
      "3;\"А245УЕ197\";\"ООО «ТАКСИ-АВТОЛАЙН»\";\"017265\";\"FORD FOCUS\";\"7734653292\";\"1117746207578\"\n",
      "```\n",
      "\n",
      "1. Загрузка первичных данныхДанные можно загружать непосредственно с сайта. В процессе загрузки сразу переименуем колонки удобным образом.\n",
      "url <- \"http://data.mos.ru/datasets/download/655\"\n",
      "colnames = c(\"RowNumber\", \"RegPlate\", \"LegalName\", \"DocNum\", \"Car\", \"INN\", \"OGRN\", \"Void\")\n",
      "rawdata <- read.table(url, header = TRUE, sep = \";\",\n",
      "             colClasses = c(\"numeric\", rep(\"character\",6), NA),\n",
      "             col.names = colnames,\n",
      "             strip.white = TRUE,\n",
      "             blank.lines.skip = TRUE,\n",
      "             stringsAsFactors = FALSE,\n",
      "             encoding = \"UTF-8\")Теперь можно приступать к анализу и визуализации…\n",
      "\n",
      "2. Преобразование данныхПредположим, что необходимо проанализировать распределение количества зарегистрированных в качестве такси автомобилей, в зависимости от организационной формы лицензиата и от марки автомобиля. Соответствующие данные не выделены отдельно, но вся нужная информация содержится в полях FULL_NAME (переименовано в LegalName) и VEHICLE_BRAND_MODEL (Car).\r\n",
      "В процессе преобразования исходных данных необходимо\n",
      "\n",
      "из поля LegalName выделить организационно-правовую форму в отдельное поле OrgType;\n",
      "из поля Car выделить марку машины в отдельное поле CarBrand;\n",
      "отбросить неиспользуемые поля.\n",
      "Для простоты считаем, что первые слова полей LegalName и Car составляют, соответственно, организационно-правовую форму и марку машины (ниже будет понятно, что делать с исключениями). Ненужные поля будут отброшены автоматически в процессе преобразования data.frame в data.table с явным указанием списка переносимых полей.\n",
      "ptn <- \"^(.+?) (.+)$\" # regexp pattern to match first word\n",
      "dt <- data.table(rawdata)[, \n",
      "            list(RegPlate, LegalName, Car, OGRN,\n",
      "                 OrgType  = gsub(ptn, \"\\\\1\" , toupper( LegalName )),\n",
      "                 CarBrand = gsub(ptn, \"\\\\1\",  toupper( Car       )))                          \n",
      "            ]\n",
      "rm(rawdata) # Clear some memory\n",
      "\n",
      "3. Первые итогиПроверим, какие организационные формы были выделены из данных.\n",
      "sort( table(dt$OrgType) )##    НП   ОАО   ЗАО   ООО    ИП \n",
      "##     1   392   649 17118 17680\n",
      "Данные сформированы вполне корректно: по количеству полученных лицензий лидируют индивидуальные предприниматели (снижение налоговой нагрузки?), есть общества с ограниченной ответственностью, открытые и закрытые акционерные общества и даже одно некоммерческое партнерство.\r\n",
      "Для того, чтобы определить, сколько независимых лицензиатов (а не автомобилей) получили лицензию, в зависимости от организационно-правовой формы, необходимо провести суммирование по полю, уникально характеризующему юридическое лицо (ОГРН).\n",
      "dt[, list( N = length( unique(OGRN) ) ), by = OrgType][order(N, decreasing = TRUE)]\n",
      "##    OrgType     N\n",
      "## 1:      ИП 12352\n",
      "## 2:     ООО   563\n",
      "## 3:     ЗАО    14\n",
      "## 4:     ОАО     6\n",
      "## 5:      НП     1\n",
      "\n",
      "Очистка данныхАвтомобили каких марок используются в качестве такси в Москве? \r\n",
      "В наборе данных представлено немало марок автомобилей: 115, но действительно ли они все уникальные? Для примера выведем все марки, начинающиеся на букву «M».\n",
      "sort( unique( dt[grep(\"^M.*\", CarBrand), CarBrand]))##  [1] \"M214\"                 \"MASERATI\"             \"MAZDA\"               \n",
      "##  [4] \"MAZDA-\"               \"MERCEDES\"             \"MERCEDES-BENZ\"       \n",
      "##  [7] \"MERCEDES-BENZ-\"       \"MERCEDES-BENZ-S500\"   \"MERCEDES-BENZC\"      \n",
      "## [10] \"MERCEDES-BENZE200K\"   \"MERCEDES-BENZE220CDI\" \"MERCEDES-BЕNZ\"       \n",
      "## [13] \"MERCERDES-BENZ\"       \"MERCRDES\"             \"MERCRDES-BENZ\"       \n",
      "## [16] \"MERSEDES-\"            \"MERSEDES-BENZ\"        \"METROCAB\"            \n",
      "## [19] \"MG\"                   \"MINI\"                 \"MITSUBISHI\"\n",
      "К сожалению, большое число марок машин во многом обусловлено ошибками в данных. К примеру, одна и та же марка — MERCEDES-BENZ — встречается под различными именами. Перед анализом данные необходимо очистить.\r\n",
      "Программная основа для очистки текстовой информации — функции поиска «расстояния между строками». Для каждой пары строк они вычисляют метрику, характеризующую трудоемкость преобразования одной строки в другую с помощью операций над буквами. Чем более похожи строки, тем меньше требуется операций. В идеале одинаковые строки должны иметь расстояние, равное нулю, а максимально непохожие — единице. Именно так и работает алгоритм Jaro-Winkler функции stringdist одноименного пакета. \r\n",
      "Сравним несколько строк, только посчитаем не расстояние, а похожесть, 1-stringdist.\n",
      "1 - stringdist( c(\"MERCEDES\",\"MERSEDES\",\"MAZDA\",\"RENAULT\",\"SAAB\"), \"MERCEDES\", method = \"jw\", p = 0.1)## [1] 1.0000 0.9417 0.5950 0.3452 0.0000\n",
      "На первый взгляд задача очистки данных решается просто: для каждой записи достаточно выбрать наиболее похожее значение из справочника. К сожалению, такой подход не всегда работает. Во-первых, справочника может не быть (как в текущем случае). Во-вторых, некоторые ситуации требуют ручной коррекции данных, даже при наличии точного справочника. Например, с точки зрения метода три марки одинаково подходят в качестве альтернативы неверному значению «BAZ»:\n",
      "1 - stringdist(\"BAZ\", c(\"VAZ\", \"UAZ\", \"ZAZ\"), method = \"jw\", p = 0.1)## [1] 0.7778 0.7778 0.7778\n",
      "Ниже использован полуавтоматический метод коррекции, позволяющий существенно облегчить труд специалиста по очистке данных за счет программной генерации вариантов исправлений, с которыми аналитик может либо согласиться, либо вручную поправить. \r\n",
      "Предполагается, что в большом объеме данных с малым количеством ошибок часто встречающиеся значения — корректные, а редко встречающиеся — ошибки. Частоты значений используются в качестве весового коэффициента, пропорционально увеличивая метрику близости строк. Чтобы часто встречающиеся марки машин не выходили вперед за счет количества, а не похожести, учитываются только метрики значений со степенью похожести выше порогового значения t (о выборе t позже). Для каждого возможного значения марки машины, таким образом, определяется рекомендованное «справочное» значение из того же набора данных. Пары «марка — предложенное исправление» выводятся в csv-файл. После анализа и внесения исправлений скорректированный csv-файл загружается и служит словарем.\r\n",
      "Начнем с конструирования функции, возвращающей наилучшее соответствие на имеющемся наборе данных.\n",
      "bestmatch.gen <- function(wc, t = 0){\n",
      "  # wc = counts of all base text words\n",
      "  # t = threshold: only the words with similarity above threshold count\n",
      "          \n",
      "  bestmatch <- function(a){\n",
      "    sim <- 1 - stringdist( toupper(a), toupper( names(wc) ) , method = \"jw\", p = 0.1 )\n",
      "    # Compute weights and implicitly cut off everything below threshold\n",
      "    weights <- sim * wc * (sim > t)\n",
      "    # Return the one with maximum combined weight\n",
      "    names( sort(weights, decr = TRUE)[1] )\n",
      "  }\n",
      "  bestmatch\n",
      "}\n",
      " Пороговое значение t подбирается опытным путем. Вот пример работы функции для порогового параметра t = 0.7.\n",
      "  bm07 <- bestmatch.gen( table( dt$CarBrand), t = 0.7 )\n",
      "  s <- c(\"FORD\",\"RENO\",\"MERS\",\"PEGO\")\n",
      "  sapply(s, bm07)\n",
      "##            FORD            RENO            MERS            PEGO \n",
      "##          \"FORD\"       \"RENAULT\" \"MERCEDES-BENZ\"       \"PEUGEOT\"\n",
      "На первый взгляд, все сработало чудесно. Однако радоваться рано. Хорошо представленные в наборе данных марки машин с похожими названиями могут «перетягивать на себя» другие корректные названия.\n",
      "s <- c(\"HONDA\", \"CHRYSLER\", \"VOLVO\")\n",
      "sapply(s, bm07)\n",
      "##        HONDA     CHRYSLER        VOLVO \n",
      "##    \"HYUNDAI\"  \"CHEVROLET\" \"VOLKSWAGEN\"\n",
      "Попробуем повысить пороговое значение t.\n",
      "bm09 <- bestmatch.gen( table( dt$CarBrand), t = 0.9 )\n",
      "  s <- c(\"HONDA\",\"CHRYSLER\",\"VOLVO\")\n",
      "  sapply(s, bm09)\n",
      "##      HONDA   CHRYSLER      VOLVO \n",
      "##    \"HONDA\" \"CHRYSLER\"    \"VOLVO\"\n",
      "Все в порядке? Почти. Слишком жесткое отсечение непохожих строк приводит к тому, что алгоритм считает некоторые ошибочные значения корректными. Подобные ошибки придется исправить вручную.\n",
      "s <- c(\"CEAT\", \"CVEVROLET\")\n",
      "sapply(s, bm09)\n",
      "##        CEAT   CVEVROLET \n",
      "##      \"CEAT\" \"CVEVROLET\"\n",
      "Теперь все готово для формирования файла словаря уникальных значений марок машин. Так как файл нужно будет править руками, удобно, если в нем будут дополнительные поля, показывающие, отличается ли предложенная замена от исходного значения (это не всегда очевидно), насколько часто встречается название марки, а также метка, привлекающая внимание к записи в зависимости от каких-то статистических характеристик набора. В данном случае мы хотим выловить ситуации, в которых алгоритм предлагает редко встречающиеся (предположительно ошибочные) значения в качестве корректных.\n",
      "ncb <- table(dt$CarBrand)\n",
      "scb <- names(ncb) # Source Car Brands\n",
      "acb <- sapply(scb, bm09) # Auto-generated replacement\n",
      "cbdict_out <- data.table(ncb)[,list(\n",
      "                SourceName = scb,\n",
      "                AutoName = acb,\n",
      "                SourceFreq = as.numeric(ncb),\n",
      "                AutoFreq = as.numeric( ncb[acb] ),\n",
      "                Action = ordered( scb == acb, labels = c(\"CHANGE\",\"KEEP\")),\n",
      "                DictName = acb\n",
      "              )]\n",
      "# Add alert flag\n",
      "# Alert when suggested is a low-frequency dictionary word\n",
      "cbdict_out <- cbdict_out[,\n",
      "                Alert := ordered( AutoFreq <= quantile(AutoFreq, probs = 0.05, na.rm = TRUE),                   \n",
      "                labels = c(\"GOOD\",\"ALERT\"))\n",
      "                ]\n",
      "write.table( cbdict_out[ order(SourceName), \n",
      "                         list( Alert, Action, SourceName, AutoName, SourceFreq, AutoFreq, DictName) ], \n",
      "      \"cbdict_out.txt\", sep = \";\", quote = TRUE, \n",
      "      col.names = TRUE, row.name = FALSE, fileEncoding = \"UTF-8\")\n",
      "Необходимо проверить и отредактировать значения поля DictName и сохранить файл под именем «cbdict_in.txt» для последующей загрузки.\r\n",
      "Анализируемый набор данных имеет особенности, на которые стоит обратить внимание:\n",
      "\n",
      "некоторые строки не содержат марки машины — пусто или «НЕТ», а некоторые модели сложно однознвчно идентицифировать: L1H1, M214; вручную меняем на UNKNOWN или аналогичное псевдо-значение;\n",
      "равноправно применяется два варианта написания: MERCEDES и MERCEDES-BENZ, оставляем одно, MERCEDES-BENZ;\n",
      "есть два визуально одинаковых независимых написания ZAZ (в выводе две строки, и обе ллгоритм предлагает сохранить как верные, Action = KEEP); видимо, где-то вкралась буква с другим кодом UTF-8;\n",
      "некоторые названия машин не содержат марки, а тольмко модель: SAMAND (IRAN KHODRO)\n",
      "неразбериха с марками TAGAZ — VORTEX и JAC; предлагается для простоты присвоить (пусть не совсем корректно) общее название TAGAZ машинам, чьи марки определились как TAGAZ, A21, SUV, SUVT11, VORTEX, JAC.\n",
      " Помимо особенностей данных есть ограничения алгоритма, которые нужно корректировать вручную. \n",
      "алгоритм предлагает некоторые ошибочные названия в качестве корректных альтернатив: CEAT, CVEVROLET;\n",
      "марки состоящие из двух слов, сокрашаются до одного: ALFA (ALFA ROMEO), GREAT (GREAT WALL), IRAN (IRAN KHODRO), LAND (LAND ROVER).\n",
      "Отредактированные данные загружаем из файла cbdict_in.txt.\n",
      "if ( file.exists(\"cbdict_in.txt\")) url <- \"cbdict_in.txt\" else url <- \"cbdict_out.txt\"\n",
      "\n",
      "cbdict_in <- read.table( url, header = TRUE, sep = \";\",\n",
      "                         colClasses = c( rep(\"character\",4), \"numeric\", \"numeric\", \"character\"),\n",
      "                         encoding = \"UTF-8\")\n",
      "\n",
      "cbdict <- cbdict_in$DictName\n",
      "names(cbdict) <- cbdict_in$SourceName                   \n",
      " И исправляем значения марок машин в таблице данных.\n",
      "dt[, CarBrand := cbdict[CarBrand]]\n",
      "dt[is.na(CarBrand), CarBrand := \"UNKNOWN\"]\n",
      "После очистки уникальных значений марок машин стало меньше практически в два раза\n",
      "length( unique(dt$CarBrand) )## [1] 72\n",
      "\n",
      "Ответы на аналитические вопросы1. Top 10 организацийОпределим 10 наиболее крупных таксопарков. В данном случае необходимо построить рейтинг по одному измерению — ОГРН.\n",
      "st <- dt[, list( NumCars = length(RegPlate)), by = list(OGRN, LegalName) ]\n",
      "head( st[order( NumCars, decreasing = TRUE)], 10)\n",
      "##              OGRN             LegalName NumCars\n",
      "##  1: 1137746197104            ООО «СОЛТ»     866\n",
      "##  2: 1037727000893    ООО «СТИЛЬ-МОТОРС»     751\n",
      "##  3: 1067746273198      ООО «РИТМ ЖИЗНИ»     547\n",
      "##  4: 1037789018849           ООО «ТАКСИ»     541\n",
      "##  5: 1127746010700      ООО «ТАКСИ-24 М»     406\n",
      "##  6: 1057748223653 ООО «ЕВРОТРАНССЕРВИС»     349\n",
      "##  7: 5067746596297        ООО «АВТОРЕЙС»     288\n",
      "##  8: 1027739272175          ОАО «14 ТМП»     267\n",
      "##  9: 1137746133250     ООО «СИТИ СЕРВИС»     255\n",
      "## 10: 5077746757688             ООО «ЦПК»     238\n",
      "К сожалению, в рассматриваемом наборе данных хранится только юридическая информация о лицензиатах, а не торговая марка. В Интернете возможно по названию организации и ОГРН найти, под каким брендом работает таксопарк, но это процесс не автоматический и довольно трудоемкий. Результаты поиска наиболее крупных таксопарков собраны в файле \"top10orgs.csv\".\n",
      "top10orgs <- data.table( read.table( \"top10orgs.csv\", \n",
      "  header = TRUE, sep = \";\", colClasses = \"character\", encoding = \"UTF-8\"))\n",
      "Воспользуемся встроенными возможностями data.table по проведению операции JOIN двух таблиц.\n",
      "setkey(top10orgs,OGRN)\n",
      "setkey(st,OGRN)\n",
      "st[top10orgs][order(NumCars, decreasing = TRUE), list(OrgBrand, EasyPhone, NumCars)]\n",
      "##               OrgBrand EasyPhone NumCars\n",
      "##  1:               СОЛТ 781 81 82     866\n",
      "##  2:           Такси956 956 8 956     751\n",
      "##  3:         Такси-Ритм 641 11 11     547\n",
      "##  4:    Городское Такси 500 0 500     541\n",
      "##  5:            Такси24 777 66 24     406\n",
      "##  6:      Формула такси 777 5 777     349\n",
      "##  7: Новое желтое такси 940 88 88     288\n",
      "##  8:       14 Таксопарк 707 2 707     267\n",
      "##  9:              Cabby 21 21 989     255\n",
      "## 10:     Глававтопрокат 927 11 11     238\n",
      "\n",
      " 2. Три наиболее популярные автомарки, в зависимости от формы юридического лицаКакие марки машин наиболее популярны, в зависимости от юридической формы лицензиата? Для ответа на этот вопрос нужно провести агрегацию данных по двум измерениям — марка машины и оргформа.\r\n",
      "Процесс идет в три этапа:\n",
      "\n",
      "Вычисление агрегированного показателя (в данном случае число машин по ОГРН).\n",
      "Вычисление ранга.\n",
      "Ограничение ранга (top 3), сортировка, перераспределение колонок и вывод данных.\n",
      "\n",
      "st <- dt[, list(AGGR = length(RegPlate)), by = list(OrgType, CarBrand) ]\n",
      "st.r <- st[, list(CarBrand, AGGR, \n",
      "                  r = ( 1 + length(AGGR) - rank(AGGR, ties.method=\"first\"))), \n",
      "           by = list(OrgType)] # ranking by one dimension\n",
      "st.out <- st.r[ r <= 3 ][, list(r, OrgType, cval = paste0(CarBrand,\" (\",AGGR,\")\"))]\n",
      "dcast(st.out, r ~ OrgType, value.var = \"cval\")[-1] # reshape data and hide r\n",
      "##             ЗАО               ИП        НП             ОАО            ООО\n",
      "## 1    FORD (212) CHEVROLET (2465) VOLVO (1)       KIA (192)    FORD (3297)\n",
      "## 2 RENAULT (175)      FORD (2238)      <NA> CHEVROLET (115) RENAULT (2922)\n",
      "## 3 HYUNDAI (122)   RENAULT (1996)      <NA>       FORD (53) HYUNDAI (2812)\n",
      "\n",
      "Визуализация1. Отображение данных в виде круговой диаграммыКруговая (секторная) диаграмма, Pie Chart, весьма популярна в бизнес-среде, но подвергается обоснованной критике профессионалами анализа данных. Тем не менее, ее нужно уметь «готовить».\r\n",
      "Пусть требуется отобразить распределение числа лицензий такси, по автомаркам. Чтобы не перегружать диаграмму покажем только марки с количеством лицензий не меньше 1000.\n",
      "st <- dt[, list(N = length(RegPlate)), by = CarBrand ] # Summary table\n",
      "st <- st[, CarBrand := reorder(CarBrand, N) ]\n",
      "piedata <- rbind(\n",
      "  st[ N >= 1000 ][ order(N, decreasing=T) ],\n",
      "  data.table( CarBrand = \"Другие марки\", N = sum( st[N < 1000]$N) )\n",
      "  )\n",
      "piedata\n",
      "##          CarBrand    N\n",
      "##  1:          FORD 5800\n",
      "##  2:       RENAULT 5093\n",
      "##  3:       HYUNDAI 4727\n",
      "##  4:     CHEVROLET 4660\n",
      "##  5:           KIA 2220\n",
      "##  6:         SKODA 2073\n",
      "##  7:        NISSAN 1321\n",
      "##  8:    VOLKSWAGEN 1298\n",
      "##  9:        TOYOTA 1075\n",
      "## 10: MERCEDES-BENZ 1039\n",
      "## 11:  Другие марки 6534\n",
      "Для построения графика хотелось бы зафиксировать именно такой порядок следования марок. Если этого не сделать, то автоматическая сортировка выведет «Другие марки» с последнего места на первое.\n",
      "piedata <- piedata[, CarBrand := factor(CarBrand, levels = CarBrand, ordered = TRUE)]Для построения диаграммы используем ggplot2.\n",
      "pie <-  ggplot(piedata, aes( x = \"\", y = N, fill = CarBrand)) + \n",
      "        geom_bar(stat = \"identity\") +\n",
      "        coord_polar(theta = \"y\")\n",
      "pie\n",
      "\r\n",
      "Вывод уже достаточно информативен. Однако хотелось бы внести ряд визуальных улучшений:\n",
      "\n",
      "убрать серый фон, границы, круговую ось, подписи и отметки;\n",
      "выбрать более различимую цветовую шкалу и обвести каждый «кусок пирога»;\n",
      "рядом с каждым сектором проставить число лицензий, соотвествующее марке;\n",
      "дать текстовое название легенде.\n",
      "Код ниже позволяет сделать все перечисленное. Для отображения надписей рядом с секторами пришлось добавить поле с расчетом точки центра сектора (подсмотрено у artelstatistikov.ru).\n",
      "piedata <- piedata[, pos := cumsum(N) - 0.5*N ]\n",
      "pie <-  ggplot(piedata, aes( x = \"\", y = N, fill = CarBrand)) +\n",
      "        geom_bar( color = \"black\", stat = \"identity\", width = 0.5) +\n",
      "        geom_text( aes(label = N, y = pos), x = 1.4, color = \"black\", size = 5) +\n",
      "        scale_fill_brewer(palette = \"Paired\", name = \"Марки авто\") +\n",
      "        coord_polar(theta = \"y\") +\n",
      "        theme_bw() +\n",
      "        theme ( panel.border = element_blank()\n",
      "              , panel.grid.major = element_blank()\n",
      "              , axis.ticks = element_blank()\n",
      "              , axis.title.x = element_blank()\n",
      "              , axis.title.y = element_blank()\n",
      "              , axis.text.x = element_blank()\n",
      "              , legend.title = element_text(face=\"plain\", size=16)\n",
      "              )\n",
      "pie\n",
      "2. Столбчатая диаграммаБолее информативная альтернатива кругу — столбчатая диаграмма, Bar Chart. Помимо того, что длины столбиков удобнее сравнивать, чем длины дуг или площади секторов круга, столбчатая диаграмма может дополнительно отобразить, например, распределении числа лицензий по оргформам.\n",
      "st <- dt[, list(N = length(RegPlate)), by = list(OrgType, CarBrand) ] # Summary table\n",
      "cbsort <- st[, list( S = sum(N) ), keyby = CarBrand ] # Order by total number\n",
      "setkey(st, CarBrand)\n",
      "st <- st[cbsort] # Join\n",
      "\n",
      "topcb <- st[ S >= 1000 ][ order(S) ]\n",
      "bottomcb <- st[S < 1000, list(CarBrand = \"Другие марки\", OrgType, N = sum(N)), by = OrgType]\n",
      "bottomcb <- bottomcb[, list(CarBrand, OrgType, N, S = sum(N))]\n",
      "\n",
      "bardata <- rbind( bottomcb, topcb)  \n",
      "bardata <- bardata[, CarBrand := factor(CarBrand, levels = unique(CarBrand), ordered=T)]\n",
      "#\n",
      "bar <-  ggplot(bardata, aes(x = CarBrand, weight = N, fill = OrgType)) +\n",
      "        geom_bar() + coord_flip() +\n",
      "        scale_fill_brewer(palette = \"Spectral\", name = \"Оргформа\") +\n",
      "        labs(list(y = \"Количество лицензий\", x = \"Марки автомобилей\")) +\n",
      "        theme_bw()\n",
      "bar\n",
      "\n",
      "3. Диаграмма Heat Map (Теплокарта)Предположим, требуется получить ответ на вопрос: «Хозяева каких марок машин (среди таксистов) больше всего подвержены моде на «красивые» номера?». Красивыми в данном случае будем считать номера с одинаковыми цифрами в тройках: 111, 222 и т.д.\r\n",
      "Анализ ведется по двум аналитическим измерениям — марка машины и тройка. Показатель — количество машин с заданным сочетанием марки и тройки. Для визуализации такого набора данных хорошо подходит визуальный аналог таблицы — диаграмма heat map. Чем более популярна тройка, тем более интенсивный цвет кодирует значение ячейки.\n",
      "ln <- dt[grep( \"^[^0-9]([0-9])\\\\1{2}.+$\" , RegPlate),\n",
      "         list(CarBrand, LuckyNum = gsub(\"^[^0-9]([0-9]{3}).+$\",\"\\\\1\", RegPlate))]\n",
      "ln <- ln[, list( N = .N),  by = list(CarBrand, LuckyNum) ]\n",
      "ln <- ln[, Luck := sum(N), by = list(CarBrand) ] # Total number of lucky regplates per car brand\n",
      "ln <- ln[, CarBrand := reorder(CarBrand, Luck) ]\n",
      "#\n",
      "heatmap <-  ggplot(ln, aes(x = CarBrand, y = LuckyNum)) +  \n",
      "            geom_tile( aes(fill = as.character(N)), color = \"black\") + \n",
      "            scale_fill_brewer(palette = \"YlOrRd\", name = \"Число «красивых» номеров:\") +\n",
      "            labs(list(x = \"Марки автомобилей\", y = \"Номерные тройки\")) +\n",
      "            theme_bw() +\n",
      "            theme ( panel.grid.major = element_blank()\n",
      "                    , axis.text.x = element_text(angle = 45, hjust = 1)\n",
      "                    , axis.title.y = element_text(vjust = 0.3)\n",
      "                    , legend.position = \"top\"\n",
      "                    , legend.title.align = 1\n",
      "            )\n",
      "heatmap\n",
      "\r\n",
      "Во всех диаграммах использованы научно обоснованные цветовые палитры проекта Color Brewer 2.0.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "При работе с big data ошибок не избежать. Вам нужно докопаться до сути данных, расставить приоритеты, оптимизировать, визуализировать данные, извлечь правильные идеи. По результатам опросов, 85 % компаний стремятся к управлению данными, но только 37% сообщают об успехах в этой области. На практике изучать негативный опыт сложно, поскольку о провалах никто не любит говорить. Аналитики с удовольствием расскажут об успехах, но как только речь зайдет об ошибках, будьте готовы услышать про «накопление шума», «ложную корреляцию» и «случайную эндогенность», и без всякой конкретики. Действительно ли проблемы с big data существуют по большей части лишь на уровне теории? \n",
      "\r\n",
      "Сегодня мы изучим опыт реальных ошибок, которые ощутимо повлияли на пользователей и аналитиков. \n",
      "\n",
      "Ошибки выборки\n",
      "\r\n",
      "В статье «Big data: A big mistake?» вспомнили об интересной истории со стартапом Street Bump. Компания предложила жителям Бостона следить за состоянием дорожного покрытия с помощью мобильного приложения. Софт фиксировал положение смартфона и аномальные отклонения от нормы: ямы, кочки, выбоины и т.д. Полученные данные в режиме реального времени отправлялись нужному адресату в муниципальные службы.\n",
      "\r\n",
      "Однако в какой-то момент в мэрии заметили, что из богатых районов жалоб поступает гораздо больше, чем из бедных. Анализ ситуации показал, что обеспеченные жители имели телефоны с постоянным подключением к интернету, чаще ездили на машинах и были активными пользователями разных приложений, включая Street Bump.\n",
      "\r\n",
      "В результате основным объектом исследования стало событие в приложении, но статистически значимой единицей интереса должен был оказаться человек, использующий мобильное устройство. Учитывая демографию пользователей смартфонов (на тот момент это в основном белые американцы со средним и высоким уровнем дохода), стало понятно, насколько ненадежными оказались данные.\n",
      "\r\n",
      "Проблема неумышленной предвзятости десятилетиями кочует из одного исследования в другое: всегда будут люди, активнее других пользующиеся соцсетями, приложениями или хештегами. Самих по себе данных оказывается недостаточно — первостепенное значение имеет их качество. Подобно тому, как вопросники влияют на результаты опросов, электронные платформы, используемые для сбора данных, искажают результаты исследования за счет воздействия на поведение людей при работе с этими платформами.\n",
      "\r\n",
      "По словам авторов исследования «Обзор методов обработки селективности в источниках больших данных», существует множество источников big data, не предназначенных для точного статистического анализа — опросы в интернете, просмотры страниц в Твиттере и Википедии, Google Trends, анализ частотности хештегов и др.\n",
      "\r\n",
      "Одной из самых ярких ошибок такого рода является прогнозирование победы Хилари Клинтон на президентских выборах в США в 2016 году. По данным опроса Reuters/Ipsos, опубликованным за несколько часов до начала голосования, вероятность победы Клинтон составляла 90%. Исследователи предполагают, что методологически сам опрос мог быть проведен безупречно, а вот база, состоящая из 15 тыс. человек в 50 штатах, повела себя иррационально — вероятно, многие просто не признавались, что хотят проголосовать за Трампа.\n",
      "\n",
      "Ошибки корреляций\r\n",
      "Непонятная корреляция и запутанная причинно-следственная связь часто ставят в тупик начинающих дата-сайнтистов. В результате появляются модели, безупречные с точки зрения математики и совершенно не жизнеспособные в реальности.\n",
      "\n",
      "\r\n",
      "На диаграмме выше показано общее количество наблюдений НЛО с 1963 года. Число зарегистрированных случаев из базы данных Национального центра отчетности по НЛО в течение многих лет оставалось примерно на одном уровне, однако в 1993 году произошел резкий скачок.\n",
      "\r\n",
      "Таким образом, можно сделать совершенно логичный вывод, что 27 лет назад пришельцы всерьез взялись за изучение землян. Реальная же причина заключалась в том, что в сентябре 1993 года вышел первый эпизод «Секретных материалов» (на пике его посмотрели более 25 млн человек в США).\n",
      "\n",
      "\r\n",
      "Теперь взгляните на данные, которые показывают частоту наблюдений НЛО в зависимости от времени суток и дня недели: желто-оранжевым окрашена наибольшая частота случаев наблюдения. Очевидно, что пришельцы чаще высаживаются на Землю в выходные, потому что в остальное время они ходят на работу. Значит, исследование людей для них — хобби?\n",
      "\r\n",
      "Эти веселые корреляции имеют далеко идущие последствия. Так, например, исследование «Доступ к печати в сообществах с низким уровнем дохода» показало, что школьники, имеющие доступ к большему количеству книг, получают лучшие оценки. Руководствуясь данными научной работы, власти Филадельфии (США) занялись реорганизацией системы образования.\n",
      "\r\n",
      "Пятилетний проект предусматривал преобразование 32 библиотек, что обеспечило бы равные возможности для всех детей и семей в Филадельфии. На первый взгляд, план выглядел великолепно, но, к сожалению, в исследовании не учитывалось, действительно ли дети читали книги — в нем лишь рассматривался вопрос, доступны книги или нет.\n",
      "\r\n",
      "В итоге значимых результатов добиться не удалось. Дети, не читавшие книги до исследования, не полюбили вдруг чтение. Город потерял миллионы долларов, оценки у школьников из неблагополучных районов не улучшились, а дети, воспитанные на любви к книгам, продолжили учиться так же, как учились.\n",
      "\n",
      "Потеря данных\n",
      "\r\n",
      "(с)\n",
      "\r\n",
      "Иногда выборка может быть верной, но авторы просто теряют необходимые для анализа данные. Так произошло в работе, широко разошедшейся по миру под названием «Фрикономика». В книге, общий тираж которой превысил 4 млн экземпляров, исследовался феномен возникновения неочевидных причинно-следственных связей. Например, среди громких идей книги звучит мысль, что причиной спада подростковой преступности в США стал не рост экономики и культуры, а легализация абортов.\n",
      "\r\n",
      "Авторы «Фрикономики», профессор экономики Чикагского университета Стивен Левитт и журналист Стивен Дабнер, через несколько лет признались, что в итоговое исследование абортов попали не все собранные цифры, поскольку данные просто исчезли. Левитт объяснил методологический просчет тем, что в тот момент «они очень устали», и сослался на статистическую незначимость этих данных для общего вывода исследования.\n",
      "\r\n",
      "Действительно ли аборты снижают количество будущих преступлений или нет — вопрос все еще дискуссионный. Однако у авторов подметили множество других ошибок, и часть из них удивительно напоминает ситуацию с популярностью уфологии в 1990-х годах.\n",
      "\n",
      "Ошибки анализа\n",
      "\r\n",
      "(с)\n",
      "\r\n",
      "Биотех стал для технологических предпринимателей новым рок-н-роллом. Его также называют «новым IT-рынком» и даже «новым криптомиром», имея ввиду взрывную популярность у инвесторов компаний, занимающихся обработкой биомедицинской информации.\n",
      "\r\n",
      "Являются ли данные по биомаркерам и клеточным культурам «новой нефтью» или нет — вопрос второстепенный. Интерес вызывают последствия накачки индустрии быстрыми деньгами. В конце концов, биотех может представлять угрозу не только для кошельков венчурных фондов, но и напрямую влиять на здоровье людей.\n",
      "\r\n",
      "Например, как указывает генетик Стивен Липкин, для генома есть возможность делать высококлассные анализы, но информация о контроле качества часто закрыта для врачей и пациентов. Иногда до заказа теста вы не можете заранее узнать, насколько велика глубина покрытия при секвенировании. Когда ген прочитывают недостаточное число раз для адекватного покрытия, программное обеспечение находит мутацию там, где ее нет. Зачастую мы не знаем, какой именно алгоритм используется для классификации аллелей генов на полезные и вредные.\n",
      "\r\n",
      "Тревогу вызывает большое количество научных работ в области генетики, в которых содержатся ошибки. Команда австралийских исследователей проанализировала около 3,6 тыс. генетических работ, опубликованных в ряде ведущих научных журналов. В результате обнаружилось, что примерно одна из пяти работ включала в свои списки генов ошибки.\n",
      "\r\n",
      "Поражает источник этих ошибок: вместо использования специальных языков для статистической обработки данных ученые сводили все данные в Excel-таблице. Excel автоматически преобразовывал названия генов в календарные даты или случайные числа. А вручную перепроверить тысячи и тысячи строк просто невозможно.\n",
      "\r\n",
      "В научной литературе гены часто обозначаются символами: например, ген Септин-2 сокращают до SEPT2, а Membrane Associated Ring Finger (C3HC4) 1 — до MARCH1. Excel, опираясь на настройки по умолчанию, заменял эти строки датами. Исследователи отметили, что не стали первооткрывателями проблемы — на нее указывали более десятилетия назад.\n",
      "\r\n",
      "В другом случае Excel нанес крупный удар по экономической науке. Знаменитые экономисты Гарвардского университета Кармен Рейнхарт и Кеннет Рогофф в исследовательской работе проанализировали 3,7 тыс. различных случаев увеличения госдолга и его влияние на рост экономики 42 стран в течение 200 лет.\n",
      "\r\n",
      "Работа «Рост за время долга» однозначно указывала, что при уровне госдолга ниже 90 % ВВП он практически не влияет на рост экономики. Если же госдолг превышает 90 % ВВП, медианные темпы роста падают на 1 %.\n",
      "\r\n",
      "Исследование оказало огромное влияние на то, как мир боролся с последним экономическим кризисом. Работа широко цитировалась для оправдания сокращения бюджета в США и Европе.\n",
      "\r\n",
      "Однако несколько лет спустя Томас Херндорн, Майкл Эш и Роберт Поллин из Университета Массачусетса, разобрав по пунктам работу Рогоффа и Рейнхарта, выявили банальные неточности при работе с Excel. Статистика, на самом деле, не показывает никакой зависимости между темпами роста ВВП и госдолгом.\n",
      "\n",
      " Заключение: исправление ошибок как источник ошибок\n",
      "\r\n",
      "(с)\n",
      "\r\n",
      "Учитывая огромное количество информации для анализа, некоторые ошибочные ассоциации возникают просто потому, что такова природа вещей. Если ошибки редки и близки к случайным, выводы итогового анализа могут не пострадать. В некоторых случаях бороться с ними бессмысленно, так как борьба с ошибками при сборе данных может привести к возникновению новых ошибок.\n",
      "\r\n",
      "Знаменитый статистик Эдвард Деминг сформулировал описание этого парадокса следующим образом: настройка стабильного процесса для компенсации небольших имеющихся отклонений с целью достижения наиболее высоких результатов может привести к худшему результату, чем если бы не происходило вмешательства в процесс.\n",
      "\r\n",
      "В качестве иллюстрации проблем с чрезмерным исправлением данных используется моделирование корректировок в процессе случайного падения шариков через воронку. Корректировать процесс можно с помощью нескольких правил, основная цель которых — предоставить возможность попасть как можно ближе к центру воронки. Однако чем больше вы будете следовать правилам, тем более разочаровывающими будут результаты.\n",
      "\r\n",
      "Проще всего эксперимент с воронкой провести онлайн, для чего создали симулятор. Пишите в комментариях, каких результатов вам удалось достичь.\n",
      "\n",
      "\r\n",
      "Правильно анализировать большие данные мы можем научить в Академии MADE — бесплатном образовательном проекте от Mail.ru Group. Заявки на обучение принимаем до 1 августа включительно.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет.В конце прошлого года GlowByte и Газпромбанк сделали большой совместный доклад на конференции Big Data Days, посвященный созданию современного аналитического хранилища данных на базе экосистемы Cloudera Hadoop. В статье мы рассказали об опыте построения системы, сложностях и вызовах, с которыми пришлось столкнуться и преодолеть, чтобы достичь успеха в проекте.  Появление технологии Hadoop десятилетние назад вызвало на рынке интеграции данных небывалый ажиотаж и оптимизм. Индустрия задалась вопросом — «а готова ли технология вытеснить традиционные системы обработки данных?». За прошедшую декаду было сломано немало копий в этой битве. Кто-то успел разочароваться, кто-то добился локальных успехов, а тем временем сама экосистема прошла короткий, но стремительный эволюционный путь, который позволяет уверенно сказать, что в настоящий момент не существует задачи и вызова в области обработки и интеграции данных, которую не способен решить Hadoop.В этой статье мы попытаемся дать ответ на главный вопрос — как создать современное аналитическое хранилище данных на базе экосистемы Cloudera на примере проекта, реализованного нами в “Газпромбанк” АО. Попутно расскажем как мы справились с основными вызовами при решении задачи.“Газпромбанк” АО — один  их ведущих системообразующих финансовых институтов РФ. Он входит в топ-3 банков по активам России и всей Восточной Европы и имеет разветвленную сеть дочерних филиалов.Банк традиционно на рынке финансовых услуг был консервативным и ориентировался на корпоративный сектор, но в 2017 году принял стратегию “Цифровой трансформации” с целью развития направления розничного бизнеса. Розничный банковский сектор является высококонкурентным в РФ и для реализации стратегии Газпромбанку потребовалось создание новой технологической платформы, которая должна удовлетворять  современным требованиям, так как основой интенсивного роста на конкурентном рынке могут быть только data driven процессы.На тот момент в Банке уже было несколько платформ интеграции данных. Основная платформа КХД занята классическими, но критичными с точки зрения бизнеса задачами: управленческой, финансовой и регуляторной отчетности. Внесение изменения в текущую архитектуру КХД несло серьезные риски и финансовые затраты. Поэтому было принято решение разделить задачи и создавать аналитическую платформу с нуля.Верхнеуровнево задачи ставились следующие:Создание озера данных (как единой среды, в которой располагаются все необходимые для анализа данные);Консолидации данных из озера в единую модель;Создание аналитический инфраструктуры;Интеграция с бизнес-приложениями;Создание витрин данных;Внедрение Self-service инструментов;Создание Data Science окружения.Этап проработки архитектуры важно начинать после консолидации и уточнения всех ключевых требований к системе. Требования мы разделили на два больших блока:Бизнес-требованияОбеспечение данными бизнес-приложений: аналитический CRM, Real Time Offer, Next Best Offer, розничный кредитный конвейер;Возможность работы с сырыми данными из систем-источников as is (функция Data Lake);Среда статистического моделирования;Быстрое подключение новых систем источников к ландшафту;Возможность обработки данных за всю историю хранения;Единая модель консолидированных данных (аналитическое ядро);Графовая аналитика;Текстовая аналитика;Обеспечение качества данных.Требования ИТ Высокая производительность при дешевом горизонтальном масштабировании;Отказоустойчивость и высокая доступность;Разделяемая нагрузка и гарантированный SLA;ELT обработка и трансформация данных;Совместимость с имеющимися Enterprise решениями (например, SAP Business Objects, SAS);Ролевая модель доступа и полное обеспечение требований информационной безопасности.Кроме этого, система должна быть линейно масштабируемой, основываться на open source технологиях, и самое главное — соотношение стоимость\\производительность должно быть самым конкурентным из всех предложений на рынке.Для создания единой аналитической платформы розничного бизнеса мы выбрали стек Hadoop на базе дистрибутива Cloudera Data HubАрхитектура решенияРассмотрим архитектуру решения.Рис. АрхитектураСистема разделена на два кластера Cloudera Data Hub. Кластер регламентных процессов и  Лаборатория данных1. Кластер регламентных процессовВсе регламентные источники данных подключаются к данному кластеру. Все регламентные ETL расчеты также работают на этом контуре. Все системы потребители данных “запитываются” из регламентного кластера. Таким образом выполняется жесткая изоляция непредсказуемой пользовательской нагрузки от критичных бизнес процессов.В настоящий момент к Hadoop подключено свыше 40-ка систем-источников с регламентом от t-1 день до t-15 минут для batch загрузки, а также real-time интеграция с процессинговым центром. Регламентный контур поставляет данные во все системы розничного бизнеса: Аналитический CRM;Розничный кредитный конвейер;Антифрод система;Система принятия решений;Collection;MDM;Система графовой аналитики;Система текстовой аналитики;BI отчетность2. Кластер пользовательских экспериментов “Лаборатория данных”В то же время, все данные, которые загружаются на регламентный контур в режиме онлайн, реплицируются на контур пользовательских экспериментов. Задержка по времени минимальная и зависит только от пропускной способности сетевого канала тк контур лаборатории данных находится в другом ЦОДе. Те пользовательский контур одновременно выполняет роль Disaster Recovery плеча в случае выхода из строя основного ЦОДа. Дата инженеры и дата science специалисты получают все необходимые данные для проведения своих исследований и проверки гипотез без задержки и без ожидания днями и неделями, когда нужные им данные для расчетов или тренировки моделей, куда-то выгрузят. Они доступны все в одном месте и всегда свежие. Дополнительно на кластере лаборатории данных создаются пользовательские песочницы, где можно создавать и свои объекты. Также ресурсы кластера распределены именно для высококонкурентной пользовательской нагрузки. На регламентный кластер у пользовательского доступа нет. После проверки гипотез, подготовки требований для регламентных расчетов либо тренировки моделей, результаты передаются для постановки на регламентный контур и сопровождения.Дополнительно на контуре лаборатории создано окружение управления жизненным циклом моделей, окружение пользовательских аналитических приложений с управлением, ресурсами на K8S, подключены два специализированных узла с GPU ускорением для обучения моделей.Система мониторинга и управления кластерами, загрузками, ETL,  реализована на дополнительных виртуальных машинах, не включенных напрямую в кластера Cloudera.Сейчас версия дистрибутива CDH 5.16.1. В архитектурный подход закладывалась ситуация выхода из строя двух любых узлов без последующей остановки системы.Характеристики Data узлов следующие: CPU 2x22 Cores 768Gb RAM SAS HDD 12x4Tb. Все собрано в HPE DL380 в соответствии с рекомендациями Cloudera Enterprise Reference Architecture for Bare Metal Deployments. Такой “необычный”, как кому-то может показаться, сайзинг связан с выбором подхода по ETL и процессингового движка для работы с данными. Об этом немного ниже. Необычность его в том, что вместо “100500” маленьких узлов, мы выбираем меньше узлов, но сами узлы “жирнее”.Основные технические вызовыВ процессе проработки и внедрения мы столкнулись с рядом технических вызовов, которые необходимо было решить, для того чтобы система удовлетворяла выше заявленным высоким требованиям.Выбор основного процессингового движка в Hadoop;Подход по трансформации данных (ETL);Репликация данных «Система-источник –> Hadoop» и «Hadoop –> Hadoop»;Изоляция изменений и консистентность данных;Управление конкурентной нагрузкой;Обеспечение требований информационной безопасностиДалее рассмотрим каждый из этих пунктов  детально.Выбор основного процессингового движкаГорький опыт первых попыток некоторых игроков реализовать ХД в Hadoop 1.0 показал, что нельзя построить систему обработки данных руками java программистов, не имеющих опыта построения классических ХД за плечами, не понимающих базовых понятий жизненного цикла данных, не способных «отличить дебет от кредита» или «рассчитать просрочку». Следовательно, для успеха нам надо сформировать команду специалистов по данным, понимающих нашу предметную область  и использовать язык структурированных запросов SQL.В целом, базовый принцип работы, с данными которого стоит придерживаться  – если задачу можно решить на SQL  то ее нужно решать только на SQL. А большинство задач с данными решаются именно с помощью языка структурированных запросов. Да и нанять и подготовить команду SQL-щиков для проектной работы быстрее и дешевле чем «специалистов по данным, окончивших курсы на диване из рекламы в инстаграм».Для нас это означало что необходимо выбрать «правильный» SQL движок для работы с данными в Hadoop. Остановили свой выбор на движке Impala так как он имеет ряд конкурентных преимуществ. Ну и собственно ориентация на Impala во многом и предопределила выбор в пользу Cloudera как дистрибутива Hadoop для построения аналитического хранилища.Чем же Impala так хороша?Impala – движок распределенных вычислений, работающий напрямую с данными HDFS, а не транслирующий команды в другой фреймворк вроде MapReduce, TEZ или SPARK.Impala – движок который большинство всех операций выполняет в памяти. Impala читает только те блоки Parquet, которые удовлетворяют условиям выборки и соединений (bloom фильтрация, динамическая фильтрация), а не поднимает для обработки весь массив данных. Поэтому в большинстве аналитических задач на практике Impala быстрее, чем другие традиционные MPP движки вроде Teradata или GreenPlum.Impala имеет хинты, позволяющие очень легко управлять планом запроса, что весьма важный критерий при разработке и оптимизации сложных ETL преобразований без переписывания запроса.Движок не разделяет общие ресурсы Hadoop  с другими сервисами так как не использует YARN и имеет свой ресурсный менеджмент. Это обеспечивает предсказуемую высоко конкурентную нагрузку. Синтаксис SQL настолько близок к традиционным движкам, что на подготовку разработчика или аналитика, имеющего опыт другой SQL системы, уходит не больше 3-4х часов. Вот как работа с Hadoop выглядит глазами аналитика:Рис. Работа с Impala SQL в HueЭто работа в веб-ноутбуке Hue, который идет вместе с Cloudera. Не обделены и те пользователи, кто предпочитает работать с классическими толстыми SQL клиентами или сводными таблицами Excel.Рис. SQL доступ к Hadoop в локальном “толстом” клиенте.Многие кто читал рекомендации Cloudera, могут задаться вопросом – а почему Impala не рекомендована как ETL движок, а только как движок пользовательского ad-hoc или BI доступа? Ответ на самом деле прост - Impala не имеет гарантии исполнения запроса «чтобы не стало» в отличие от Hive. Eсли падает запрос или узел, то запрос автоматически не перезапустится и поднимать его надо вручную. Это проблема легко  решаема – ETL поток или запрос в приложении должны уметь перезапускаться в таких ситуациях.ETL потоки в нашем решении перезапускаются без вмешательства администратора автоматически:При падении запроса происходит автоматический анализ причины;При необходимости автоматически подбираются параметры конкретного запроса или параметры сессии чтобы повторный перезапуск отработал без ошибок;Выполняется сбор статистической информации по ошибкам для дальнейшего анализа и настройки потока чтобы в будущем по данному запросу или job’у таких ситуаций не возникало.У нас на проекте сложилась парадоксальная ситуация - команда аналитиков и инженеров по данным, работающих над проектом, знала про Hadoop только то, что на логотипе есть желтый слоник. Для них Hadoop - это привычный SQL. Уже после “уборки урожая” (завершения разработки аналитического слоя, о котором речь пойдет ниже), ребята попросили провести для них обучение по Hadoop чтобы быть “в теме”.Подход по трансформации данныхВ разработке трансформации данных важно не только выбрать правильный движок, но и принять правильные стандарты разработки. У нас давно сформировался подход к таким задачам как metadata driven E-L-T при котором трансформация данных отрисовывается в диаграмме ETL инструмента, который в свою очередь генерирует SQL и запускает его в среде исполнения. При этом SQL должен быть максимально оптимальным с точки зрения конкретной среды исполнения. На рынке не так много ETL инструментов, позволяющих управлять генерацией SQL. В данном внедрении использовался инструмент SAS Data Integration.Весь регламентный ETL выполнен в подходе metadata driven ELT. Никаких ручных скриптов с планировкой на airflow!Такой подход позволяет Автоматизировать процессы управления метаданными;Автоматизировать процесс построения lineage данных как средствами самого ETL инструмента, так и средствами доступа к API;Повысить качество процессов внесения изменений и управления данными т.к. вся информация о зависимостях всех объектов и всех job’в хранится в метаданных ETL инструмента.Использовать CI/CD процессы в разработкеРис. Примеры диаграмм ETL процессовSAS DI позволяет визуализировать граф зависимостей в штатном функционале или можно выгрузить метаданные через API и использовать их для анализа в других средах.Рис. Граф зависимостей объектовРепликация данныхЗагрузка данных в систему – ключевая отправная точка реализации функциональных бизнес требований системы.Для этой функции был разработан специализированный инструмент – Data Replicator. Инструмент позволяет в очень короткие сроки подключать системы источники и настраивать загрузку данных в Hadoop. Из возможностейСинхронизация метаданных с источника;Встроенные механизмы контроля качества загруженных данных;Загрузка в различных режимах работы в т.ч. полная копия, извлечение и загрузка инкремента (по любой скалярной детерминированной функции), архивация данных источника и т.д.Решение имеет гибкие настройки позволяющие приоритизировать задания загрузки, балансировку, контроль многопоточности. Это позволяет бережно относится к источнику при извлечении данных, но в то же время гарантировать SLA доступности данных в Hadoop.Другая очень важная функция Data Replicator’а  - автоматическая репликация данных с регламентного кластера Hadoop на DR кластер. Данные, загружаемые из систем-источников реплицируются автоматически, для деривативных данных существует API. Все регламентные ETL процессы, при обновлении целевой таблицы вызывают API которое запускает процесс мгновенного копирования изменений на резервный контур. Таким образом, DR кластер, который так же выполняет роль пользовательской песочницы, всегда имеет «свежие» данные.Нами реализовано множество конфигураций для различных СУБД используемых как источники в ГПБ, также  для других процессинговых движков Hadoop (для случаев когда другой кластер Hadoop является источником данных для системы) и  есть возможность обрабатывать данные, загруженные в систему другими инструментами, например kafka, flume, или промышленный ETL tool.Изоляция изменений и консистентностьЛюбой кто работал в Hadoop сталкивался с проблемой конкурентного доступа к данным. Когда пользователь читает таблицу, а другая сессия пытается туда записать данные, то происходит блокировка таблицы (в случае Hive) либо пользовательский запрос падает (в случае Impala). Самое распространенное решение на практике – выделение регламентных окон на загрузку во время которых не допускается работа пользователей, либо каждая новая порция загрузки записывается в новую партицию.  Для нас первый подход неприемлем тк мы должны гарантировать доступность данных 24х7 как по загрузке так и по доступу. Второй подход не применим т.к. он предполагает секционирование данных только по дате\\порции загрузке, что неприемлемо если требуется отличное секционирование (по первичному ключу, по системе источнику и т.д.). Так же второй метод приводит к избыточному хранению данных.Забегая вперед хочется отметить, что в настоящее время в HIVE 3 проблемы решена путем добавления поддержки ACID транзакционности, но, в нашей версии дистрибутива у нас далеко не третий Hive (да еще и на Map Reduce), а хотим получить высокую производительность и конкурентную нагрузку и  поэтому нам пришлось реализовать ACID для Impala  в Hadoop самостоятельно.В нашем решении изоляция выполнена с применением подхода HDFS snapshot и разделения слоя хранения и доступа к данным через VIEW.Когда данные записываются в HDFS, сразу, мгновенно создается снапшот на который переключается VIEW.Пользователь читает данные с VIEW, а не напрямую с таблицы, поэтому следующая сессия записи никак не влияет на его текущий запрос. Все что остается делать – это переключать VIEW на новые HDFS снапшоты, число которых определяется максимальной длительностью пользовательских запросов и частотой обновления данных в Hadoop. Те в сухом остатке мы получаем аналог UNDO в Oracle, retention период которого зависит от количества снапшотов и регламента загрузки данных.Основной секрет в том, что как только процессинговый  движок определил какие данные из HDFS он должен прочитать, после этого DDL VIEW или таблицы может быть изменен т.к. оптимизатор больше не будет обращаться к словарю metastore. Т.е. можно выполнить переключение VIEW на другую директорию.Функционал HDFS Snapshot настолько легковесный и быстрый что позволяет создавать сотни снапшотов в минуту и никак не влияет на производительность системы.Изоляции изменений в нашем решении также является функцией DataReplictor’а. Все загружаемые данные изолируются автоматически, причем на обеих контурах системы, а производные ETL данные изолируются через вызов API. Каждое изменение целевого объекта, которое происходит в рамках ETL процесса завершается вызовом API по созданию снапшота и переключению VIEW.Благодаря такому решению, все загрузки и все данные доступны в режиме 24х7 без регламентных окон. HDFS снапшоты не приводят к большому избыточному хранению данных в HDFS. Наш опыт показал, что для часто меняющихся регламентных данных хранение снапшотов за трое суток приводит к увеличению размера максимум на 25%.Управление конкурентной нагрузкойСледующий большой блок требований – управление конкурентной нагрузкой.На практике это означает что нужно обеспечитьПредсказуемую работу регламентных процессов;Приоритизация пользователей в зависимости от принадлежности к ресурсной группе;Отсутствие, минимизация или управление отказами в обслуживании;Как это обеспечено на практикеНастроено разделение ресурсов между сервисами Hadoop на уровне ОС через cgroups;Правильное распределение памяти между нуждами ОС и Hadoop;Правильное распределение памяти внутри кластера между служебными сервисами Hadoop, YARN приложениями и Impala;Выделение ресурсных пулов Impala отдельным пользовательским группам – для гарантии обслуживания и приоритизации запросов.Результат – предсказуемая высококонкурентная нагрузка десятков пользователей одновременно и десятков тысяч ETL запросов в сутки без влияния на другие составляющие экосистемы Cloudera.Ри. Количество SQL запросов, завершающихся каждую секунду.В настоящий момент на кластере регламентных расчетов в сутки регистрируется и успешно выполняется в среднем 900 тыс SQL запросов по трансформации и загрузке данных. В дни массовых загрузок и расчетов эта цифра поднимается до полутора миллионов. Рис. Средняя утилизация CPU за суткиПри этом мы видим, что остается внушительный запас по производительности с тз возможностей повышения конкурентной работы. Есть понимание что это может быть и 1,5 млн и 2 млн запросов. Это означает что выбранный подход оказался верным и пропускная способность системы как и ее предсказуемость под нагрузкой показывает выдающиеся результаты.Информационная безопасностьВ финансовом секторе традиционно вопросы информационной безопасности являются одними из самых ключевых тк приходится работать с данными, которые не только подлежат защите с тз федерального законодательства, но и с требованиями, которые периодически ужесточаются госрегулятором. При выборе дистрибутива Hadoop стоит особое внимание уделять этим требованиям, так как большинство не вендорских сборок, либо сборок, спроектированных на базе популярных open source дистрибутивов (например Apache Big Top) не позволяют закрывать часть требований и при выводе системы в промышленную эксплуатацию можно столкнуться с неприятными сюрпризами недопуска системы от службы ИБ.В кластере Cloudera нами были реализованные следующие требования:Ролевая модель доступа к даннымВсе пользователи включены в группы Active Directory (AD) каталога;Группы AD зарегистрированы в Sentry;В Sentry выполнено разграничение доступа для баз Impala и директорий HDFS;Каждый Target слой данных имеет ролевые слои VIEW с ограничениями на чувствительные данные в соответствии с ролевой моделью доступа;Кластеры керберизированы;Подключение клиентских приложений только с применением SSL шифрования. Также шифрование используется при передачи данных внутри кластера.Выполняется парсинг и приведение всех журналов сервисов Hadoop к единому реляционному формату стандартного журнала ИБ (единая точка интеграции для системы сбора данных ИБ)Пользовательские запросы;Запросы ETL;Точки интеграции Hadoop с другими системами;Все серверы, ОС, компоненты и прикладное ПО настроены в соответствии с согласованными профилями информационной безопасности и периодически проходят проверку на предмет известных уязвимостей.Единый аналитический слой данныхНаличие общего слоя консолидированных данных – основное требование аналитического ХД. Без этого Hadoop (как и любое другое ХД) – озеро данных, которое пользователи начинают превращать со временем в неуправляемое болото. Поэтому важно иметь общую версию правды над этим озером чтобы все задачи решались в единой системе координат.Был разработан единый аналитический слой консолидированных данных. Источником для него является копия детального слой КХД, которая регулярно реплицируется в среду Hadoop, а также дополнительные источники, подключаемые напрямую, минуя КХД.Модель ориентирована на пользовательский ad-hoc доступ  и проектировалась с учетом требований типовых задач клиентской аналитики, риск моделей, скоринга.Реализованы все области данных, необходимые для решения задач розничного бизнеса и моделирования такие как:Аккредитивы;Депозиты;Залоги;Заявки;Карты;Контрагенты;MDM;Кредиты;Сегмент клиента;Рейтинги;Агрегаты;Справочники;Счета;Эквайринг;Векселя;РЕПО;Резервы.В настоящий момент, слой состоит из 177 целевых объектов и порядка 2350 бизнес-атрибутов. В snappy сжатии объем данных порядка 20 Тб (не менее 100 Тб в RAW).В модель загружена история с 2010 года. Ведь точность моделей зависит от глубины истории данных, на которых она обучается. Более того, история очищалась аналитическими алгоритмами. В банке разветвленная филиальная сеть и часть филиалов мигрировали друг в друга, клиенты переходили из одного филиала в другой, производили пролонгацию сделок и тд. Все это составляет определенные сложности для анализа данных. Но в конечном целевом слое вся история отношений с каждым клиентом, все сделки, имеют непрерывную историю в рамках одного суррогатного ключа без пересекающихся интервалов историчности.Реализованный единый слой - источник данных для производных прикладных витрин под бизнес-приложения, отчетность и модели. Сейчас у нас около 40 производных регламентных витрин, состоящих из 550 целевых таблиц и примерно 13200 атрибутов.НадежностьЧасто приходится слушать о ненадежности решений, спроектированных на Hadoop. За два года эксплуатации Cloudera Data Hub у нас практически не было каких-либо проблем, связанных с простоем системы. Случилось буквально пара инцидентов, повлиявших не регламентные процессы.Один раз у нас забилось место, выделенное под БД metastore (недостатки мониторинга).В другой раз была попытка выгрузить несколько сотен миллионов транзакций через Impala. В результате “прилег” координатор и другие пользователи и процессы не могли подключиться на этот координатор. Как результат выработали правило – каждый отдельный вид процессов (загрузка данных, ETL, пользователи, приложения) подключается к своему координатору, который еще имеет дублера для балансировки. Ну и конечно большие выгрузки данных в системы потребители лучше делать через sqoop export. Ну и в последних релизах Impala уже без проблем может отдавать десятки миллионов записей на подключение.Да, случаются выходы из строя дисков, приходится иногда делать decommission узлов для их замены, но все это проходит прозрачно для пользователей без остановки работы, ведь наш архитектурный подход сразу подразумевал устойчивость к выходу из строя как минимум двух любых узлов.ИтогиВ настоящий момент система является фабрикой данных всех розничных процессов Банка и аналитических приложений. Платформой ежедневно пользуется 36 департаментов и примерно 500 пользователей для самостоятельного решения задач по аналитике и моделированию. Реализованный нами проект стал финалистом номинации Cloudera Data Impact 2020 в категории Data For Enterprise AI.ВыводыПосле двух лет промышленной эксплуатации нашей Системы мы сегодня с уверенностью можем сказать, то экосистема Hadoop полностью позволяет реализовать все современные требования к аналитической платформе при использовании дистрибутива Cloudera и при правильных архитектурных подходах. Система может полностью вытеснить все традиционные аналитические СУБД без какого-либо ущерба к накопленному опыту разработчиков и аналитиков. Нужно всего лишь принять правильные решения и сделать “прыжок веры”. Традиционно консервативный Газпромбанк сделал с нами этот “прыжок веры” и смог построить современную аналитическую платформу, ввязавшись в гонку на розничном рынке в кратчайшие сроки.Об успехах в цифрах можно посмотреть в записи нашего совместно доклада.Для проектирования современной аналитической системы не требуется гетерогенная архитектура слоеного пирога с пропитками из “гринпламов”, “тарантулов”, “игнайтов” и так далее. Все данные и сервисы работы с данными должны находится под управлением одной целостной системы. Такой подход снижает наличие дополнительных точек интеграции, а следовательно, и потенциальные отказы. Не требуются  дополнительные работы и длительные сроки по интеграции и «пропитке» этих слоев данными. Наш  архитектурный подход позволяет ускорить внедрение нового функционала и как следствие улучшить time to market новых продуктов, основанных на data driven процессах.В современных аналитических задачах не существует понятий горячих и холодных данных. Ситуация “прилета” пачки проводок, за диапазон t - 3-5 лет - это каждодневная регламентная ситуация. И для такого случая вы должны пересчитать остатки, обороты, просрочки и предоставить данные для модели или определения сегмента клиента в аналитическом CRM. Как я уже писал выше, чем глубже в истории данные, тем точнее ваши модели. Такие задачи можно решить только если все данные в одном месте и в одной системе. Наш принцип  - все данные горячие!Для успешной реализации проектной команде недостаточно опыта знания технологии Hadoop. Hadoop это всего лишь инструмент. Необходимо применять подходы проектирования классического ХД на базе SQL MPP, иначе ваша система навсегда останется “помойкой” под архивные данные, нарисованной внизу слоеного пирога как “хранилище неструктурированных и холодных данных” на архитектурной картинке.Наши ближайшие планыВ настоящий момент мы находимся в завершающей стадии миграции на новую платформу Cloudera Data Platform 7.1. Вполне вероятно, что на момент публикации мы уже на CDP и в ближайшее время тут будут опубликованы результаты. Пока, можно с уверенностью сказать, что после проведенных тестов, мы ожидаем ряд оптимизационных улучшений, связанных с Impala 3.4, появлением страничных индексов в parquet, наличием Zstd компрессии. Новые сервисы вроде Atlas и Cloudera Data Flow позволят закрывать функции управления данными и потоковой аналитики «из коробки». В ближашее время мы также планируем пилотировать родной для Cloudera BI инструмент - Cloudera Data Visualization.Что еще мы сделали в нашем ландшафте Hadoop:Real-time интеграция системы с процессинговым центром с использованием Kudu (real-time клиентские данные, доступные для работы с минимальной задержкой наступления события). Горячие данные в Kudu, холодные в Parquet, общий «склеивающий» интерфейс доступа для пользователей через SQL Impala. Результат - данные в реальном времени о состоянии карточных транзакций и остатков по карточному счету открывают для бизнеса новые возможности.Историзируемый слой ODSПостроение слоя ODS с использованием Oracle Golden Gate с сохранением истории изменения источника с возможностью задания гранулярности истории по каждому объекту репликации, а также архивированием в Hadoop с возможностью «схлопывания» интервалов «холодных» данных.Графовая аналитикаПостроение витрины property графа в Hadoop;Загрузка в графовую БД Arango;Интерфейс работы с графом для андерайтеров над Arango;Графовые модели (анализ окружения клиента при скоринге);Текстовая аналитикаРабота моделей по распознаванию первичных документов клиента и поиска в них аномалий (контроль фронта, антифрод, автоматизация работы с заявкой);Анализ новостных лент, тематических форумовГеоаналитикаАнализ удаленности и проходимости офисов от основных пешеходных маршрутов, автомобильных проездов и парковок;Оптимизация курьерских маршрутовСистема управления качеством данных, позволяющая оценить качество всех загружаемых и производных данных для принятия решений об использовании этих данных на прикладном уровне. Результат - мониторинг через визуальные дашборды и почтовые рассылки состояния качества данных аналитического слоя, поставка данных в системы потребители вместе с “паспортом качества”.Контейнеризация пользовательских приложений и моделей с использованием окружения K8SКаждый пункт из этого списка достоин отдельной развернутой статьи, которые обязательно появятся в будущем. Следите за обновлениями, задавайте ваши вопросы и делитесь своим опытом.Авторы:Евгений Вилков, ГлоуБайт.Колесникова Елена, Газпромбанк (АО).    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем привет! В этой статье расскажем про инструментарий для анализа мобильных приложений, который мы используем каждый день. Для начала поговорим про то, как запускать мобильные приложения, чем смотреть трафик, а также рассмотрим инструменты для статического и динамического анализа мобильных приложений.ДевайсыНастоящее устройствоВ Digital Security примерно 99% аудитов проводятся на настоящих девайсах. Это избавляет от множества проблем. С ходу могу назвать следующие проблемы, которые решает использование реального устройства:Проще пройти аттестацию среды, если таковая реализована в приложенииНе тратятся полезные ресурсы на рабочей машинеВстречаются приложения, у которых некоторые функциональности вынесены в нативные библиотеки, и при этом они скомпилированы только для архитектуры ARM. Большинство виртуальных девайсов — это x86_64 или x64; в Android Studio есть ARM-девайс, но версия Android и быстродействие оставляют желать лучшегоБыстродействие эмулятора всегда ниже, чем у реального устройстваВ основном мы используем Samsung с Android 12, пару Xiaomi и пикселей. Также недавно у нас появился планшет на Harmony OS.Виртуальный девайсAndroid Studio EmulatorЧтобы создать виртуальное устройство, создаем пустой проект в Android Studio, выбираем вкладку Tools, где нам нужен инструмент Device Manager. Откроется боковая вкладка с девайсами.На следующем шаге мы можем выбрать из двух версий  —  с Google Play и без. Если выбрать первую, то у вас не будет возможности выйти в режим с правами суперпользователя.После скачивания необходимых файлов на вкладке Device Manager появится ваше устройство. Можем запускать.Windows Subsystem for AndroidПро этот интересный инструмент мы уже рассказали в прошлой статье по настройке WSA — обязательно посмотрите :)ПроксиАнализ мобильных приложений неразрывно связан с анализом трафика между сервером и клиентом (мобильным приложением). Для этого можно найти множество (нет) инструментов. Но у нас в основном встречается три. О них ниже.BurpSuitehttps://portswigger.net/burp/releases/Это наш главный инструмент, который запущен почти всегда. Как бы это странно ни звучало, но у него самый понятный интерфейс и есть все, что нужно, из коробки. Распространяется в двух версиях — Community и Professional. Для базового анализа хватит и первой версии, к тому же всегда можно добавить нужные расширения через Extender.MitmProxyhttps://mitmproxy.org/Консольная тулза, которая позволяет, как и Burp, завернуть в нее трафик и изучать его. Есть еще веб-интерфейс, но более интересна другая функциональность.MitmProxy позволяет легко дописывать функционал при помощи Python API. Например, у нас был кейс, когда было реализовано шифрование данных, пересылаемых через вебсокеты, и хотелось посмотреть на них. А еще больше хотелось их модифицировать.Буквально за несколько минут был написан скрипт, который расшифровывал данные (у нас были необходимые ключи), передавал их дальше в Burp, который был указан как upstream proxy, и был еще один upstream proxy, который зашифровывал данные и передавал уже на бэкенд.Получился примерно такой флоу:OWASP ZAPhttps://www.zaproxy.org/download/Еще один мощный инструмент, который практически не уступает BurpSuite. Единственный его минус — миллиард настроек, в которых можно бесконечно искать необходимую галку.Хочется выделить одну из функций, которой нет в BurpSuite и которая при этом иногда  необходима — возможность ставить точку остановки для вебсокетов. В современном мире встречаются сайты или приложения, у которых почти все клиент-серверное взаимодействие основано на вебсокетах. В BurpSuite вы можете включить Interceptor и прокликивать каждое сообщение, пока не появится то самое, нужное вам, которое вы хотите модифицировать и отправить дальше. Но что делать, если таких сообщений десятки или сотни? Здесь и пригодится этот функционал.Добавить брейкпойнт можно на вкладке с вебсокетами. Мы определяем тип передаваемых данных и данные для поиска.Как только появится сообщение, которое удовлетворяет нашему шаблону, произойдет прерывание, а ZAP покажет искомое сообщение.Статический анализJADX https://github.com/skylot/jadxИнструмент, с которым знакомы почти все аналитики безопасности мобильных приложений и не только. Это самый настоящий мастхэв при анализе приложений. Если при изучении трафика мы 70% времени проводим в BurpSuite, то при исследовании мобильного приложения «‎черным ящиком» 80% времени открыт JADX.Позволяет на лету декомпилировать код, просматривать классы, ресурсы и делать поиск по всему приложению. Самый настоящий комбайн в хорошем смысле этого слова.Также позволяет подключиться к приложению в режиме отладки и ставить брейкпойнты.Apktoolhttps://ibotpeaches.github.io/Apktool/Используется ровно в двух случаях — когда нужно достать используемые ресурсы или ассеты, а также когда JADX не хватает памяти для того, чтобы открыть и декомпилировать все приложение, чтобы поискать по нему. В таком случае распаковываем приложение при помощи apktool, делаем поиск обычным grep/ripgrep и открываем класс либо в jadx, либо файл со smali-кодом в любом текстовом редакторе.ByteCode Viewerhttps://github.com/Konloch/bytecode-viewerAll-In-One тулза для анализа мобильных приложений. Можно закинуть APK и сравнить результаты сразу нескольких декомпиляторов. Очень полезно, когда JADX не может привести некоторые методы к нормальному виду, а через smali ничего не понятно :)Androguardhttps://androguard.readthedocs.io/en/latest/Очень мощный инструмент для реверс-инжиниринга мобильных приложений. Несколько раз использовали его для автоматизации проверок, когда необходимо было пройтись по функциям и XREF'ам.Также умеет рисовать графы для классов, что визуально может помочь разобраться в флоу работы мобильного приложения.Mariana Trenchhttps://github.com/facebook/mariana-trenchОдин из наиболее \"свежих\" инструментов для анализа мобильных приложений. Производит Taint-анализ по байткоду Dalvik. Легко устанавливается, из коробки уже имеет некоторый набор проверок, что поможет найти низко висящие уязвимости. Естественно, под каждый проект лучше писать свои правила — это повысит эффект от анализа и позволит сэкономить время.Главное — учесть, что если у вас приложение большое и имеет кучу функционала, то нужно запастись временем и терпением — анализ может занять до получаса. Еще из замеченного: mariana trench не учитывает, экспортируемый ли компонент. Представим базовую уязвимость, когда у нас в активити есть обработка диплинка, из него берется параметр url и передается в функцию loadUrl какой-нибудь webview. Mariana Trench подсветит это как уязвимость, однако если точка входа (Activity) у нас неэкспортируемая, то это будет False Positive.Хоть это и стоит иметь в виду, в отчет такое не напишешь.Hbctoolhttps://github.com/bongtrop/hbctool https://suam.wtf/posts/react-native-application-static-analysis-en/Консольная утилита для работы с байткодом Hermes. Одним из популярных фреймворков для кроссплатформенной разработки является React Native. Когда вы анализируете такое приложение, у вас есть три варианта развития событий:В файле ./assets/index.android.bundle будет минифицированный javascript-код, который можно попробовать прогнать через beautifier и прочесть.Рядом с файлом  ./assets/index.android.bundle будет лежать файл  ./assets/index.android.bundle.map с маппингами для собранного файла, и вы можете при помощи https://github.com/rarecoil/unwebpack-sourcemap распаковать его и прочесть оригинальный исходный код.В файле ./assets/index.android.bundle будет бинарщина. Это и есть Hermes Bytecode.Эта тулза на крайний случай. При помощи нее можно попробовать превратить код в так называемый HASM. Читать его все еще трудно, но это явно лучше, чем бинарный файл. И через кровь и слезы можно попытаться изучить логику работы, изменить и собрать обратно.Динамический анализFridahttps://frida.re/Самый известный инструмент, который позволяет на лету подключаться к приложению и внедрять в него свой код. Если сравнивать с чем-то по назначению, то в голову приходит только Xposed Framework. Если сравнивать по времени, то написание скрипта для Frida занимает намного меньше времени, чем модуля для Xposed.Чтобы установить необходимые вещи на хост, нужно выполнить следующую простую команду:pip install frida-toolsСкачиваем frida-server для нужной архитектуры, переносим его на устройство и запускаем с правами суперпользователя:su\n",
      "cd /data/local/tmp\n",
      "/data/local/tmp/frida-server-15.2.2-android-arm64После этого мы сможем увидеть его в консоли и начать писать скрипты для приложений.Objectionhttps://github.com/sensepost/objectionObjection — это тулкит, который основан на Frida и предоставляет уже готовые функции для работы с мобильными приложениями. Имеет следующие готовые функции:Работа с файловой системойБайпасс SSL-pinningДамп keychain & keystoreРабота с памятьюМанипуляции с интентамиУстановка: pip3 install objectionЗапуск: objection -g \"package_name\" exploreА еще у него просто шикарнейшие автоподсказки.MedusaВ моем представлении, это андерграунд среди средств для анализа мобильных приложений. Он мало где упоминается, что очень зря. Это самая большая коллекция скриптов для Frida из тех, что я видел. Можно сравнить с metasploit по подходу: у вас есть коллекция скриптов, вы выбираете необходимые, они компилируются вместе и инжектятся в процесс.Очень удобно и иногда экономит кучу времени. Однако пару раз в модулях находились опечатки и ошибки, из-за чего скрипты ломались, так что приходилось править руками :)Magisk-модулиЕсли посмотреть на Magisk-модули, то не так уж и много необходимых нам, которые позволяют сэкономить время.MagiskTrustUserCertshttps://github.com/NVISOsecurity/MagiskTrustUserCertsТаким модулем является MagiskTrustUserCerts. Современные версии Android не дают возможности добавить системный CA. Это надо делать вручную или при помощи таких модулей, которые на этапе загрузки системы перенесут клиентские сертификаты в системный каталог. Это необходимо, чтобы приложения доверяли нам, когда мы заворачиваем трафик на прокси для последующего анализа.Данный модуль уже упоминался нами, когда мы настраивали WSA.XPosed-модулиВвиду устаревания оригинального XPosed, мы используем его форк — LSposed. Данный форк — это модуль для Magisk, из-за чего установка производится буквально в несколько кликов.WebViewDebugHookhttps://github.com/feix760/WebViewDebugHookМало что можно написать про этот модуль, но он делает буквально следующее:WebView.setWebContentsDebuggingEnabled(true);Удобно, если приложение использует WebView и нужно взглянуть на нее \"изнутри\".Устанавливаем данный модуль, через LSposed выбираем скоуп приложений, на которые будет распространяться данный модуль.Теперь, когда у нас на активном экране будет WebView, мы сможем посмотреть на нее через Chrome (chrome://inspect).XintentДанный модуль позволяет видеть работу системы с интентами, которыми обмениваются приложения или компоненты приложения. Если вы хотите понять лучше логику работы приложения в динамике, то это отличный вариант. Как и раньше, нужно выбрать целевое приложение — и вперед смотреть логи.ADBНаверное, странно видеть тут ADB, но не упомянуть его нельзя. Он умеет многое и играет роль чуть ли не основного инструмента при взаимодействии с устройством.Проброс портов# пробрасываем 7777 порт на устройстве, чтобы трафик шел на 8080 порт хоста\n",
      "adb reverse tcp:7777 tcp:8080Манипуляции с настройкамиКомандами ниже можно указать HTTP proxy и убрать его соответственно.adb shell settings put global http_proxy 127.0.0.1:7777\n",
      "adb shell settings put global http_proxy :0Текущая on-top activity и fragmentadb shell \"dumpsys activity activities | grep mResumedActivity\"Вывести список всех приложений и путь до base APKadb shell pm list packages -fADB logcat по package namelinux:\n",
      "adb logcat --pid=`adb shell pidof -s com.example.app`\n",
      "\n",
      "windows:\n",
      "adb logcat --pid=$(adb shell pidof -s ru.dsec.mobiletask)Запустить intentБазовый пример:adb shell am start -a android.intent.action.VIEW -d \"dsec://open\"am умеет практически все. У него длинный, но крайне понятный manual :)ПрочееСюда попало все, что не вошло в остальные категории.Scrcpyhttps://github.com/Genymobile/scrcpyЭто приложение позволяет отобразить экран Android-девайса, доступного через ADB. Также позволяет вводить данные через клавиатуру. Очень удобно, когда нужно вводить огромный пароль от тестовой УЗ и делать это через телефон крайне лениво :)Есть под все ОС, что довольно редко для наших дней.ScreenStreamКазалось бы, зачем он нужен, когда есть scrcpy. Но что если нет возможности подключиться по проводу и вы находитесь в одной сети? Данное приложение нужно не очень часто, но помнить о нем полезно.Работает с полпинка, главное — иметь сетевой доступ.ЗаключениеВ этой статье представлены не все, но большинство инструментов, которые мы используем каждый день при работе с мобильными приложениями. Надеемся, что это поможет людям, которые только погружаются в анализ мобильных приложений и хотят собрать первое рабочее окружение.Есть что дополнить? Пишите в комментарии.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Если у Вас есть данные о связях людей в XML формате, то пора применять графовую аналитику.Вы наверняка часто слышали об XML и вам известно хотя бы одно приложение, экспортирующее данные в этот формат. XML имеет большую совместимость и благодаря этому применяется для обмена данными между базами данных и пользовательскими компьютерами. Но как именно с ним работать и анализировать? В этой статье разберем практическую задачу с экспортированными данными в XML и визуализацией этих данных. Некоторое время назад пришлось столкнуться с большим количеством данных контактов людей, полученных с мобильных телефонов. Для анализа данные были экспортированы в XML формат. Данные необходимо было проанализировать на наличие связей между людьми. В данном случае связью считалась запись контакта в мобильном телефоне. Зачастую все мошеннические схемы и преступные действия, производимые при помощи мобильных телефонов, а также с использованием транзакций в банке, производимых по номеру телефона, можно отследить по хранящимся данным, и для анализа возможно применение графовой аналитики. В случае признания судом тех или иных лиц преступниками не всегда возможно определить всех причастных лиц при помощи обычных методов анализа.Перейдем к анализу социальных сетей. Сеть в простейшем виде представляет собой набор точек, соединенных попарно линиями. Точки называются вершинами или узлами, а линии - ребрами. Большинство реальных сетей, таких как транспортные, социальные или компьютерные сети, являются сложными. Социальная сеть, как правило, представляет собой сеть людей, хотя иногда это может быть сеть групп людей, таких как компании. Люди или группы образуют вершины сети, а края представляют собой какие-то связи между ними, например, дружба между людьми или деловые отношения между компаниями. Объединение вершин в сообщества сети является частной характеристикой этой сети. Сообщества объясняют взаимодействие ближайших соседей вершин. В социальных сетях это объясняет, что если точка А связана с точкой B, а точка В – с точкой С, то в большинстве случаев это означает, что точка A, соединена с точкой C (обычно друзья наших друзей – это и наши друзья тоже).Для объединения вершин в сообщества применяется кластеризация. Кластеризация выделяет из большого множества объектов подмножества, которые называются кластерами таким образом, что объекты, входящие в один кластер, были подобны друг другу больше в сравнении с объектами из другого кластера.Про анализ социальных сетей и графовую аналитику есть несколько статей на сайте NTA, поэтому перейдем непосредственно к практической части.  При анализе файлов XML были найдены теги, под которыми хранятся данные мобильных контактов. В результате для извлечения необходимых номеров была написана, следующая функция, на языке python:import networkx as nx\n",
      "import os\n",
      "import re\n",
      "import sys\n",
      "import xml.etree.ElementTree as etree\n",
      "\n",
      "\n",
      "def import_contacts(root):\n",
      "    # импортирует список контактов из XML (пустые значения отбрасываются)\n",
      "    contacts = []\n",
      "    tag = root.find(\"./MobileDevice \")\n",
      "\n",
      "    if tag is None:\n",
      "        return None\n",
      "\n",
      "    tag = tag.attrib\n",
      "\n",
      "    if tag.get('Name') == 'PHONEBOOK':\n",
      "        for phone in root.findall(\"./MobileDevice /Item/Field/[@Type='FLD_PB_NUMTYPE_MOBILE']\"):\n",
      "            if phone.text is None:\n",
      "                continue\n",
      "            else:\n",
      "                contacts.append(phone.text)\n",
      "    elif tag.get('Name') == 'AGGREGATED_CONTACTS':\n",
      "        for phone in root.findall(\"./MobileDevice /Item/Field/[@Type='FLD_AGGRCONT_NUMTYPE_MOBILE']\"):\n",
      "            if phone.text is None:\n",
      "                continue\n",
      "            else:\n",
      "                contacts.append(phone.text)\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "    return contactsПолученные данные необходимо было нормализовать к единому формату телефонных номеров, т.к. где-то они хранятся в написании с 8-ки, где-то с 7, где-то вообще цифры, не похожие на номера телефонов. Для преобразования была написана следующая функция:def normalize_phones_list(phones_list):\n",
      "    \"\"\"\n",
      "   приводим № телефонов в единый формат (начинается с 7-ки вместо 8-ки)\n",
      "   остальные отбрасываются\n",
      "    \"\"\"\n",
      "    i = 0\n",
      "    phones = []\n",
      "    while i < len(phones_list):\n",
      "        phones_list[i] = phones_list[i].replace(u'\\xa0', u' ')\n",
      "        phones_list[i] = re.sub(r'[^0-9]', r'', phones_list[i])\n",
      "        if len(phones_list[i]) == 11:\n",
      "            if phones_list[i][0] == '8':\n",
      "                phones_list[i] = '7' + phones_list[i][1:]\n",
      "            phones.append(int(phones_list[i]))\n",
      "        i += 1\n",
      "    return phonesВсе данные были экспортированы в XML формат с названиями в виде номера телефона владельца, для того чтобы получить эту вершину (источник связей) с его контактами был написан следующий код: def get_owner_phone_from_filename(filename):\n",
      "    \"\"\"\n",
      "    получает № тел. (исп. для указания телефона-владельца) из имени файла TXT\\XML (имя файла должно содержать № тел)\n",
      "   \"\"\"\n",
      "    match = re.search(r'(\\d{11})\\.(xml|txt)', filename)\n",
      "    if match is not None:\n",
      "        owner_phone = match[1]\n",
      "    else:\n",
      "        owner_phone = None\n",
      "    return owner_phone\n",
      "Для сбора списка контактов со всех файлов и преобразования к необходимому формату была написана следующая функция:def xml_to_txt(pathname):\n",
      "    \"\"\"\n",
      "    конвертация всех XML файлов в папке pathname в TXT (каждый № тел. записывается в отдельную строку)\n",
      "   возвращается список полученных № тел. с группировкой по номерам-владельцам\n",
      "    \"\"\"\n",
      "    filenames = []\n",
      "    owners_dict = dict()\n",
      "    path = os.walk(pathname)\n",
      "    for (dirpath, dirnames, files) in path:\n",
      "        for filename in files:\n",
      "            match = re.search(r'(\\d{11})\\.xml', filename)\n",
      "            if match is not None:\n",
      "                filenames.append(match[0])\n",
      "        break\n",
      "\n",
      "    try:\n",
      "        for filename in filenames:\n",
      "            tree = etree.parse(pathname + '\\\\' + filename)\n",
      "            root = tree.getroot()\n",
      "\n",
      "            if root.tag == 'OFExport':\n",
      "                contacts_list = import_contacts(root)\n",
      "\n",
      "            contacts_list = normalize_phones_list(contacts_list)\n",
      "            with open(pathname + '\\\\' + get_owner_phone_from_filename(filename) + '.txt', 'w') as f:\n",
      "                for item in contacts_list:\n",
      "                    f.write(\"%s\\n\" % item)\n",
      "\n",
      "            owner_phone = int(get_owner_phone_from_filename(filename))\n",
      "            owners_dict[owner_phone] = contacts_list\n",
      "    except etree.ParseError as e:\n",
      "        print(\"Ошибка при обработке XML-файла:\", filename)\n",
      "        print(e)\n",
      "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
      "\n",
      "    return owners_dict\n",
      "Далее для формирования графа по вершинам-владельцам и их связям с контактами был написан следующий код:def import_from_txt(pathname):\n",
      "    \"\"\"\n",
      "    загрузка № тел. из TXT файлов, расположенных в папке pathname и её подпапок\n",
      "    имена подпапок используются в качестве установленных сообществ\n",
      "    номер-владелец берется из имени файла\n",
      "    (каждый № тел. в файле должен быть в отдельной строке, имя файла из 11 цифр № тел)\n",
      "    возвращается список полученных № тел. с группировкой по номерам-владельцам\n",
      "   \"\"\"\n",
      "    case = 'without'\n",
      "    contacts_list = []\n",
      "    G = nx.Graph()\n",
      "    for dirName, subdirList, fileList in os.walk(pathname):\n",
      "        if dirName != pathname:\n",
      "            match = re.search(r'(TXTdata\\\\(\\w+))', dirName)\n",
      "            if match is not None:\n",
      "                case = match[2]\n",
      "\n",
      "        for filename in fileList:\n",
      "            try:\n",
      "                with open(dirName + '\\\\' + filename) as f:\n",
      "                    for line in f:\n",
      "                        contacts_list.append(line.rstrip('\\n'))\n",
      "\n",
      "                contacts_list = normalize_phones_list(contacts_list)\n",
      "                owner_phone = int(get_owner_phone_from_filename(filename))\n",
      "                G.add_node(owner_phone, case=case)\n",
      "                G.add_nodes_from(contacts_list, case=case)\n",
      "                for phone in contacts_list:\n",
      "                    G.add_edge(owner_phone, phone)\n",
      "                contacts_list = []\n",
      "            except etree.ParseError as e:\n",
      "                print(\"Ошибка при обработке TXT-файла:\", filename)\n",
      "                print(e)\n",
      "            except:\n",
      "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
      "    return G\n",
      "И наконец запускаем весь скрипт для обработки всех файлов и сохраняем результат в формат graphml:data = import_from_txt('TXTdata')\n",
      "nx.write_graphml(data, 'graph.graphml')\n",
      "sys.exit()По результатам экспорта все вершины и связи были сформированы при помощи библиотеки NetworkX, а результат записан в файл: «graph.graphml». Далее, вершины и связи были импортированы из файла в Gephi.В результате импорта была получена социальная сеть, состоящая из 12084 вершин и 12752 связей. После того, как были отфильтрованы «листы» осталось 606 вершин и 1274 связи. Далее, посредством алгоритма «Betweeness Centrality», был проведен анализ центральности, после чего, в соответствии с данной метрикой, устанавливался размер каждой вершины. Также социальная сеть была раскрашена в соответствующий цвет для каждого сообщества: Для поиска сообществ (кластеризации) был применен метод Лувена, анализ при помощи данного алгоритма показал высокую скорость при большом количестве вершин. Этот метод имеет высокую точность при тестировании на известных сообществах. Метод использует иерархическую структуру, поэтому подходит для сетей различного масштаба. Разберем, как работает алгоритм. Сначала каждая вершина образует отдельное сообщество. Итерация состоит из двух этапов. На первом этап для каждой вершины алгоритм пытается найти сообщество, перемещение в которое даст лучшее положительное изменение модулярности. Вершину переместить возможно только по смежным связям, то есть только в те сообщества, которым принадлежат вершины, смежные с данной. Итерация проверки всех вершин продолжается до тех пор, пока происходит хотя бы одно перемещение.На втором этапе происходит сжатие сети: вершины, которые входят в одно сообщество, образуют новую супервершину с соответствующим преобразованием связей. Алгоритм останавливается, когда сеть перестает изменяться.На следующем рисунке представлен результат применения этого метода.Таким образом, собранные сведения показывают тесную связь между различными сообществами, хотя и присутствуют автономные компоненты связности. Для дальнейшего анализа полученных результатов данные были переданы аналитикам.В результате анализа контактов с мобильных телефонов, которые предварительно экспортированы в XML формат, были построены связи владельцев мобильных телефонов с его контактами в записной книжке. Методы анализа социальных сетей и кластеризации помогли выявить изменения в заранее определенных сообществах. Этот метод анализа помогает выявить мошенников, которые были в связи с уже известными мошенниками.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " К нам на диагностику поступил накопитель Seagate ST4000DM000 семейства Lombard. Со слов клиента можно было понять, что накопитель использовался на компьютере Apple Macintosh и был на нем отформатирован, и не один раз, за все время эксплуатации. Вопросы касательно состояния накопителя или типа файловой системы остаются без ответа. Клиентом дается лишь сбивчивое пояснение, что необходимо восстановление файлов с оригинальной структурой каталогов. Также клиент уточняет, что в одном из сервисов были получены файлы без оригинальных имен с помощью какой-то программы восстановления данных, но его такой результат не устраивает.\n",
      "\n",
      "\n",
      "\r\n",
      "Приступая к диагностике, первым делом необходимо оценить состояние накопителя. Так как это Seagate, то нам необходимо увидеть терминальный лог с момента подачи питания, S.M.A.R.T. и оценить способность чтения головок в зонах разной плотности. Как правило, такой коротенький тест позволяет выявить многие неисправности.\n",
      "\r\n",
      "Подключаем, подаем питание. В терминале накопитель кратко сообщает, что процедура старта прошла успешно, и по регистрам DRD и DSC демонстрирует готовность к принятию команд.\n",
      "\n",
      "\n",
      "Рис. 2 Терминальный лог старта HDD Seagate ST4000DM000\n",
      "\r\n",
      "Следом необходимо проверить показания S.M.A.R.T. (что такое SMART и на что в нем обращать внимание я уже описывал в своей заметке).\n",
      "\n",
      "\n",
      "Рис. 3 Показания S.M.A.R.T.\n",
      "\r\n",
      "Первое, что мы смотрим, так это время наработки (атрибут 0х09), так как если окажется, что оно близкое к нулевому значению, то нет смысла обращать внимание на показания S.M.A.R.T., поскольку будет высока вероятность, что статистика кем-то была сброшена технологическими командами, и текущие показания не показывают всех событий, зафиксированных за время работы накопителя. В нашем случае время наработки 3 696 часов, что свидетельствует о том, что скорее всего в показания S.M.A.R.T. вмешательства не было.\n",
      "\r\n",
      "Далее обращаем внимание на показания атрибутов 0х05, 0хС5 (197), 0хС6 (196) в столбце RAW. Нулевые значения свидетельствуют о том, что за время работы накопителем не были зафиксированы серьезные проблемы с чтением с поверхности и никаких переназначений (remap) не выполнялось.\r\n",
      "Показание атрибута 0хС7 (199) намекает на возможные проблемы с трансфером данных в высокоскоростных режимах. С учетом того, что количество ошибок невелико, пока не будем делать преждевременных выводов.\n",
      "\r\n",
      "Так как это не накопитель с черепичной записью (SMR), то способность чтения всех головок в зонах разной плотности оценить достаточно просто. Для этого достаточно знать количество головок, ориентировочный размер минизон и их порядок чередования в выстраивании логического пространства накопителя. Для демонстрации используем Data Extractor. Построим карту минизон.\n",
      "\n",
      " \n",
      "Рис. 4 Карта минизон в логическом пространстве Seagate ST4000DM000\n",
      "\r\n",
      "Из списка виден порядок использования минизон для выстраивания логического пространства:\r\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 7, 6, 5, 4, 3, 2, 1, 0 и далее цикличное повторение. Исходя из размера одной минизоны у данного накопителя, очевидно, что достаточно прочитать несколько участков логического пространства (обычно это начало, середина и конец логического диапазона) протяженностью около 500 000 секторов, чтобы убедиться, что накопитель не зависает и скорость сканирования резко не проседает ни по одной из поверхностей. \n",
      "\r\n",
      "Использовалось именно чтение с поверхности, а не верификации, чтобы заодно и проверить не будут ли возникать ошибки при передаче данных. В данном случае каких-либо ошибок при чтении не обнаружено. Данный набор действий позволяет считать накопитель условно исправным и приступить к анализу структур файловых систем. \n",
      "\r\n",
      "Первоначально оценим, существуют ли на данный момент на диске какие-либо разделы и какие файловые системы там используются. \n",
      "\r\n",
      "Хочу обратить внимание, что на дисках, где количество секторов больше чем 4 294 967 296 секторов, необходимо использовать GPT, чтобы задействовать всю емкость, так как в классической таблице разделов используются 32-битные величины, разрядности которых недостаточно. В нашем случае ST4000DM000 представляет из себя накопитель емкостью 4Тб, в котором логический диапазон состоит из 7 814 037 168 секторов по 512 байт. \n",
      "\r\n",
      "Начнем с просмотра содержимого в LBA 0.\n",
      "\n",
      "\n",
      "Рис. 5 Таблица разделов описывающая наличие GPT.\n",
      "\r\n",
      "Здесь мы обнаруживаем классическую таблицу разделов, с описанием одного тома. По смещению 0x1C2 указан тип раздела 0xEE со смещением 0х00000001 сектор от начала диска и размером 0x3A3817D5. \n",
      "\r\n",
      "Назначение этой записи – указать, что все содержимое диска доступное для классической таблицы разделов занято, чтобы различные старые дисковые утилиты, не имеющие понятия о GPT, не могли создать раздел. Но в случае дисков, где количество секторов больше, чем 4 294 967 296, защищенная область должна быть 0xFFFFFFF, а не 0x3A3817D5.\n",
      "\r\n",
      "Обращаем внимание, что величина 0x3A3817D5 (976 754 645) приблизительно в 8 раз меньше 7 814 037 168 – общего количества секторов на диске. Это нам позволяет сделать предположение, что вероятнее всего диск использовался как устройство, размер сектора у которого равен 4096 байт, а не 512 байт. Проверим предположение и попробуем поискать регулярное выражение 0x45 0x46 0x49 0x20 0x50 0x41 0x52 0x54 (EFI PART). Если оно будет в секторе 1, то предположение неверно, если будет в секторе 8, то предположение подтвердится.\n",
      "\n",
      " \n",
      "Рис. 6 Заголовок GPT\n",
      "\r\n",
      "Также проверим, описаны ли какие-либо тома в данной GPT, для чего перейдем к сектору 16\n",
      "\n",
      "\n",
      "Рис. 7 Разделы описанные в GPT\n",
      "\r\n",
      "Здесь обнаруживаются две записи.\n",
      "\r\n",
      "Первая запись – это том размером 76 800 (614 400) секторов, где используется файловая система FAT32. Данный том резервируется для EFI нужд.\n",
      "\r\n",
      "Вторая запись – это том размером 976 644 858 (7 813 158 864) секторов, где используется файловая система HFS+.\n",
      "\r\n",
      "Так как версия с тем, что диск использовался как устройство с размером сектора 4096 байт подтверждена, то следующим шагом будет продолжение анализа с использованием Data Extractor.\n",
      "\r\n",
      "После создания задачи меняем параметр размера сектора с 512 на 4096 и получаем следующую картину.\n",
      "\n",
      " \n",
      "Рис. 8 Параметры HFS+\n",
      "\r\n",
      "На диске видим два тома с корректными файловыми системами. Первый том, исходя из роли и размера, нас не интересует. А вот второй том уже представляет интерес.\n",
      "\r\n",
      "Из временных меток можно сделать вывод, что данный том создан 19 октября 2020 года, что является относительно близкой датой к моменту поступления диска к нам.\n",
      "\r\n",
      "Сканирование CatalogFile+Journal (структур HFS+) показывает, что диск пуст на 99,9% и признаков пользовательских данных, описанных этой файловой системой, нет.\n",
      "\r\n",
      "Теперь необходимо проверить предположение, что, возможно, на данном диске были иные тома и файловые системы, а не только те, что представлены сейчас. Для этого воспользуемся инструментом поиска различных регулярных выражений, характерных для различных структур файловых систем и файлов.\n",
      "\n",
      " \n",
      "Рис. 9 Результат поиска регулярных выражений.\n",
      "\r\n",
      "Анализ области около 2Гб, который длится менее 2 минут, показывает нам, что кроме существующих FAT32 и HFS+ в наличии признаки существования тома с файловой системой ExFAT. Первое, что нас интересует, – это просмотр корневого каталога тома (ExFAT Root Directory).\n",
      "\n",
      "\n",
      "Рис. 10 Корневой каталог ExFAT\n",
      "\r\n",
      "Бросается в глаза метка тома «Transcend». Странность в том, что широко распространены внешние диски этого производителя в форм-факторе 2,5 дюйма, а не 3,5. И достаточно невысокая вероятность, что пользователь сам когда-либо решил поставить подобную метку тома.\n",
      "\r\n",
      "Выпишем названия директорий, которые описаны в корневом каталоге и зададим вопросы клиенту на предмет того, не они ли являются искомой информацией.\n",
      "\r\n",
      "Итак, по прошествии чуть более 10 минут переходим к продолжению беседы с клиентом, в процессе которой выясняется, что он не является владельцем информации и не может пролить свет на то, какие данные содержались на диске, и что ему требуется сделать звонок менеджеру для уточнения задания.\n",
      "\r\n",
      "В процессе диалога можно предположить, что клиент является курьером организации-посредника на рынке услуг восстановления данных. Дальнейшие переговоры эту версию подтверждают, так как после озвучивания информации клиентом своему менеджеру следует пауза. Видимо менеджер также не в курсе того, что именно должно быть на диске. Но спустя минут 15 клиенту поступает звонок, в котором сообщается, что это именно те данные, которые необходимо извлечь, и что их объем должен быть около 2Тб. Также мы информируемся о том, что нам предоставлена для анализа посекторная копия оригинального носителя, сделанная с помощью WinHex.\n",
      "\r\n",
      "Наконец-то становится понятным задание и то, что мы на верном пути. Можно снова брать накопитель у клиента и переходить к продолжению диагностических мероприятий. Конечно, будь у нас вся эта информация с самого начала, процедура экспресс-диагностики была бы значительно короче.\n",
      "\r\n",
      "Для реконструкции ExFAT нам необходимо знать, каким был размер кластера у данной файловой системы, и определить позицию нулевого кластера (точки отсчета). Далее поискать останки таблицы размещения файлов и битовую карту занятого пространства (Bitmap).\n",
      "\r\n",
      "Отдельное нелестное слово нужно сказать о разработчиках ExFAT. В угоду производительности файловой системы было принято решение, что в таблицу помещается информация только о фрагментированных цепочках. Линейно расположенные данные никак не проявляются в таблице. При создании данной файловой системы на непустом диске таблица расположения файлов не очищается и в ней могут присутствовать мусорные данные. К сожалению, такая идеология не лучшим образом сказывается на сложности восстановления данных.\n",
      "\r\n",
      "При анализе первых 2Гб были найдены части директорий ExFAT. Оценив размер этих структур и дальнейшее заполнение нулями до начала иных данных несложно установить размер кластера. После просмотра нескольких директорий видим ярко выраженные интервалы по 256 (2048) секторов. Это позволяет нам предположить, что размер кластера был 1 048 576 (0х100000) байт или 1 Мб.\n",
      "\r\n",
      "Для определения начала отсчета посмотрим на позиции близлежащих директорий. Обратимся вновь к рисунку 10. В частности, нас интересует директория «$RECYCLE.BIN», так как она располагается практически в самом начале. Номер ее кластера указан в смещении 0х94 и представляет из себя двойное слово (DW), в котором записано значение 0x00000005, то есть директория расположена в кластере 5. Также обратим внимание на директорию «Ххххххххх Хххх.photoslibrary», которая согласно значения указанного в смещении 0xF4, расположена в кластере 7. Данные директории тем хороши, что там с высокой вероятностью ожидается предсказуемый набор директорий или файлов.\n",
      "\r\n",
      "Далее от корневой директории с шагом 0x100000 байт или 256 (2048) секторов полистаем вперед по адресному пространству.\n",
      "\n",
      " \n",
      "Рис. 11 Директория ExFAT, возможно $RECYCLE.BIN\n",
      "\r\n",
      "Содержимое похоже на вариант пустой папки корзины, где кроме файла «desktop.ini» ничего не описано. Расположение файла по смещению 0х34 указывает на кластер 6 и размер 0х81 (129) байт. Перейдем еще на 1 кластер вперед\n",
      "\n",
      "\n",
      "Рис. 12 Содержимое файла desktop.ini\n",
      "\r\n",
      "Содержимое очень похоже на то, что обычно можно увидеть в файлах «desktop.ini» и соответствует размеру 0х81 (129) байт. Есть основания предположить, что на рис.11 папка $RECYCLE.BIN, а на рис. 12 описанный в ней файл. Если предположение верно, то в следующем кластере мы должны увидеть директорию, и ее содержимое, вероятно, должно быть похожим на типичную папку photoslibrary для MacOS на Apple Macintosh.\n",
      "\n",
      " \n",
      "Рис. 13 Директория ExFAT, возможно ХххххххххХххх.photoslibrary\n",
      "\r\n",
      "Как видим, предположение оказалось верным, и мы увидели имена ожидаемых директорий. Количество совпадений на данном участке можно посчитать достаточным и сделать расчет нулевой точки и позиции корневой директории некогда существовавшего тома. \r\n",
      "Корневая директория находится в кластере 4. Так как она предшествует директории $RECYCLE.BIN, номер кластера которой 5.\n",
      "\r\n",
      "Нулевая точка по отношению к $RECYCLE.BIN должна находится на расстоянии минус 5 кластеров. Позиция $RECYCLE.BIN 37 888 (303 104) сектор. 5 кластеров это 1280 (10 240) секторов. Выполнив простое вычитание получаем искомую позицию: 37 888 (303104) – 1 280 (10240) = 36 608 (292864) или смещение от начала логического пространства в байтах это 292 864 * 512 = 149 946 368 (0x8F00000).\n",
      "\r\n",
      "Далее, имея начальную точку отсчета, размер кластера и позицию корневого каталога, попробуем значительно большим числом проверок подтвердить корректность нашего предположения.\n",
      "\r\n",
      "Средствами Data Extractor это не так быстро сделать для раздела ExFAT, поэтому монтируем диск в ОС (с запретом записи).\n",
      "\n",
      " \n",
      "Рис. 14 Меню монтирования дисков в ОС в утилите PC3000 Win 7 Disk\n",
      "\r\n",
      "Используем бесплатный Image Explorer от софт-центр, где, открыв диск, мы можем быстро написать параметры виртуальной файловой системы и произвести оценку правильности предположений.\n",
      "\n",
      "\n",
      "рис. 15 Развернутое древо каталогов ExFAT\n",
      "\r\n",
      "Как видим на скриншоте, директории и файлы на своих местах, что позволяет сделать вывод, что параметры файловой системы определены верно.\n",
      "\r\n",
      "На этом диагностические мероприятия можно остановить и далее согласовывать с клиентом следующий перечень работ:\n",
      "\n",
      "1. Поиск регулярных выражений в рамках всего логического пространства для установления возможного расположения различных типов данных.\n",
      "\n",
      "2. Как минимум реконструкция одного раздела ExFAT.\n",
      "\n",
      "3. Анализ пересечений с новыми данными, записанными поверх.\n",
      "\n",
      "4. Построение инвертированной карты по отношению к существующим данным по реконструированной файловой системе в рамках пересечения с Bitmap и поиск пользовательских данных в этих участках с последующей сортировкой найденного.\n",
      "\r\n",
      "В случае компаний-посредников, как обычно, начинается перезвон, и лишь только после согласия конечного владельца (который вряд ли подозревает, что его данные будут восстанавливаться в нашей лаборатории) дается согласие на проведение работ.\n",
      "\r\n",
      "Любая работа даже с исправными дисками все равно начинается с создания посекторной копии на другой накопитель. Данная мера необходима для того, чтобы накопитель клиента остался в неизменном виде и никакие инициативы ОС не привели к безвозвратному искажению данных. Для диска 4Тб копирование через порты PC3000Express составит около 10-12 часов.\n",
      "\r\n",
      "После создания копии приступаем к поиску различных регулярных выражений, чтобы иметь представление о распределении данных в логическом пространстве, а также посмотреть есть ли признаки иных разделов и файловых систем на этом диске.\n",
      "\n",
      " \n",
      "Рис. 16 Результат поиска регулярных выражений в границах всего накопителя\n",
      "\r\n",
      "По результатам сканирования выясняется, что пользовательских данных на диске однозначно значительно меньше, чем 2Тб, заявленные клиентом. Последнее регулярное выражение находится на 539 877 376 секторе и до конца диска более не встречается ничего похожего на данные пользователя, кроме конечного маркера вновь созданной HFS+, хотя диск до самого конца не в нулях. Вероятно, до того, как на диске был создан раздел ExFAT, этот накопитель содержал шифрованный том. Ничем другим наличие только «шумных» данных не объясняется.\n",
      "\r\n",
      "В таком случае важно сопоставить результат поиска регулярных выражений с битовой картой.\n",
      "\n",
      "\n",
      "Рис. 17 Фрагмент сектора корневой директории ExFAT\n",
      "\r\n",
      "По смещению 0х34 указан номер кластера 2 – это позиция битовой карты (Bitmap) на разделе ExFAT. По смещению 0х38 указан размер структуры 0х0746F1 (476 913 байт или 3 815 304 бита). При анализе данной структуры установлено, что возведенные биты в карте есть только для первых 270Гб, а далее, согласно карте, раздел пуст. То есть битовая карта соответствует результатам поиска регулярных выражений, но и то, и другое расходится со словами клиента.\n",
      "\r\n",
      "Разумеется, при обнаружении такого серьезного несоответствия работы приостанавливаются и приходится снова связываться с клиентом-посредником и пытаться получить ответы на вопросы:\n",
      "\n",
      "1. Действительно ли им была создана полная посекторная копия, которую передали нам для анализа?\n",
      "\n",
      "2. Действительно ли владелец уверен, что на этом диске было 2Тб данных?\n",
      "\n",
      "3. И если уверен, то согласен ли на продолжение работ по восстановлению данных зная, что на этом диске не может быть получено данных более 270Гб?\n",
      "\r\n",
      "На первый вопрос мы получили ответ посредством удаленного доступа к оригинальному диску. И в дисковом редакторе, пролистав его с некоторым крупным шагом, сравнили с той копией, что у нас. Выяснилось, что копия была полной.\n",
      "\r\n",
      "На второй вопрос был передан ответ, что владелец информации считает, что он точно видел, что диск заполнен на 2Тб, но уже не очень в этом уверен.\n",
      "\r\n",
      "Но при все уверенности клиента в том, что данных было больше, все же дается согласие на продолжение работ.\n",
      "\r\n",
      "Перед реконструкцией файловой системы желательно получить представление, как много фрагментированных директорий нас ожидает. Для этого возьмем результаты чернового анализа и просмотрим размер найденных директорий. Если обнаружатся директории, размер записей которых равен размеру кластера, то вероятнее всего фрагментация имеет место, если размер записей меньше размера кластера, то можно считать, что задача заметно упрощается и не потребуется ручного сращивания фрагментов директорий.\n",
      "\n",
      "\n",
      "Рис. 18 Список найденных директорий ExFAT\n",
      "\r\n",
      "В данном случае дополнительных сложностей не обнаружено, максимальный размер записей в директории составил 629 984 байт, что заметно меньше размера кластера. \n",
      "\r\n",
      "Также необходимо произвести маркирование всех областей, занятых вновь созданными файловыми структурами. Для этого мы построим карты расположения всех структур и файлов на разделах FAT32 и HFS+.\n",
      "\n",
      "\n",
      "Рис. 19 Карта структур и данных на томе HFS+\n",
      "\r\n",
      "Выполним заполнение этих мест на копии паттерном, который легко будет отличить от любых пользовательских данных, а также в задаче копирования для этих участков изменим легенду с успешно прочитанных на прочитанные с ошибкой. Это будет необходимо для дальнейшего выявления испорченных перезаписями файлов.\n",
      "\r\n",
      "Для дальнейшего более удобного использования аналитических инструментов Data Extractor необходимо описать раздел в таблице разделов и создать загрузочный сектор для ExFAT раздела.\n",
      "\n",
      "\n",
      "Рис. 20 Таблица разделов с прописанным томом ExFAT\n",
      "\r\n",
      "По смещению 0x1D2 впишем тип тома 0х07. Данный тип используется как для NTFS так и для ExFAT.\r\n",
      "По смещению 0х1D6 указатель на начало тома ExFAT. Пусть это будет 32 сектор (0х20).\r\n",
      "По смещению 0х1DA впишем максимально допустимый для классической таблицы разделов размер тома (хоть это значение и меньше реального размера тома, но в данном случае допустимо, так как мы не планируем этот поврежденный том монтировать в какую-либо ОС, и ненулевое значение нам нужно лишь для нормальной работы инструментов Data Extractor).\n",
      "\n",
      "\n",
      "Рис. 21 Загрузочный сектор ExFAT\n",
      "\r\n",
      "Так как Data Extractor весьма чувствителен к содержимому загрузочного сектора ExFAT, то зачастую заполнение только важных полей окажется недостаточным (что не очень логично), и так легко отобразить раздел во внутреннем проводнике, как это было на диагностике в Image Explorer, не получится. Поэтому в случае загрузочного сектора ExFAT лучше взять стандартный шаблон, и в него ввести правильные значения.\n",
      "\r\n",
      "Для своего удобства напишем загрузочный сектор в таком виде, каким бы он был, если бы накопитель использовался бы как устройство с сектором 512 байт. Это обеспечит нам корректную работу всех инструментов комплекса без лишних перестроений карт.\n",
      "\r\n",
      "Заполним поля:\n",
      "Bytes Per Block – количество байт в секторе. В ExFAT указывается степень, в которую нужно возвести 2, чтобы получить размер. \n",
      "Block per Cluster – количество секторов кластере. Здесь также указывается степень, в которую нужно возвести 2, чтобы получить количество.\n",
      "Total Clusters – количество кластеров, доступных на томе. Вписываем значение 3 815 304. Оно получается из умножения размера битовой карты на 8.\n",
      "Total Blocks – количество секторов. Значение получается из умножения Total Clusters на размер кластера (который, в свою очередь, получается из перемножения Bytes per Block на Blocks per Cluster)\n",
      "FAT offset – смещение от загрузочного сектора до таблицы размещения файлов. Создадим пустую структуру и положим с сектора 64. Впишем в нее стандартный заголовок.\n",
      "Block Per FAT – количество секторов, которое занимает FAT таблица. Ее размер несложно посчитать исходя из количества кластеров. Block Per FAT = Total Clusters / (Bytes per Block /4) c дальнейшим округлением до целого в большую сторону. 3 815 304 / (512/4) = 29 807, 0625 = 29 808.\r\n",
      "(Как бы ни пытались в некоторых источниках назвать ExFAT 64-битной файловой системой, таблица размещения файлов используется 32-битная, но, в отличие от FAT32, для адресации используются 32 бита, а не 28.)\n",
      "Number of FATs – количество копий таблиц. К сожалению, при создании разделов оно обычно равно 1.\n",
      "Cluster Heap Offset – указывается смещение до битовой карты в секторах. \n",
      "Root Dir Cluster – номер кластера корневой директории.\n",
      "\r\n",
      "После того, как раздел стал доступен в проводнике Data Extractor, мы построим карту занятого пространства по битовой карте.\n",
      "\n",
      " \n",
      "Рис. 22 Карта занятого пространства структурами ExFAT и пользовательскими данными согласно битовой карты.\n",
      "\r\n",
      "Также построим карту расположения файлов по существующим файловым записям, отсортируем по порядку расположения файлов на диске и сопоставим с данными битовой карты.\n",
      "\n",
      "\n",
      "Рис. 23 Фрагмент карты расположения файлов доступных на томе ExFAT\n",
      "\r\n",
      "По результатам построения карты расположения файлов наблюдаем достаточно обширную «дыру» в логическом диапазоне с 718 528 до 57 131 008. На битовой карте очевидно, что эта область занята пользовательским данными. Также при поиске регулярных выражений по всему диску в этой области обнаруживались признаки данных. \n",
      "\r\n",
      "В таком случае подтверждается факт повреждения данной файловой системы и необходимость дальнейших аналитических действий.\n",
      "\r\n",
      "Выполняем инверсию карты расположения файлов, чтобы получить список цепочек пространства, которое не описано существующими файловыми записями. Удаляем всем цепочки, размер которых меньше размера кластера, так как это будут свободные фрагменты кластеров, которые не полностью заняты записанными в них пользовательскими данными. Сопоставляем с битовой картой и оставляем только пересекающиеся цепочки из этих диапазонов.\n",
      "\r\n",
      "Оставшийся результат подлежит дальнейшему анализу — поиску директорий ExFAT. Создадим директорию, в которой сформируем записи – указатели на найденные каталоги, а также впишем записи из найденных фрагментов каталогов. Найденные директории необходимо проверить на предмет содержания записей, пересекающихся с доступными директориями, установить их взаимосвязи, а также проверить соответствие заголовков файлов, на которые указывают записи в этих директориях, и отсеять неактуальные директории. Потеря директорий могла быть вызвана как ошибками в файловой системе при эксплуатации диска, так и частичным перекрытием новыми данными, записанными на диск.\n",
      "\n",
      "\n",
      "Рис. 24 Директория с указателями на найденный структуры у которых отсутствует родительский объект.\n",
      "\r\n",
      "Далее, дополнив карту расположения файлов объектами, обнаруженными в потерянных директориях, перейдем к повторению процедур с построением инвертированный карты с учетом пересечения с битовой картой. В полученных таким образом цепочках необходимо поискать регулярные выражения для различных типов пользовательских файлов. \n",
      "\r\n",
      "Это финальная стадия аналитических работ, результатом которой будут остатки пользовательских данных, для которых отсутствуют элементы файловой системы, описывающие их расположение. Обращаем внимание на то, что эти меры помогли нам не включить в конечный результат различный мусор из данных, которые могли быть удалены ранее самим пользователем. \n",
      "\r\n",
      "По завершении этих операций можно приступить к копированию найденных данных с учетом наличия в карте расположения файла секторов «прочитанных с ошибкой» и таким образом отсеять файлы, которые однозначно перезаписаны новыми данными. Пометки «прочитанных с ошибкой» мы создали после построения карт объектов FAT32 и HFS+.\n",
      "\r\n",
      "На этом работы завершены. Получен максимально возможный результат не фрагментированных файлов с сохранением оригинальной иерархии каталогов, а также найдены практически все возможные потерянные файлы без включения в этот результат различных мусорных данных, характерных для программ автоматического восстановления.\n",
      "\n",
      "Предыдущая публикация: Самостоятельная диагностика жестких дисков и восстановление данных    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "В последние пару лет меня достаточно сильно интересовал вопрос ценообразования в российских интернет-магазинах. Каждый раз при заявлении интернет-магазина о большой скидке в душу закрадывается сомнение… Действительно ли такая большая скидка? Была ли реальна цена которая сейчас зачеркнута?\n",
      "Резкие изменения курса доллара в конце 2014г. подлили масла в огонь. Очень захотелось получить ответ на вопрос как зависят цены от курса доллара в реальности. \n",
      "В итоге, я решил покончить с этими вопросами и собрать историю изменения цен по российским интернет-магазинам. По катом результаты работы + несколько интересных закономерностей.\n",
      "\n",
      "Немного технический подробностей\n",
      "На данный момент в системе работают несколько десятков парсеров написанных на python.\n",
      "Хранить данные в лоб мне показалось очень расточительным, решил хранить только изменения цены. Если цена не меняется — записи в БД не создаются, такой подход позволяет очень хорошо экономить ресурсы. На данный момент в таблице всего 200 000 000 строк, что не так много для данных по ~100 000 товаров в >1000 магазинов за 8 месяцев. \n",
      "В качестве хранилища используется MySQL 5.6. Недавно пришлось переехать на SSD, так как обычные HDD на Hetzner не очень справлялись с большой нагрузкой на запись.\n",
      "\n",
      "В данной статье я хотел бы описать интересные закономерности найденные при анализе собранных данных:\n",
      "\n",
      "1. Синхронное изменение цены\n",
      "Собрав базу за несколько месяцев, я решил проанализировать коэффициент корреляции между предложениями одного и того же товара от разных магазинов. Для этого был быстренько набросан скрипт на python + pandas. Pandas в данном случае очень помог наличием функции resample.\n",
      "\n",
      "sql = \"\"\"\n",
      "SELECT pr.date, pr.shopitemid, price from prices AS pr\n",
      "JOIN shopitems AS si\n",
      "ON pr.shopitemid = si.id\n",
      "WHERE si.itemid = 1\n",
      "AND si.shopid > 10\n",
      "AND si.last_price IS NOT NULL\n",
      "ORDER BY pr.date\n",
      "\"\"\" \n",
      "df = pd.read_sql_query(sql, engine)\n",
      "for item in df['shopitemid'].unique():\n",
      "    x= df[df['shopitemid'] == item]\n",
      "    nans = x.isnull().sum()['price']/float(len(x))\n",
      "    if nans > 0.2 or len(x['price'].unique()) < 10 or \\\n",
      "    x['date'].min() >  (datetime.now() - relativedelta(months=3)):\n",
      "        df = df.drop(df[df['shopitemid'] == item].index)\n",
      "df = df.dropna()\n",
      "\n",
      "df = df.pivot(index='date', columns='shopitemid', values='price')\n",
      "df = df.fillna(method='pad')\n",
      "df = df.dropna()\n",
      "df = df.resample('24h', fill_method='pad', how='last', loffset='24h')\n",
      "mtrx =  df.as_matrix().T\n",
      "columns = df.columns.values\n",
      "\n",
      "corr = np.corrcoef(mtrx)\n",
      "z = np.where(corr > 0.90)\n",
      "for x,y in zip(z[0],z[1]):\n",
      "    if x<y:\n",
      "        print columns[x],columns[y]\n",
      "        myplot(mtrx[x])\n",
      "        myplot(mtrx[y])\n",
      "        \n",
      "        plt.show()\n",
      "\n",
      "Проанализируем историю изменения цен на примере холодильника Indesit SB 185.\n",
      "На выходе получились достаточно интересные графики типа такого:\n",
      "\n",
      "\n",
      "Ещё графики\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Здесь можно посмотреть данный график в более удобном формате.\n",
      "В данном примере видно что у трех магазинов цена изменяется абсолютно синхронно в течение 8 месяцев. Мне видятся такие вероятные причины такого явления:\n",
      "\n",
      "В двух из трёх используются системы автоматического выставления цены на основе цен конкурентов.\n",
      "Магазины как-то связаны организационно и имеют доступ к общей БД цен\n",
      "\n",
      "\n",
      "2. Появление нового смартфона.\n",
      "В момент анализа я наткнулся на график цен по Samsung Galaxy S6. \n",
      "\n",
      "\n",
      "\n",
      "Мне показался интересным момент появления телефона.\n",
      "Первая неделя: одно-два предложения с высокой ценой.\n",
      "Дальше, в течение двух месяцев, постепенно подключаются остальные магазины и разброс цен становится значительным.\n",
      "\n",
      "3. Самые дешевые интернет-магазины для каждой категории товаров\n",
      "Собрав достаточно большую базу цен, появилась идея сформировать ТОП-10 самых дешевых магазинов для каждой категории товаров.\n",
      "Разберем принципы формирования данного списка на примере категории Холодильники:\n",
      "Пробегаемся по каждому товару категории.\n",
      "Каждому магазину, продающему данный товар, начисляем баллы от 0-дорого, до 1-дешево.\n",
      "Алгоритм расчета баллов score = (maxprice-price)/(maxprice-minprice)\n",
      "Вычисляем среднее от баллов набранных каждым магазином.\n",
      "Удаляем магазины продающие очень мало товаров данной категории.\n",
      "\n",
      "Например, для категории телевизоры, получился такой список:\n",
      "\n",
      "\n",
      "Название\n",
      "\n",
      "Кол-во баллов\n",
      "\n",
      "Рейтинг на Yandex.Market\n",
      "\n",
      "Кол-во оценок на Yandex.Market\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "КРАСНАЯ КНОПКА\n",
      "\n",
      "0,877\n",
      "\n",
      "5\n",
      "\n",
      "697\n",
      "\n",
      "\n",
      "\n",
      "Топтел\n",
      "\n",
      "0,854\n",
      "\n",
      "5\n",
      "\n",
      "1358\n",
      "\n",
      "\n",
      "\n",
      "Pleer.ru\n",
      "\n",
      "0,853\n",
      "\n",
      "4\n",
      "\n",
      "52711\n",
      "\n",
      "\n",
      "\n",
      "Greenbook\n",
      "\n",
      "0,853\n",
      "\n",
      "5\n",
      "\n",
      "200\n",
      "\n",
      "\n",
      "\n",
      "ОГО\n",
      "\n",
      "0,832\n",
      "\n",
      "5\n",
      "\n",
      "4009\n",
      "\n",
      "\n",
      "\n",
      "Technosteps.ru\n",
      "\n",
      "0,832\n",
      "\n",
      "5\n",
      "\n",
      "294\n",
      "\n",
      "\n",
      "\n",
      "Soundbreeze\n",
      "\n",
      "0,812\n",
      "\n",
      "5\n",
      "\n",
      "662\n",
      "\n",
      "\n",
      "\n",
      "ELECTROGOR\n",
      "\n",
      "0,808\n",
      "\n",
      "5\n",
      "\n",
      "6445\n",
      "\n",
      "\n",
      "\n",
      "ЦИФРОВИК\n",
      "\n",
      "0,805\n",
      "\n",
      "5\n",
      "\n",
      "460\n",
      "\n",
      "\n",
      "\n",
      "ЭЛЕКТРОЗОН\n",
      "\n",
      "0,804\n",
      "\n",
      "5\n",
      "\n",
      "1664\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4. Зачем же я всё это писал?\n",
      "В качестве фидбэка от уважаемого сообщества мне очень хотелось бы получить ваши идеи по анализу данного датасета.\n",
      "Что ещё можно проанализировать, какие интересные закономерности попробовать найти?\n",
      "Чтобы предоставить возможность каждому посмотреть историю цен, тем более скоро черная пятница, я набросал простенькую веб-морду с возможностью поиска по товарам. Можете поиграться тут, надеюсь, не ляжет.\n",
      "\n",
      "UPD 01.12.2015: Добавил возможность уведомления при снижении цены ниже определенного порога.    \n",
      " В нашей статье вы узнаете, как восстановить данные с программного btrfs RAID ОС Linux. Как его создать, заменить нерабочий диск и восстановить утерянную информацию с поврежденного массива.\n",
      "\n",
      "\n",
      "\n",
      "Обычно для создания RAID массива в ОС Linux используют mdadm и lvm, подробнее об этих системах можно посмотреть в одном из наших видео уроков.\n",
      "\n",
      "\n",
      "Помимо этих инструментов, о которых рассказано в видео, еще встроенная поддержка RAID есть в файловой системе Btrfs. Она обходится собственными средствами для построения и работы с дисковыми массивами, далее давайте более детально рассмотрим все ее возможности.\n",
      "\n",
      "Файловая система btrfs\n",
      "Btrfs – это новая файловая система с поддержкой функции копирования при записи (Copy on Write) со встроенной поддержкой RAID. Суть этой замечательной функции состоит в отсутствии перезаписи старых данных при копировании. Это огромный плюс, она значительно упрощает восстановление удаленных файлов после сбоев. Так как любой сбой или ошибка в процессе переписывания никак не повлияют на предыдущее состояние файлов.\n",
      "\n",
      "ФС хранит метаданные отдельно от данных файловой системы, и вы можете одновременно использовать разные уровни RAID для этой информации, это главное ее преимущество. Также целью этой журналируемой системы, является обеспечение более эффективного управления хранилищем и увеличенной безопасностью целостности данных в ОС Linux.\n",
      "\n",
      "Перед началом использования btrfs, нам понадобиться доустановить необходимые инструменты управления ФС, выполнив следующую команду:\n",
      "\n",
      "sudo apt install btrfs-tools\n",
      "\n",
      "\n",
      "\n",
      "Создание точки монтирования\n",
      "И прежде чем собрать RAID необходимо создать каталог точки монтирования.\n",
      "\n",
      "Создаем каталог:\n",
      "\n",
      "Sudo mkdir –v /data\n",
      "\n",
      "Где «data» – это его название.\n",
      "\n",
      "\n",
      "\n",
      "Теперь приступаем к созданию массива.\n",
      "\n",
      "Как создать RAID5\n",
      "Создавая массив, не нужно в обязательном порядке размечать носители на разделы, ФС не требует этого. Объединять накопители в массив, можно как целые, так и отдельные разделы, даже объединять носители с имеющимися разделами. Для примера я покажу как создать RAID5 из пяти накопителей.\n",
      "\n",
      "Чтобы постоянно не вводить пароль root выполните команду sudo -i, после чего все команды будут выполняться от администратора.\n",
      "\n",
      "\n",
      "\n",
      "Для построения массива введите в терминале такую команду:\n",
      "\n",
      "sudo mkfs.btrfs -L data -m raid5 -d raid5 -f /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf\n",
      "\n",
      "где: L — метка или имя файловой системы,\n",
      "\n",
      "Параметр:\n",
      "\n",
      "\n",
      "d — устанавливаем тип RAID5 для данных.\n",
      "m — устанавливаем тип RAID5 для метаданных.\n",
      "f — служит для принудительного создания btrfs, даже если какой-либо из накопителей отформатирован в другой ФС.\n",
      "\n",
      "\n",
      "\n",
      "Теперь, можно монтировать, используя любой из накопителей который входит в состав.\n",
      "\n",
      "Как смонтировать RAID диск\n",
      "Я использовал 5 накопителей для создания RAID: sdb, sdc, sdd, sde и sdf. Поэтому я могу смонтировать данные ФС в каталоге data с помощью диска sdb. Открываем управление дисками и монтируем наш носитель, после чего он станет доступным.\n",
      "\n",
      "\n",
      "\n",
      "Или же можно смонтировать диск через терминал.\n",
      "\n",
      "sudo mount /dev/sdb1 /data\n",
      "\n",
      "Чтобы проверить введите sudo df -h\n",
      "\n",
      "Как видите, наш массив смонтирован в каталог /data\n",
      "\n",
      "\n",
      "\n",
      "А чтобы посмотреть информацию о занятом и свободном пространстве массива, вводим:\n",
      "\n",
      "sudo btrfs filesystem usage /data\n",
      "\n",
      "\n",
      "\n",
      "А для размонтирования массива достаточно ввести:\n",
      "\n",
      "sudo umount /data\n",
      "\n",
      "\n",
      "\n",
      "Как заменить или добавить накопитель\n",
      "Для замены накопителя нужно в терминале ввести btrfs replace. Она запускается асинхронно, то есть выполняется постепенно:\n",
      "\n",
      "\n",
      "start — для запуска,\n",
      "cancel — для остановки,\n",
      "status — а для просмотра состояния.\n",
      "\n",
      "Прежде нужно определить номер поврежденного накопителя:\n",
      "\n",
      "sudo btrfs filesystem show\n",
      "\n",
      "\n",
      "\n",
      "Затем заменить его на новый:\n",
      "\n",
      "btrfs replace start <удаляемое устройство или его ID> <добавляемое устройство> <путь, куда смонтирована btrfs>\n",
      "\n",
      "В моем случае вводим:\n",
      "\n",
      "btrfs replace start 3 /dev/sdg \n",
      "\n",
      "где: 3 – это номер отсутствующего диска, а sdg – код нового накопителя.\n",
      "\n",
      "\n",
      "\n",
      "Как восстановить поврежденный том\n",
      "Для восстановления Btrfs массива, нужно использовать встроенную опцию монтирования — recovery:\n",
      "\n",
      "sudo mount -o recovery /dev/sdb /mnt\n",
      "\n",
      "\n",
      "\n",
      "Далее начнется процесс восстановления.\n",
      "\n",
      "Восстанавливаем данные с btrfs RAID5\n",
      "Даже самая надежная и отказоустойчивая система мажет выйти из строя. Сбой системы, выход из строя накопителя, аппаратной части, повреждение метаданных, случайное удаление, неправильная настройка все это может повлечь за собой поломку RAID и утерю важных данных. Если вы столкнулись с этим воспользуйтесь программой Hetman RAID Recovery. Она способна восстановить любую информацию с нерабочих массивов или носителей, которые входили в массив. Утилита воссоздаст разрушенный RAID, пошагово вычитывая всю известную информацию, далее вы сможете скопировать все найденные данные.\n",
      "\n",
      "Подключите носители к ПК с установленной Windows, воспользуйтесь виртуальной машиной, или установите ее второй системой.\n",
      "\n",
      "Программа автоматически просканирует дисковое пространство, и отобразит всю возможную информацию о массиве.\n",
      "\n",
      "\n",
      "\n",
      "Как видите, в нашем случае с btrfs RAID, программа не собирает носители в массив, это связано со спецификой его построения, при этом вся информация хранится в соответствии с типом RAID.\n",
      "\n",
      "Для начала процесса восстановления откройте менеджер дисков, кликните правой кнопкой мыши по любому из дисков из которых состоял массив и запустите «Быстрое сканирование».\n",
      "\n",
      "\n",
      "\n",
      "При сканировании любого из дисков результат будет идентичным, так как они все являются частью одного массива.\n",
      "\n",
      "По завершении анализа программа отобразит найденные файлы, здесь доступен их пред.просмотр, с помощью которого легче найти нужные изображения или видео.\n",
      "\n",
      "\n",
      "\n",
      "Отметьте файлы, которые нужно вернуть и нажмите «Восстановить», укажите путь куда их сохранить и еще раз «Восстановить». По завершении все файлы будут лежать в указанной папке.\n",
      "\n",
      "\n",
      "\n",
      "Если в результате быстрого сканирования программе не удалось найти нужных файлов выполните «Полный анализ». Данный тип анализа займет больше времени, но при этом найдет всю информацию, которая осталась на диске, даже ту которая была давно удалена с диска.\n",
      "\n",
      "\n",
      "\n",
      "Так как это пятый RAID вся информация остается целой при отсутствии одного накопителя, но, если вышли из строя 2 диска часть информации будет повреждена.\n",
      "\n",
      "\n",
      "\n",
      "Особенности файловой системы btrfs\n",
      "\n",
      "Btrfs поддерживает сжатие данных. То есть, вся имеющаяся информация на носителях будет автоматически сжиматься. А при обращении к данным, каждый конкретный файл будет автоматически распакован.\n",
      "\n",
      "Эта функция позволяет значительно экономить пространство на накопителях, а также время на поиски стороннего ПО для сжатия данных. ФС поддерживает 3 основных метода сжатия файлов – это zlib, lzo и zstd. Их основное отличие заключается в степени, а также скорости сжатия.\n",
      "\n",
      "Наша программа поддерживает восстановление сжатых файлов любым из этих трех методов. В интерфейсе программы сжатые тома и файлы отображаются следующим образом, как видите они подсвечены другим цветом.\n",
      "\n",
      "\n",
      "\n",
      "Еще одним из главных преимуществ является создание подтомов (subvolum-ов). Простыми словами на одном накопителе можно создать к примеру 3 разных диска (subvolum-а). Эти подтома способны расширяться самостоятельно, за счет свободного пространства другого тома. Такая возможность при необходимости позволяет расширить один диск за счет другого без сжатия и переноса данных. Эти подтома отображаются в программе следующим образом, это диски внутри основного накопителя.\n",
      "\n",
      "\n",
      "\n",
      "Таким же образом в программе отображены и папки со snap-shot-ами.\n",
      "\n",
      "Для создания подтомов используйте команду:\n",
      "\n",
      "btrfs subvolume create /mnt/btrfs/my-subvolume\n",
      "\n",
      "\n",
      "\n",
      "А для просмотра списка подтомов на этом накопителе выполните:\n",
      "\n",
      "btrfs subvolume list /mnt/btrfs\n",
      "\n",
      "\n",
      "\n",
      "Для монтирования подтома:\n",
      "\n",
      "mount -o subvol=my-subvolume /dev/sdd2 /mnt/btrfs2\n",
      "\n",
      "где sdb1 — это код диска.\n",
      "\n",
      "\n",
      "\n",
      "Заключение\n",
      "Существует небольшое количество инструментов для восстановления данных, которые умеют читать btrfs RAID. При выборе стоит учитывать, что в процессе работы информация может затереться, обратите внимание на наличие функции создания образа массива и сканирования с образа. И все же лучшим решением уберечь себя от потери важных данных будет их регулярное резервное копирование.\n",
      "\n",
      "Полную версию статьи со всеми дополнительными видео уроками смотрите в источнике. Если у вас остались вопросы, задавайте их в комментариях. А также зайдите на наш Youtube канал, там собраны более 400 обучающих видео.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Всем привет. В этом топике я хотел бы провести обзор относительно нового и довольно мощного метода нелинейной динамики – метода Recurrence plots или рекуррентного анализа в приложении к анализу временных рядов. А, кроме того, поделится кодом короткой программы на языке Matlab, которая реализует все нижеописанное.\n",
      "\n",
      "Итак, начнем. По долгу службы я занимаюсь нелинейной динамикой, обработкой видео и изображений, я бы даже сказал, довольно узкой частью нелинейной динамики – нелинейными колебаниями роторов. Как известно, вибросигнал представляет собой ничто иное, как временной ряд, где в качестве сигнала выступает значение амплитуды отклонения, ну например, ротора турбины самолета. Как известно, не только колебания ротора можно представить в таком виде. Колебания биржевых котировок, активность Солнца и множество других процессов описываются простым вектором чисел, выстроенным по времени. Скажу даже больше, все эти процессы объединяет один важный фактор – они нелинейны, а некоторые даже хаотичны, что означает на практике невозможность предсказать состояние в системе на сколь угодно большой отрезок времени даже зная точно закон ее движения в виде дифференциальных уравнений. А самое главное, в большинстве случаев мы не можем даже записать эти самые уравнения в каком-либо виде. И тут на помощь приходит эксперимент и нелинейная динамика.\n",
      "\n",
      "Нелинейная динамика\n",
      "Снимая показания с датчика (скачивая файл с котировкой валют), мы имеем на выходе одномерный сигнал, как правило, сложной формы. Хорошо еще если сигнал периодический. А если нет? В противном случае мы имеем дело со сложной системой, которая к тому же, может находиться в режиме нелинейных и хаотических колебаний. Под сложной системой в данном случае понимают систему с большим числом степеней свободы. Но мы-то получаем с датчика одномерный сигнал. Как узнать о других степенях свободы этой системы?\n",
      "\n",
      "Прежде чем мы переедем к главному, стоит упомянуть о еще одном мощном методе анализа нелинейной динамики, который плавно перекочевал в методы анализа временных рядов. Это так называемое восстановление траектории системы по сигналу от всего лишь одной степени свободы. К примеру, вращается по кругу шарик на нити, и если мы смотрим в одной плоскости на это процесс, то сможем снимать сигнал лишь с по одной оси, для нас шарик будет бегать туда-сюда. И вот, выяснилось, что можно полностью восстановить N – мерный сигнал всей системы. Но не в полном объеме, а лишь с сохранением топологических свойств системы (проще говоря ее геометрии). В данном случае, если шарик движется по периодической траектории, то и восстановленная траектория будет периодической. Сам метод основан на простой формуле (в случае двумерной системы):\n",
      "\n",
      "Y(i) = X(t), Y1(i) = X(t+n), где n – это временная задержка, а Y, Y1 это уже восстановленный сигнал. Доказывается это все в теореме Такенса. В примере пространство двумерно, а в реальности оно может представлять из себя пространство сколь угодно большего числа измерений. Метод оценки размерности пространства не входит в тему данного топика, упомяну лишь о том, что это может быть, к примеру, метод ложных соседей.\n",
      "\n",
      "Метод Recurrence plots\n",
      "Итак, мы получили траекторию системы, в которой сохранены топологические свойства реальной физической системы, что очень важно. Теперь мы можем натравить на нее целый арсенал методов распознавания образов для выявления закономерностей. Но все они так или иначе имеют свои недостатки вычислительные или просто сложны в реализации. И вот в 1987 году Экман с коллегами разработали новый метод, суть которого сводиться к следующему. Полученная выше траектория, представляющая собой набор векторов размерности N отображается на двумерную плоскость по формулам:\n",
      "\n",
      "R[i,j,m,e] = O(e[i], — ||S[i] – S[j] ||), где i,j – индексы точки на плоскости, m – размерность пространства вложения, O – функция Хевисайда ||…|| — норма или расстояние (например евклидово). Внешний вид полученной и визуализированной матрицы и даст нам представление о динамике системы, изначально представленной в виде временного ряда. Все вышесказанное проиллюстрируем в виде эксперимента с числами Вольфа, которые описывают активность Солнца (вместо этого ряда легко можно взять котировки валют).\n",
      "\n",
      "Результаты\n",
      "Это собственно сам сигнал:\n",
      "\n",
      "\n",
      "\n",
      "Это восстановленная траектория в двумерном пространстве (мы берем двумерное пространство для простоты, хотя на самом оно содержит большее число измерений, но как показывает практика этого вполне достаточно для качественной оценки)\n",
      "\n",
      "\n",
      "\n",
      "Это так называемая матрица расстояний (по смыслу всего лишь расстояния от i-ой точки доя j-ой). Часто эти диаграммы имеют замысловатый рисунок, что может быть использовано (и используется) в дизайне.\n",
      "\n",
      "\n",
      "\n",
      "Ну вот собственно и сама рекуррентная диаграмма. Для описания всех методов анализа такой диаграммы не хватит и 400 страниц. Помимо качественного анализа, данный метод допускает и количественные показатели, что может с успехом использоваться в случае использования нейронных сетей. Ну а самое важное, это то, что уже бросив беглый взгляд на эту диаграмму, мы можем сказать о ней многое. Во-первых наличие полос, которые перпендикулярны главной диагонали говорит о наличии хаотического или стохастического процессов в системе (что бы сказать точно необходимы дополнительные исследования). Наличие неравномерно заполненных черными точками зон говорит о нестационарных процессах и позволяет точно определить границы этих процессов во времени. \n",
      "\n",
      "\n",
      "\n",
      "Код в Matlab\n",
      "\n",
      "clear; clc;\n",
      "%Построение рекуррентной диаграммы\n",
      "[X1,X2,X3,X4]=textread('data.txt','%f %f %f %f');\n",
      "plot(X2,X4);grid;hold; ; ylabel('Wolf');xlabel('Time');\n",
      "N = length(X1);\n",
      "M = round(0.3*N);\n",
      "M1 = N - M;\n",
      "m = 2; %Размерность пространства вложения\n",
      "t = 10; %Временная задержка\n",
      "%Сюда будем писать восстановленный аттрактор\n",
      "X(1,1) = 0; \n",
      "X(1,2) = 0\n",
      "j = 1;\n",
      "%Здесь восстанавливаем траекторию\n",
      "for i=M1:(N - t)\n",
      " X(j,1) = X3(i);\n",
      " X(j,2) = X3(i + t);\n",
      " j = j + 1;\n",
      "end\n",
      "figure;\n",
      "plot(X(:,1),X(:,2));grid;hold; ; ylabel('Wolf');xlabel('Time');\n",
      "N1 = length(X(:,1))\n",
      "D1(1,1) = 0;\n",
      "D2(1,1) = 0;\n",
      "e = 30;\n",
      "%Строим RP - диаграмму\n",
      "for i = 1:N1\n",
      " for j = 1:N1\n",
      " D1(i,j)=sqrt((X(i,1)-X(j,1))^2+(X(i,2)-X(j,2))^2);\n",
      " if D1(i,j) < e\n",
      " D2(i,j) = 0;\n",
      " else\n",
      " D2(i,j) = 1;\n",
      " end;\n",
      " end;\n",
      "end;\n",
      "figure;\n",
      " pcolor(D2) ; \n",
      " shading interp;\n",
      " colormap(pink);\n",
      " figure;\n",
      " pcolor(D1) ; \n",
      " shading interp;\n",
      " colormap(pink);\n",
      "hold on\n",
      "\n",
      "PS. Это всего лишь элементарный пример, который, естественно, не охватывает всю мощь этого метода. В простейшем случае для автоматизации процесса вы можете использовать алгоритмы поиска фигур на плоскости. Такие системы могут быть актуальны в системах мониторинга турбин самолета, а так же системах мониторинга различных финансовых процессов. Я не затронул самую интересную часть данной тематики — вычисление количественных показателей этих диаграмм, что постараюсь компенсировать в ближайших топиках.\n",
      "\n",
      "Литература\n",
      "\n",
      " J.-P. Ecmann N S. Oliffson Kamphorst and D. Ruelle, Recurrence Plots of Dynamical Systems\n",
      " www.recurrence-plot.tk\n",
      " en.wikipedia.org/wiki/Recurrence_plot\n",
      "    \n",
      " \n",
      "\n",
      "У начинающего специалиста по данным (data scientist) есть возможность выбрать один из множества языков программирования, который поможет ему быстрее освоить данную науку. \n",
      "\n",
      "Тем не менее, никто точно не скажет вам, какой язык программирования лучше всего подходит для этой цели. Ваш успех как специалиста в данной области будет зависить от множества факторов и сегодня мы постараемся их рассмотреть, а в конце статьи вы сможете проголосовать за тот язык программирования, который вы считаете наиболее подходящим для работы с данными.\n",
      "\n",
      "Специфичность\n",
      "Будьте готовы к тому, что по мере углубления в область науки о данных, вам раз за разом прийдется заново «изобретать велосипед». Кроме того, вам необходимо будет в совершенстве овладеть различными пакетами программ и модулями для выбранного вами языка программирования. Насколько хорошо вы сможете все это усвоить, зависит, в первую очередь, от наличия предметно-ориентированных пакетов программ для выбранного ЯП. \n",
      "\n",
      "Универсальность\n",
      "Ведущий специалист по данным обладает хорошими всесторонними навыками программирования, а также умением проводить расчеты и анализировать. Большая часть повседневной работы в области науки о данных направлена на поиск и обработку исходных данных или корректировку данных. К сожалению, никакие новороченные пакеты для машинного обучения вам не помогут для данных целей.\n",
      "\n",
      "Эффективность\n",
      "В быстро развивающемся мире коммерческой науки о данных есть множество возможностей быстро получить желаемую работу. Тем не менее, именно благодаря быстрому развитию области науки о данных ее постоянно сопровождают технические недароботки, и только упорная практика сможет свести к минимуму такие недочеты. \n",
      "\n",
      "Производительность \n",
      "В некоторых случаях очень важно оптимизировать производительность вашего кода, тем более при работе с большими объемами особо важных данных. Однако скомпилированные языки обычно намного быстрее, чем интерпретируемые. Аналогично, статически типизированные языки значительно более отказоустойчивы, чем динамически типизированные. Таким образом, единственным компромиссом является снижение производительности. \n",
      "\n",
      "В некоторой степени, каждый из представленных ниже языков программирования обладает одним параметром в каждой из двух групп: универсальность — специфичность; производительность — удобство.\n",
      "\n",
      "Учитывая эти основные принципы, давайте рассмотрим некоторые из наиболее популярных языков программирования, которые используются в науке о данных. Вся информация, о приведенных ниже языках программирования, основывается на моих собственных наблюдениях и опыте, а также опыте моих друзей и коллег.\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "\n",
      "R, который является прямым потомком старшего языка программирования S, был выпущен в далеком 1995 году и с тех пор становится все совершеннее. Написанный на таких языках как C и Fortran данный проект сегодня поддерживается Фондом языка R для статистических вычислений (R Foundation for Statistical Computing).\n",
      "\n",
      "Лицензия:\n",
      "\n",
      "Бесплатная\n",
      "\n",
      "Преимущества:\n",
      "\n",
      "\n",
      "Отличный набор высококачественных предметно-ориентированных пакетов с открытым исходным кодом. R имеет в своем распоряжении пакеты практически для любого количественного и статистического применения, которое можно только себе представить. Сюда входят нейронные сети, нелинейная регрессия, филогенетика, построение сложных диаграмм, графиков и многое-многое другое.\n",
      "Вместе с базовой установкой в довесок нам предоставляется возможность установки обширных встроенных функций и методов. Кроме того, R прекрасно обрабатывает данные матричной алгебры.\n",
      "Возможность визуализации данных является немаловажным преимуществом наряду с возможностью использования различных библиотек, например ggplot2.\n",
      "\n",
      "Недостатки:\n",
      "\n",
      "\n",
      "Низкая производительность. Здесь нечего сказать: R не является быстрым ЯП.\n",
      "Специфичность. R прекрасно подходит для статистических исследований и науки о данных, но он не так хорош, когда дело доходит до программирования для общих целей. \n",
      "Другие особенности. R имеет несколько необычных особенностей, которые могут сбить с толку программистов, привыкших работать с другими ЯП: индексирование начинается с 1, использование нескольких операторов присваивания, нетрадиционные структуры данных.\n",
      "\n",
      "Наш вердикт – идеальный вариант для первоначальных целей\n",
      "\n",
      "R – мощный язык, который отличается наличием огромного выбора приложений для сбора статистических данных и визуализации данных, а тот факт, что он является ЯП с открытым исходным кодом, позволяет ему собрать большое количество поклонников среди разработчиков. Именно благодаря своей эффективности для первоначальных целей этому языку программирования удалось достичь широкой популярности.\n",
      "\n",
      "Python\n",
      "\n",
      "\n",
      "\n",
      "В 1991 году Гвидо ван Россум представил язык программирования Python. С тех пор этот язык стал чрезвычайно популярным ЯП общего назначения и широко используется в сообществе специалистов по данным. В настоящее время основными версиями являются Python 3,6 и Python 2,7.\n",
      "\n",
      "Лицензия:\n",
      "\n",
      "Бесплатная\n",
      "\n",
      "Преимущества:\n",
      "\n",
      "\n",
      "Python – это очень популярный, широко используемый язык программирования общего назначения. Он имеет обширный набор специально разработанных модулей и широко используется разработчиками. Многие онлайн-сервисы предоставляют API для Python.\n",
      "Python очень прост в изучении. Низкий порог вхождения делает его идеальным первым языком для тех, кто занимается программированием.\n",
      "Такие программные пакеты как pandas, scikit-learn и Tensorflow, делают Python надежным вариантом для современных приложений в области машинного обучения.\n",
      "\n",
      "Недостатки:\n",
      "\n",
      "\n",
      "Типобезопасность. Python – это динамически типизированный язык, а это значит, что вы должны быть осторожными при работе с ним. Ошибки несоответствия типов (например, передача строки (string) в качестве аргумента методу, который ожидает целое число (integer)) могут время от времени случаться.\n",
      "Например, в случае если имеются конкретные цели статистического анализа и анализа данных, то обширный набор пакетов языка R дает ему преимущество перед Python. Кроме того, существуют более быстрые и безопасные альтернативы Python среди языков программирования.\n",
      "\n",
      "Наш вердикт – удобен во всех отношениях\n",
      "\n",
      "Python является хорошим вариантом для целей науки о данных (data science), и это утверждение справедливо как для начального, так и для продвинутого уровней работы в данной области. Большая часть науки о данных сосредоточена вокруг процесса ETL (извлечение-преобразование-загрузка). Эта особенность делает Python идеально подходящим для таких целей языком программирования. Библиотеки, такие как Tensorflow от Google, делают Python очень интересным языком для работы в области машинного обучения.\n",
      "\n",
      "SQLimg align=«center» src=«habrastorage.org/web/f7e/2cf/42d/f7e2cf42d60b4b8fa6f442504828fe57.png»/>\n",
      "\n",
      "SQL («язык структурированных запросов») определяет, управляет и запрашивает реляционные базы данных. Язык появился в 1974 году и с тех пор претерпел множество видоизменений, но основные его принципы остаются неизменными.\n",
      "\n",
      "Лицензия:\n",
      "\n",
      "Есть бесплатные и платные варианты.\n",
      "\n",
      "Преимущества\n",
      "\n",
      "\n",
      "Очень эффективен при работе с запросами, обновлениями, а также при обработке реляционных баз данных.\n",
      "Декларативный синтаксис делает SQL очень читаемым языком. Нет никакой неопределенности в том, что SELECT name FROM users WHERE age > 18 должен делать!\n",
      "SQL очень часто используется в различных приложениях, так что знакомство с ним может очень пригодиться. Модули, такие как SQLAlchemy, упрощают интеграцию SQL с другими языками.\n",
      "\n",
      "Недостатки:\n",
      "\n",
      "\n",
      "Синтаксис SQL может показаться достаточно сложной задачей для тех, кто привык к императивному программированию.\n",
      "Существует множество различных вариаций SQL, таких как PostgreSQL, SQLite, MariaDB. Все они достаточно разные, поэтому ни о какой совместимости не может быть и речи.\n",
      "\n",
      "Наш вердикт – эффективный, несмотря на время\n",
      "\n",
      "SQL более полезен в качестве языка для обработки данных, чем в качестве передового аналитического инструмента. Тем не менее, так много процессов в области науки о данных зависит от ETL, а долговечность и эффективность SQL лишний раз свидетельствуют о том, что такой ЯП должен знать каждый специалист по данным (data scientist).\n",
      "\n",
      "Java\n",
      "\n",
      "\n",
      "Java – это чрезвычайно популярный язык общего назначения, который работает на виртуальной машине Java Virtual Machine (JVM). Это абстрактная вычислительная система, которая обеспечивает плавную переносимость между платформами. В настоящее время поддерживается корпорацией Oracle.\n",
      "\n",
      "Лицензия:\n",
      "\n",
      "8-я версия – бесплатная\n",
      "\n",
      "Преимущества:\n",
      "\n",
      "\n",
      "Универсальность. Многие современные системы и приложения разработаны с помощью языка Java. Огромным преимуществом такого ЯП является способность интегрировать методы науки о данных непосредственно в существующую кодовую базу.\n",
      "Строгая типизация. Обеспечение типобезопасности не пустой звук для Java, и в случае разработки критически важных приложений для работы с большими данными эта особенность как никогда важна.\n",
      "Java – это высокопроизводительный, скомпилированный язык общего назначения. Это делает его пригодным для написания эффективного производственного кода ETL, а также алгоритмов машинного обучения с использованием вычислительных средств.\n",
      "\n",
      "Недостатки: \n",
      "\n",
      "\n",
      "«Многословность» языка Java делает его не лучшим вариантом для проведения специальных анализов и разработки более специализированных статистических приложений. \n",
      "Java не имеет большого количества библиотек для передовых статистических методов по сравнению с некоторыми предметно-ориентированными языками, например R.\n",
      "\n",
      "Наш вердикт — серьезный претендент на звание лучшего языка для работы в области науки о данных\n",
      "\n",
      "Много чего можно сказать в пользу изучения Java как языка для работы в области науки о данных. Многие компании оценят возможность беспрепятственной интеграции готового кода программного продукта в собственную кодовую базу, а производительность и типобезопасность Java являются его неоспоримыми преимуществами. Тем не менее, к недостаткам такого языка можно отнести тот факт, что у него отсутствуют наборы специфических пакетов, которые доступны для других языков. Несмотря на такой недостаток, Java является языком программирования, которому обязательно стоит уделить внимание, особенно если вы уже знаете R или Python.\n",
      "\n",
      "Scala\n",
      "\n",
      "Функционирующий на JVM язык программирования Scala был разработан Мартином Одерски в 2004 году. Это язык с несколькими парадигмами, позволяющий использовать как объектно-ориентированные, так и функциональные подходы. Кроме того, структура кластерных вычислений Apache Spark написана на Scala.\n",
      "\n",
      "Лицензия: \n",
      "\n",
      "Бесплатная\n",
      "\n",
      "Преимущества:\n",
      "\n",
      "\n",
      "Используя Scala и Spark, у вас появляется возможность работать с высокопроизводительными кластерными вычислениями. Scala – это идеальный выбор для тех, кто работает с большими объемами данных.\n",
      "Мультипарадигматический. Для программистов, работающих со Scala, доступны как объектно-ориентированные, так и функциональные парадигмы программирования.\n",
      "Scala компилируется в байт-код Java и работает на JVM. Это позволяет ему взаимодействовать с языком Java, делая Scala очень мощным языком общего назначения. Кроме того, он также хорошо подходит для работы в области науки о данных.\n",
      "\n",
      "Недостатки:\n",
      "\n",
      "\n",
      "Если вы только-только собрались работать со Scala, то будьте готовы изрядно «поломать» голову. Лучше всего загрузить sbt и настроить IDE, например Eclipse или IntelliJ, с помощью специального плагина Scala.\n",
      "Есть мнение, что синтаксис и система типов Scala являются достаточно сложными. Таким образом, программистам, привыкшим работать с динамическими языками, например Python, придется несладко.\n",
      "\n",
      "Наш вердикт – идеальный вариант для работы с большими данными\n",
      "\n",
      "Если вы решили использовать кластерные вычисления для работы с большими данными, то пара Scala + Spark – это идеальное решение. Более того, если у вас уже есть опыт работы с Java и другими статически типизированными языками программирования, то вы непременно оцените эти возможности Scala. Однако, если ваше приложение не имеет ничего общего с большими объемами данных, работа с которыми может оправдать добавление всех составляющих Scala, вы, скорее всего, добьетесь большей производительности, используя другие языки, такие как R или Python.\n",
      "\n",
      "Julia\n",
      "\n",
      "\n",
      "\n",
      "Выпущенный чуть более 5 лет назад, Julia произвела впечатление на мир вычислительных методов. Язык добился такой популярности благодаря тому, что несколько крупных организаций, включая некоторые в финансовой отрасли, почти сразу начали использовать его для своих целей. \n",
      "\n",
      "Лицензия:\n",
      "\n",
      "Бесплатная\n",
      "\n",
      "Преимущества:\n",
      "\n",
      "\n",
      "Julia – это скомпилированный язык JIT («точно в срок»), благодаря которому удается достичь хорошей производительности. Этот язык является достаточно простым, он предусматривает возможности динамической типизации и сценариев интерпретируемого языка, такого как Python.\n",
      "Julia был предназначен для проведения численного анализа, он также может рассматриваться в качестве языка программирования общего назначения.\n",
      "Читаемость. Многие программисты, работающие с данным языком, считают, что такая особенность является его наибольшим преимуществом.\n",
      "\n",
      "Недостатки:\n",
      "\n",
      "\n",
      "Незрелость. Поскольку Julia является достаточно новым языком, некоторые разработчики сталкиваются с нестабильностью во время работы с его пакетами. Тем не менее, базовые средства языка считаются стабильными.\n",
      "Еще одним признаком незрелости языка является ограниченное количество пакетов программ, а также небольшое число поклонников среди разработчиков. В отличие от устоявшихся R и Python язык программирования Julia не располагает большим количеством пакетов программ (пока что).\n",
      "\n",
      "Наш вердикт – язык, который себя еще проявит\n",
      "\n",
      "Да, главная проблема языка Julia – это его молодость, однако его нельзя за это винить. Поскольку Julia был создан лишь недавно, он пока что не может конкурировать со своими основными конкурентами, Python и R. Будьте терпеливыми и вы поймете, что существует множество причин обратить пристальное внимание на этот язык, который, непременно, сделает выдающиеся шаги в ближайшем будущем.\n",
      "\n",
      "MATLAB\n",
      "\n",
      "MATLAB – это признанный язык для численных расчетов, используемый как в научных целях, так и в индустрии. Он был разработан и лицензирован MathWorks, компанией, созданной в 1984 году, основной целью которой являлось коммерциализация программного обеспечения.\n",
      "\n",
      "Лицензия:\n",
      "\n",
      "\n",
      "Цены варьируются в зависимости от выбранного вами варианта языка\n",
      "\n",
      "Преимущества:\n",
      "\n",
      "\n",
      "MATLAB, предназначенный для численных вычислений, хорошо подходит для использования количественного анализа со сложными математическими требованиями, такими как обработка сигналов, преобразования Фурье, матричная алгебра и обработка изображений.\n",
      "Визуализация данных. MATLAB имеет ряд встроенных возможностей построения графиков и диаграмм.\n",
      "MATLAB часто можно встретить во многих курсах бакалавриата по точным наукам, таким как физика, инженерия и прикладная математика. Таким образом, он широко используется в этих областях.\n",
      "\n",
      "Недостатки:\n",
      "\n",
      "\n",
      "Платная лицензия. Вне зависимости от выбранного вами варианта (для научных, личных целей или целей компании) вам придется раскошелиться на дорогостоящую лицензию. Наш совет: обратите внимание на бесплатную альтернативу – Octave.\n",
      "MATLAB – это не лучший язык программирования для общего назначения.\n",
      "\n",
      "Наш вердикт – лучший вариант для целей, требующий значительных математических расчетов\n",
      "\n",
      "Благодаря своему широкому использованию в различных количественных вычислениях как для научных целей, так и для целей индустрии, MATLAB стал достойным вариантом для применения в области науки о данных. Он прийдется вам как нельзя кстати, если для ваших ежедневных целей необходима интенсивная, продвинутая математическая функциональность, собственно, для чего MATLAB и был разработан.\n",
      "\n",
      "Другие языки\n",
      "\n",
      "Существуют и другие популярные ЯП, которые могут представлять интерес для специалистов по данным. В этом разделе представлен их краткий обзор.\n",
      "\n",
      "C++\n",
      "\n",
      "Зачастую, C++ не используется в области науки о данных. Тем не менее, он имеет молниеносную производительность и широкую популярность. Главной причиной, по которой C++ не обрел популярности в области науки о данных, является его неэффективность для такой цели. \n",
      "\n",
      "Как написал один из участников форума:\n",
      "«Предположим, что вы пишете код для проведения какого-либо специального анализа, который, вероятно, будет запускаться только один раз. Так вот, вы предпочли бы потратить 30 минут на создание программы, которая будет работать в течение 10 секунд или потратить 10 минут на программу, которая будет работать в течение 1 минуты?»\n",
      "\n",
      "И этот парень прав! Тем не менее, C++ станет отличным выбором для реализации алгоритмов машинного обучения, оптимизированных на низком уровне.\n",
      "\n",
      "Наш вердикт – не лучший выбор для повседневной работы, но если дело касается производительности...\n",
      "\n",
      "JavaScript\n",
      "Ввиду того, что за последние несколько лет платформа Node.js активно развивалась, язык программирования JavaScript все больше и больше обретал черты серверного языка. Однако его возможности в области науки о данных и машинного обучения на сегодняшний день достаточно скромны (тем не менее, не стоит забывать про brain.js и synaptic.js!). К недостаткам JavaScript можно отнести:\n",
      "\n",
      "\n",
      "Слишком рано для него, чтобы вступить в игру (Node.js всего 8 лет!)...\n",
      "Платформа Node.js и вправду быстрая, но всегда найдутся те, кто будет активно критиковать JavaScript.\n",
      "\n",
      "К неоспоримым преимуществам Node.js можно отнести его асинхронный ввод-вывод, его растущую популярность, а также тот факт, что существует множество языков, которые компилируются с JavaScript. Так что вполне возможно, что в недалеком будущем мы увидим полезный фрейморк для работы в области науки о данных с возможностью обработки с помощью ETL в режиме реального времени. Другой вопрос: будет ли это актуально на то время…\n",
      "\n",
      "Наш вердикт – предстоит еще много чего сделать, для того чтобы JavaScript считался достойным языком для работы в области науки о данных\n",
      "\n",
      "Perl\n",
      "Perl известен как «швейцарский армейский нож языков программирования» из-за его универсальности как языка сценариев общего назначения. Он имеет много общего с Python, являясь динамически типизированным языком сценариев. Но ему еще очень далеко до той популярности, которую имеет Python в области науки о данных.\n",
      "\n",
      "Это немного удивительно, учитывая его применение в областях, в которых используются методы количественного анализа, например в биоинформатике. Что касается науки о данных, то у Perl есть несколько недостатков: у него не получится быстро стать популярным в данной области, а его синтаксис считается недружелюбным. Кроме того, со стороны его разработчиков не наблюдается никаких попыток создания библиотек, которые могли бы быть использованы в области науки о данных. А как мы с вами знаем: зачастую все решают правильные действия в подходящий момент.\n",
      "\n",
      "Наш вердикт – полезный язык сценариев общего назначения, но с его помощью вам уж точно не устроится на работу специалиста по данным...\n",
      "\n",
      "Ruby\n",
      "Ruby – это еще один динамически типизированный интерпретируемый язык общего назначения. Тем не менее, похоже, что у его создателей нет никакого желания сделать его пригодным для работы в области науки о данных, как в случае с Python.\n",
      "\n",
      "Это может показаться странным, но все вышеуказанное так или иначе связано с доминирующим положением Python в области научных исследований, а также с положительными отзывами людей, пишущих на этом языке. Чем больше людей выбирают Python, тем больше разрабатывается для него модулей и фреймворков, и тем больше программистов отдают свое предпочтение Python. Проект SciRuby был создан для того, чтобы внедрить в Ruby функциональность научных вычислений, например, матричной алгебры. Но, несмотря на все эти потуги, Python на данный момент по-прежнему лидирует.\n",
      "\n",
      "Наш вердикт – не совсем правильный выбор для науки о данных, но в вашем резюме знание Ruby не помешает\n",
      "\n",
      "Заключение\n",
      "Ну вот мы с вами и рассмотрели короткое руководство по языкам программирования, которые ближе всего подступили к области науки о данных. Важным моментом здесь является понимание того, что вам больше нужно: специфичность или универсальность языка, его удобство или эффективность.\n",
      "\n",
      "Я регулярно использую R, Python и SQL, так как моя текущая работа в основном сосредоточена на разработке существующих конвейеров данных и ETL-процессов. Эти языки совмещают правильный баланс общности и эффективности для выполнения этой работы с возможностью использования более совершенных статистических пакетов R, когда это необходимо.\n",
      "\n",
      "Однако, возможно, вы уже неплохо набили руку в Java, или вам не терпится испробовать в действии Scala для работы с большими данными, или, может быть, вы без ума от проекта Julia.\n",
      "\n",
      "А может вы зубрили MATLAB на парах в институте или не прочь дать SciRuby шанс показать себя? Да у вас могут быть сотни разных причин! Если так, то оставьте свой комментарий внизу – ведь для нас действительно важно знать мнение каждого из вас!\n",
      "\n",
      "Спасибо за внимание!\n",
      "\n",
      "— Маркетинг для вашего проекта на Reddit, Medium и Bitcointalk.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Какой из представленных в статье языков программирования вы считаете наиболее подходящим для работы с данными? \n",
      "            28.31%\n",
      "           R \n",
      "            216\n",
      "           \n",
      "            61.73%\n",
      "           Python \n",
      "            471\n",
      "           \n",
      "            22.54%\n",
      "           SQL \n",
      "            172\n",
      "           \n",
      "            10.09%\n",
      "           Java \n",
      "            77\n",
      "           \n",
      "            10.09%\n",
      "           Scala \n",
      "            77\n",
      "           \n",
      "            4.33%\n",
      "           Julia \n",
      "            33\n",
      "           \n",
      "            6.95%\n",
      "           MATLAB \n",
      "            53\n",
      "           \n",
      "            9.44%\n",
      "           C++ \n",
      "            72\n",
      "           \n",
      "            1.57%\n",
      "           Perl \n",
      "            12\n",
      "           \n",
      "            1.57%\n",
      "           Ruby \n",
      "            12\n",
      "           \n",
      "            10.22%\n",
      "           Другой \n",
      "            78\n",
      "            \n",
      "       Проголосовали 763 пользователя.  \n",
      "\n",
      "       Воздержался 231 пользователь. \n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Мы продолжаем цикл статей о SAP HANA Data Management Suite – гибриде локальных и облачных технологий, который включает в себя четыре компонента-продукта: SAP Data Hub, SAP HANA, SAP Enterprise Architecture Designer и SAP Cloud Platform Big Data Services.\n",
      "\r\n",
      "Сочетание этих решений позволяет создать целостную структуру управления данными с следующими функциями:\n",
      "\n",
      "\n",
      "отслеживание происхождения данных\n",
      "отслеживание изменений в данных и их структуре\n",
      "комплексное понимание метаданных\n",
      "поддержка необходимого уровня безопасности\n",
      "централизованный мониторинг\n",
      "\r\n",
      "Но сегодня мы поговорим про «ядро» этой системы – платформу SAP HANA.\n",
      "\r\n",
      "SAP проводил и продолжает проводить исследования, инвестирует большие ресурсы и средства в развитие направления по обработке данных. В результате появилась платформа SAP HANA – High-Performance Analytic Appliance. У нашей компании уже был накоплен многолетний и по-своему уникальный опыт по разработке технологий и сервисов для бизнеса – и в SAP применили его при создании платформы для бизнеса для realtime обработки данных. В результате появилась SAP HANA, которая стала основой и ядром для разработки и построения интеллектуальных предприятий нового типа (intelligent enterprise). Платформу используют для разработки приложений как внутри SAP, так и наши клиенты и партнёры.\n",
      "\n",
      "\n",
      "\r\n",
      "SAP HANA – это многоцелевое решение для хранения и обработки информации. Одна из особенностей SAP HANA – это встроенный механизм вычислений, который позволяет переносить выполнение операций по планированию с уровня приложений на уровень базы данных SAP HANA. С помощью современной архитектуры аппаратной платформы вычисления проходят эффективнее – вся «лавина» обрабатываемых данных разбивается на строго определённое количество потоков, число которых равно общему количеству ядер платформы. Такой подход позволяет максимально эффективно использовать вычислительную мощность каждого ядра каждого процессора. \n",
      "\r\n",
      "SAP HANA также предоставляет технологии для хранения и обработки данных in-memory. SAP HANA как база данных позволяет хранить данные в построчном и в поколоночном виде. Технология хранения и обработки данных in-memory обеспечивает быструю обработку транзакций, а вместе технологией анализа данных Calculation View гарантирует высокое быстродействие при выполнении аналитических запросов.\n",
      "\r\n",
      "Аналитики Forrester начали использовать новое понятие – «транслитическая база данных». По их определению, такая платформа «поддерживает многие типы использования, включая информацию в режиме реального времени, машинное обучение, поточную аналитику и экстремальную транзакционную обработку». \n",
      "\r\n",
      "В недавнем отчёте Forrester говорится следующее: «SAP HANA – это shared-nothing (без общего использования ресурсов), in-memory платформа. Это основа платформы SAP для транзакций и аналитики по данным, она поддерживает множество сценариев применения: приложения для обработки данный в режиме реального времени, аналитика, транслитические приложения, системы глубокой и продвинутой аналитики. Предприятия используют платформу для организации in-memory витрин данных, для работы с realtime-хранилищем данных SAP Business Warehouse, а также при работе с SAP S/4HANA и SAP Business Suite».\n",
      "\r\n",
      "Транслитические платформы подходят для поддержки realtime-приложений и сервисов: для торговли акциями, обнаружения мошенничества, борьбы с терроризмом, мониторинга здоровья пациентов, анализа данных от различных сенсоров, мониторинга землетрясений и много другого. С помощью транслитической платформы приложения могут обмениваться данными в реальном времени, обеспечивают согласованность и точность информации, хранимой на предприятии. \n",
      "\r\n",
      "Ещё одна сфера применения SAP HANA – это поддержка машинного обучения, что позволяет применять к данным сложные аналитические модели для более точного прогнозирования операций, бизнес-процессов, поведения клиентов и т.д.\n",
      "\n",
      "Как SAP HANA поддерживает данную функциональность? \n",
      "\n",
      "\r\n",
      "Начнём с сервиса баз данных. Если рассматривать HANA с точки зрения архитектуры и технологий, то здесь применяются два способа хранения данных – построчный и поколоночный. \n",
      "\r\n",
      "Построчное хранение данных в таблице позволяет обеспечивать высокую скорость записи данных. Если вы хотите добавить новую строку в таблицу, то вам достаточно найти свободное место в памяти для этой строки и записать туда новые данные. Однако при построчном хранении возникает проблема с анализом данных: необходимо использовать индексирование или материализованное представление данных в форме, которая будет удобна для анализа. При этом индексирование приводит к задержкам из-за того, что необходимо дополнительное время на перестроение индекса, материализацию данных в ином формате в процессе вставки строки. \n",
      "\r\n",
      "Если же данные хранятся поколоночно, то для добавления новой строки необходимо потратить время на разнесение значений строки по колонкам, затем – подождать, пока данные будут разнесены в разные места в памяти. Всё это приводит к снижению производительности во время записи данных.\n",
      "\r\n",
      "База данных с поколоночным хранением позволяет значительно быстрее обрабатывать запросы, потому что в этом случае данные из запрошенных колонок расположены в памяти компактно и сжато. Т.е. при запросе нет необходимости сканировать всю таблицу – достаточно просмотреть только колонки, используемые в запросе. Такая база данных оптимизирована для чтения, а поколоночное хранение информации позволяет организовывать данные в оперативной памяти определенным образом, с использованием группировки. При этом подходе можно с большей эффективностью использовать различные техники компрессии, что приводит к многократному сжатию исходной информации. \n",
      "\r\n",
      "Для решения этой проблемы был разработан подход Unified Tables, который обеспечивает высокую скорость чтения и записи данных в таблицу поколоночного хранения. Такой механизм позволяет быстро осуществлять транзакции (то есть запись новых строк), анализировать данные с высокой скоростью за счёт поколоночного хранения в сжатом виде, параллельной обработки данных, а также хранить все данные в оперативной памяти (in-memory).\n",
      "\r\n",
      "При проведении записи изменения не сразу вносятся в основное место хранения таблиц. Вместо этого все правки заносятся в отдельную структуру данных – дельта-хранилище (на картинке L1-delta). Здесь данные хранятся в оптимизированном для записи формате. Когда необходимо перенести изменения из дельта-хранилища, то запускается специальный процесс Delta merge – слияние дельты. Сначала данные из L1-delta преобразуются в поколоночный формат в L2-delta, а затем объединяются с основным хранением данных (main store). А для механизма чтения данных все три области хранения информации (L1-delta, L2-delta и main store) предоставляют данные в целостном виде. Благодаря этому процессу получается обеспечить высокую скорость записи и анализа данных.\n",
      "\n",
      "\n",
      "\r\n",
      "Одно из существенных преимуществ SAP HANA – все расчеты агрегированных данных производятся непосредственно при формировании аналитического запроса и выводятся сразу в виде результата. Возможности по хранению детальных или исходных данных в оперативной памяти (а не агрегированных значений) позволяют отказаться от предварительного расчета и хранения агрегатных таблиц, которые являются неотъемлемой частью классических аналитических систем. \n",
      "\r\n",
      "SAP HANA также поддерживает различные внутренние языки программирования: R – для создания прогнозных моделей, SQL Script – для написания логики вычислений. На уровне сервера приложений XSA, встроенного в SAP HANA 2.0, можно выполнять разработку на многих других языках благодаря поддержки концепции Bring Your Own Language (и за счёт использования Cloud Foundry). С помощью этих языков можно производить необходимые вычисления и прогнозы непосредственно на уровне хранения данных. Это позволяет избавиться от лишних этапов передачи больших объёмов данных и выдавать готовый результат расчетов на уровень приложения.\n",
      "\r\n",
      "Теперь рассмотрим платформенные сервисы SAP HANA.\n",
      "\n",
      "Сервисы SAP HANA Platform\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "В SAP HANA есть не только база данных, но и целый набор сервисов для разработки приложений, средства интеграции и очистки данных, библиотеки для аналитической обработки данных, включая Machine Learning, а также возможности для хранения и обработки специальных типов данных. SAP HANA позволяет без дополнительных инструментов загружать данные из различных источников, разрабатывать различные формы для ввода, редактирования и анализа данных. Также доступны инструменты для сложной интеллектуальной обработки данных: преобразование, трансформация, поиск закономерностей, исследования. И, конечно, платформа открыта для визуального анализа данных через различные инструменты.\n",
      "\r\n",
      "Чтобы рассказать о всех возможностях SAP HANA, потребуется написать несколько дополнительных статей. Многие из них уже описаны в нашем блоге.\n",
      "\n",
      "\n",
      "\r\n",
      "Давайте рассмотрим некоторые доступные сервисы:\n",
      "\r\n",
      "SAP HANA включает в себя движок для хранения и обработки геоданных – данных, которые описывают положение, форму и ориентацию объектов в пространстве. SAP HANA поддерживает пространственные типы данных и методы их обработки. Существует специальный метод для обработки такой структуры – граф. SAP HANA в этом случае предоставляет возможности для обработки гиперсвязанных данных и их отношений. Движок для обработки данных имеет встроенные алгоритмы поиска окрестностей, кратчайших путей, сильно связанных компонент, сопоставления образцов и многое другое.\n",
      "\r\n",
      "В SAP HANA также есть сотня предварительно упакованных алгоритмов машинного обучения и прогнозирования с такими возможностями, как объединение, кластеризация, классификация, регрессия, распределение вероятности, временные ряды и многое другое. Кроме этого, вы можете использовать библиотеку TensorFlow и язык R.\n",
      "\r\n",
      "SAP HANA имеет встроенные возможности для обработки и анализа текстовых файлов, включая различные функции по интеллектуальному анализу текстов – например, нечеткая логика, поиск синонимов, семантический разбор и т.д.\n",
      "\r\n",
      "SAP HANA Streaming Analytics может фиксировать, фильтровать, анализировать и воздействовать на миллионы событий в секунду в режиме реального времени, сохраняя данные или результаты в базу данных SAP HANA и направляя менее критические данные в более дешевые решения для хранения — такие, как Hadoop. SAP HANA Streaming Analytics также интегрирована с системой сообщений Apache Kafka.\n",
      "\n",
      "Полезные материалы и ресурсы для начала работы с SAP HANA:\n",
      "Бесплатная ознакомительная версия SAP HANA, express edition доступна для скачивания на нашем официальном сайте. Также в начале работы вы можете изучить набор туториалов перед началом работы с SAP HANA:\r\n",
      " — виртуальная машина и версия Server + XSA Applications для SAP HANA и видеоинструкция по её установке\r\n",
      " — в наборе туториалов есть широкий выбор. Например, для работы с пространственными данными: первый и второй    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " По запросу R или Python в интернете вы найдёте миллионы статей и километровых обсуждений по теме какой из них лучше, быстрее и удобнее для работы с данными. Но к сожалению особой пользы все эти статьи и споры не несут.\n",
      "\n",
      "Цель этой статьи — сравнить основные приёмы обработки данных в наиболее популярных пакетах обоих языков. И помочь читателям максимально быстро овладеть тем, который они ещё не знают. Для тех кто пишет на Python узнать как выполнять всё то же самое в R, и соответственно наоборот.\n",
      "В ходе статьи мы разберём синтаксис наиболее популярных пакетов на R. Это пакеты входящие в библиотеку tidyverse, а также пакет data.table. И сравним их синтаксис с pandas, наиболее популярным пакетом для анализа данных в Python.\n",
      "Мы пошагово пройдём весь путь анализа данных от их загрузки до выполнения аналитических, оконных функций средствами Python и R.\n",
      "Содержание\n",
      "Данная статья может использоваться как шпаргалка, в случае если вы забыли как в одном из рассматриваемых пакетов выполнить некоторую операцию по обработке данных.\n",
      "\n",
      "\n",
      "\n",
      "Основные отличия синтаксиса в R и Python\r\n",
      "1.1. Обращение к функциям пакетов\r\n",
      "1.2. Присваивание\r\n",
      "1.3. Индексация\r\n",
      "1.4. Методы и ООП\r\n",
      "1.5. Пайплайны\r\n",
      "1.6. Структуры данных\n",
      "Несколько слов о пакетах которые мы будем использовать\r\n",
      "2.1. tidyverse\r\n",
      "2.2. data.table\r\n",
      "2.3. pandas\n",
      "Установка пакетов\n",
      "Загрузка данных\n",
      "Создание датафреймов\n",
      "Выбор нужных столбцов\n",
      "Фильтрация строк\n",
      "Группировка и агрегация\n",
      "Вертикальное объединение таблиц (UNION)\n",
      "Горизонтальное объединение таблиц (JOIN)\n",
      "Простейшие оконные функции и вычисляемые столбцы\n",
      "Таблица соответствия методов обработки данных в R и Python\n",
      "Заключение\n",
      "Небольшой опрос о том какой пакет вы используете\n",
      "\n",
      "Если вы интересуетесь анализом данных возможно вам будут полезны мои telegram и youtube каналы. Большая часть контента которых посвящены языку R.\n",
      "Основные отличия синтаксиса в R и Python\n",
      "Что бы вам было проще с переходом из Python к R, или наоборот, приведу несколько основных моментов, на которые необходимо обратить внимание.\n",
      "Обращение к функциям пакетов\n",
      "После загрузки пакета в R, для обращения к его функциям нет необходимости указывать имя пакета. В большинстве случаев в R это не принято, но допустимо. Вы вообще можете не импортировать пакет если в коде вам понадобится какая-либо одна его функция, а просто вызвать её указав название пакета и имя функции. Разделителем между названием пакета и функции в R служит двойное двоеточие package_name::function_name().\n",
      "В Python наоборот, классикой считается обращение к функциям пакета, явно указав его имя. При загрузке пакета, как правило, ему присваивают сокращённое имя, например для pandas обычно используется псевдоним pd. Обращение к функции пакета идёт через точку package_name.function_name().\n",
      "Присваивание\n",
      "В R для присваивания значения объекту принято использовать стрелку obj_name <- value, хотя допускается и одинарный знак равенства, но одинарный знак равенства в R используют в основном для передачи значений аргументам функций.\n",
      "В Python присваивание осуществляется исключительно одинарным знаком равенства obj_name = value.\n",
      "Индексация\n",
      "Тут тоже есть довольно весомые отличия. В R индексация начинается с единицы и включает в результирующий диапазон все указанные элементы, \n",
      "в Python индексация начинается с нуля и выбираемый диапазон не включается последний элемент указанный в индексации. Так конструкция x[i:j] в Python не будет включать элемент j.\n",
      "Также есть различия в отрицательной индексации, в R запись x[-1] вернёт все элементы вектора, кроме первого. В Python аналогичная запись вернёт только последний элемент.\n",
      "Методы и ООП\n",
      "В R по своему реализовано ООП, об этом я писал в статье \"ООП в языке R (часть 1): S3 классы\". В целом R функциональный язык, и всё в нём построено на функциях. Поэтому к примеру для пользователей Excel перейти на tydiverse будет проще, чем на pandas. Хотя возможно это моё субъективное мнение. \n",
      "Если вкратце, то объекты в R не имеют методов (если говорить про S3 классы, но есть и другие реализации ООП, которые встречаются значительно реже). Есть лишь обобщённые функции, которые в зависимости от класса объекта по-разному их обрабатывают.\n",
      "Пайплайны\n",
      "Возможно это название для pandas будет не совсем корректно, но я попробую объяснить смысл. \n",
      "Что бы не сохранять промежуточные вычисления и не плодить в рабочем окружении ненужные объекты вы можете использовать своеобразный конвейер. Т.е. передавать результат вычисления из одной функции в следующую, и не сохранять промежуточные результаты.\n",
      "Возьмём следующий пример кода, в котором мы сохраняем в отдельные объекты промежуточные вычисления:\n",
      "temp_object <- func1()\n",
      "temp_object2 <- func2(temp_object )\n",
      "obj <- func3(temp_object2 )\n",
      "Мы последовательно выполнили 3 операции, и результат каждой сохранили в отдельный объект. Но на самом деле эти промежуточные объекты нам не нужны.\n",
      "Либо ещё хуже, но привычнее пользователям Excel.\n",
      "obj  <- func3(func2(func1()))\n",
      "В данном случае мы не сохраняли промежуточные результаты вычислений, но читать код с вложенными друг, в друга функциями крайне не удобно.\n",
      "Мы будем рассматривать несколько подходов к обработке данных в R, и в них по разному выполняются подобные операции.\n",
      "Пайплайны в библиотеке tidyverse реализуются оператором %>%.\n",
      "obj <- func1() %>% \n",
      "            func2() %>%\n",
      "            func3()\n",
      "Таким образом мы берём результат работы func1() и передаём его в качестве первого аргумента в func2(), далее результат этого вычисления передаём в качестве первого аргумента func3(). И в конце концов, все выполненные вычисления записываем в объект obj <-.\n",
      "Лучше слов всё вышеописанное иллюстирирует этот мем:\n",
      "\n",
      "В data.table похожим образом используются цепочки.\n",
      "newDT <- DT[where, select|update|do, by][where, select|update|do, by][where, select|update|do, by]\n",
      "В каждой из квадратных скобок вы можете использовать результат предыдущей операции.\n",
      "В pandas такие операции разделяются точкой.\n",
      "obj = df.fun1().fun2().fun3()\n",
      "Т.е. мы берём нашу таблицу df и используем её метод fun1(), далее к полученному результату применяем метод fun2(), после fun3(). Полученный результат сохраняем в объект obj .\n",
      "Структуры данных\n",
      "Структуры данных в R и Python схожи, но имеют разные названия. \n",
      "\n",
      "\n",
      "\n",
      "Описание\n",
      "Название в R\n",
      "Название в Python / pandas\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Табличная структура\n",
      "data.frame, data.table, tibble\n",
      "DataFrame\n",
      "\n",
      "\n",
      "Одномерный список значений\n",
      "Вектор\n",
      "Series в pandas или список (list) в чистом Python\n",
      "\n",
      "\n",
      "Многоуровневая не табличная структура\n",
      "Список (List)\n",
      "Словарь (dict)\n",
      "\n",
      "\n",
      "\n",
      "Некоторые другие особенности и различия синтаксиса мы рассмотрим далее.\n",
      "Несколько слов о пакетах которые мы будем использовать\n",
      "Для начала расскажу немного о пакетах с которыми в ходе этой статьи вы познакомитесь.\n",
      "tidyverse\n",
      "Официальный сайт: tidyverse.org\n",
      "\r\n",
      "Библиотека tidyverse написана Хедли Викхемом, старшим научным сотрудником RStudio. tidyverse состоит из внушительного набора пакетов упрощающих обработку данных, 5 из которых входят в топ 10 загружаемых из репозитория CRAN.\n",
      "Ядро библиотеки состоит из следующих пакетов: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats. Каждый из этих пакетов направлен на решение определённой задачи. Например dplyr создан для манипуляции с данными, tidyr для приведения данных к аккуратному виду, stringr упрощает работу со строками, а ggplot2 является одним из наиболее популярных инструментов для визуализации данных.\n",
      "Преимуществом tidyverse является простота и легко читаемость синтаксиса, который во многом похож на язык запросов SQL. \n",
      "data.table\n",
      "Официальный сайт: r-datatable.com\n",
      "Автором data.table является Мэтт Доул из H2O.ai. \n",
      "Первый релиз библиотеки состоялся в 2006 году. \n",
      "Синтаксис пакета не так удобен как в tidyverse и больше напоминает классические датафреймы в R, но при этом значительно расширенные по функционалу. \n",
      "Все манипуляции с таблицей в данном пакете описываются в квадратных скобках, и если перевести синтаксис data.table на SQL, то получится примерно следующее: data.table[ WHERE, SELECT, GROUP BY ]\n",
      "Сильной стороной данного пакета является скорость обработки больших объёмов данных.\n",
      "pandas\n",
      "Официальный сайт: pandas.pydata.org \n",
      "Название библиотеки происходит от эконометрического термина «панельные данные» (англ. panel data), используемого для описания многомерных структурированных наборов информации.\n",
      "Автором pandas является американец Уэс Мак-Кинни. \n",
      "Когда речь идёт об анализе данных на Python, равных pandas нет. Очень многофункциональный, высокоуровневый пакет, который позволяет вам провести с данными любые манипуляции начиная от загрузки данных из любых источников до их визуализации.\n",
      "Установка дополнительных пакетов\n",
      "Пакеты о которых пойдёт речь в этой статье не входят в базовые дистрибутивы R и Python. Хотя есть небольшая оговорка, если вы установили дистрибутив Anaconda то ставить дополнительно pandas не требуется.\n",
      "Установка пакетов в R\n",
      "Если вы хотя бы раз открывали среду разработки RStudio наверняка вы и так уже знаете как установить нужный пакет в R. Для установки пакетов воспользуйтесь стандартной командой install.packages() запустив её непосредственно в самом R.\n",
      "# установка пакетов\n",
      "install.packages(\"vroom\")\n",
      "install.packages(\"readr\")\n",
      "install.packages(\"dplyr\")\n",
      "install.packages(\"data.table\")\n",
      "После установки пакеты необходимо подключить, для чего в большистве случаев используется команда library().\n",
      "# подключение или импорт пакетов в рабочее окружение\n",
      "library(vroom)\n",
      "library(readr)\n",
      "library(dplyr)\n",
      "library(data.table)\n",
      "Установка пакетов в Python\n",
      "Итак, если у вас установлен чистый Python, то pandas вам необходимо доустанавливать руками. Открываем командную строку, или терминал, в зависимости от вашей операционной системы и вводим следующую команду.\n",
      "pip install pandas\n",
      "После чего возвращаемся в Python и импортируем установленный пакет командой import.\n",
      "import pandas as pd\n",
      "Загрузка данных\n",
      "Добыча данных является одним из важнейших этапов анализа данных. И Python и R при желании предоставляют вам обширные возможности для получения данных из любых источников: локальные файлы, файлы из интернета, веб сайты, всевозможные базы данных.\n",
      "\n",
      "В ходе статьи мы будем использовать несколько наборов данных:\n",
      "\n",
      "Две выгрузки из Google Analytics.\n",
      "Набор данных о пассажирах титаника.\n",
      "\n",
      "Все данные лежат у меня на GitHub в виде csv и tsv файлов. От куда мы их и будем запрашивать.\n",
      "Загрузка данных в R: tidyverse, vroom, readr\n",
      "Для загрузки данных в библиотеке tidyverse предназначены два пакета: vroom, readr. vroom более современный, но в будущем возможно пакеты будут объединены.\n",
      "Цитата из официальной документации vroom.\n",
      "vroom vs readr\r\n",
      "What does the release of vroom mean for readr? For now we plan to let the two packages evolve separately, but likely we will unite the packages in the future. One disadvantage to vroom’s lazy reading is certain data problems can’t be reported up front, so how best to unify them requires some thought.\n",
      "\r\n",
      "vroom против readr\r\n",
      "Что означает выпуск vroom для readr? На данный момент мы планируем развивать оба пакета отдельно, но, вероятно, мы объединим их в будущем. Одним из недостатков ленивого чтения vroom является то, что о некоторых проблемах с данными нельзя сообщить заранее, поэтому, необходимо подумать о том как лучше их объединить.В этой статье мы рассмотрим оба пакета для загрузки данных:\n",
      "\n",
      "Загрузка данных в R: пакет vroom\n",
      "# install.packages(\"vroom\")\n",
      "library(vroom)\n",
      "\n",
      "# Чтение данных\n",
      "## vroom\n",
      "ga_nov  <- vroom(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/ga_nowember.csv\")\n",
      "ga_dec  <- vroom(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/ga_december.csv\")\n",
      "titanic <- vroom(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/titanic.csv\")\n",
      "\n",
      "\n",
      "Загрузка данных в R: readr\n",
      "# install.packages(\"readr\")\n",
      "library(readr)\n",
      "\n",
      "# Чтение данных\n",
      "## readr\n",
      "ga_nov  <- read_tsv(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/ga_nowember.csv\")\n",
      "ga_dec  <- read_tsv(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/ga_december.csv\")\n",
      "titanic <- read_csv(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/titanic.csv\")\n",
      "\n",
      "В пакете vroom, не зависимо от формата данных csv / tsv загрузка осуществляется одноимённой функцией vroom(), в пакете readr мы используем под каждый формат свою функцию read_tsv() и read_csv().\n",
      "Загрузка данных в R: data.table\n",
      "В data.table для загрузки данных присутствует функция fread().\n",
      "\n",
      "Загрузка данных в R: пакет data.table\n",
      "# install.packages(\"data.table\")\n",
      "library(data.table)\n",
      "\n",
      "## data.table\n",
      "ga_nov  <- fread(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/ga_nowember.csv\")\n",
      "ga_dec  <- fread(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/ga_december.csv\")\n",
      "titanic <- fread(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/r_python_data/titanic.csv\")\n",
      "\n",
      "Загрузка данных в Python: pandas\n",
      "Если сравнивать с R пакетами, то в данном случае наиболее близким по синтаксису к pandas будет readr, т.к. pandas умеет запрашивать данные от куда угодно, и в этом пакете присутствует целое семейство функций read_*().\n",
      "\n",
      "read_csv()\n",
      "read_excel()\n",
      "read_sql()\n",
      "read_json()\n",
      "read_html()\n",
      "\n",
      "И много других функций предназначенных для чтения данных из всевозможных форматов. Но для наших целей достаточно read_table() или read_csv() с использованием аргумента sep для указания разделителя столбцов.\n",
      "\n",
      "Загрузка данных в Python: pandas\n",
      "import pandas as pd\n",
      "\n",
      "ga_nov  = pd.read_csv(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/russian_text_in_r/ga_nowember.csv\", sep = \"\\t\")\n",
      "ga_dec  = pd.read_csv(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/russian_text_in_r/ga_december.csv\", sep = \"\\t\")\n",
      "titanic = pd.read_csv(\"https://raw.githubusercontent.com/selesnow/publications/master/data_example/russian_text_in_r/titanic.csv\")\n",
      "\n",
      "Создание датафреймов\n",
      "В таблице titanic, которую мы загрузили, есть поле Sex, в котором хранится идентификатор пола пассажира.\n",
      "Но для более удобного представления данных в разрезе пола пассажира следует использовать не код пола, а название. \n",
      "Для этого мы создадим небольшой справочник, таблицу в которой будет всего 2 столбца (код и название пола) и 2 строки соответственно. \n",
      "Создание датафрейма в R: tidyverse, dplyr\n",
      "В приведённом ниже примере кода мы создаём нужный датафрейм с помощью функции tibble() .\n",
      "\n",
      "Создание датафрейма в R: dplyr\n",
      "## dplyr\n",
      "### создаём справочник\n",
      "gender <- tibble(id = c(1, 2),\n",
      "                 gender = c(\"female\", \"male\"))\n",
      "\n",
      "Создание датафрейма в R: data.table\n",
      "\n",
      "Создание датафрейма в R: data.table\n",
      "## data.table\n",
      "### создаём справочник\n",
      "gender <- data.table(id = c(1, 2),\n",
      "                    gender = c(\"female\", \"male\"))\n",
      "\n",
      "\n",
      "Создание датафрейма в Python: pandas\n",
      "В pandas создание фреймов осуществляется в несколько этапов, сперва мы создаём словарь, а потом преобразуем словарь в датафрейм.\n",
      "\n",
      "Создание датафрейма в Python: pandas\n",
      "# создаём дата фрейм\n",
      "gender_dict = {'id': [1, 2],\n",
      "               'gender': [\"female\", \"male\"]}\n",
      "# преобразуем словарь в датафрейм\n",
      "gender = pd.DataFrame.from_dict(gender_dict)\n",
      "\n",
      "Выбор столбцов\n",
      "Таблицы с которыми вы работаете могут содержать десятки, и даже сотни столбцов с данными. Но для проведения анализа, как правило, вам нужны далеко не все столбцы которые доступны в исходной таблице.\n",
      "\n",
      "Поэтому одной из первых операций которые вы будете выполнять с исходной таблицей, это очистка её от ненужной информации, и освобождение памяти которую эта информация занимает.\n",
      "Выбор столбцов в R: tidyverse, dplyr\n",
      "Синтаксис dplyr очень похож на язык запросов SQL, если вы с ним знакомы то довольно быстро овладеете этим пакетом.\n",
      "Для выбора столбцов используется функция select().\n",
      "Ниже примеры кода с помощью которого вы можете выбрать столбцы следующими способами:\n",
      "\n",
      "Перечислив названия нужных столбцов\n",
      "Обратиться к именам столбцов используя регулярные выражения\n",
      "По типу данных или любому другому свойству данных которые содержатся в столбце\n",
      "\n",
      "\n",
      "Выбор столбцов в R: dplyr\n",
      "# Выбор нужных столбцов\n",
      "## dplyr\n",
      "### выбрать по названию столбцов\n",
      "select(ga_nov, date, source, sessions)\n",
      "### исключь по названию столбцов\n",
      "select(ga_nov, -medium, -bounces)\n",
      "### выбрать по регулярному выражению, стобцы имена которых заканчиваются на s\n",
      "select(ga_nov, matches(\"s$\"))\n",
      "### выбрать по условию, выбираем только целочисленные столбцы\n",
      "select_if(ga_nov, is.integer)\n",
      "\n",
      "Выбор столбцов в R: data.table\n",
      "Те же операции в data.table выполняются несколько иначе, в начале статьи я привёл описание того, какие аргументы есть внутри квадратных скобок в data.table.\n",
      "DT[i,j,by]\n",
      "Где:\r\n",
      "i — where, т.е. фильтрация по строкам\r\n",
      "j — select|update|do, т.е. выбор столбцов и их преобразование\r\n",
      "by — группировка данных\n",
      "\n",
      "Выбор столбцов в R: data.table\n",
      "## data.table\n",
      "### выбрать по названию столбцов\n",
      "ga_nov[ , .(date, source, sessions) ]\n",
      "### исключь по названию столбцов\n",
      "ga_nov[ , .SD, .SDcols = ! names(ga_nov) %like% \"medium|bounces\" ]\n",
      "### выбрать по регулярному выражению\n",
      "ga_nov[, .SD, .SDcols = patterns(\"s$\")]\n",
      "\n",
      "Переменная .SD позволяет вам обратиться ко всем столбцам, а .SDcols отфильтровать нужные столбцы используя регулярные выражения, или другие функции для фильтрации названий нужных вам столбцов.\n",
      "Выбор столбцов в Python, pandas\n",
      "Для выбора столбцов по названию в pandas достаточно передать список их названий. А для выбора или исключения столбцов по названию используя регулярные выражения необходимо использовать функции drop() и filter(), и аргумент axis=1, с помощью которого вы указываете, что обрабатывать надо не строки а столбцы.\n",
      "Для выбора поля по типу данных используйте функцию select_dtypes(), и в аргументы include или exclude передайте список типов данных соответствующие тем, какие поля вам необходимо выбрать. \n",
      "\n",
      "Выбор столбцов в Python: pandas\n",
      "# Выбор полей по названию\n",
      "ga_nov[['date', 'source', 'sessions']]\n",
      "# Исключить по названию\n",
      "ga_nov.drop(['medium', 'bounces'], axis=1)\n",
      "# Выбрать по регулярному выражению\n",
      "ga_nov.filter(regex=\"s$\", axis=1)\n",
      "# Выбрать числовые поля\n",
      "ga_nov.select_dtypes(include=['number'])\n",
      "# Выбрать текстовые поля\n",
      "ga_nov.select_dtypes(include=['object'])\n",
      "\n",
      "Фильтрация строк\n",
      "Например, в исходной таблице могут храниться данные за несколько лет, а вам необходимо проанализировать только прошлый месяц. Опять же, лишние строки замедлят процесс обработки данных и будут засорять память ПК.\n",
      "\n",
      "Фильтрация строк в R: tydyverse, dplyr\n",
      "В dplyr для фильтрации строк используется функция filter(). В качестве первого аргумента она принимает датафрейм, далее вы перечисляете условия фильтрации. \n",
      "При написании логических выражений для фильтрации таблицы в данном случае имена столбцов указываете без кавычек, и без объявления имени таблицы.\n",
      "Применяя для фильтрации несколько логических выражений используйте следующие операторы:\n",
      "\n",
      "& или запятая — логическое И\n",
      "| — логическое ИЛИ\n",
      "\n",
      "\n",
      "Фильтрация строк в R: dplyr\n",
      "# фильтрация строк\n",
      "## dplyr\n",
      "### фильтрация строк по одному условию\n",
      "filter(ga_nov, source == \"google\")\n",
      "### фильтр по двум условиям соединённым логическим и\n",
      "filter(ga_nov, source == \"google\" & sessions >= 10)\n",
      "### фильтр по двум условиям соединённым логическим или\n",
      "filter(ga_nov, source == \"google\" | sessions >= 10)\n",
      "\n",
      "Фильтрация строк в R: data.table\n",
      "Как я уже писал выше, в data.table синтаксис преобразования данных заключён в квадратные скобки. \n",
      "DT[i,j,by]\n",
      "Где:\r\n",
      "i — where, т.е. фильтрация по строкам\r\n",
      "j — select|update|do, т.е. выбор столбцов и их преобразование\r\n",
      "by — группировка данных\n",
      "Для фильтрации строк используется аргумент i, который имеет первую позицию в квадратных скобках. \n",
      "Обращение к столбцам в логических выражениях осуществляется без кавычек и указания имени таблицы.\n",
      "Логические выражения связываются между собой так же как и в dplyr через операторы & и |.\n",
      "\n",
      "Фильтрация строк в R: data.table\n",
      "## data.table\n",
      "### фильтрация строк по одному условию\n",
      "ga_nov[source == \"google\"]\n",
      "### фильтр по двум условиям соединённым логическим и\n",
      "ga_nov[source == \"google\" & sessions >= 10]\n",
      "### фильтр по двум условиям соединённым логическим или\n",
      "ga_nov[source == \"google\" | sessions >= 10]\n",
      "\n",
      "Фильтрация строк в Python: pandas\n",
      "Фильтрация по строкам в pandas схожа с фильтрацией в data.table, и осуществляется в квадратных скобках. \n",
      "Обращение к столбцам в данном случае осуществляется обязательно с указанием имени датафрейма, далее название столбца можно так же указать в кавычках в квадратных скобках (пример df['col_name']), либо без кавычек после точки (пример df.col_name).\n",
      "В случае, если вам необходимо отфильтровать датафрейм по нескольким условиям, каждое из условий необходимо взять в круглые скобки. Связываются между собой логический условия операторами & и |.\n",
      "\n",
      "Фильтрация строк в Python: pandas\n",
      "# Фильтрация строк таблицы\n",
      "### фильтрация строк по одному условию\n",
      "ga_nov[ ga_nov['source'] == \"google\" ]\n",
      "### фильтр по двум условиям соединённым логическим и\n",
      "ga_nov[(ga_nov['source'] == \"google\") & (ga_nov['sessions'] >= 10)]\n",
      "### фильтр по двум условиям соединённым логическим или\n",
      "ga_nov[(ga_nov['source'] == \"google\") | (ga_nov['sessions'] >= 10)]\n",
      "\n",
      "Группировка и агрегация данных\n",
      "Одна из наиболее часто используемых операций в анализе данных — группировка и агрегация.\n",
      "\n",
      "Синтаксис для выполнения этих операций разрознен во всех рассматриваемых нами пакетах.\n",
      "В данном случае в качестве примера мы возьмём датафрейм titanic, и посчитаем количество и среднюю стоимость билетов в зависимости от класса каюты.\n",
      "Группировка и агрегация данных в R: tidyverse, dplyr\n",
      "В dplyr для группировки используется функция group_by(), а для агрегации summarise(). На самом деле у dplyr есть целое семейство функций summarise_*(), но цель этой статьи сравнить базовый синтаксис, поэтому не будем лезть в такие дебри.\n",
      "Основные агрегирующие функции:\n",
      "\n",
      "sum() — суммирование\n",
      "min() / max() — минимальное и максимальное значение\n",
      "mean() — среднее арифметическое\n",
      "median() — медиана\n",
      "length() — количество\n",
      "\n",
      "\n",
      "Группировка и агрегация в R: dplyr\n",
      "## dplyr\n",
      "### группировка и агрегация строк\n",
      "group_by(titanic, Pclass) %>%\n",
      "  summarise(passangers = length(PassengerId),\n",
      "            avg_price  = mean(Fare))\n",
      "\n",
      "В функцию group_by() в качестве первого аргумента мы передали таблицу titanic, и далее указали поле Pclass, по которому мы будем группировать нашу таблицу. Результат этой операции с помощью оператора %>% передали в качестве первого аргумента в функцию summarise(), и добавили ещё 2 поля: passangers и avg_price. В первом, используя функцию length() рассчитали количество билетов, а во втором с помощью функции mean() получили среднюю стоимость билета.\n",
      "Группировка и агрегация данных в R: data.table\n",
      "В data.table для агрегации служит аргумент j который имеет вторую позицию в квадратных скобках, а для группировки by или keyby, которые имеют третью позицию.\n",
      "Список агрегирующих функций в данном случае идентичен описанному в dplyr, т.к. это функции из базового синтаксиса R.\n",
      "\n",
      "Группировка и агрегация в R: data.table\n",
      "## data.table\n",
      "### фильтрация строк по одному условию\n",
      "titanic[, .(passangers = length(PassengerId),\n",
      "            avg_price  = mean(Fare)),\n",
      "        by = Pclass]\n",
      "\n",
      "Группировка и агрегация данных в Python: pandas\n",
      "Группировка в pandas схожа с dplyr, а вот агрегация не похожа ни на dplyr ни на data.table.\n",
      "Для группировки используйте метод groupby(), в который необходимо передать список столбцов, по которым будет сгруппирован датафрейм. \n",
      "Для агрегации можно использовать метод agg(), который принимает словарь. Ключами словаря являются столбцы к которым вы будете применять агрегирующие функции, а значениями будут имена агрегирующих функций.\n",
      "Агрегирующие функции:\n",
      "\n",
      "sum() — суммирование\n",
      "min() / max() — минимальное и максимальное значение\n",
      "mean() — среднее арифметическое\n",
      "median() — медиана\n",
      "count() — количество\n",
      "\n",
      "Функция reset_index() в примере ниже используется для того, что бы сбросить вложенные индексы, которые pandas по умолчанию устанавливает после агрегации данных.\n",
      "Символ \\ позволяет вам переходить на следующую строку.\n",
      "\n",
      "Группировка и агрегация в Python: pandas\n",
      "# группировка и агрегация данных\n",
      "titanic.groupby([\"Pclass\"]).\\\n",
      "    agg({'PassengerId': 'count', 'Fare': 'mean'}).\\\n",
      "        reset_index()\n",
      "\n",
      "Вертикальное объединение таблиц\n",
      "Операция, при которой вы объединяете две или более таблиц одинаковой структуры. В загруженных нами данными есть таблицы ga_nov и ga_dec. Эти таблицы одинаковы по структуре, т.е. имеют одинаковые столбцы, и типы данных в этих столбцах. \n",
      "\n",
      "Это выгрузка из Google Analytics за ноябрь и декабрь месяц, в этом разделе мы объедим эти данные в одну таблицу.\n",
      "Вертикальное объединение таблиц в R: tidyverse, dplyr\n",
      "В dplyr объединить 2 таблицы в одну можно с помощью функции bind_rows(), передав в качестве её аргументов таблицы.\n",
      "\n",
      "Фильтрация строк в R: dplyr\n",
      "# Вертикальное объединение таблиц\n",
      "## dplyr\n",
      "bind_rows(ga_nov, ga_dec)\n",
      "\n",
      "Вертикальное объединение таблиц в R: data.table\n",
      "Так же ничего сложного, используем rbind().\n",
      "\n",
      "Фильтрация строк в R: data.table\n",
      "## data.table\n",
      "rbind(ga_nov, ga_dec)\n",
      "\n",
      "Вертикальное объединение таблиц в Python: pandas\n",
      "В pandas для объединения таблиц служит функция concat(), в которую необходимо передать список фреймов для их объединения.\n",
      "\n",
      "Фильтрация строк в Python: pandas\n",
      "# вертикальное объединение таблиц\n",
      "pd.concat([ga_nov, ga_dec])\n",
      "\n",
      "Горизонтальное объединение таблиц\n",
      "Операция при которой, к первой таблице добавляются столбцы из второй по ключу. Зачастую используется при обогащении таблицы фактов (например таблице с данными о продажах), некоторыми справочными данными (например стоимостью товара).\n",
      "\n",
      "Есть несколько типов объединения:\n",
      "\n",
      "В загруженной ранее таблице titanic у нас имеется столбец Sex, который соответствует коду пола пассажира:\n",
      "1 — женский\r\n",
      "2 — мужской\n",
      "Также, мы с вами создали таблицу — справочник gender. Для более удобного представления данных по полу пассажиров нам необходимо добавить название пола, из справочника gender в таблицу titanic.\n",
      "Горизонтальное объединение таблиц в R: tidyverse, dplyr\n",
      "В dplyr для горизонтального объединения присутствует целое семейство функций:\n",
      "\n",
      "inner_join()\n",
      "left_join()\n",
      "right_join()\n",
      "full_join()\n",
      "semi_join()\n",
      "nest_join()\n",
      "anti_join()\n",
      "\n",
      "Наиболее часто используемой в моей практике является left_join(). \n",
      "В качестве первых двух аргументов перечисленные выше функцию принимают две таблицы для объединения, а в качестве третьего аргумента by необходимо указать столбцы для объединения.\n",
      "\n",
      "Горизонтальное объединение таблиц в R: dplyr\n",
      "# объединяем таблицы\n",
      "left_join(titanic, gender,\n",
      "          by = c(\"Sex\" = \"id\"))\n",
      "\n",
      "Горизонтальное объединение таблиц в R: data.table\n",
      "В data.table объединять таблицы по ключу необходимо с помощью функции merge().\n",
      "Аргументы функции merge() в data.table\n",
      "\n",
      "x, y — Таблицы для объелинения\n",
      "by — Столбец, который является ключом для объединения, если в обеих таблицах он имеет одинаковое название\n",
      "by.x, by.y — Имена столбцов для объединения, в случае если в таблицах они имеют разное название\n",
      "all, all.x, all.y — Тип соединения, all вернёт все строки из обеих таблиц, all.x соответствует операции LEFT JOIN (оставит все строки первой таблицы), all.y — соответствует операции RIGHT JOIN (оставит все строки второй таблицы).\n",
      "\n",
      "\n",
      "Горизонтальное объединение таблиц в R: data.table\n",
      "# объединяем таблицы\n",
      "merge(titanic, gender, by.x = \"Sex\", by.y = \"id\", all.x = T)\n",
      "\n",
      "Горизонтальное объединение таблиц в Python: pandas\n",
      "Так же как и в data.table, в pandas для объединения таблиц используется функция merge().\n",
      "Аргументы функции merge() в pandas\n",
      "\n",
      "how — Тип соединения: left, right, outer, inner\n",
      "on — Столбец, который является ключом, в случае если имеет одинаковое название в обеих таблицах\n",
      "left_on, right_on — Имена столбцов ключей, в случае если они имеют разные имена в таблицах\n",
      "\n",
      "\n",
      "Горизонтальное объединение таблиц в Python: pandas\n",
      "# объединяем по ключу\n",
      "titanic.merge(gender, how = \"left\", left_on = \"Sex\", right_on = \"id\")\n",
      "\n",
      "Простейшие оконные функции и вычисляемые столбцы\n",
      "Оконные функции по смыслу похожи на агрегирующие, и так же часто используются в анализе данных. Но в отличие от агрегирующих функций, оконные не меняют количество строк исходящего датафрейма. \n",
      "\n",
      "По сути с помощью оконных функцию мы разбиваем входящий датафрейм на части по какому-то признаку, т.е. по значению поля, или нескольких полей. И проводим над каждым окном арифметические операции. Результат этих операций будет возвращён в каждую строку, т.е. не изменяя общего количества строк в таблице.\n",
      "Для примера возьмём таблицу titanic. Мы можем посчитать какой процент составила стоимость каждого билета в рамках его класса кают. \n",
      "Для этого нам необходимо в каждой строке получить общую стоимость билета по текущему классу кают, к которому относится билет в данной строке, потом разделить стоимость каждого билета на общую стоимость всех билетов этого же класса кают.\n",
      "Оконные функции в R: tidyverse, dplyr\n",
      "Для добавления новых столбцов, без применения группировки строк, в dplyr служит функция mutate().\n",
      "Решить описанную выше задачу можно сгруппировав данные по полю Pclass и просуммировав в новом столбце поле Fare. Далее разгруппировываем таблицу и делим значения поля Fare на то, что получилось в прошлом шаге.\n",
      "\n",
      "Оконные функции в R: dplyr\n",
      "group_by(titanic, Pclass) %>%\n",
      "  mutate(Pclass_cost = sum(Fare)) %>%\n",
      "  ungroup() %>%\n",
      "  mutate(ticket_fare_rate = Fare / Pclass_cost)\n",
      "\n",
      "Оконные функции в R: data.table\n",
      "Алгоритм решения остаётся такой же, как в dplyr, нам необходимо разбить таблицу на окна по полю Pclass. Вывести в новом столбце сумму по соответствующей каждой строке группе, и добавить столбец в котором мы рассчитаем долю стоимости каждого билета в его группе.\n",
      "Для добавления новых столбцов в data.table присутствует оператор :=. Ниже приведён пример решения задачи с помощью пакета data.table\n",
      "\n",
      "Оконные функции в R: data.table\n",
      "titanic[,c(\"Pclass_cost\",\"ticket_fare_rate\") := .(sum(Fare), Fare / Pclass_cost), \n",
      "        by = Pclass]\n",
      "\n",
      "Оконные функции в Python: pandas\n",
      "Один из способов добавить новый столбец в pandas — использовать функцию assign(). Для суммирования стоимости билетов по классу кают, без группировки строк мы будем использовать функцию transform().\n",
      "Ниже пример решения, в котором мы добавляем в таблицу titanic те же 2 столбца.\n",
      "\n",
      "Оконные функции в Python: pandas\n",
      "titanic.assign(Pclass_cost      =  titanic.groupby('Pclass').Fare.transform(sum),\n",
      "               ticket_fare_rate = lambda x: x['Fare'] / x['Pclass_cost'])\n",
      "\n",
      "Таблица соответствия функций и методов\n",
      "Далее привожу таблицу соответствия методов для выполнения различных операций с данными в рассмотренных нами пакетах.\n",
      "\n",
      "\n",
      "\n",
      "Описание\n",
      "tidyverse\n",
      "data.table\n",
      "pandas\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Загрузка данных\n",
      "vroom()/ readr::read_csv() / readr::read_tsv()\n",
      "fread()\n",
      "read_csv()\n",
      "\n",
      "\n",
      "Создание датафреймов\n",
      "tibble()\n",
      "data.table()\n",
      "dict() + from_dict()\n",
      "\n",
      "\n",
      "Выбор столбцов\n",
      "select()\n",
      "аргумент j, вторая позиция в квадратных скобках\n",
      "передаём список нужных столбцов в квадратных скобках / drop() / filter() / select_dtypes()\n",
      "\n",
      "\n",
      "Фильтрация строк\n",
      "filter()\n",
      "аргумент i, первая позиция в квадратных скобках\n",
      "перечисляем условия фильтрации в квадратных скобках / filter()\n",
      "\n",
      "\n",
      "Группировка и агрегация\n",
      "group_by() + summarise()\n",
      "аргументы j + by\n",
      "groupby() + agg()\n",
      "\n",
      "\n",
      "Вертикальное объединение таблиц (UNION)\n",
      "bind_rows()\n",
      "rbind()\n",
      "concat()\n",
      "\n",
      "\n",
      "Горизонтальное объединение таблиц (JOIN)\n",
      "left_join() / *_join()\n",
      "merge()\n",
      "merge()\n",
      "\n",
      "\n",
      "Простейшие оконные функции и добавление рассчитываемых столбцов\n",
      "group_by() + mutate()\n",
      "аргумент j с использованием оператора := + аргумент by\n",
      "transform() + assign()\n",
      "\n",
      "\n",
      "\n",
      "Заключение\n",
      "Возможно в статье я описал не самые оптимальные реализации обработки данных, поэтому буду рад если исправите мои ошибки в комментариях, или же просто дополните приведённую в статье информацию другими приёмами работы с данными в R / Python.\n",
      "Как я уже писал выше, цель статьи заключась не в том, что бы навязывать своё мнение о том, какой из языков лучше, а упростить возможность изучить оба языка, либо по необходимости мигрировать между ними.\n",
      "Если вам понравилась статья буду рад новым подписчикам на моих youtube и телеграм каналах.\n",
      "Опрос\n",
      "А какие из перечисленных пакетов вы используете в работе?\n",
      "В комментариях можете написать причину своего выбора.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Какой пакет для обработки данных вы используете (можно выбрать несколько вариантов) \n",
      "            46.58%\n",
      "           tidyverse \n",
      "            34\n",
      "           \n",
      "            28.77%\n",
      "           data.table \n",
      "            21\n",
      "           \n",
      "            54.79%\n",
      "           pandas \n",
      "            40\n",
      "            \n",
      "       Проголосовали 73 пользователя.  \n",
      "\n",
      "       Воздержались 16 пользователей. \n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 ноября 2020 года была выпущена новая версия .NET Core, официально названная .NET 5. Обновлённая платформа предоставляет множество различных возможностей. К примеру, она позволяет C#-разработчикам использовать нововведения C# 9: records, relational pattern matching и т. д. К сожалению, есть и минусы – корректно проанализировать такой проект с помощью PVS-Studio нельзя. Ну... Раньше было нельзя. Ведь теперь эта проблема в прошлом – следующий релиз PVS-Studio 7.13 будет поддерживать анализ проектов, ориентированных на .NET 5!Aka .NET Core 4.NET 5 является, по сути, продолжением .NET Core 3.1. Изменив название на \".NET\", сотрудники Microsoft решили подчеркнуть, что именно эта реализация теперь является основной и именно она будет далее развиваться. И ведь не обманули – на официальном сайте уже на момент написания статьи можно было найти целых 3 preview-версии .NET 6. Однако почему же версия, следующая после 3.1, стала 5? Что ж, это не кажется таким удивительным, если вспомнить, что после Windows 8.1 идёт Windows 10 :) На самом деле, номер 4 пропущен лишь для того, чтобы не было путаницы с разделением .NET Framework и .NET. Несмотря на это, \"Entity Framework Core 5.0\", базирующийся на .NET 5, сохранит постфикс \"Core\". Это связано с тем, что в противном случае нельзя будет отличить Core и Framework версии \"Entity Framework 5.0\"..NET 5 даёт разработчикам много новых возможностей. К примеру, в C# серьёзно улучшен механизм сопоставления с шаблоном. Кроме того, появилась необычная, но интересная возможность писать код вне функций и классов. Пожалуй, далеко не каждому разработчику такое нужно, но наверняка и у этой фичи найдутся свои поклонники. Полный список нововведений C# 9 можно найти на официальном сайте.Почитать про общие нововведения .NET 5 можно здесь.PVS-Studio и .NET 5Пользователи нередко писали нам с просьбой поддержать анализ .NET 5 проектов. Да и сами мы вполне понимали, что крайне важно идти в ногу со временем. Однако поддержка новых версий платформы – хоть и важный, но далеко не единственный вектор развития PVS-Studio.  Во многом именно поэтому пришлось так долго ждать :( Но сейчас мы, наконец-то, готовы представить вам новую версию, поддерживающую анализ проектов под .NET 5.Новые возможности будут доступны для версий анализатора под Windows, Linux и macOS. Версии под Windows для работы, как и раньше, требуется установленный .NET Framework 4.7.2. А вот для использования анализатора под Linux и macOS теперь требуется наличие .NET 5 (раньше был нужен .NET Core 3.1).Зачем анализатор под Linux и macOS перешёл на .NET 5?Вообще изначально мы не планировали переводить анализатор для Linux/macOS с .NET Core 3.1 на .NET 5. Нет, конечно же, мы собирались перейти на новую версию платформы, но думали сделать это несколько позже. Что ж, планы пришлось поменять, так как перед нами возникла проблема. При проведении анализа проектов, ориентированных под .NET Core (и, соответственно, .NET 5), PVS-Studio активно взаимодействует с SDK соответствующей версии. Сложности возникли из-за того, что библиотеки из .NET 5 SDK зависят от \"System.Runtime\" версии 5. В то же время анализатор, ориентированный на .NET Core 3.1, загружает в память \"System.Runtime\" версии 3.1. В результате возникал конфликт – анализатор не мог взаимодействовать с библиотекой из SDK и анализ не проходил.Переход с .NET Core на .NET 5 полностью решил данную проблему :).Мгновенные улучшенияДля реализации поддержки анализа проектов под .NET 5 нам понадобилось обновить некоторые зависимости. В частности, PVS-Studio теперь использует более новые версии Roslyn и MSBuild. Это позволяет анализатору корректно разбирать код, использующий возможности C# 9. К примеру, ранее анализатор мог выдавать ложные срабатывания на код видаuser = user with { Name = \"Bill\" }\n",
      "Анализатор не мог корректно обработать такой код, так как в него не была заложена информация о WithExpression. В результате выдавалось предупреждение о том, что переменная user присваивается сама себе. Конечно, в том же самом отчёте было и предупреждение о том, что анализ данного проекта не поддерживается. Однако от того не особо легче :(. К счастью, после обновления все подобные проблемы решились сами собой. Другие же моменты нам пришлось обдумывать отдельно.Проблемы новых версийРазвивающийся статический анализатор вынужден подстраиваться под новые версии языка. Очевидно, что при разработке такого инструмента невозможно учесть всех особенностей, которые появятся в языке в будущем. Поддержка анализа новой версии языка – это не только обеспечение корректного парсинга и построения семантики. Не менее важно провести обзор существующего кода, оценить работу различных внутренних технологий и диагностических правил. Скорее всего, после обновления появится необходимость внести в существующий функционал некоторые правки.Одно из потенциально проблемных нововведений я упоминал ранее – top-level statements. При работе над C#-анализатором мы в некоторых случаях рассчитывали, что объявления локальных переменных, условия, циклы и т. д. всегда будут находиться внутри методов. Появление в языке возможности написания C#-кода даже без объявления класса... Это нечто интересное, конечно, но для нас это порождает некоторые сложности. Впрочем, едва ли все разработчики вдруг начнут писать весь код вне классов :)Другое нововведение, заставившее нас изменить код, – init-аксессор. Его появление повлекло за собой необходимость доработки диагностического правила V3140.Правило срабатывает в случаях, когда в аксессорах свойства используются разные внутренние переменные. Ниже приведён пример срабатывания из статьи о проверке RunUO:private bool m_IsRewardItem;\n",
      "\n",
      "[CommandProperty( AccessLevel.GameMaster )]\n",
      "public bool IsRewardItem\n",
      "{\n",
      "  get{ return m_IsRewardItem; }\n",
      "  set{ m_IsRewardItem = value; InvalidateProperties(); }\n",
      "}\n",
      "\n",
      "private bool m_East;\n",
      "\n",
      "[CommandProperty( AccessLevel.GameMaster )]\n",
      "public bool East                                       // <=\n",
      "{\n",
      "  get{ return m_East; }\n",
      "  set{ m_IsRewardItem = value; InvalidateProperties(); } \n",
      "}\n",
      "Предупреждение PVS-Studio: V3140 Property accessors use different backing fields. WallBanner.cs 77При написании диагностики V3140 предполагалось, что у свойства может быть только 2 аксессора – get и set. Появление init застало существующую реализацию врасплох. Диагностика не просто некорректно работала – она падала с исключением! К счастью, эта проблема была обнаружена ещё на этапе тестирования и успешно решена.Ждём релиза?А это вовсе не обязательно! Конечно, релиз уже скоро, поэтому ждать осталось недолго, но всё-таки... Хочется попробовать новую версию как можно раньше, не правда ли? Что ж, никаких проблем – мы с радостью предоставим такую возможность! Переходите на страницу обратной связи и отправляйте свой запрос новой версии. Мы постараемся ответить максимально быстро, а значит, уже скоро вы сможете анализировать свои (и не только) .NET 5 проекты!Призываем вас также делиться своими впечатлениями о PVS-Studio. Каким бы ни был ваш опыт использования анализатора, мы хотели бы о нём узнать. Ведь во многом именно благодаря отзывам пользователей PVS-Studio развивается и становится лучше.Если хотите поделиться этой статьей с англоязычной аудиторией, то прошу использовать ссылку на перевод: Nikita Lipilin. Finally! PVS-Studio Supports .NET 5 Projects.    \n",
      " Вы идете в банк за кредитом на развитие бизнеса, на покупку авто или на другие цели. Давать или не давать – в каждом случае специалисты банка решают этот вопрос индивидуально, принимая во внимание кредитную историю клиента, размер его дохода и другие факторы. Казалось бы, система кредитования давно настроена и исправно работает. Можно ли придумать что-то новое в этом отношении? Мы в рознице ВТБ отвечаем на этот вопрос утвердительно. Исследования подтверждают: данные о клиентском поведении, которые имеются в распоряжении банка, задействованы далеко не полностью, и в этом направлении использование IT дает очень хороший эффект!\n",
      "\n",
      "\n",
      "\n",
      "Как мы интегрируем IT в бизнес и какие преимущества получают клиенты – читайте под катом.\n",
      "\n",
      "В 2016 году для розничного направления Группы ВТБ мы реализовали первый этап крупного проекта по обработке и анализу клиентской информации. Благодаря этому проекту наши клиенты стали получать персональные предложения, основанные на анализе их поведения в прошлом. На первом этапе мы собрали и используем до 60% данных, и результаты превзошли все ожидания. Большинство клиентов охотно приняли индивидуальные предложения и, что самое главное – остались довольны. Значит, идея избирательного подхода сработала, система функционирует на «отлично».\n",
      "\n",
      "Сейчас на очереди второй этап – запуск новой платформы DataResearchPlatform на основе DataLake («озера данных»), которая в перспективе должна охватить 99,9% всех имеющихся в банке данных о клиентской активности.\n",
      "\n",
      "Почему DataLake?\n",
      "Как и все современные решения в области Big Data, наша новая платформа DataResearchPlatform построена на основе «озера данных». Почему мы выбрали именно эту технологию? DataLake хорош тем, что позволяет хранить огромные объемы «сырых» данных в их первоначальном формате. Эти данные могут использоваться как угодно: сопоставляться, смешиваться, организовываться по различным критериям. В отличие от стандартного хранилища данных, данные DataLake доступны аналитикам сразу в полном объеме и со всеми исходными связями. Это дает больше возможностей для поиска самых неожиданных вариантов их использования, но для этого нужны соответствующие технологии и инструменты.\n",
      "\n",
      "Клиентская информация обрабатывается с использованием интеллектуального анализа данных data mining. Благодаря этому специалисты банка могут проверять свои гипотезы о клиентском поведении и его влиянии на платежеспособность, а также разрабатывать новые предсказательные модели.\n",
      "\n",
      "Есть и другие «фишки», которые мы планируем получить при работе с DataLake:\n",
      "\n",
      "\n",
      "вырастить собственных пользователей профилей DataArchitect и DataScientist в корпоративной среде;\n",
      "получить отличный опыт в глубинном анализе данных;\n",
      "полностью пересмотреть и улучшить системы управления информацией о клиентах (CRM);\n",
      "научиться точнее предсказывать риск для каждого конкретного клиента.\n",
      "\n",
      "Когда система налажена, банк может брать самые современные удочки и отправляться на рыбалку на свое «озеро». И можно не сомневаться: каждый раз улов будет превосходный, и им захочется поделиться с клиентами. Благодаря глубокому анализу клиентского поведения, банк может предлагать заемщикам специальные предложения, лучшие кредитные условия и индивидуальные (более лояльные) процентные ставки по кредитам.\n",
      "\n",
      " Как работает DataResearchPlatform?\n",
      " До того, как было принято решение перейти на DataLake, в ВТБ уже существовало хранилище данных, поэтому первое, что мы сделали – интегрировали с ним новую платформу.\n",
      "\n",
      "Кроме этого, на первом этапе мы работали над отладкой технологической среды для моделирования: были отработаны механизмы обновления всего установленного ПО и расширен кластер Hadoop. Также было важно выработать новые подходы к работе пользователей, поскольку новая платформа накладывает определенные требования к разграничению доступа к данным.\n",
      "\n",
      "В итоге нынешняя версия DataResearchPlatform развернута на 12 узлах BDA объемом до 288 ТБ (в планах ее расширение до 18 узлов до конца года). Платформа работает на основе экосистемы Hadoop, технологий OpenSource и промышленных Enterprise-решений. Она базируется на программно-аппаратном решении Oracle BigData Appliance. Для работы с данными используются аналитические инструменты SAS HPDM, SAS EG, Python, R.\n",
      "\n",
      "Пользователи профиля DataArchitect и DataScientist получили полностью безопасный доступ к данным, а объемы данных были расширены. Теперь в DataResearchPlatform уже собирается практически вся информация о клиентской активности, которая имеется в распоряжении банка. Ее в любой момент можно «выловить» из «озера» и использовать во благо клиента.\n",
      "\n",
      "Рабочая команда проекта: члены правления ВТБ24 — А.Соколов и С.Русанов.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Исходные данные\r\n",
      "Очистка данных – это одна из проблем стоящих перед задачами анализа данных. В этом материале отразил наработки, решения, которые возникли в результате решения практической задачи по анализу БД при формировании кадастровой стоимости. Исходники здесь «ОТЧЕТ № 01/ОКС-2019 об итогах государственной кадастровой оценки всех видов объектов недвижимости (за исключением земельных участков) на территории Ханты-Мансийского автономного округа — Югры».\n",
      "\r\n",
      "Рассматривался файл «Сравнительный модель итог.ods» в «Приложение Б. Результаты определения КС 5. Сведения о способе определения кадастровой стоимости 5.1 Сравнительный подход». \n",
      "\r\n",
      "Таблица 1. Статпоказатели датасета в файле «Сравнительный модель итог.ods»\r\n",
      "Общее количество полей, шт. — 44\r\n",
      "Общее количество записей, шт. — 365 490\r\n",
      "Общее количество символов, шт. — 101 714 693\r\n",
      "Среднее количество символов в записи, шт. — 278,297\r\n",
      "Стандартное отклонение символов в записи, шт. — 15,510\r\n",
      "Минимальное количество символов в записи, шт. — 198\r\n",
      "Максимальное количество символов в записи, шт. — 363\n",
      "\n",
      "2. Вводная часть. Базовые нормы\r\n",
      "Занимаясь анализом указанной БД сформировалась задача по конкретизации требований к степени очистки, так как, это понятно всем, указанная БД формирует правовые и экономические последствия для пользователей. В процессе работы оказалось, что особо никаких требований к степени очистки больших данных не сформировано. Анализируя правовые нормы в этом вопросе пришел к выводу, что все они сформированы от возможностей. То есть появилась определенная задача, под задачу комплектуются источники информации, далее формируется датасет и, на основе создаваемого датасета, инструменты для решения задачи. Полученные решения являются реперными точками в выборе из альтернатив. Представил это на рисунке 1.\n",
      "\n",
      "\n",
      "\r\n",
      "Так как, в вопросах определения каких-либо норм, предпочтительно опираться на проверенные технологии, то выбрал за основу критериев анализа, требования изложенные в «MHRA GxP Data Integrity Definitions and Guidance for Industry», потому что посчитал этот документ наиболее целостным для этого вопроса. В частности в этом документе раздел написано «It should be noted that data integrity requirements apply equally to manual (paper) and electronic data.» (пер. «… требования к целостности данных распространяются в равной степени на ручные (бумажные) и электронные данные»). Такая формулировка достаточно конкретно связывается с понятием «письменное доказательство», в нормах ст.71 ГПК, ст. 70 КАС, ст.75 АПК, «письменном виде» ст. 84 ГПК. \n",
      "\r\n",
      "На рисунке 2 представил схему формирования подходов к видам информации в юриспруденции.\n",
      "\n",
      "\n",
      "Рис. 2. Источник здесь.\n",
      "\r\n",
      "На рисунке 3 представлен механизм рисунка 1, для задач вышеуказанного «Guidance». Несложно, проводя сопоставление, увидеть, что используемые подходы, при выполнении требований к целостности информации, в современных нормах к информационным системам, существенно ограничены, в сравнении с правовым понятием информации. \n",
      "\n",
      "\n",
      "Рис.3\n",
      "\r\n",
      "В указанном документе (Guidance) привязка к технической части, возможностей по обработке и хранению данных, хорошо подтверждается цитатой из главы 18.2. Relational database: «This file structure is inherently more secure, as the data is held in a large file format which preserves the relationship between data and metadata». \n",
      "\r\n",
      "По сути, в таком подходе – от существующих технических возможностей, нет ничего не нормального и, сам по себе, это естественный процесс, так как расширение понятий происходит от наиболее изученной деятельности – проектирование баз данных. Но, с другой стороны, появляются правовые нормы, в которых не предусмотрено скидки на технические возможности имеющихся систем, например: GDPR — General Data Protection Regulation.\n",
      "\n",
      "\n",
      "Рис. 4. Воронка технических возможностей (Источник).\n",
      "\r\n",
      "В указанных аспектах становится понятным, что первоначальный датасет (рис. 1) должен будет, в первую очередь, сохраняться, а во вторую очередь быть базой для извлечения из него дополнительной информации. Ну как пример: повсеместно распространены камеры фиксации ПДД, системы информационной обработки отсеивают нарушителей, но остальная информация также может быть предложена другим потребителям, допустим как маркетинговый мониторинг структуры потока покупателей к торговому центру. А это источник дополнительной добавленной стоимости при использовании Бигдата. Вполне можно допустить, что собираемые сейчас датасеты, где-то в будущем, будут иметь ценность по механизму аналогичному ценности раритетных изданий 1700 годов в настоящее время. Ведь, по сути, временные датасеты уникальны и маловероятно, что повторяться в будущем.\n",
      "\n",
      "3. Вводная часть. Критерии оценок\r\n",
      "В процессе обработки была выработана следующая классификация ошибок.\n",
      "\r\n",
      "1. Класс ошибки (за основу взят ГОСТ Р 8.736-2011): а) систематические ошибки; б) случайные ошибки; в) грубая ошибка.\n",
      "\r\n",
      "2. По множественности: а) моноискажение; б) мультиискажение.\n",
      "\r\n",
      "3. По критичности последствий: а) критическая; б) не критическая.\n",
      "\r\n",
      "4. По источнику возникновения: \n",
      "\r\n",
      "А) Техническая – ошибки возникающие в процессе работы оборудования. Достаточно актуальная ошибка для IoT-систем, систем со значительной степенью влияния качества связи, оборудования (железа).\n",
      "\r\n",
      "Б) Операторские – ошибки в широком диапазоне от опечатки оператора при вводе до ошибок в техзадании на проектирование БД.\n",
      "\r\n",
      "В) Пользовательские – тут ошибки пользователя во всем диапазоне от «забыл переключить раскладку» до того что метры принял за футы.\n",
      "\r\n",
      "5. Выделил в отдельный класс:\n",
      "\r\n",
      "а) «задачу разделителя», то есть пробела и «:» (в нашем случае) когда его продублировали;\r\n",
      "б) слитно написанных слов;\r\n",
      "в) отсутствия пробела после служебных символов\r\n",
      "г) симметрично-множественные символы: (), «», «…».\n",
      "\r\n",
      "В совокупностью, с систематизацией ошибок БД представленных на рисунке 5, складывается достаточно эффективная система координат для поиска ошибок и выработки алгоритма очистки данных, для этого примера.\n",
      "\n",
      "\n",
      "Рис. 5. Типичные ошибки, соответствующие структурным единицам БД (Источник: Орешков В.И., Паклин Н.Б. «Ключевые понятия консолидации данных»).\n",
      "\r\n",
      "Accuracy (Точность), Domain Integrity (Целостность), Data Type (Тип данных), Consistency (Консистенция), Redundancy (Избыточность), Completeness (Полнота), Duplication (Дублирование), Conformance to Business Rules (Соблюдение бизнес-правил), Structural Definiteness (Структурная Определенность), Data Anomaly (Аномалия Данных), Clarity (Ясность), Timely (Своевременность), Adherence to Data Integrity Rules (Соблюдение правил целостности данных). (Стр. 334. Data warehousing fundamentals for IT professionals / Paulraj Ponniah.—2nd ed.)\n",
      "\r\n",
      "Представил английские формулировки и в скобках русский машинный перевод.\n",
      "\r\n",
      "Accuracy. The value stored in the system for a data element is the right value for that occurrence of the data element. If you have a customer name and an address stored in a record, then the address is the correct address for the customer with that name. If you find the quantity ordered as 1000 units in the record for order number 12345678, then that quantity is the accurate quantity for that order.\r\n",
      "[Точность. Значение, сохраненное в системе для элемента данных, является правильным значением для этого вхождения элемента данных. Если у вас есть имя клиента и адрес, сохраненные в записи, то адрес является правильным адресом для клиента с этим именем. Если вы найдете количество, заказанное как 1000 единиц в записи для заказа номер 12345678, то это количество является точным количеством для этого заказа.]\n",
      "\r\n",
      "Domain Integrity. The data value of an attribute falls in the range of allowable, defined values. The common example is the allowable values being “male” and “female” for the gender data element.\r\n",
      "[Целостность Домена. Значение данных атрибута попадает в диапазон допустимых, определенных значений. Общий пример-допустимые значения «мужской” и „женский“ для элемента гендерных данных.]\n",
      "\r\n",
      "Data Type. Value for a data attribute is actually stored as the data type defined for that attribute. When the data type of the store name field is defined as “text,” all instances of that field contain the store name shown in textual format and not numeric codes.\r\n",
      "[Тип данных. Значение атрибута данных фактически сохраняется как тип данных, определенный для этого атрибута. Если тип данных поля имя магазина определен как „текст“, все экземпляры этого поля содержат имя магазина, отображаемое в текстовом формате, а не в числовых кодах.]\n",
      "\r\n",
      "Consistency. The form and content of a data field is the same across multiple source systems. If the product code for product ABC in one system is 1234, then the code for this product is 1234 in every source system.\r\n",
      "[Консистенция. Форма и содержание поля данных одинаковы в разных системах-источниках. Если код продукта для продукта ABC в одной системе равен 1234, то код для этого продукта равен 1234 в каждой исходной системе.]\n",
      "\r\n",
      "Redundancy. The same data must not be stored in more than one place in a system. If, for reasons of efficiency, a data element is intentionally stored in more than one place in a system, then the redundancy must be clearly identified and verified.\r\n",
      "[Избыточность. Одни и те же данные не должны храниться более чем в одном месте системы. Если по соображениям эффективности элемент данных намеренно хранится в нескольких местах системы, то избыточность должна быть четко определена и проверена.]\n",
      "\r\n",
      "Completeness. There are no missing values for a given attribute in the system. For example, in a customer file, there must be a valid value for the “state” field for every customer. In the file for order details, every detail record for an order must be completely filled.\r\n",
      "[Полнота. В системе нет пропущенных значений для данного атрибута. Например, в файле клиента должно быть допустимое значение поля „состояние“ для каждого клиента. В файле сведений о заказе каждая запись сведений о заказе должна быть полностью заполнена.]\n",
      "\r\n",
      "Duplication. Duplication of records in a system is completely resolved. If the product file is known to have duplicate records, then all the duplicate records for each product are identified and a cross-reference created.\r\n",
      "[Дублирование. Дублирование записей в системе полностью устранено. Если известно, что файл продукта содержит повторяющиеся записи, то все повторяющиеся записи для каждого продукта идентифицируются и создается перекрестная ссылка.]\n",
      "\r\n",
      "Conformance to Business Rules. The values of each data item adhere to prescribed business rules. In an auction system, the hammer or sale price cannot be less than the reserve price. In a bank loan system, the loan balance must always be positive or zero.\r\n",
      "[Соблюдение бизнес-правил. Значения каждого элемента данных соответствуют установленным бизнес-правилам. В аукционной системе цена молотка или продажи не может быть меньше резервной цены. В банковской кредитной системе баланс кредита всегда должен быть положительным или нулевым.]\n",
      "\r\n",
      "Structural Definiteness. Wherever a data item can naturally be structured into individual components, the item must contain this well-defined structure. For example, an individual’s name naturally divides into first name, middle initial, and last name. Values for names of individuals must be stored as first name, middle initial, and last name. This characteristic of data quality simplifies enforcement of standards and reduces missing values.\r\n",
      "[Структурная Определенность. Там, где элемент данных может быть естественным образом структурирован на отдельные компоненты, элемент должен содержать эту четко определенную структуру. Например, имя человека естественным образом делится на имя, средний инициал и фамилию. Значения для имен физических лиц должны храниться в виде имени, среднего инициала и фамилии. Эта характеристика качества данных упрощает применение стандартов и уменьшает недостающие значения.]\n",
      "\r\n",
      "Data Anomaly. A field must be used only for the purpose for which it is defined. If the field Address-3 is defined for any possible third line of address for long addresses, then this field must be used only for recording the third line of address. It must not be used for entering a phone or fax number for the customer. \r\n",
      "[Аномалия Данных. Поле должно использоваться только для той цели, для которой оно определено. Если поле Address-3 определено для любой возможной третьей строки адреса для длинных адресов, то это поле должно использоваться только для записи третьей строки адреса. Он не должен использоваться для ввода номера телефона или факса для клиента.]\n",
      "\r\n",
      "Clarity. A data element may possess all the other characteristics of quality data but if the users do not understand its meaning clearly, then the data element is of no value to the users. Proper naming conventions help to make the data elements well understood by the users.\r\n",
      "[Ясность. Элемент данных может обладать всеми другими характеристиками качественных данных, но если пользователи не понимают его значения ясно, то элемент данных не представляет ценности для пользователей. Правильные соглашения об именовании помогают сделать элементы данных хорошо понятными пользователям.]\n",
      "\r\n",
      "Timely. The users determine the timeliness of the data. lf the users expect customer dimension data not to be older than one day, the changes to customer data in the source systems must be applied to the data warehouse daily.\r\n",
      "[Своевременно. Пользователи определяют своевременность данных. если пользователи ожидают, что данные измерения клиента не будут старше одного дня, изменения данных клиента в исходных системах должны применяться к хранилищу данных ежедневно.]\n",
      "\r\n",
      "Usefulness. Every data element in the data warehouse must satisfy some requirements of the collection of users. A data element may be accurate and of high quality, but if it is of no value to the users, then it is totally unnecessary for that data element to be in the data warehouse.\r\n",
      "[Полезность. Каждый элемент данных в хранилище данных должен удовлетворять некоторым требованиям коллекции пользователей. Элемент данных может быть точным и иметь высокое качество, но если он не представляет ценности для пользователей, то совершенно необязательно, чтобы этот элемент данных находился в хранилище данных.]\n",
      "\r\n",
      "Adherence to Data Integrity Rules. The data stored in the relational databases of the source systems must adhere to entity integrity and referential integrity rules. Any table that permits null as the primary key does not have entity integrity. Referential integrity forces the establishment of the parent–child relationships correctly. In a customer-to-order relationship, referential integrity ensures the existence of a customer for every order in the database.\r\n",
      "[Соблюдение правил целостности данных. Данные, хранящиеся в реляционных базах данных исходных систем, должны соответствовать правилам целостности сущностей и ссылочной целостности. Любая таблица, допускающая null в качестве первичного ключа, не обладает целостностью сущности. Ссылочная целостность заставляет правильно устанавливать отношения между родителями и детьми. В отношениях клиент-заказ ссылочная целостность обеспечивает существование клиента для каждого заказа в базе данных.]\n",
      "\n",
      "4. Качество очистки данных\r\n",
      "Качество очистки данных достаточно проблематичный вопрос в бигдата. Ответить на вопрос какая степень очистки данных необходима при выполнении поставленной задачи, является основным для каждого датаанлитика. В большинстве текущих задач каждый аналитик устанавливает это сам и вряд ли кто-то со стороны способен оценить этот аспект в его решении. Но для поставленной задачи в этом случае этот вопрос был крайне важен, так как достоверность правовых данных должна стремиться к единице. \n",
      "\r\n",
      "Рассматривая технологии тестирования программного обеспечения по определению надежности в работе. Этих моделей на сегодняшний день более 200. Многие из моделей используют модель обслуживания заявок:\n",
      "\n",
      "\n",
      "Рис. 6\n",
      "\r\n",
      "Размышляя следующим образом: «Если найденная ошибка это событие аналогичное событию отказа в данной модели, то как найти аналог параметра t?» И составил следующую модель: Представим, что время которое необходимо тестировщику для проверки одной записи равно 1 минута (для рассматриваемой БД), тогда чтобы отыскать все ошибки ему потребуется 365 494 минут, что приблизительно составляет 3 года и 3 месяца рабочего времени. Как мы понимаем это очень не малый объем работы и затраты за проверку базы данных будут неподъемны для составителя этой БД. В данном размышлении появляется экономическое понятие затраты и после анализа пришел к выводу, что это достаточно эффективный инструмент. Опираясь на закон экономики: «Объем производства (в ед.), при котором достигается максимальная прибыль фирмы, находится в той точке где предельные затраты на выпуск новой единицы продукции сравниваются с ценой, которую эта фирма может получить за новую единицу». Опираясь на постулат, что нахождение каждой следующей ошибки, требует все больше и больше проверки записей, то это и есть фактор затрат. То есть принятый в моделях тестирования постулат принимает физически смысл, в следующей закономерности: если для нахождение i-той ошибки потребовалось проверить n записей, то для нахождения следующей (i+1) ошибки уже потребуется проверить m записей и при этом n<m. Этот постулат, в моделях тестирования, формулируются, в основном тем требованием, что найденные ошибки нужно фиксировать, но не исправлять, чтобы ПО тестировалось в своем естественном состоянии, то есть поток отказов был однороден. Соответственно, для нашего случая, проверка записей может проявить два варианта однородности:\n",
      "\n",
      "\n",
      "Когда количество проверенных записей до нахождения новой ошибки стабилизируется;\n",
      "Когда количество проверенных записей до нахождения следующей ошибки будет увеличиваться.\n",
      "\r\n",
      "Для определения критического значения обратился к понятию экономической целесообразности, которое в данном случае, при использовании понятия общественных затрат можно сформулировать следующим образом: «Затраты по исправлению ошибки должен нести тот экономический агент, который сможет это сделать с наименьшими издержками». Одного агента мы имеем – это тестировщик, который тратит на проверку одной записи 1 минуту. В денежном эквиваленте, при заработке 6000 руб./день, это составит 12,2 руб. (приблизительно на сегодняшний день). Осталось определить вторую сторону равновесия в экономическом законе. Рассуждал так. Существующая ошибка потребует от того, кого она касается потратить усилия по ее исправлению, то есть владельца недвижимости. Допустим для этого нужно 1 день действий (отнести заявление, получить исправленный документ). Тогда с общественной точки зрения его затраты будут равны средней з/п за день. Средняя начисленная з/п в ХМАО по «Итоги социально-экономического развития Ханты-Мансийского автономного округа – Югры за январь-сентябрь 2019 года» 73285 руб. или 3053,542 руб./день. Соответственно получаем критическое значение равное:\r\n",
      "3053,542: 12,2 = 250,4 ед.записей.\n",
      "\r\n",
      "Это означает, с общественной точки зрения, если тестировщик проверил 251 запись и нашел одну ошибку это равноценно тому, что пользователь исправил эту ошибку самостоятельно. Соответственно если тестировщик потратил на нахождение следующей ошибки время равное проверке 252 записей, то в этом случае затраты на исправление лучше переложить на пользователя. \n",
      "\r\n",
      "Здесь представлен упрощенный подход, так как с общественной точки зрения необходимо учитывать всю дополнительную стоимость генерируемую каждым специалистом, то есть затраты с учетом налогов и соцплатежей, но модель понятна. Следствием из этой взаимосвязи становится требование к специалистам следующее: специалист из IT отрасли должен иметь з/п большую чем в среднем по стране. Если его з/п меньше, чем в среднее значение з/п потенциальных пользователей БД, то он сам должен проверить всю БД в рукопашную. \n",
      "\r\n",
      "При использовании описанного критерия формируется первое требование к качеству БД:\r\n",
      "I(тр). Доля критических ошибок не должна превышать величины 1/250,4 = 0,39938%. Чуть меньше чем аффинажная очистка золота в промышленности. И в натуральном измерении не более 1459 записей с ошибками.\n",
      "\r\n",
      "Экономическое отступление.\n",
      "\r\n",
      "По сути, допуская такое количество ошибок в записях, общество соглашается на экономические потери в объеме:\n",
      "\r\n",
      "1459*3053,542 = 4 455 118 руб.\n",
      "\r\n",
      "Данная сумма определяется тем фактом, что у общества отсутствуют инструменты позволяющие снизить эти издержки. Отсюда следует, если у кого-то появится технология, которая позволяет снизить количество записей с ошибками до, например, 259, то это позволяет обществу экономить:\r\n",
      "1200*3053,542 = 3 664 250 руб.\n",
      "\r\n",
      "Но при этом он может попросить за свой талант и труд, ну допустим — 1 млн. руб.\r\n",
      "То есть общественные издержки сокращаются на:\n",
      "\r\n",
      "3 664 250 – 1 000 000 = 2 664 250 руб.\n",
      "\r\n",
      "По сути, этот эффект является добавленной стоимостью, от использования технологий Бигдата. \n",
      "\r\n",
      "Но тут следует учитывать, что это общественный эффект, а владельцем БД являются муниципальные органы власти, их доход от использования имущества, зафиксированного в данной БД, при ставке 0,3% составляет: 2,778 млрд. руб./год. А эти издержки (4 455 118 руб.) его сильно не волнуют, так как переложены на владельцев имущества. И, в этом аспекте, разработчик, более аффинажных технологий в Бигдата, должен будет проявить умение убедить владельца этой БД, а на такие вещи нужен немалый талант.\n",
      "\r\n",
      "В данном примере алгоритм оценки ошибок был выбран на основе модель Шумана [2] проверки ПО при тестировании на безотказность. По причине ее распространённости в сети и возможности получить необходимые статистические показатели. Методология взята из Монахов Ю.М. «Функциональная устойчивость информационных систем», смотрите под спойлером на рис. 7-9. \n",
      "\n",
      "Рис. 7 – 9 Методология модели Шумана\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Во второй части данного материала представлен пример очистки данных, в котором получены результаты использования модели Шумана.\r\n",
      "Представлю полученные результаты:\r\n",
      "Предполагаемое количество ошибок N = 3167 шN.\r\n",
      "Параметр С, лямбда и функция надежности:\n",
      "\n",
      "\n",
      "Рис.17\n",
      "\r\n",
      "По сути, лямбда — это фактический показатель с какой интенсивностью на каждом этапе обнаруживаются ошибки. Если посмотреть, во второй части, то оценка этого показателя составляла 42,4 ошибки в час, что, достаточно, сравнимо с показателем Шумана. Выше, было определено, что интенсивность нахождения ошибок разработчиком должна быть не ниже чем 1 ошибка на 250,4 записей, при проверке 1 записи в минуту. Отсюда критическое значение лямбда для модели Шумана:\n",
      "\r\n",
      "60/250,4 = 0,239617. \n",
      "\r\n",
      "То есть необходимость проведения процедур нахождения ошибок нужно проводить до тех пор, пока лямбда, с имеющихся 38,964, не снизится до 0,239617.\n",
      "\r\n",
      "Либо пока показатель N (потенциальное количество ошибок) минус n (исправленное количество ошибок) не снизится меньше принятого нами порога – 1459 шт.\n",
      "\n",
      "Литература\n",
      "\n",
      "Монахов, Ю. М. Функциональная устойчивость информационных систем. В 3 ч. Ч. 1. Надежность программного обеспечения: учеб. пособие / Ю. М. Монахов; Владим. гос. ун-т. – Владимир: Издво Владим. гос. ун-та, 2011. – 60 с. – ISBN 978-5-9984-0189-3. \n",
      "Martin L. Shooman, «Probabilistic models for software reliability prediction».\n",
      "Data warehousing fundamentals for IT professionals / Paulraj Ponniah.—2nd ed.\n",
      "\n",
      "Часть вторая. Теоретическая    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Часть 1. Технология гетерогенных ROLAP-кубовВсем привет. В этой публикации мы начнем рассказ о том, как наша BI-платформа «Форсайт» работает с данными. Как организовано взаимодействие платформы с СУБД и какие объемы информации мы можем эффективно обрабатывать. Что такое связка «BI+Data Lake» и как можно ее сформировать. Как в OLAP-кубах получать сведения из разных слоев данных: сырые/неструктурированные, детальные, консолидированные, валидированные, аналитические и т.п. Зачем для BI нужно деление на горячие, теплые и холодные данные. Ответы на все эти вопросы вы найдете в цикле наших статей.Осуществлять аналитическую обработку данных, причем зачастую разной природы и масштабов – дело достаточно сложное. Поэтому мы разделили наш рассказ на несколько частей.  Итак, давайте начнем первую часть нашего рассказа. Welcome под кат!1.1. Как работает связка «BI+Data Lake»?За последние 20 лет уровень программных продуктов обработки, хранения и анализа данных значительно вырос. Сначала были простые локальные базы данных, автономные под каждый проект. Затем сложные корпоративные хранилища данных с единой версией правды, гибкими методологиями ХД, медленно меняющимися справочниками второго типа (SCD2), сложными агрегациями первичных транзакционных данных и многое другое. Но количество информации в цифровом виде из года в год все росло и увеличивалось. Постепенно объем разнородных источников стал настолько велик, что появился устойчивый термин «большие данные» (Big Data). Далее возникает двойственная ситуация. Чем больше данных, тем дороже построить качественное КХД. Все эти «сырые» разнородные данные из разных источников нужно проверять, валидировать, систематизировать и так далее. Но, возможно, не всегда эту проверку требуется делать качественно и скрупулёзно. Представьте себе, что из большого водоема данных вам потребуется вода, но для двух разных ситуаций. Первый случай – 100 куб. м воды для тушения лесного пожара, и второй случай – 150 мл бутилированной воды для питья. Во втором случае над водой из водоема будет произведена большая работа по её фильтрации, очистке, разливу по бутылкам. И только когда вода достигнет определенного качества, только тогда ей можно утолить жажду. В первом случае, наоборот, вам будет совершенно не важно, какого вода качества. Будет интересен только общий объем водоема и хватит ли его запасов на всю площадь пожара.Примерно на такой же идее и основываются «Озёра данных» (Data Lakes). Мелкими ручейками разные данные в их натуральном формате (RAW, от англ. сырой/необработанный) «стекаются» в место их общего хранения. Это и есть Big Data. Затем информация в нужном объеме проходит систематизацию. В случае наличия «единой версии правды» информация попадает в хранилище данных (DWH) или, если с сырыми данными было произведено только минимум преобразований, то информация перемещается в слой хранения оперативных данных (ODS, Operation Data Storage). Основная разница между DWH и ODS в скорости и стоимости обработки данных. Следующим шагом по модификации информации является консолидация и итоговое систематизирование. Это проводит к формированию слоя детальных данных (DDS, Detail Data Storage). Пример трансформации «ODS->DDS»Когда в консолидированной модели крупного холдинга разнородная информация от дочерних компаний приводится к единому корпоративному плану счетов. При этом в DDS с одной стороны сохраняется первичная детализация по каждой дочерней компании (кто-то специализируется на производстве шариковых авторучек, а кто-то выпускает производственные станки), но появляются новые общие признаки, которые становятся общими для всех. Например, код корпоративного счета «Производство продукции». Далее все три потока информации (ОDS, DDS и DWH) собираются в общий слой представления аналитических данных (ADS, Analytical Data Store) или витрины предподготовленных данных (Data Marts). Но это все про информацию и данные. При чем тут BI, совершенно справедливо спросите вы? Немного терпения. Доведем наш рассказ про «стекание» воды в озера данных до логической точки. Как же понять, в каких случаях нам нужно тщательно фильтровать воду для питья методом многоступенчатой очистки, а когда черпать ее бочками для тушения пожаров прямо из озера сразу вместе с живущей в ней биомассой? С точки зрения ИТ точнее будет даже сказать, не в каких случаях делать «захват» данных, а какими программными инструментами? Когда для обработки данных использовать миниатюрные щипчики для тонкой ювелирной работы, а когда и кувалда подойдет? Ответы на эти вопросы дает гибкая система бизнес-аналитики, или Business Intelligence. В ней как раз сосредоточен тот набор необходимых аналитических инструментов, способных решить ту или иную задачу «колдовства над водой». Например, для многомерного и всестороннего анализа данных подходит гибкая технология аналитической обработки данных (OLAP). Для анализа текстовой информации можно воспользоваться технологией text mining, которая, например, на основе лингвистического анализа сможет определить тональность высказывания и структурировать эти сведения. Совместив ее с инструментами интеллектуального анализа (data mining), можно выявить скрытые правила и закономерности в наборах данных. Интересный пример использования озёр данных рассказали сотрудники компании «Газпром нефть» в онлайн-журнале «Сибирская нефть» (см. здесь). Создание платформы «Умных озёр данных», в которой BI является неотъемлемой частью, позволяет этой нефтяной компании «детально анализировать потребительские предпочтения, разрабатывать максимально персонализированные предложения для клиентов», как пишет директор по региональным продажам Александр Крылов.Поэтому хорошо «плавающая» BI-платформа сможет быстро и эффективно решить самые разнообразные задачи. Но, как и при любом «купании», все во многом зависит от водоема. В первую очередь от технологии хранения самих данных. 1.2. Для разных типов данных - разные технологии их храненияПлатформа «Форсайт» не является средством постоянного хранения больших объемов данных. Для этого мы всегда используем какую-либо внешнюю СУБД. Тут важно сделать уточнение, что для быстрой работы в нашем BI реализованы средства кэширования данных в ОЗУ (in-memory). Но это все же не способ длительного хранения данных, а скорее механизм быстрой обработки данных нашей платформой своими инструментами с высокой производительностью.Таким образом, мы идем «по старинке» и для хранения данных используем БД от разных производителей: Oracle, MS SQL Server, PostgreSQL, Greenplum, Teradata, Vertica, ClickHouse, Hive, а также ряд проприетарных российских СУБД (PostgrePRO, Jatoba и линейку продуктов Arenadata). Для этого в платформе «Форсайт» реализован набор коннекторов. Для распространенных СУБД это нативные коннекторы на C++, остальные через ODBC. В случае с озёрами данных каждый источник целесообразно хранить в предназначенных для этого видах СУБД: реляционных, распределенных, колоночных и др. Далее объединение разных данных происходит уже на уровне многомерных кубов в ОЗУ средствами самой BI-платформы. Комбинация обработки данных «СУБД + оперативная память на BI» дает неплохой результат. Часто разные источники одного многомерного OLAP-куба состоят из нескольких совершенно разных физических таблиц из разных баз данных. Например, исторические данные с длительной динамикой размещены в MPP-СУБД (что обеспечивает быстрое извлечение информации), а сценарии с эконометрическими расчетами прогнозов по этой динамике хранятся в реляционной СУБД (т.к. сценарии часто перечитываются и измененные данные требуется все время перезаписывать в БД маленькими порциями, а MPP плохо воспринимает micro-batch). В связи с этим для разных слоев данных приходится комбинировать разные СУБД, а итоговый аналитический срез данных объединяется уже на стороне BI в оперативной памяти. Давайте более детально рассмотрим механизмы такой комбинации источников данных в OLAP-кубах нашей платформы «Форсайт».1.3. Многомерные OLAP-кубы – основа любой BIСамая первая и важная характеристика для OLAP-куба – это N измерений (или часто еще их называют аналитиками или справочниками). Они определяют размерность куба. Второе – это структура физического хранения. Давайте рассмотрим самый простой случай – одна таблица с фактами и N таблиц-справочников (классическая модель «звездочка»). Все таблицы расположенная в одной СУБД, например, PostgreSQL.«Звездочка» – простой сценарий работы многомерного OLAP-кубаВ платформе «Форсайт» вы легко сможете настроить такой источник данных с помощью отдельных конструкторов для измерений и многомерных кубов. Вся информация с настройками сохранится в репозитории метаданных. Измерения и кубы создаются как отдельные (самостоятельные) метаобъекты и могут быть переиспользованы. Например, справочник территорий создается один раз и может быть добавлен во все кубы в вашей ИТ-системе.  Далее для чтения исходных данных через многомерный куб платформа (используя метаданные) определяет необходимые измерения - см. шаг 1 на рис. ниже. И затем формирует запрос в БД к таблицам-справочникам (шаг 2,3).На основе ответа из БД определяется состав элементов для каждого измерения (шаг 4). Поддерживаются линейные или иерархические (сбалансированные/несбалансированные) измерения. У каждого элемента в измерении часть атрибутов заполняется данными из запроса к БД (ключ, наименование и т.п.), а часть может рассчитываться уже на стороне BI (например, порядок следования и элемент-владелец при вычисляемых иерархиях или какой-нибудь сложный алгоритм транслитерации ФИО сотрудников в латиницу для англоязычного наименования).Структура сформированного справочника может быть размещена в сессионный кэш платформы (шаг 5). Тогда при последующих обращениях к этому измерению в рамках одной сессий запросов к БД в таблицу-справочник больше не будет. Это позволяет повысить скорость работы с источниками данных, когда в рамках одного сеанса работы с платформой пользователь обращается к одним и тем же данным много раз. Режим кэширования является опциональным (причем для каждого справочника), и его можно отключить. Например, если данные обновляются в БД в реальном времени.Получив информацию о всех измерения (шаг 6), мы определим структуру куба, и можно переходить к этапу выборки данных. Содержание выборки определяется «отметкой» (SelectionSet), см. шаг 7. Отметка - это комбинация выбранных элементов в каждом измерении куба (Selection Dim 1…N). Установленная пользователем «отметка» напрямую влияет на структуру sql-запроса к таблицам с данными (шаг 9 и 10). И вот тут начинается самая сложная часть для OLAP-движка. С одной стороны, такой движок должен быть универсальный и автоматически адаптироваться под любую «отметку» пользователя. Т.е. генерировать максимально (насколько это возможно сделать автоматически) оптимальный sql-запрос с разной структурой условия where. С другой стороны, как уже говорили выше, должны поддерживаться разные типы СУБД, где в общем случае синтаксис PL/SQL и принципы их оптимальной работы могут сильно различаться.Универсальный «движок» генерации sql-запросов к многомерным данным Как решить вопрос оптимальности? Часто в разных статьях про производительность СУБД или даже BI упоминаются TPC-H тесты (http://www.tpc.org). Они действительно хорошо показывают скорость реакции на разные аналитические запросы. Но в первую очередь они больше ориентированы на агрегированные ad-hoc отчеты, когда из многомилионной выборки данных требуется получить всего лишь десятки агрегированных и отфильтрованных значений. При этом условия (фильтрация) выборки организованы простыми логическими правилами: равно константа, between между двумя значениями, больше или меньше заданной величины и т.д.При использовании в BI-платформе универсальной ROLAP модели, которая выполняет прямые реляционные запросы в БД, тесты намного сложнее. Например, для каждого четного измерения куба выбрать все их нечетные элементы, а для нечетных измерений – все четные элементы. Как вам такой тест? Он, конечно же, легко выполняется для справочников, состоящих из небольшого количества элементов. Например, пол, возраст, валюта. Здесь транслировать произвольную «отметку» пользователя (SelectionSet) из интерфейса BI в условия sql-запроса достаточно просто, т.к. количество элементов в справочниках небольшое. Обычный in (…) с парой значений или between (для случая, когда ключи выбранных элементов идут подряд), плюс правильный индекс на таблицу БД – любой OLAP «движок» сделает такое без особого труда и с высокой производительностью.Совершенно другая история для справочников с большим количеством элементов: товарная номенклатура, реестр контрагентов, список студентов ВУЗа за последние 20 лет и т.д.  Фильтрация в таблице фактов для таких условий выполняется по полям с высокой кардинальностью. Простой пример – структура дебиторской или кредиторской задолженности. Для крупного холдинга это сотни товарных позиций, десятки и сотни тысяч контрагентов (покупатели, поставщики, подрядчики). Еще добавляются виды деятельности, структура дочерних обществ холдинга и другие аналитические разрезы. Затем в хаосе этой разряженной многомерной модели данных пользователь по известной и понятной только ему бизнес-логике выбрал какие-то элементы в каждом измерении куба. Может быть, один элемент, а может быть тысяча. И все эти условия должны автоматически транслироваться из BI-платформы на язык sql-запросов. Причем с проверкой каждого элемента, а не просто «все товары с ключом больше X или между Y и Z». Ведь в сводной таблице (pivot table) мы хотим увидеть «дебиторку/кредиторку» в разрезе конкретных выбранных товарных позиций и контрагентов, а не общую сумму всего агрегата целиком. И выполниться такой запрос должен быстро, а не часами «перебирать» на сервер БД декартово произведение комбинаций из всех возможных пересечений этих условий каждого из измерений. При этом ожидается, что пользователь не пишет сложные и/или вложенные MDX запросы (разбитые на атомарные условия), а просто проставляет отметку в интересующих его разрезах куба и сразу получает плоскую сводную таблицу. Именно такая задача часто возникает в enterprise BI при реализации отчетов со сложной структурой. И тут тест производительности со всеми четными товарами и нечетными контрагентами очень показателен.ROLAP vs MOLAP vs HOLAP – что лучше для Data LakeУспех решения такой задачи (сформировать из BI-платформы быстро работающий sql-запрос с большим количеством условий) состоит из двух составляющий. Первое – это правильный выбор СУБД. Для обработки разных видов данных важно и нужно использовать разные типы СУБД. Именно поэтому мы остаемся приверженцами ROLAP, сочетая его с частичным HOLAP или in-memory OLAP. Только такой вариант обработки информации (прямое обращение к исходным данным с их опциональным кэшированием в рамках каждой сессии), является самым оптимальным для связи «озеро данных + BI». Конечно, намного проще для BI-платформы пойти по пути внутреннего хранения данных. Регулярно «перекладывать» 100% всей необходимой информации в адаптированные структуры данных (MOLAP), а также использовать только «удобную» для себя технологию СУБД или собственную систему полного кэширования данных. Таким путем сейчас идет ряд российских и международных BI-платформ. Но этот подход не всегда эффективно встраивается в уже существующую систему озера данных заказчика. Точнее, рядом создается второе озеро-дублер, и информация по заданному расписанию «клонируется» во второй водоем. Дополнительно, такой подход с озером-дублером совершенно не работает для потоковых данных или real-time BI. Именно поэтому вы получите только первую половину успеха, если для конкретного слоя данных (горячие, теплые или холодные данные) правильно подберете тот или иной тип СУБД. Точнее, этот выбор лучше делать сразу, на этапе проектирования архитектуры озера данных, рассматривая его в сочетании с целевой BI-платформой, которую вы планируете использовать в будущем.  Например, для создания фабрики данных и аналитического КХД мы рекомендуем использовать комбинацию линейки продуктов «Форсайт» и «Аренадата».  Стратегии фильтрации в sql-запросахВторая составляющая успеха – это принципы работы в BI-платформе многомерного «движка» данных (rolap data engine). Механизмы формирования sql-запросов должны быть адаптированы под структуру и сценарии использования ROLAP-куба. В первую очередь это фильтрация данных, которые попадут в куб. Здесь важно найти оптимальный баланс между условиями выборки данных на стороне СУБД, и финальной фильтрацией и обработкой результатов sql-запросов уже на уровне бизнес-логики BI. Например, если «отметка» состоит из идущих подряд элементов (ключи элементов отсортированы), но есть «дырки», то в определенных случаях для sql-запроса лучше работает условие in (1,2,…,9,10, 25,26,…,42,43, 89,90,…), а в каких-то случаях комбинация нескольких between…and…. Дополнительно часть условий (замедляющих выполнение sql-запроса) можно просто исключить из sql-условия, а уже финальное сокращение выборки из БД выполнить на стороне BI-платформы (уже не средствами СУБД, а программной логикой самой BI). Такие комбинации часто тоже показывают неплохой результат.Для выбора оптимального sql-запроса в платформе «Форсайт» реализованы механизмы стратегии фильтрации данных многомерного куба (см. шаг 8 на OLAP-схеме выше). Суть такой стратегии заключается в том, чтобы в зависимости от структуры «отметки» куба (SelectionSet) по-разному генерировать текст условия в sql-запросах. Доступно несколько вариантов стратегии:Используя ту или иную стратегию фильтрации, текст sql-запросов автоматически генерируется платформой. Причем это не единичное обращение к БД (select * from …) а целая серия sql-команд и запросов для организации работы разных вариантов стратегии фильтрации. Например, для варианта №4 требуется создание временных таблиц для сохранения информации по фильтрам с большим количеством элементов. Но автоматическая генерация такого комплексного скрипта требует учитывать синтаксис разных СУБД. Например, для СУБД PostgreSQL скрипт может быть таким:Для других СУБД синтаксис sql-скрипта будет изменяться платформой «Форсайт» автоматически. Таким образом, настроив в репозитории платформы логическую структуру многомерных кубов, можно без проблем мигрировать эти кубы на разные СУБД. Потребуется только изменить настройки подключения к БД и привязку к целевым таблицам. Все необходимые sql-операции для извлечения данных платформа сделает самостоятельно, с учетом оптимальной стратегии фильтрации.Сложные связи или как «подстроиться» под существующую структуру БДУчитывая, что озёра данных формируются на основе разных источников, часто связи в базе данных между таблицами могут основываться на нескольких полях. Рассмотрим ситуацию, когда в каждом филиале используется своя локальная БД и первичный ключ для сотрудников по всей компании начинает пересекаться. Чтобы объединить информацию, в озере данных нужно уже создавать уникальный индекс из двух полей – branch_id и employee_id. И это вполне нормально. На таком «сдвоенном» индексе платформа «Форсайт» также может сформировать измерение. ROLAP-куб на основе такой логической связи двух атрибутов с таблицей фактов сформируется, но стратегия фильтрации для sql-запросов будет требовать корректировки. Тут уже нельзя для каждого поля применять условие фильтрации отдельно. Например, в измерении сотрудников мы выбрали одного специалиста из филиала A, а из филиала B выбрали 100 сотрудников (один из которых использует тот же самый ключ что и сотрудник из филиала A). Простое условие в sql-запросе будет следующим: …… where branch_id in (‘branch_A’, ‘branch_B’) and employee_id in (1,2,…,100) and …...Но оно не будет эффективным. Запрос будет явно избыточным, в случае если ключи всех 100 сотрудников из филиала «B» есть и в филиале «A».  Замена команды in на between не даст нужного результата. Конечно BI-платформа при финальном размещении данных в OLAP-кубе (см. шаг 11) обязательно «отфильтрует» только 101 сотрудника (а не оставит все 200 человек), но драгоценное время будет потеряно, а производительность снижена.На первом шаге мы пробовали усложнять структуру условия в sql-запросе, (branch_id = ‘branch_A’ and  employee_id = 1) or (branch_id = ‘branch_B’ and ……), но ни к чему хорошему это не приводило. Для примеров с большой размерностью куба и высокой кардинальностью полей (с которыми связаны измерения) скорость работы БД была очень медленной. Но и упрощать условия sql-запроса и «тащить» с сервера БД на сервер BI лишние данные для последующей обработки в ОЗУ, тоже не очень хотелось. Чтобы решить эту задачу, мы расширили четвертый вариант стратегии фильтрации, создавая временную таблицу сразу для всех атрибутов индекса измерения. Это в ряде случаев очень сильно помогло.Такие комплексные одновременные связи не ограничены по количеству полей-атрибутов в индексе измерения. Их может быть и три, и четыре и т.д. Также в одном индексе можно сочетать разные типы данных у атрибутов. Простой пример – календарное измерение. В платформе «Форсайт» индекс этого измерения по умолчанию состоит из двух атрибутов: дата начала периода и уровень календаря (год, квартал, месяц, день). Так сделано потому, что в случае многоуровневого календаря только дата «01.01.2022» не позволяет отличать «2022 год» и «1 января 2022 года».1.4. Одна СУБД – несколько таблицПродолжая усложнять структуру данных в озере, представим ситуацию, когда таблиц фактов уже несколько. Плюс есть измерения, состоящие из нескольких таблиц-справочников, и нам необходимо объединить их в общую иерархию элементов. Т.е. простая модель «звездочки»/«снежинки» начинает усложняться.Составные измерения из нескольких таблиц-справочниковНесколько таблиц-справочников в платформе «Форсайт» могут быть объединены в одно составное измерение. Такое измерение разделяется на блоки (см. шаг 2 на рисунке ниже). Каждый блок – это отдельный источник данных (dataSet), из которых получается общий список элементов составного измерения. Один dataSet может соответствовать только одной таблице-справочнику. Принцип работы измерения такой же, как и на предыдущей схеме – генерируется несколько запросов в БД, но уже к нескольким таблицам (см. шаги 3, 4).По отношению друг к другу блоки измерения могут быть или линейные (тогда все их элементы расположены на одном уровне) или соподчиненные (элементы одного блока являются родителями элементов другого блока). Внутри одного блока элементы также могут быть расположены линейно или иерархически. Итоговое формирование элементов справочника происходит в оперативной памяти на стороне BI сервера на основе данных из ответа от БД (см. шаг 5, 6). При необходимости может использоваться кэш платформы (см. шаги 7-8).Cвязь одного ROLAP-куба с несколькими таблицами фактамиАналогичным образом вы можете добавить в ROLAP-куб и несколько таблиц фактов (см. шаги 11-13). Тогда каждую таблицу вы должны связать с индексами измерений, а точнее с атрибутами измерения, входящими в этот индекс. Эту связь можно настроить с одним и тем же общим индексом справочника или для каждой таблицы указать свой индекс. С одним индексом все просто. Все три таблицы связаны с одним и тем же атрибутом измерения – например, с числовым ключом элемента.Рассмотрим связь по нескольким индексам. Например, в одном измерении есть три уникальных атрибута: ключ, классификационный код и мнемоника. Все они могут быть использованы для кодировки элементов справочника. Для каждого атрибута в измерении создан свой индекс. В каждой из трех таблиц фактов используется только одна из этих кодировок. Тогда при добавлении каждой таблицы в ROLAP-куб вы связываете её поле (содержащее значение какой-то из трех кодировок) с соответствующем индексом. При извлечении данных все три sql-запроса к трем таблицам фактам выполняются независимо. Каждый из запросов возвращает свою подвыборку данных в определенной кодировке (ключ, код или мнемоника). Далее все три массива данных объединяются в общую многомерную матрицу куба. Каждая полученная из БД точка данных (запись из курсора sql-запроса) будет размещена в том элементе измерения, которому соответствует указанный в связи атрибут (кодировка).При такой настройке возможна ситуация, когда на одну и ту же точку в многомерной матрице куба будут претендовать значения из разных таблиц фактов (т.е., например, и код и мнемоника соответствуют одной точке данных). Для таких случаев можно настроить правила вычисления фактов на уровне BI-платформы. Например, простые агрегации (сумма, среднее, первый или последний элемент, количество точек и т.п.) или сложные функциональные правила (в том числе с учетом приоритетов таблиц-источников). Такие BI-вычисления можно сочетать с агрегацией на БД. Тогда в sql-запрос к каждой таблице фактов будет транслироваться group by вместе с sum, avr, count и др. (в том числе можно настроить собственную процедуру на PL/SQL). Затем все результаты из БД еще дополнительно при финальном формировании куба будет суммироваться (или выполняться иное правило), но уже на стороне BI-платформы.Денормализация данных в ROLAP-кубе Как уже обсуждали выше, индекс может состоять из нескольких атрибутов. Это потребует связать уже несколько полей таблицы фактов с атрибутами справочника (входящих в этот индекс). Также, в общем случае, можно связать поля таблицы фактов и с неуникальным индексом. Тогда данные будут дублироваться по всем элементам измерения, для которых установлено соответствующее значение неуникального атрибута. Такой вариант тоже практикуется. Еще один из способов – совсем не устанавливать связь между атрибутами измерения и полями таблицы фактов. Тогда в матрице данных ROLAP-куба значения будут «размножаться» по элементам несвязанных измерений или «фиксироваться» на одном из заданных элементов. Такое поведение востребовано при соединении в кубе нескольких таблиц, часть из которых имеет меньшую размерность, т.е. вам просто нечего связать с атрибутами измерений. Пример - объединение фактических и прогнозных (плановых) данныхПример такого случая денормализации – объединение в одном кубе фактических и прогнозных данных, расположенных в двух разных таблицах. Для прогнозов обычно используют измерение сценариев: оптимистичный, пессимистичный, позитивный, негативный и т.п. Разный сценарий – разные цифры. Для фактических данных поле сценариев в таблице фактов бессмысленно. Но выбрав несколько сценариев, вам важно увидеть и динамику ретроспективной части временного ряда. Причем не важно, сколько сценариев вы выбрали – один или несколько.Для разных фрагментов (блоков) измерения своя таблица фактовЕсли в измерении куба создано несколько блоков, то для каждого из них можно определить отдельную связь с таблицей фактов. Например, хранить данные разной гранулярности (год, кв., месяц, день) в отдельных таблицах БД. В календарном измерении платформы уже выделены соответствующие блоки. Тогда привязку таблиц фактов нужно по-прежнему делать к атрибутам индексов, но уже не всего измерения в целом, а отдельно каждого блока. Другой такой пример – это территориальное разделение (страны, регионы, города). При таком подходе вы можете таблицы-справочники связывать с таблицами фактов не только через общие индексы всего измерения (как уже обсуждали выше), но и через локальные индексы каждого блока. Но тогда в чем разница иcпользования индексов всего измерения и индексов отдельных блоков?  При такой «блочной» привязке в нашей BI-платформе для разной «отметки» (selectionSet для выборки данных) автоматические определяется нужные sql-запросы только к задействованным таблицам фактов. Каждый элемент измерения входит только в один блок. Поэтому определив «отметку» элементов в каждом из измерений, мы перед началом генерации текста sql-запросов можем однозначно сказать, какие из блоков используются. Соответственно, если часть блоков не попадает в «отметку», то и запросов к соответствующим незадействованным таблицам БД не будет. Связь по общему индексу измерения такое деление сделать не сможет, т.к. общие индексы всего измерения определяются для всех его элементов сразу.Деление измерения на блоки не обязательно должно быть связано с разными уровнями иерархии (страны-регионы-города). Можно и линейные справочники разделить на несколько блоков и для соответствующих элементов отображать данные из разных таблиц фактов. Например, для показателей социально-экономического развития регионов данные о внешнеэкономической деятельности расположены в одной таблице БД, а показатели бюджета - в другой.Гибкие возможности для работы с гибкими архитектурами КХДВсе указанные выше гибкие настройки в измерениях и ROLAP-кубах позволяют определять связи между таблицами-справочниками и таблицами фактов в совершенно разных архитектурах проектирования корпоративного хранилища данных: «звезда», «снежинка», Data Vault, частично «якорная» модель. Это обеспечивает интеграцию таких ROLAP-кубов с уже существующей физической архитектурой озера данных компании. Это является одним из важных условий при определении возможности внедрения BI-платформы в существующий ИТ-ландшафт организации.1.5. Большой гиперкуб сразу для нескольких СУБДВажно отметить, что взаимодействие с таблицами БД через BI позволяет организовать гетерогенную взаимосвязь между разными типами СУБД (в том числе, когда эта гетерогенность самой СУБД не поддерживается или ее настройка/производительность сильно ограничена). Так как вся логика sql-запросов и финальная обработка данные обрабатываются в оперативной памяти BI сервера - то прямых связей или DB-link между СУБД не требуется. С точки зрения нескольких баз данных каждый шаг делается независимо.Одновременное чтение данных из разных СУБДДавайте рассмотрим эти шаги. Сначала платформа запросила из БД одну порцию данных, сформировали на ней измерение. Каждое измерение формируется независимо друг от друга, а значит, и СУБД могут быть разные. Далее отмеченные в SelectionSet элементы («отметка») через стратегию фильтрации используются в sql-запросах к данным. Результаты запросов с данными платформа тоже получает независимо (отдельно от каждой СУБД), и из них в оперативной памяти BI-сервера формирует итоговую многомерную матрицу куба. В том числе обрабатывает (агрегирует) пересечения точек-фактов – это тоже уже обсуждали выше.Такой подход позволяет создавать в платформе «Форсайт» гетерогенные ROLAP-кубы разной степени сложности. Используя эту возможность, вы практически всегда сможете подключиться к любому озеру данных, не важно, какой технологический стек оно использует и какая структура хранения данных в нем организована. Большие озера данных - это источник информации для большого количества ИТ-систем организации. И адаптировать физическую структуру хранения отдельно для каждой системы – такой возможности в озере данных часто нет. При внедрении BI-платформы придется «подстраиваться» под то, что есть. Часто с точки зрения BI – это полный «зоопарк» СУБД (в хорошем смысле этого слова, рассматривая контекст горячих, теплых и холодных слоев данных). Подход гетерогенных ROLAP-кубов в этом случае очень хорошее и эффективное решение, которое становится посредником между разными источниками данных. Остается правильно его «приготовить» настроить. «Быстро читать нельзя часто изменять» = «казнить нельзя помиловать»Еще один интересный момент при таком гетерогенном обращении BI-платформы к разным БД – это режим чтения и записи (корректировки) данных. Разные типы СУБД по-разному работают в этих режимах. Например, реляционная СУБД PostgreSQL быстро записывает небольшие порции измененных данных (micro-batch), но медленно считывает сложные аналитические запросы. Для MPP-СУБД Greenplum противоположная ситуация. За счет массово-параллельной архитектуры чтение данных выполняется с высокой скоростью, но обновление данных возможно только большим порциями (batch), что совершенно не подходит для режима ручной корректировок информации в отчетности или при расчете целого среза данных при помощи «продвинутой» аналитики (advanced analytics). Эти две взаимоисключающие возможности при работе разных СУБД с операциями чтения и записи похоже на крылатое выражение «казнить нельзя помиловать». Только «казнить» и «помиловать» нужно заменить «быстро читать» и «часто изменять» данные. К сожалению, для текущих реалий Business Intelligence это так. Для enterprise BI крайне важно одновременно работать с данными и на чтение, и на изменение (запись). Но большинство СУБД не предоставляют такую возможность одновременно, к сожалению.  Технология гетерогенного ROLAP-куба позволяет «разрубить» этот узел противоречия. Совместив в таком кубе таблицы фактов из разных СУБД, можно управлять потоками данных при чтении и записи. При чтении учитывать приоритеты слоев данных. Один слой данных - это выделенная отдельная СУБД, которая отвечает за источник оперативно изменяемых данных (назовем их оперативный слой данных). Сюда будем записывать все измененные данные. Остальные слои – это специализированные СУБД, где хранится весь исторический массив данных только для чтения (при необходимости можно разделить их на горячие/теплые/холодные данные и использовать СУБД от разных производителей). Далее при выполнении операции чтения данных первый приоритет отдать оперативному слою.  Если там данных нет, значит, их не изменяли, и можно проверять все остальные слои исторических данных. Источник изменяемых оперативных данных можно не «раздувать», и на регулярной основе (в периоды неактивного использования системы, например, каждую ночь, или чаще) реплицировать их в соответствующие источники для чтения. После этого оперативный слой можно очистить и начинать заполнять его новыми изменениями. Если репликацию делать часто, то в источнике для записи всегда будет немного данных. Поэтому операция чтения (в том числе при сложных аналитических sql-запросах) будет выполняться быстро, даже в случае применения обычной реляционной СУБД. Но это достаточно сложная тема и мы вынесем ее в отдельную часть цикла наших статей про работу BI и озера данных.На этом наша первая часть цикла публикаций про практику работы платформы «Форсайт» с озером данных завершена. Следите за нашими следующими выпусками. Далее мы обязательно расскажем о других интересных особенностях работы нашей платформы с разными СУБД, сложными КХД и большими данными.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Действительно ли big data – это объективная насущная проблема для бизнеса?\n",
      "Может быть это лишь красивый маркетинговый ход разработчиков мощных компьютеров и продуктов по хранению и обработке цифровых данных.\n",
      "Может быть это лишь привлекательная реклама консультантов по исследованию рынков и поведенческих моделей клиентов.\n",
      "А может это всего лишь модный тренд в сфере тотального наблюдения за субъектами рынка и прогнозирования их реакций.\n",
      "\r\n",
      "Возможно и нет никаких «больших» данных, а есть большая иллюзия о том, что удастся каким-то образом собрать настолько огромный массив цифровой информации, обработать его неким волшебным образом и получить ответы на все вопросы, волнующие бизнесмена.\n",
      "\n",
      "\n",
      "Кадр из к/ф «Особое мнение» (Minority Report) Стивена Спилберга по повести Филипа Дика (2002 — 20th Century Fox, DreamWorks SKG).\n",
      "\n",
      "\n",
      "… ресурс\n",
      "Большие данные – это по сути ресурс аналитика. Это ресурс для людей, осуществляющих исследования и подготовку принятия решения. И как любой ресурс, большие данные без умения, знаний и технологий их использования не работают. Кто-то называет такое умение «добычей данных» (data mining) – по аналогии с добычей полезных ископаемых, делая акцент на глубоком проникновении и трудоемкости. Кто-то называет такое умение «интеллектом бизнеса» (business intelligent) – показывая насколько важным является «умственная» составляющая в этом процессе. Кому-то понравиться название «большая аналитика».\n",
      "\r\n",
      "Но известно из теории и практики, что даже наличие ресурса в большом количестве не означает его успешное и эффективное использование. Иногда избыточный объем ресурса позволяет строить бизнес-модель, не на глубокой его переработке в определенный набор продуктов, а на простом упаковывании и реализации в сыром виде. Зачем искать дополнительные варианты, когда, можно не прикладывая чрезмерных усилий, просто сбывать необработанный ресурс.\n",
      "\n",
      "… необходим высокий уровень менеджмента\r\n",
      "Большие данные, как информационная категория, имеют одну особенность в отличии от материальных ресурсов. Для их применения необходим по-настоящему высокий уровень организации бизнес-объектов и бизнес-процессов компании. Без такого уровня подготовки, без наличия определенной квалификации у бизнеса, покупка (или сбор) больших данных будет отличаться низкой эффективностью. Настолько низкой, что не оправдает вложенные в них средства.\n",
      "\r\n",
      "Зачем бизнесу тратить средства на большие данные, если не создан бизнес-слой управления (принятия решений) на основе аналитики? Абсолютно верно – незачем. К этому в той или иной степени приходят те компании, которые начали использование больших данных без принятия в контур управления аналитических технологий, техник принятия подготовленных аналитически решений и которые, по большому счету, не готовы к переменам. Такие субъекты рынка рано или поздно откажутся от больших данных. Особенно вопрос остро встанет при повышенной конкуренции за финансовые ресурсы внутри бизнеса.\n",
      "\r\n",
      "Сегодня рынок больших данных сосредоточился на информационных технологиях. Это понятно и приятно, что развиваются инструменты работы с большими данными. Но интенсивный рост информационных сетей и совершенствование информационных технологий снимает барьеры по вычислительным мощностям. Это заставит передовые амбициозные бизнесы пересмотреть своё нынешнее увлечение и сместить акцент в сторону новых эффективных методик, инструментов, технологий менеджмента, базирующихся на знаниях и обучении.\n",
      "\r\n",
      "Собственно, когда презентуют большие данные, часто речь идет о возможностях их хранения, транспортировки и обработки. Поисковые технологии гигантов сети Интернет яркий пример того, что бизнесу дают большие данные. Алгоритмы поиска – это мощнейшая обработка гигантских растущих объемов информации. Они постоянно находятся в процессе оптимизации, повышения производительности индексирования и структурирования информации. Но ведь за поисковыми технологиями в сети стоят не только большие данные. За ними стоят команды аналитиков, которые владеют высокотехнологичными знаниями в предметных областях.\n",
      "\r\n",
      "Поэтому разумная политика использования больших данных – это построение команды анализа данных, но никак не исключительное выстраивание серверов, облаков, систем добычи данных, машинного обучения и т.п.\n",
      "\n",
      "… добыча данных\r\n",
      "Стоит заметить, что не очень показательно само определение «добыча данных». Оно рисует несколько упрощенную картину действительности: есть «бесценные залежи» разнородных и перемешанных данных, а профессионал (или инструмент) берет и «раскапывает» в этих данных именно те, которые при «проникновенном» взгляде менеджера открывают ему глаза на всё происходящее и его вдруг осеняет праведная мысль о скрытых резервах бизнес-модели.\n",
      "\n",
      "Чудес не бывает и в больших данных тоже. Чтобы добыть ценную информацию из некоторого хранилища, её нужно туда сначала положить, потом извлечь, обработать и визуализировать. Акцент не корректно смещать на извлечение информации из хранилища, оставляя вне фокуса такие вещи как сбор (получение) данных, структурирование данных, упаковывание данных в хранилище, проверку качества данных, организацию процесса анализа данных, проблемы принятия решений на основе анализа больших данных и многое другое.\n",
      "\r\n",
      "Кроме того, даже для несложного data mining не помешает корректная постановка цели. Без грамотной постановки цели может выйти всё что угодно, а не осмысленный результат. Пусть эта цель выражена в виде гипотезы или вопросов, в виде проблемной ситуации или числовых показателей.\n",
      "\r\n",
      "Любые данные имеют контекст и метаданные, которые существенно ограничивают их использование в определенных ситуациях. Если условие контекста для задачи не задано, аналитик не в состоянии принять решение по добыче данных и о соответствии данных поставленной задачи.\n",
      "\n",
      "… временной лаг\r\n",
      "Не смотря на старания бизнеса сократить время от снятия информации о его состоянии до принятия решения об изменении такого состояния существуют объективные причины непреодолимого временного лага.\n",
      "\r\n",
      "Задержка между принятием решения и изменением состояния бизнеса в соответствии с принятым решением может быть также весьма существенной. Процессы и объекты перестраиваются, изменяется взаимодействие, корректируется поведения работников, подстраивается окружение. Поэтому любые данные и даже большие данные – это всегда данные о прошлом. Но руководство хочет принимать на их базе решения для будущего. Здесь главное не переоценить возможности больших данных и аналитики.\n",
      "\n",
      "… внешние и внутренние\r\n",
      "Одно из заблуждений в отношении больших данных – это то, что они преимущественно внешние по отношению к бизнесу. Считается, что большие данные – это данные о клиентах (их поведении), данные о конкурентах, данные о разных факторах существования бизнеса (политические, социальные, культурные), данные о рынках и потребительских тенденциях, данные об активности других бизнесов. Частично это так.\n",
      "\r\n",
      "Но большие данные для бизнеса от внешних источников увязываются с данными о внутреннем состоянии, причем увязываются строго и контекстно. Это крайне необходимо, чтобы совместно оценивать самочувствие бизнес-модели и внешней среды. Внутренние данные также могут быть большими и весомыми для большой эффективной аналитики. Ведь ответ на вопрос что делать менеджменту для исправления ситуации могут дать исключительно внутренние данные.\n",
      "\n",
      "… большие или не очень\r\n",
      "Ещё одна иллюзия которая способна помешать бизнесу – это то, что профессиональная результативная аналитика основана только на больших данных. Существует реальная возможность и опыт, помноженный на талант некоторых экспертов, предлагать решения в рамках традиционных объёмов внутренних данных, особенно когда речь идет о явных или типичных проблемах в бизнес-модели.\n",
      "\r\n",
      "Отрицать огромное значение сбора и анализа больших данных для развития бизнеса невозможно. Особенно важны большие данные для распределённого и информационно-активного бизнеса. Пожалуй, большие данные – единственный эффективный инструмент быть в курсе всех дел для крупных корпораций и объединений с разветвленной сетью бизнес-единиц. Средний и малый бизнес также с учетом некоторых особенностей может оказаться в выигрыше от больших данных, особенно в кооперации с крупными компаниями и сообществами.\n",
      "\n",
      "Но нельзя подменять большими данными решение насущных проблем. Лучше рассматривать их как направление, которое поддерживает центральную стратегию бизнеса и позволяет быть в курсе произошедшего, происходящего и частично прогнозировать развитие ситуации в будущем. Но если у бизнеса нет вразумительной стратегии и если бизнес-модель видится примитивно и запутанно, то никакие большие данные не в состоянии помочь даже пассивному развитию. Некоторые менеджеры, понимая для себя отсутствие потребности в больших данных и не готовности к переменам, которые они сулят, не пытаются инициировать работу с ними – это тоже пример обоснованного разумного поведения.\n",
      "\n",
      "… как способ думать\r\n",
      "Как бы мы ни старались, большие данные не способны решить все проблемы. Никак нельзя с помощью большой аналитики построить эффективную бизнес-модель. Но они все-таки смогут помочь оптимизировать её в рамках выбранной стратегии.\n",
      "\r\n",
      "«Волшебство» больших данных, которое несколько остается в стороне от общего внимания заключается в очевидном и обоснованном способе размышлять о бизнесе и искать пути его улучшения. Действительно проект больших данных улучшает бизнес и не столько из-за ценности каких-то массивов информации, а вследствие того, что менеджмент начинает смотреть на свою бизнес-модель с критической точки зрения, в том числе основываясь на заданных информационных показателях и индикаторах.\n",
      "\r\n",
      "Если руководство вплотную интересуется большой аналитикой, то значит хочет понимать больше о своей компании и это – начало оптимизации бизнеса.\n",
      "\r\n",
      "Вместо больших данных можно выбрать иное средство развития бизнеса, например, маркетинговые исследования, статистические расчеты, экономико-математическое моделирование. Результат получится различным, но работа, нацеленная на «понимание» бизнес-модели, будет начата и несомненно даст какой-то, но чаще — положительный эффект. Если конечно она выполняется объективно, разумно, профессионально и с учетом воздействующих факторов.\n",
      "\n",
      "… под брендом «большие данные»\r\n",
      "Некоторые компании накопили ресурс – данные, а другие разработали мощные программные и аппаратные ИТ-решения. Этот ресурс и эти решения они постараются под тем или иным «маркетинговым соусом подать к столу бизнесменов» и заработать «хорошие чаевые».\n",
      "\r\n",
      "В ход пойдет активный сбыт и изощренный маркетинг, вежливые консультанты и веселые клиентские мероприятия богато «приправленные» красивым брендом и впечатляющей терминологией. Они будут говорить о построении надежнейших систем обработки абсолютно не структурированных данных, о великолепных алгоритмах построения многоуровневых графов информации, о быстродействующих выборках на искусственном интеллекте, о самообучающихся нейросетевых механизмах.\n",
      "\n",
      "Не верьте на слово. Просите разъяснений, пояснений, демонстраций, документацию, независимые экспертные заключения, отзывы клиентов, нагрузочные тесты, разумного пробного бесплатного периода.\n",
      "\r\n",
      "Посудите сами, даже бренд «большие данные» выглядит выигрышно.\r\n",
      "Во-первых, в названии есть слово «большой», а значит это что-то хорошее, положительное, выгодное, впечатляющее, убедительное, ценное.\r\n",
      "Во-вторых, слово «данные», как бы указывает на что-то правильное, интеллектуальное, инновационное, эффективное, упорядоченное.\n",
      "\n",
      "Сама суть иллюзии больших данных происходит от их названия.\r\n",
      "Кажется, имея big data, бизнес решает вопросы наивысшего (большого) порядка на профессиональном (большом) уровне. И чем больше накопить данных, тем эффективней и быстрее будут решаться всё более сложные вопросы.\n",
      "\n",
      "Не поддавайтесь иллюзиям – большие данные не всегда способны сделать то, о чем мечтает бизнес.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Меня зовут Павел Куницын, я работаю главным специалистом по анализу данных и машинному обучению в ПГК. На нашем примере расскажу, с какими трудностями мы сталкивались при внедрении ML-инициатив и как их преодолевали.Решил написать серию статей. В первых статьях я подробно опишу проблемы, с которыми может встретиться DS-команда, чтобы помочь участникам проектов с машинным обучением проводить более грамотную предпроектную проработку. В дальнейшем поделюсь нашим опытом в решении некоторых из них.В одном из своих выступлений Эндрю Ын (Andrew Ng), возможно, самый известный популяризатор deep learning, предложил концепцию Data-Centric AI (https://www.youtube.com/watch?v=06-AZXmwHjo). В отличие от ранее распространенного подхода (Model-Centric AI), согласно которому улучшение качества ML-решения достигается за счет совершенствования алгоритма, новая концепция ставит во главу угла постоянное совершенствование используемых данных.Первая статья, которую вы сейчас читаете, — о препятствиях, которые могут возникнуть при работе с данными.Процесс разработки моделейОбычно последовательность операций, которые должны быть выполнены для создания алгоритма, выглядит так*: *В этой схеме я намеренно оставляю только те этапы, которые традиционно входили в пул работ data scientist’а, и исключаю стадии проработки инициативы, формализации задачи, деплоя, тестирования, мониторинга и др. Они заслуживают отдельного внимания.Как мы видим, загрузка и подготовка данных находятся в самом начале цепочки, следовательно, качество реализации этих стадий окажет прямое влияние на эффективность последующих. Ниже я раскрою основные проблемы первых двух этапов. Созданию признаков, обучению моделей и оценке качества будет посвящена отдельная статья.Загрузка данныхКак известно, модели строят свои оценки на основании закономерностей в данных, выявленных в процессе обучения. Поэтому ML-конвейер всегда начинается с получения необходимого массива. К частым сложностям, с которыми может столкнуться data scientist на этой стадии, можно отнести:1. Неожиданные изменения данных в источникахКак показывает практика, данные — это очень подвижная субстанция. Много проблем в Data Science связано именно с изменениями данных: обновлением справочников и нормативов, изменением распределений или логики обработки, вводом новых категорий, ручной правкой и т.д. В большинстве случаев контроль версий этих данных отсутствует, и сами изменения происходят без уведомления пользователя. Учитывая, что стабильные и воспроизводимые конвейеры являются залогом высокого качества DS-решения, вопрос постоянства данных нужно всегда учитывать.2. Отсутствие документацииСитуация, когда у data scientist'а сразу есть информация о данных для анализа и обучения, возможна, наверное, только на хакатонах. На деле наименования столбцов в БД могут быть абсолютно нечитаемыми, а их описание — отсутствовать. В то же время зависимости в данных намного проще искать, понимая заранее, с чем вы имеете дело. К сожалению, эта проблема является повсеместной и отнимает очень много времени, поэтому, берясь за проект, обязательно имейте это в виду.3. Большие объемы данныхНе всегда вычислительные ресурсы в распоряжении специалистов по машинному обучению соответствуют тем массивам, с которыми нужно будет работать. Можно также столкнуться с ограниченной пропускной способностью сети. Если вы отвечаете за проработку проекта, то обязательно обращайте внимание на то, с какими объемами данных вы будете иметь дело, чтобы адекватно оценивать ресурсы и определять подходящий целевой стэк.4. Большое количество источниковИсточники данных могут быть абсолютно разными: HDFS, S3, реляционная база данных или даже excel-файл на рабочем столе. Работа с одним таким источником через какой-нибудь API на Python сегодня уже не представляет сложности. Однако чаще информация распределена между различными хранилищами, и необходимо искать способы их агрегации. Другой классической проблемой, связанной с количеством источников, является расхождение данных по одним и тем же сущностям в разных системах.Это доставляет огромное число неприятностей в машинном обучении, особенно, когда дело доходит до деплоя. Конечно, если в команде есть инженер данных, эти задачи перекладываются на него, но далеко не каждый проект укомплектован DE, и даже когда он есть, на выстраивание потоков данных обычно требуется время. Поэтому такие трудности регулярно становятся частью работы аналитиков.5. Ошибки в ETL-процессахКогда заходишь на проект, часть потоков данных, которые должны готовить датасеты для обучения, могут быть уже выстроены. При этом гарантии, что логика предварительной обработки соответствует задаче, никто не дает, и часто это вскрывается не на старте проекта, а когда он уже идет полным ходом. Опыт показал, что детальная проработка существующих потоков данных на первых порах может существенно снизить риски задержек проекта.Подготовка данныхСырые данные практически никогда не пригодны для моделирования, и именно эта стадия является самой трудоемкой. Здесь часто приходится тратить много сил на аналитику, процесс подготовки данных может отнять большую часть времени проекта. К основным преградам на пути к формированию чистых данных можно отнести следующие проблемы:1. Низкое качество данныхВ реальной жизни чистых данных не бывает. Даже если вам достался чистый, на первый взгляд, датасет, будьте уверены, что перед этим он мог пройти не одну итерацию обработки. К основным проблемам качества, на которые можно сразу проверить исходный массив, можно отнести:Пропуски;Дубликаты;Аномально высокие или низкие значения;Несоответствие типов данных;Нарушение логики (например, отрицательное время или возраст).Когда ошибка выявлена, нужно понять причину и устранить ее. К сожалению, этот процесс может растянуть срок реализации проекта на несколько недель.2. Сложности в разметке данныхБольшинство ML-алгоритмов обучаются на размеченных данных, т.е. используют исторические значения прогнозируемой величины (например, объем спроса или время в пути) в качестве ориентира при построении прогнозов. Здесь также кроются потенциальные сложности. Во-первых, разметки может просто не быть. Особенно часто это встречается в узкоспециализированных областях. Во-вторых, проведение разметки может требовать привлечения экспертов ввиду особенностей решаемой задачи. В-третьих, разметка может быть выполнена некорректно, что отразится на качестве модели в продуктивной среде. В результате эта задача может отнять очень много денег и времени.3. Нехватка данныхВыше я писал, что большие массивы данных могут доставить неприятности. Возможна и обратная ситуация, когда данных слишком мало для выявления закономерностей и построения качественных моделей. Иногда не хватает только части данных, например, в случае дисбаланса классов в задачи классификации, когда прогнозируемое событие случается слишком редко. Как правило, чем больше исторических данных, тем более высокие результаты ожидаются. Но практика показывает, что не все ML-инициативы могут позволить себе такую роскошь, и часто приходится работать в ограниченных условиях.4. Отсутствие бизнес-экспертизыВ каждой сфере бизнеса существуют устоявшиеся процессы, в том числе связанные со сбором, хранением и обработкой данных. Когда принимаешься за проект в новой для себя области, также сталкиваешься с незнакомыми профессиональными терминами и понятиями. Поэтому стоит быть готовым к тому, что в процессе подготовки датасетов у data scientist’ов будут регулярно возникать вопросы по предметной области.Приведу пример из железнодорожной области. Одна из основных сущностей, с которой работают железнодорожники, — накладная. Это документ, который содержит сведения о грузе и особенностях перевозки, информацию по контрагентам, станциях отправления и назначения, дате прибытия вагона и т.д. В исторических данных можно встретить накладные, которые были закрыты не на станции назначения. В этом случае реальное время в пути может не совпадать с расчетным в накладной. Такие подводные камни нелегко интерпретировать, опираясь на классические способы анализа качества данных, и помочь может только знание отраслевых процессов.Надеюсь, что учет трудностей, которые я перечислил, поможет вам избежать лишних проблем при подготовке данных для ваших алгоритмов и позволит сделать свои ML-решения качественнее и стабильнее. Если вы сталкивались с проблемами в загрузке и подготовке датасетов для анализа и моделирования, о которых я не упомянул, буду рад узнать о вашем опыте в комментариях.Источник иллюстраций: Freepik.com    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Хорошего дня, Хаброжители! \n",
      "\r\n",
      "Методы управления данными и их интеграции быстро развиваются, хранение данных в одном месте становится все сложнее и сложнее масштабировать. Пора разобраться с тем, как перевести сложный и тесно переплетенный ландшафт данных вашего предприятия на более гибкую архитектуру, готовую к современным задачам.\n",
      "\r\n",
      "Архитекторы и аналитики данных, специалисты по соблюдению требований и управлению узнают, как работать с масштабируемой архитектурой и внедрять ее без больших предварительных затрат. Питхейн Стренгхольт поделится с вами идеями, принципами, наблюдениями, передовым опытом и шаблонами. \n",
      "\n",
      "\n",
      "Для кого эта книга\n",
      "Книга ориентирована на крупные предприятия, хотя может пригодиться и в небольших организациях. Ею могут особенно заинтересоваться:\r\n",
      "● руководители и архитекторы: директора по обработке и анализу данных, технические директора, архитекторы предприятия и ведущие архитекторы данных;\r\n",
      "● группы контроля и управления: руководители службы информационной безопасности, специалисты по защите данных, аналитики информационной безопасности, руководители по соблюдению нормативных требований, операторы баз данных и бизнес-аналитики;\r\n",
      "● аналитические группы: специалисты по теории и методам анализа данных, аналитики и руководители аналитических отделов;\r\n",
      "● команды разработчиков: дата-инженеры, бизнес-аналитики, разработчики и проектировщики моделей данных, а также другие специалисты по данным. \n",
      "\n",
      "\n",
      "\n",
      "Качество данных\r\n",
      "Состояние и качество данных — еще один важный аспект проектирования. Когда наборы данных загружаются в RDS, они проверяются на соответствие определенным метрикам с использованием правил оценки качества данных. В первую очередь должна оцениваться целостность данных и возможность их технической проверки на соответствие опубликованным схемам. Затем выполняется проверка данных на соответствие таким функциональным показателям качества, как полнота, точность, согласованность, достоверность и т. д.\n",
      "\r\n",
      "Преимущество использования общей инфраструктуры для RDS заключается в возможности использовать всю мощь больших данных для оценки качества. Например, Apache Spark (https://spark.apache.org/) может проверить и обработать сотни миллионов строк данных в течение нескольких минут. Существуют также фреймворки, которые можно использовать для проверки качества данных. В Amazon Web Services (AWS), например, был разработан Deequ (https://github/awslabs/deequ.com), — инструмент с открытым исходным кодом для расчета показателей качества данных. Другой пример — инструмент Delta Lake (https://delta.io), разработанный в DataBricks (https://databricks.com), — его можно использовать как для проверки схемы, так и для проверки данных. Эти инструменты позволяют глубже понять качество данных, что важно для всех сторон (рис. 3.7).\n",
      "\r\n",
      "Если мониторинг качества данных реализован как надо, он не только обнаруживает и отслеживает проблемы с качеством данных, но и становится частью общей структуры управления, которая добавляет новые данные к уже существующим. Если по какой-то причине качество упадет ниже указанного порогового значения, структура может зарегистрировать данные и попросить владельцев рассмотреть и принять данные с текущим качеством или отклонить их и выполнить повторную доставку.\n",
      "\r\n",
      "Качество данных можно контролировать в двух местах: в источнике или в RDS. Преимущество управления качеством данных в RDS с использованием нескольких источников данных на одной платформе заключается в возможности проверки ссылочной целостности. Системы-источники часто ссылаются на данные из других приложений или систем. Путем перекрестной проверки ссылочной целостности, а также сравнения и сопоставления всех наборов данных из всех RDS можно найти ошибки и корреляции, о существовании которых и не подозревали.\n",
      "\n",
      "\r\n",
      "Оценка качества данных означает наличие в системе управления ими замкнутой цепочки обратной связи, которая постоянно исправляет и предотвращает повторное появление проблем с качеством. Благодаря этому качество данных постоянно контролируется и изменения его уровня должны немедленно устраняться. Проблемы качества данных должны решаться в системах-источниках, а не в RDS. Если данные исправлены в источнике, то проблемы с качеством больше не будут появляться в других местах.\n",
      "\r\n",
      "Мой опыт показывает, что нельзя недооценивать влияние плохого качества данных. Если качество данных не обеспечивается должным образом, все потребители данных будут вынуждены снова и снова проводить работы по их очистке и исправлению.\n",
      "\n",
      "Уровни RDS\r\n",
      "У потребителей могут быть самые разные нужды — от простого исследования данных до принятия решений в режиме реального времени. Чтобы облегчить использование различных схем потребления, я рекомендую разбить RDS на разные уровни: входящий уровень и уровень доступа.\n",
      "\r\n",
      "Входящие уровни, как показано слева на рис. 3.8, обычно основаны на недорогом (объектном) хранилище и используются для проверки качества входящих наборов данных, обработки для получения метаданных, а также архивирования и создания наборов исторических данных (вскоре мы обсудим каждую из этих прикладных функций). Естественно, что во входящих уровнях со временем накопятся большие объемы данных; обычно в виде набора файлов CSV или JSON. Наконец, данные могут передаваться на входящий уровень различными способами: пакетной передачей, загрузкой по событиям, загрузкой через API, захватом изменений и т. д.\n",
      "\n",
      "\n",
      "RDS используют разделение, предполагающее размещение данных от разных поставщиков в разных сегментах, папках или логических экземплярах базы данных. Разделение обеспечивает управляемость и изоляцию.\r\n",
      "Уровни доступа к данным, как показано справа на рис. 3.8, оптимизированы для удобства чтения и могут предлагать несколько возможностей обработки запросов для улучшения взаимодействия с пользователем или повышения производительности. Хотя данные все еще находятся в контексте предметной области, они лучше подходят для решения бизнес-вопросов. Очевидно, что этот уровень дороже и часто приходится использовать несколько таких уровней для поддержки различных вариантов использования с разными моделями потребления. Например, для быстрых операций наборы данных могут находиться в хранилищах типа «ключ — значение» или в базах данных в памяти, а текстовые запросы могут располагаться в хранилищах, оптимизированных для поиска. Это также означает, что данные можно преобразовать в форму, которая лучше всего подходит для анализа. Их можно объединять, фильтровать или обрабатывать с использованием исторических данных или только подмножества данных входящего уровня.\n",
      "\r\n",
      "Наконец, уровни доступа к данным предоставляют дополнительные возможности. Они могут предлагать самообслуживание и улучшенные средства управления безопасностью, а также инструменты бизнес-аналитики и аналитики для быстрого доступа к данным, и могут использовать метаданные для управления перемещением данных и их синхронизации.\n",
      "\n",
      "Получение данных\r\n",
      "Давайте подробнее рассмотрим входящие уровни и то, как данные принимаются, собираются и накапливаются. Для перемещения данных приложений из золотых источников в RDS используются два основных метода.\n",
      "\r\n",
      "Первый — событийно-ориентированная обработка, или загрузка данных по событиям. Предполагает передачу данных в виде потока небольших событий или наборов данных. Событийно-ориентированная обработка позволяет собирать и обрабатывать данные относительно быстро, поэтому ее часто называют обработкой «в реальном времени». По мере поступления данных могут применяться критерии и преобразования данных для обнаружения изменений в потоке событий (мы коснемся передачи состояния приложения в главе 5 и в частности в разделе «Потоковое взаимодействие» на с. 184). Прием потоковой передачи хорошо походит для случаев, когда объемы данных относительно малы, вычисления, выполняемые с данными, относительно просты и задержки должны быть относительно короткими, близкими к реальному времени. Многие сценарии использования допускают применение событийно-ориентированной обработки, но если полнота и объем данных являются серьезной проблемой, то предпочтительнее использовать традиционную пакетную обработку.\n",
      "\r\n",
      "Второй метод — пакетная обработка — подразумевает процесс одновременной передачи большого объема данных, например всех транзакций из крупной платежной системы в конце дня. Пакет обычно выглядит как набор данных с миллионами записей, хранящихся в виде файла. Почему может понадобиться пакетная обработка? Потому что во многих случаях это единственный способ собрать данные. Наиболее важные RDBM имеют пакетные компоненты для поддержки пакетного перемещения данных. Согласование данных, процесс проверки данных во время перемещения, часто вызывает беспокойство1. Проверка наличия всех данных важна, особенно в финансовых процессах. Кроме того, многие системы сложны и содержат огромное количество таблиц, и применение языка структурированных запросов (Structured Query Language, SQL) — единственный надежный способ выбрать и извлечь все необходимые данные. Вот почему такие утверждения, как «пакеты не нужны» или «событийно-ориентированные данные — единственный вариант», слишком обобщены.\n",
      "\r\n",
      "Хотя загрузка данных по событиям продолжает набирать популярность, этот метод никогда не сможет полностью заменить пакетную обработку во всех случаях использования. Я предполагаю, что на современном предприятии одновременно будут использоваться оба метода. Еще ожидается, что для предметных областей будет реализовано много дополнительных компонентов предоставления данных, учитывая большое разнообразие приложений и БД. Они будут различаться в зависимости от особенностей процессов ETL, захвата изменений в данных, средств планирования и т. д.\n",
      "\n",
      "Метаданные схемы\r\n",
      "При пакетной обработке старых систем одним из слабых мест обычно является управление метаданными схемы. Для обеспечения целостности и полноты данных в файлах TXT и CSV (значения, разделенные запятыми) также можно добавить вверху заголовок, описывающий схему и количество строк внизу в файле CSV. Другой метод описания файлов — разработать определение интерфейса с использованием метаданных схемы. Файл метаданных, который показан в примере 3.1, описывает не только схему данных, но и владение, контрольные суммы и версию. Контрольные суммы могут использоваться для проверки полноты после преобразования данных, а версия — для проверки эволюции и обратной совместимости.\n",
      "\n",
      "\r\n",
      "Еще один подход к предоставлению метаданных с данными — разрешение областям регистрировать свои интерфейсы и выгружать свои метаданные на центральный портал регистрации. Этот реестр метаданных можно использовать и для хранения информации об интерфейсах, данных и связанных концепциях в одном месте. Мы обсудим это в главе 6.\n",
      "\r\n",
      "Я уделяю так много внимания метаданным, потому что они важны как для совместимости, так и для поддержки передачи данных в хранилища RDS. В отсутствие метаданных вам придется вручную разрабатывать множество конвейеров. Метаданные помогут автоматизировать обработку и организовать создание дополнительных контрольных точек проверки, работающих согласованно, независимо от используемой базовой технологии.\n",
      "\r\n",
      "В следующих двух разделах мы рассмотрим некоторые продукты и службы, требующие дополнительного внимания для правильного приема данных. Сюда входит сбор данных из готовых коммерческих решений и внешних сервисов.\n",
      "\n",
      "Интеграция готовых коммерческих решений\r\n",
      "Дополнительного внимания требуют интеграция и сбор данных из готовых коммерческих (commercial off-the-shelf, COTS) продуктов. Многие из них чрезвычайно трудно интерпретировать или использовать. Схемы БД часто бывают очень сложными, а ссылочная целостность обычно поддерживается программно, через приложение, а не базу данных. Часто данные защищены и могут извлекаться только с помощью стороннего решения.\n",
      "\r\n",
      "Во всех ситуациях я рекомендую реализовать дополнительные службы, которые позволят сначала выгрузить данные во вторичное хранилище (рис. 3.9), а затем настроить конвейер для переноса данных из этого хранилища в RDS. Преимущество такого подхода в том, что решение поставщика отделено от конвейера данных. Обычно схема данных продукта COTS напрямую не контролируется. Если поставщик выпустит обновление продукта и изменит структуры данных, то конвейер данных перестанет работать. Подход с промежуточным хранилищем обеспечивает гибкость, позволяющую поддерживать совместимость интерфейса.\n",
      "\n",
      "\n",
      "Извлечение данных из внешних API и SaaS\r\n",
      "Внешние API или SaaS, играющие роль поставщиков, тоже обычно требуют особого внимания. Бывают случаи, когда нужно получить полный набор данных, но API позволяет извлечь только относительно небольшую часть. Другие API могут применять регулирование, используя квоты или ограничения на количество запросов. Есть также API с дорогими тарифными планами для каждого вашего вызова.\n",
      "\r\n",
      "Во всех этих ситуациях я рекомендую создавать небольшие службы или приложения, которые обращаются к API и хранят данные во вторичном хранилище, как показано на рис. 3.10.\n",
      "\n",
      "\r\n",
      "Создавая обертки, инкапсулирующие API поставщика SaaS, можно спроектировать интересный шаблон. Все вызовы сначала будут передаваться обертке, инкапсулирующей API. Если запрос был выполнен недавно или только что, то обертка немедленно вернет результаты из вторичной БД. В противном случае будет вызван API поставщика SaaS, а результаты переданы потребителю и одновременно сохранены во вторичном хранилище для любого последующего использования. С помощью этого шаблона можно в конечном итоге получить полный набор данных и заполнить хранилище данных только для чтения (RDS).\n",
      "\n",
      "Служба исторических данных\r\n",
      "Аспект, который я хочу обсудить более подробно, — это управление жизненным циклом данных путем сбора и хранения исторических данных. Удаление нерелевантных данных делает системы более быстрыми и экономичными.\n",
      "\r\n",
      "Архитектура RDS берет на себя роль хранения и управления большими объемами исторических данных. Основное отличие от архитектуры корпоративного хранилища состоит в том, что RDS хранят данные в исходном контексте. Никакого преобразования в модель данных предприятия не ожидается, поэтому ценность не будет потеряна. Это серьезное преимущество: в оперативных сценариях использования, требующих сохранения большого количества исторических данных, не нужно преобразовывать данные обратно в исходный контекст.\n",
      "\r\n",
      "Хотя RDS не зависит от технологии, вероятность того, что все входящие уровни RDS будут спроектированы с использованием дорогостоящих систем управления AWS, очень мала. Когда хранилище и вычислительные ресурсы разделены, ваши RDS, скорее всего, будут размещены в недорогих распределенных файловых системах, доступных только для добавления. Это означает, что любое изменение или обновление существующих таблиц повлечет за собой полное переписывание файлов или уровней доступа к данным. Поэтому для распределенных файловых систем, доступных только для добавления, я советую использовать один из описанных ниже подходов, так как существует компромисс между затратами в управление входящими данными и простотой их потребления. У каждого подхода есть свои плюсы и минусы.\n",
      "\n",
      "Разбиение на разделы полноразмерных моментальных снимков файловых систем\r\n",
      "Первый подход — сохранять все доставляемые данные путем логического разбиения на разделы и группировки. Разбиение на разделы — распространенный метод организации файлов или таблиц в отдельные группы (разделы) для повышения управляемости, производительности или доступности. Разбиение обычно выполняется по некоторым атрибутам данных, таким как географическое местоположение (город, страна), значения (уникальные идентификаторы, коды сегментов) или время (дата и время доставки). Пример разбиения на разделы показан на рис. 3.11, слева.\n",
      "\n",
      "\r\n",
      "Разбиение полных моментальных снимков на разделы выполнить проще. По мере поступления данных каждый снимок добавляется как новый неизменяемый раздел. Таблица — это набор всех снимков, в которых каждый раздел сохраняет полный размер на определенный момент времени. Недостаток такого подхода — дублирование данных. Я не считаю это проблемой в наши дни, когда облачные хранилища стоят относительно дешево. Полные моментальные снимки также упрощают повторную доставку. Если исходная система обнаруживает, что были доставлены неверные данные, их можно отправить снова и раздел будет перезаписан. Основной недостаток этого подхода — усложнение анализа данных. Сравнение между конкретными периодами времени может быть затруднено из-за необходимости обрабатывать все данные при чтении. Это может стать проблемой, если потребители требуют, чтобы все исторические данные были обработаны и сохранены. Обработка исторических данных за три года может привести к последовательной обработке как минимум тысячи файлов и занять много времени, в зависимости от размера данных.\n",
      "\n",
      "Обслуживание исторических данных\r\n",
      "Второй подход — оптимизация всех наборов данных для использования исторических данных. Например, обработка всех наборов данных в SCD, которые показывают все изменения, имевшие место с течением времени. Обработка и создание исторических данных требуют дополнительного процесса ETL для обработки и объединения различных доставок данных (см. правую часть рис. 3.11).\n",
      "\r\n",
      "Подход к созданию исторических данных позволяет организовать их хранение более эффективно, обрабатывая, удаляя дубликаты и объединяя данные. Как можно видеть на рис. 3.11, медленно меняющееся измерение занимает половину количества строк. Поэтому запросы, например, при использовании реляционной БД выглядят проще и выполняются быстрее. Очистка данных или удаление отдельных записей, что может потребоваться для соблюдения требований GDPR, также упростится, так как вам не придется обрабатывать все наборы данных. Еще одно преимущество — возможность выбора более простых и быстрых реляционных БД благодаря более эффективному хранению данных.\n",
      "\r\n",
      "Однако есть и недостаток: создание SCD требует большего управления. Все данные нужно обработать. Изменения в исходных данных необходимо обнаруживать сразу после их появления и затем обрабатывать. Это требует дополнительного кода и вычислительной мощности. Еще хочу заметить, что потребители данных имеют разные требования. Поэтому, несмотря на наличие медленно меняющихся измерений, потребителям все равно необходимо обрабатывать данные. Например, данные могут поступать и обрабатываться каждый час, но, если потребитель ожидает, что данные будут сравниваться по дням, все равно потребуется дополнительная работа по преобразованию.\n",
      "\n",
      "Один из недостатков создания исторических данных для общего потребления состоит в том, что потребителям все равно может понадобиться обрабатывать данные всякий раз, когда они пропускают столбцы в выборках. Например, если потребитель запрашивает более узкий набор данных, могут появиться повторяющиеся записи и придется снова выполнить обработку для удаления дубликатов.\r\n",
      "Наконец, повторная доставка может быть трудновыполнимой, так как при этом могут быть обработаны и добавлены в измерения некорректные данные. Это можно исправить с помощью логики повторной обработки, дополнительных версий или сроков действия, но в любом случае потребуется дополнительное управление. Проблема здесь в том, что правильное управление данными может стать обязанностью центральной группы, поэтому такая масштабируемость нуждается в обширном интеллектуальном самообслуживающемся функционале.\n",
      "\r\n",
      "Для масштабируемости и помощи потребителям вы также можете рассмотреть возможность смешивания двух подходов — сохранение всех полных моментальных снимков и создание «исторических данных как услуги». В этом сценарии всем предметным областям потребителей предлагается небольшая вычислительная инфраструктура, с помощью которой они смогут планировать создание исторических данных в зависимости от объема (ежедневно, еженедельно или ежемесячно), временного интервала, атрибутов и необходимых им наборов данных. Используя краткосрочные экземпляры обработки в общедоступном облаке, вы даже можете сделать такой подход рентабельным. Большим преимуществом этого решения является сохранение гибкости при повторной доставке при отсутствии необходимости привлекать команду инженеров для работы с данными. Еще одно преимущество — возможность адаптировать временные рамки, объемы и атрибуты к потребностям каждого клиента.\n",
      "\n",
      "\n",
      "Об авторе\n",
      "Питхейн Стренгхольт (Piethein Strengholt) — главный архитектор ABN AMRO — курирует стратегию обработки данных и изучает ее влияние на деятельность организации. Ранее он работал стратегическим консультантом, проектировал множество архитектур и участвовал в крупных программах управления данными, а также был внештатным разработчиком приложений. Живет в Нидерландах со своей семьей.\n",
      "\n",
      "\r\n",
      "Более подробно с книгой можно ознакомиться на сайте издательства:\n",
      "\r\n",
      "» Оглавление\r\n",
      "» Отрывок\n",
      "\r\n",
      "По факту оплаты бумажной версии книги на e-mail высылается электронная книга.\r\n",
      "Покупка электронной книги вне РФ доступна на Google Play.\n",
      "\r\n",
      "Для Хаброжителей скидка 25% по купону — Данные    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Когда мы говорим про открытые данные, то всегда важно помнить что они невозможны без принципиального наличия данных вообще. Я как человек, занимающийся анализом данных госданных в области госфинансов, и вся команда нашего проекта Госзатраты, занимаемся тем, что регулярно пытаемся убедить ведомства, ответственные за госполитику в этой области, в том, чтобы открытые данные были доступны и чтобы с ними было максимально удобно работать. \n",
      "\n",
      "\n",
      "\n",
      "Во многом именно в этом и является залог успеха общественных проектов. Найти «топливо» в виде данных на которых проект может быть построен и найти «топливо» в виде финансирования которое позволило бы проекту появится и поддерживаться. Например, проект Госзатраты где мы анализируем данные госконтрактов поддерживается Комитетом Гражданских инициатив (http://komitetgi.ru). И Госзатраты это один из немногих технологических некоммерческих проектов  КГИ и технологических некоммерческих проектов в России в принципе.\n",
      "\n",
      "Я хочу затронуть сразу несколько важных тем. И прошу каждую из этих тем рассматривать как вопрос.\n",
      "\n",
      "Негосударственные проекты создающие открытые данные\n",
      "\n",
      " Наверняка многие из Вас видели уже многочисленные государственные порталы с открытыми данными, страницы на сайтах госорганов и многое другое что находится в публичном пространстве. с одной стороны кажется что данных много, а с другой оказывается что чаще всего эти данные не совсем или совсем не то что мы ждем. Не данные о качестве жизни, не данные на основе которых можно быстро реализовать коммерческую идею, а нечто гораздо более прозаичное вроде вакансий и верхнеуровневой статистики на сайтах ведущих госорганов. \n",
      "\n",
      "При этом есть несколько проектов в рамках которых создаются открытые данные самостоятельно. Собирая и переупаковывая госданные или формируя данные краудсорсингом. Википедия является известнейшим таким проектом. Другой пример — российское сообщество OSM. Также есть такие проекты как OpenCorpora, Декларатор, Открытая полиция, Госзатраты и Metro4All. Все они, либо созданы группами активистов, либо поддерживаются некоммерческими организациями в внутри России. \n",
      "\n",
      "Вопрос в том — полон ли этот список? Можете ли Вы привести примеры проектов которые не потребляют или не только потребляют, но и создают открытые данные/API с доступом под свободными лицензиями? \n",
      "\n",
      "Открытость данных о госфинансах\n",
      "\n",
      "Когда меня спрашивают о том что нужно чтобы появились те или иные госданные, я всегда отвечаю что главное — это _настойчивость_ и готовность к тому часто получение данных может занимать годы. По этой причине важно всегда, при любой возможности, «бить в одну точку» и говорить об одном и том же — это еще и показатель того что добиваясь чего-то Вы четко понимаете что эти данные Вам нужны. \n",
      "\n",
      "Это одна из главных причин почему я всегда призываю участвовать во встречах с разработчиками которые проводят госорганы и участвовать в их опросах. Например, Минфин России проводит очередной опрос о востребованности их данных. Это простой и короткий опрос, с немногочисленными вопросами на которые можно дать очень четкие ответы. Не скрою что я лично (Иван Бегтин) заинтересован чтобы данных о госфинансах было бы как можно больше в открытом доступе. Это топливо которое питает наши проекты, мы используем их не только для поиска ответов на вопрос «Куда идут наши деньги?», но и для анализа того как _на самом деле_ устроено государство нашей страны. \n",
      "\n",
      "Например доступность таких данных как:\n",
      "\n",
      "Реестр расходных обязательств Российской Федерации \n",
      "Реестры расходных обязательств субъектов Российской Федерации и муниципальных образований, входящих в состав субъектов Российской Федерации Информация официального сайта Министерства финансов Российской Федерации \n",
      "\n",
      "\n",
      "Позволит загрузить нам в Госзатраты больше данных о бюджетах и сделать такой специальный раздел с муниципальными и региональными бюджетами. \n",
      "\n",
      "А данные о бюджетах СНГ и союзного государства будут интересны не только гражданам России, но и всем гражданам СНГ. Поэтому я прошу всех не полениться и потратить немного времени на этот опрос и всегда участвовать в любых опросах госорганов по теме открытости данных. \n",
      "\n",
      "И, в продолжение этого опроса, нечто не менее важное. 11 ноября Минфин и Федеральное Казначейство будут проводить встречу с разработчиками где они будут рассказывать о том что уже сделано и что уже доступно по открытым данным в области госфинансов. Мы, Инфокультура, помогаем им в организации мероприятия и я всех кто уже работал или хочет работать с этими данными зарегистрироваться на мероприятие. \n",
      "\n",
      "Поиск финансирования\n",
      "\n",
      "В 2010 году когда я только-только начинал искать данные для анализа госконтрактов я собирал их извлекая из официальных сайтов закупок по всем субъектам федерации. Тогда была ситуация полного отсутствия возможности найти на это хоть какие-то средства и дефицит данных не позволяющий создать первый более менее действующий проект. Самую-самую первую версию прототипа тогда можно было создать только на собственные средства и собственными усилиями. \n",
      "\n",
      "У многих других проектов создающих или использующих открытые данные похожая история. Они, либо создаются волонтерами, либо получают грантовое финансирование, либо изначально являются коммерческими и коммерческими и остаются. \n",
      "Если Вы обратите внимание на проекты поддержанные Общественной палатой (http://grants.oprf.ru) на открытых данных или создающие данные, то Вы их не найдете. И за все время был только один такой проект по прозрачности НКО выигранный Трансперенси Интернейшнл http://grants.oprf.ru/grants2014-1/winners/rec3722/ с соучастием Инфокультуры. И где мы собрали из разрозненных источников максимально возможные базы данных о НКО по всей России на портале Про НКО (http://prongo.ru/). \n",
      "\n",
      "В мире найти финансирование общественных проектов на открытых данных непросто не из-за отсутствия финансирования, а из-за конкуренции. Такие организации как Knights Foundation, Omidyar Network и десятки фондов в США, Великобритании, странах ЕС и Латинской Америки поддерживают технологические некоммерческие проекты. В первую очередь проекты с открытым кодом и далее от них проекты создающие и публикующие открытые данные. \n",
      "\n",
      "В России таких негосударственных фондов практически нет. Как я упоминал ранее, я сталкивался всего с двумя случаями явной поддержки таких проектов — это Комитет Гражданских Инициатив. А вот ни одного другого фонда мне найти не удалось и, проводя исследование по источникам финансирование тематики открытых данных в России, оказывается весьма удручающая картина. \n",
      "\n",
      "Почему так происходит и что с этим делать? Одной из важнейших причин является само состояние некоммерческого сектора в России. \n",
      "\n",
      "А его можно описать так:\n",
      "\n",
      "некоммерческих организаций создающих технологические проекты — единицы;\n",
      "в составах комиссий по распределению грантов практически полностью отсутствуют люди разбирающиеся в технологиях;\n",
      "ни у одного из фондов я нет стратегии на создание экосистемы в областях его специализации;\n",
      "кооперация проектов, невозможная без доступности данных, не является приоритетом ни для одного из фондов\n",
      "\n",
      "\n",
      "И это касается, как государственного, так и негосударственного финансирования. \n",
      "\n",
      "И вопрос в том что нас с этим делать. \n",
      "\n",
      "\n",
      "Убеждать частные фонды в поддержке проектов создающих экосистему открытых данных?\n",
      "Убеждать чиновников в смене приоритетов господдержки некоммерческих организаций?\n",
      "Искать способы народного финансирования и краудфандинга технологических проектов?\n",
      "\n",
      "\n",
      "Лично у меня нет окончательных и полных ответов на все эти вопросы. Но очень хочется услышать всех, у кого был опыт поиска финансирования на свои проекты на открытых данных. И узнать, что удавалось, а что нет.    \n",
      " Яндекс.Метрика сегодня это не только система веб-аналитики, но и AppMetrica — система аналитики для приложений. На входе в Метрику мы имеем поток данных — событий, происходящих на сайтах или в приложениях. Наша задача — обработать эти данные и представить их в подходящем для анализа виде. \n",
      "\n",
      "\n",
      "\n",
      "Но обработка данных — это не проблема. Проблема в том, как и в каком виде сохранять результаты обработки, чтобы с ними можно было удобно работать. В процессе разработки нам приходилось несколько раз полностью менять подход к организации хранения данных. Мы начинали с таблиц MyISAM, использовали LSM-деревья и в конце концов пришли к column-oriented базе данных. В этой статье я хочу рассказать, что нас вынуждало это делать.\n",
      "\n",
      "Яндекс.Метрика работает с 2008 года — более семи лет. Каждый раз изменение подхода к хранению данных было обусловлено тем, что то или иное решение работало слишком плохо — с недостаточным запасом по производительности, недостаточно надёжно и с большим количеством проблем при эксплуатации, использовало слишком много вычислительных ресурсов, или же просто не позволяло нам реализовать то, что мы хотим.\n",
      "\n",
      "В старой Метрике для сайтов, имеется около 40 «фиксированных» отчётов (например, отчёт по географии посетителей), несколько инструментов для in-page аналитики (например, карта кликов), Вебвизор (позволяет максимально подробно изучить действия отдельных посетителей) и, отдельно, конструктор отчётов.\n",
      "\n",
      "В новой Метрике, а также в Appmetrica вместо «фиксированных» отчётов, каждый отчёт можно произвольным образом изменять. Можно добавлять новые измерения (например, в отчёт по поисковым фразам добавить ещё разбиение по страницам входа на сайт), сегментировать и сравнивать (можно сравнить источники трафика на сайт для всех посетителей и посетителей из Москвы), менять набор метрик и так далее. Конечно, это требует совершенно разных подходов к хранению данных.\n",
      "\n",
      "MyISAM\n",
      "В самом начале Метрика создавалась, как часть Директа. В Директе для решения задачи хранения статистики использовались MyISAM таблицы, и мы тоже с этого начали. Мы использовали MyISAM для хранения «фиксированных» отчётов с 2008 по 2011 год.\n",
      "\n",
      "Давайте, я расскажу, какой должна быть структура таблицы для отчёта, например, по географии. Отчёт показывается для конкретного сайта (точнее, номера счётчика Метрики). Значит, в первичный ключ должен входить номер счётчика — CounterID. Пользователь может выбрать произвольный отчётный период. Сохранять данные для каждой пары дат было бы неразумно, поэтому они сохраняются для каждой даты и затем при запросе суммируются для заданного интервала. То есть в первичный ключ входит дата — Date.\n",
      "\n",
      "В отчёте данные отображаются для регионов в виде дерева из стран, областей, городов, либо в виде списка. Разумно поместить в первичный ключ таблицы идентификатор региона (RegionID), а собирать данные в дерево уже на стороне прикладного кода, а не базы данных.\n",
      "\n",
      "Ещё считается, например, средняя продолжительность визита. Значит, в столбцах таблицы должно быть количество визитов и суммарная продолжительность визитов.\n",
      "\n",
      "В итоге, структура таблицы такая: CounterID, Date, RegionID -> Visits, SumVisitTime,… Рассмотрим, что происходит, когда мы хотим получить отчёт. Делается запрос SELECT с условием WHERE CounterID = c AND Date BETWEEN min_date AND max_date. То есть происходит чтение по диапазону первичного ключа.\n",
      "\n",
      "Как реально хранятся данные на диске? \n",
      "MyISAM таблица представляет собой файл с данными и файл с индексами. Если из таблицы ничего не удалялось и строчки не меняли своей длины при обновлении, то файл с данными будет представлять собой сериализованные строчки, уложенные подряд в порядке их добавления. Индекс (в том числе, первичный ключ) представляет собой B-дерево, в листьях которого находятся смещения в файле с данными. Когда мы читаем данные по диапазону индекса, из индекса достаётся множество смещений в файле с данными. Затем по этому множеству смещений делаются чтения из файла с данными.\n",
      "\n",
      "Предположим естественную ситуацию, когда индекс находится в оперативке (key cache в MySQL или системный page cache), а данные не закэшированы в ней. Предположим, что мы используем жёсткие диски. Время для чтения данных зависит от того, какой объём данных нужно прочитать и сколько нужно сделать seek-ов. Количество seek-ов определяется локальностью расположения данных на диске.\n",
      "\n",
      "События в Метрику поступают в порядке, почти соответствующем времени событий. В этом входящем потоке данные разных счётчиков разбросаны совершенно произвольным образом. То есть, входящие данные локальны по времени, но не локальны по номеру счётчика. При записи в MyISAM таблицу данные разных счётчиков будут также расположены совершенно случайным образом, а это значит, что для чтения данных отчёта необходимо будет выполнить примерно столько случайных чтений, сколько есть нужных нам строк в таблице. \n",
      "\n",
      "Обычный жёсткий диск 7200 RPM умеет выполнять от 100 до 200 случайных чтений в секунду, RAID-массив при грамотном использовании — пропорционально больше. Один SSD пятилетней давности умеет выполнять 30 000 случайных чтений в секунду, но мы не можем позволить себе хранить наши данные на SSD. Таким образом, если для нашего отчёта нужно прочитать 10 000 строк, то это вряд ли займёт меньше 10 секунд, что полностью неприемлемо.\n",
      "\n",
      "Для чтений по диапазону первичного ключа лучше подходит InnoDB, так как в InnoDB используется кластерный первичный ключ (то есть, данные хранятся упорядоченно по первичному ключу). Но InnoDB было невозможно использовать из-за низкой скорости записи. Если читая этот текст, вы вспомнили про TokuDB, то продолжайте читать этот текст.\n",
      "\n",
      "Для того чтобы MyISAM работала быстрее при выборе по диапазону первичного ключа, применялись некоторые трюки.\n",
      "\n",
      "Сортировка таблицы. Так как данные необходимо обновлять инкрементально, то один раз отсортировать таблицу недостаточно, а сортировать её каждый раз невозможно. Тем не менее, это можно делать периодически для старых данных.\n",
      "\n",
      "Партиционирование. Таблица разбивается на некоторое количество более маленьких по диапазонам первичного ключа. При этом есть надежда, что данные одной партиции будут храниться более-менее локально, и запросы по диапазону первичного ключа будут работать быстрее. Этот метод можно называть «кластерный первичный ключ вручную». При этом вставка данных несколько замедляется. Выбирая количество партиций, как правило, удаётся достичь компромисса между скоростью вставок и чтений.\n",
      "\n",
      "Разделение данных на поколения. При одной схеме партиционирования могут слишком тормозить чтения, при другой — вставки, а при промежуточной — и то, и другое. В этом случае возможно разделение данных на несколько поколений. Например, первое поколение назовём оперативными данными — там партиционирование производится в порядке вставки (по времени) или не производится вообще. Второе поколение назовём архивными данными — там оно производится в порядке чтения (по номеру счётчика). Данные переносятся из поколения в поколение скриптом, но не слишком часто (например, раз в день). Данные считываются сразу из всех поколений. Это, как правило, помогает, но добавляет довольно много сложностей.\n",
      "\n",
      "Все эти трюки (и некоторые другие) использовались в Яндекс.Метрике когда-то давно для того, чтобы всё хоть как-то работало.\n",
      "\n",
      "Резюмируем, какие имеются недостатки:\n",
      "\n",
      "\n",
      "локальность данных на диске очень сложно поддерживается;\n",
      "таблицы блокируются при записи данных;\n",
      "репликация работает медленно, реплики зачастую отстают;\n",
      "консистентность данных после сбоя не обеспечивается;\n",
      "такие агрегаты, как количество уникальных пользователей, очень сложно рассчитываются и хранятся;\n",
      "сжатие данных использовать затруднительно; оно работает неэффективно;\n",
      "индексы занимают много места и зачастую не помещаются в оперативку;\n",
      "данные необходимо шардировать вручную;\n",
      "много вычислений приходится делать на стороне прикладного кода после SELECT-а;\n",
      "сложная эксплуатация.\n",
      "\n",
      "\n",
      "\n",
      "Локальность данных на диске, образное представление\n",
      "\n",
      "В целом использование MyISAM было крайне неудобным. В дневное время серверы работали со 100% нагрузкой на дисковые массивы (постоянное перемещение головок). В таких условиях диски выходят из строя чаще, чем обычно. На серверах мы использовали дисковые полки (16 дисков) — то есть, довольно часто приходилось восстанавливать RAID-массивы. При этом репликация отставла ещё больше и иногда реплику приходилось наливать заново. Переключение мастера крайне неудобно. Для выбора реплики, на которую отправляются запросы, мы использовали MySQL Proxy, и это использование было весьма неудачным (потом мы заменили его на HAProxy).\n",
      "\n",
      "Несмотря на эти недостатки, по состоянию на 2011 год мы хранили в MyISAM таблицах более 580 миллиардов строк. Потом всё переконвертировали в Metrage, удалили и в итоге освободили много серверов.\n",
      "\n",
      "Metrage\n",
      "Мы используем Metrage для хранения фиксированных отчётов с 2010 года по настоящее время. Предположим, у вас имеется следующий сценарий работы:\n",
      "\n",
      "\n",
      "данные постоянно записываются в базу небольшими batch-ами;\n",
      "поток на запись сравнительно большой — несколько сотен тысяч строк в секунду;\n",
      "запросов на чтение сравнительно мало — десятки-сотни запросов в секунду;\n",
      "все чтения — по диапазону первичного ключа, до миллионов строк на один запрос;\n",
      "строчки достаточно короткие — около 100 байт в несжатом виде.\n",
      "\n",
      "Для такого хорошо подходит достаточно распространенная структура данных LSM-Tree. Она представляет собой сравнительно небольшой набор «кусков» данных на диске, каждый из которых содержит данные, отсортированные по первичному ключу. Новые данные сначала располагаются в какой-либо структуре данных в оперативке (MemTable), затем записываются на диск в новый сортированный кусок. Периодически в фоне несколько сортированных кусков объединяются в один более крупный сортированный (compaction). Таким образом постоянно поддерживается сравнительно небольшой набор кусков.\n",
      "\n",
      "Среди встраиваемых структур данных LSM-Tree реализуют LevelDB, RocksDB. Она используется в HBase и Cassandra.\n",
      "\n",
      "\n",
      "\n",
      "Metrage также представляет собой LSM-Tree. В качестве «строчек» в нём могут использоваться произвольные структуры данных (фиксированы на этапе компиляции). Каждая строчка — это пара ключ, значение. Ключ — это структура с операциями сравнения на равенство и неравенство. Значение — произвольная структура с операциями update (добавить что-нибудь) и merge (агрегировать, объединить с другим значением). Короче говоря, это CRDT.\n",
      "\n",
      "В качестве значений могут выступать как простые структуры (кортеж чисел), так и более сложные (хэш-таблица для рассчёта количества уникальных посетителей, структура для карты кликов). С помощью операций update и merge постоянно выполняется инкрементальная агрегация данных:\n",
      "\n",
      "\n",
      "во время вставки данных, при формировании новой пачки в оперативке;\n",
      "во время фоновых слияний;\n",
      "при запросах на чтение.\n",
      "\n",
      "Также Metrage содержит нужную нам domain-specific логику, которая выполняется при запросах. Например, для отчёта по регионам ключ в таблице будет содержать идентификатор самого нижнего региона (город, посёлок), и если нам нужно получить отчёт по странам, то доагрегация данных в данные по странам будет произведена на стороне сервера БД.\n",
      "\n",
      "Перечислю достоинства этой структуры данных:\n",
      "\n",
      "\n",
      "Данные расположены достаточно локально на жёстком диске, чтения по диапазону первичного ключа работают быстро.\n",
      "Данные сжимаются по блокам. За счёт хранения в упорядоченном виде, сжатие достаточно сильное при использовании быстрых алгоритмов сжатия (в 2010 году использовали QuickLZ, с 2011 используем LZ4).\n",
      "Хранение данных в упорядоченном виде позволяет использовать разреженный индекс. Разреженный индекс — это массив значений первичного ключа для каждой N-ой строки (N порядка тысяч). Такой индекс получается максимально компактным и всегда помещается в оперативку.\n",
      "\n",
      "Так как чтения выполняются не очень часто, но при этом читают достаточно много строк, то увеличение latency из-за наличия многих кусков и разжатия блока данных и чтение лишних строк из-за разреженности индекса не имеют значения.\n",
      "\n",
      "Записанные куски данных не модифицируются. Это позволяет производить чтение и запись без блокировок — для чтения берётся снапшот данных. Используется простой и единообразный код, но при этом мы можем легко реализовать всю нужную нам domain-specific логику.\n",
      "\n",
      "Нам пришлось написать Metrage вместо доработки какого-либо существующего решения, потому что какого-либо существующего решения не было. Например, LevelDB не существовала в 2010 году. TokuDB в то время была доступна только за деньги.\n",
      "\n",
      "Все системы, реализующие LSM-Tree подходили для хранения неструктурированных данных и отображения типа BLOB -> BLOB с небольшими вариациями. Для адаптации подобной к работе с произвольными CRDT потребовалось бы гораздо больше времени, чем на разработку Metrage.\n",
      "\n",
      "Конвертация данных из MySQL в Метраж была достаточно трудоёмкой: чистое время на работу программы конвертации всего лишь около недели, но выполнить основную часть работы удалось только за два месяца.\n",
      "\n",
      "После перевода отчётов на Metrage мы сразу же получили преимущество в скорости работы интерфейса Метрики. Так 90% перцентиль времени загрузки отчёта по заголовкам страниц уменьшился с 26 секунд до 0.8 секунд (общее время, включая работу всех запросов к базам данных и последующих преобразований данных). Время обработки запросов самой Metrage (для всех отчётов) составляет: медиана — 6 мс, 90% — 31 мс, 99% — 334 мс.\n",
      "\n",
      "Мы использовали Metrage в течение пяти лет, и она показала себя как надёжное беспроблемное решение. За всё время было всего лишь несколько незначительных сбоев. Преимущества в эффективности и в простоте использования, по сравнению с хранением данных в MyISAM, являются кардинальными.\n",
      "\n",
      "Сейчас мы храним в Metrage 3.37 триллиона строк. Для этого используется 39 * 2 серверов. Мы постепенно отказываемся от хранения данных в Metrage и уже удалили несколько наиболее крупных таблиц. Но и у этой системы есть недостаток — эффективно работать можно только с фиксированными отчётами. Metrage выполняет агрегацию данных и хранит агрегированные данные. А для того чтобы это делать, нужно заранее перечислить все способы, которыми мы хотим агрегировать данные. Если мы будем делать это 40 разными способами, значит, в Метрике будет 40 отчётов, но не больше.\n",
      "\n",
      "OLAPServer\n",
      "В Яндекс.Метрике объём данных и величина нагрузки являются достаточно большими, чтобы основной проблемой было сделать решение, которое хотя бы работает — решает задачу и при этом справляется с нагрузкой в рамках адекватного количества вычислительных ресурсов. Поэтому зачастую основные усилия тратятся на то, чтобы создать минимальный работающий прототип.\n",
      "\n",
      "Одним из таких прототипов был OLAPServer. Мы использовали OLAPServer с 2009 по 2013 год в качестве структуры данных для конструктора отчётов.\n",
      "\n",
      "Наша задача — получать произвольные отчёты, структура которых становится известна лишь в тот момент, когда пользователь хочет получить отчёт. Для этого невозможно использовать предагрегированные данные, потому что невозможно предусмотреть заранее все комбинации измерений, метрик, условий. А значит, нужно хранить неагрегированные данные. Например, для отчётов, вычисляемых на основе визитов, необходимо иметь таблицу, где каждому визиту будет соответствовать строчка, а каждому параметру, по которому можно рассчитать отчёт, — столбец.\n",
      "\n",
      "Имеем такой сценарий работы:\n",
      "\n",
      "\n",
      "есть широкая «таблица фактов», содержащая большое количество столбцов (сотни);\n",
      "при чтении вынимается достаточно большое количество строк из БД, но только небольшое подмножество столбцов;\n",
      "запросы на чтение идут сравнительно редко (обычно не более сотни в секунду на сервер);\n",
      "при выполнении простых запросов допустимы задержки в районе 50мс;\n",
      "значения в столбцах достаточно мелкие — числа и небольшие строки (пример — 60 байт на URL);\n",
      "требуется высокая пропускная способность при обработке одного запроса (до миллиардов строк в секунду на один сервер);\n",
      "результат выполнения запроса существенно меньше исходных данных — то есть, данные фильтруются или агрегируются;\n",
      "\n",
      "сравнительно простой сценарий обновления данных, обычно append-only batch-ами; нет сложных транзакций.\n",
      "\n",
      "Для такого сценария работы (назовём его OLAP сценарий работы), наилучшим образом подходят столбцовые СУБД (column-oriented DBMS). Так называются СУБД, в которых данные для каждого столбца хранятся отдельно, а данные одного столбца — вместе.\n",
      "\n",
      "Столбцовые СУБД эффективно работают для OLAP сценария работы по следующим причинам:\n",
      "\n",
      "1. По I/O.\n",
      "\n",
      "\n",
      "Для выполнения аналитического запроса требуется прочитать небольшое количество столбцов таблицы. В столбцовой БД для этого можно читать только нужные данные. Например, если вам требуется только 5 столбцов из 100, то следует рассчитывать на 20-кратное уменьшение ввода-вывода.\n",
      "Так как данные читаются пачками, то их проще сжимать. Данные, лежащие по столбцам, также лучше сжимаются. За счёт этого дополнительно уменьшается объём ввода-вывода.\n",
      "За счёт уменьшения ввода-вывода, больше данных влезает в системный кэш.\n",
      "\n",
      "Например, для запроса «посчитать количество записей для каждой рекламной системы» требуется прочитать один столбец «Идентификатор рекламной системы», который занимает 1 байт в несжатом виде. Если большинство переходов было не с рекламных систем, то можно рассчитывать хотя бы на десятикратное сжатие этого столбца. При использовании быстрого алгоритма сжатия возможно разжатие данных со скоростью более нескольких гигабайт несжатых данных в секунду. То есть, такой запрос может выполняться со скоростью около нескольких миллиардов строк в секунду на одном сервере.\n",
      "\n",
      "\n",
      "\n",
      "2. По CPU.\n",
      "Так как для выполнения запроса надо обработать достаточно большое количество строк, становится актуальным диспетчеризовывать все операции не для отдельных строк, а для целых векторов (пример — векторный движок в СУБД VectorWise) или реализовать движок выполнения запроса так, чтобы издержки на диспетчеризацию были примерно нулевыми (пример — кодогенерация с помощью LLVM в Cloudera Impala). Если этого не делать, то при любой не слишком плохой дисковой подсистеме интерпретатор запроса неизбежно упрётся в CPU. Имеет смысл не только хранить данные по столбцам, но и обрабатывать их по возможности тоже по столбцам.\n",
      "\n",
      "Существует достаточно много столбцовых СУБД. Это, например, Vertica, Paraccel (Actian Matrix) (Amazon Redshift), Sybase IQ (SAP IQ), Exasol, Infobright, InfiniDB, MonetDB (VectorWise) (Actian Vector), LucidDB, SAP HANA, Google Dremel, Google PowerDrill, Metamarkets Druid, kdb+ и т. п.\n",
      "\n",
      "В традиционно строковых СУБД последнее время тоже стали появляться решения для хранения данных по столбцам. Примеры — column store index в MS SQL Server, MemSQL, cstore_fdw для Postgres, форматы ORC-File и Parquet для Hadoop.\n",
      "\n",
      "OLAPServer представляет собой простейшую и крайне ограниченную реализацию столбцовой базы данных. Так OLAPServer поддерживает всего лишь одну таблицу, заданную в compile time, — таблицу визитов. Обновление данных делается не в реальном времени, как везде в Метрике, а несколько раз в сутки. В качестве типов данных поддерживаются только числа фиксированной длины 1-8 байт. А в качестве запроса поддерживается лишь вариант SELECT keys..., aggregates... FROM table WHERE condition1 AND condition2 AND... GROUP BY keys ORDER BY column_nums....\n",
      "\n",
      "Несмотря на такую ограниченную функциональность, OLAPServer успешно справлялся с задачей конструктора отчётов. Но не справлялся с задачей реализовать возможность кастомизации каждого отчёта Яндекс.Метрики. Например, если отчёт содержал URL-ы, то его нельзя было получить через конструктор отчётов, потому что OLAPServer не хранил URL-ы; не удавалось реализовать часто необходимую нашим пользователям функциональность — просмотр страниц входа для поисковых фраз.\n",
      "\n",
      "По состоянию на 2013 год мы хранили в OLAPServer-е 728 миллиардов строк. Потом все данные переложили в ClickHouse и удалили.\n",
      "\n",
      "ClickHouse\n",
      "Используя OLAPServer, мы успели понять, насколько хорошо столбцовые СУБД справляются с задачей ad-hoc аналитики по неагрегированным данным. Если любой отчёт можно получить по неагрегированным данным, то возникает вопрос, нужно ли вообще предагрегировать данные заранее, как мы это делаем, используя Metrage?\n",
      "\n",
      "С одной стороны, предагрегация данных позволяет уменьшить объём данных, используемых непосредственно в момент загрузки страницы с отчётом. С другой стороны, агрегированные данные являются очень ограниченным решением. Причины следующие:\n",
      "\n",
      "\n",
      "вы должны заранее знать перечень отчётов, необходимых пользователю;\n",
      "то есть, пользователь не может построить произвольный отчёт;\n",
      "при агрегации по большому количеству ключей объём данных не уменьшается и агрегация бесполезна;\n",
      "при большом количестве отчётов получается слишком много вариантов агрегации (комбинаторный взрыв);\n",
      "при агрегации по ключам высокой кардинальности (например, URL) объём данных уменьшается не сильно (менее чем в 2 раза);\n",
      "из-за этого объём данных при агрегации может не уменьшиться, а вырасти;\n",
      "пользователи будут смотреть не все отчёты, которые мы для них посчитаем. — то есть, большая часть вычислений бесполезна;\n",
      "сложно поддерживать логическую целостность при хранении большого количества разных агрегаций.\n",
      "\n",
      "Как видно, если ничего не агрегировать и работать с неагрегированными данными, то это даже может уменьшить объём вычислений. Но работа только с неагрегированными данными накладывает очень высокие требования к эффективности работы той системы, которая будет выполнять запросы.\n",
      "\n",
      "Так, если мы агрегируем данные заранее, то делаем это хоть и постоянно (в реальном времени), но зато асинхронно по отношению к пользовательским запросам. Мы должны всего лишь успевать агрегировать данные в реальном времени — на момент получения отчёта используются уже по большей части подготовленные данные.\n",
      "\n",
      "Если не агрегировать данные заранее, то всю работу нужно делать в момент запроса пользователя — пока он ждёт загрузки страницы с отчётом. Это значит, что во время запроса может потребоваться обработать многие миллиарды строк и чем быстрее, тем лучше.\n",
      "\n",
      "Для этого нужна хорошая столбцовая СУБД. На рынке не существует ни одной столбцовой СУБД, которая могла бы достаточно хорошо работать на задачах интернет-аналитики масштаба Рунета и при этом имела бы не запретительно высокую стоимость лицензий. Если бы мы использовали некоторые из решений, перечисленных в предыдущем разделе, то стоимость лицензий многократно превысила бы стоимость всех наших серверов и сотрудников.\n",
      "\n",
      "В последнее время в качестве альтернативы коммерческим столбцовым СУБД стали появляться решения для эффективной ad-hoc аналитики по данным, находящимся в системах распределённых вычислений: Cloudera Impala, Spark SQL, Presto, Apache Drill. Хотя такие системы могут эффективно работать на запросах для внутренних аналитических задач, достаточно трудно представить их в качестве бэкенда для веб-интерфейса аналитической системы, доступной внешним пользователям.\n",
      "\n",
      "В Яндексе разработана своя столбцовая СУБД — ClickHouse. Рассмотрим основные требования, которые у нас к ней были до того, как приступить к разработке. \n",
      "\n",
      "Умение работать с большими данными. В новой Яндекс.Метрике ClickHouse используется для хранения всех данных для отчётов. Объём базы данных на декабрь 2015 составлял 11,4 триллионов строк (и это только для большой Метрики). Строчки — неагрегированные данные, которые используются для получения отчётов в реальном времени. Каждая строчка в наиболее крупных таблицах содержит более 200 столбцов.\n",
      "\n",
      "Система должна линейно масштабироваться. ClickHouse позволяет увеличивать размер кластера путём добавления новых серверов по мере необходимости. Например, основной кластер Яндекс.Метрики был увеличен с 60 до 394 серверов в течение двух лет. Для отказоустойчивости, серверы располагаются в разных дата-центрах. ClickHouse может использовать все возможности железа для обработки одного запроса. Так достигается скорость более 1 терабайта в секунду (данных после разжатия, только используемые столбцы).\n",
      "\n",
      "Высокая эффективность работы. Высокая производительность базы является нашим отдельным предметом гордости. По результатам внутренних тестов, ClickHouse обрабатывает запросы быстрее, чем любая другая система, которую мы могли достать. Например, ClickHouse в среднем в 2,8-3,4 раза быстрее, чем Vertica. В ClickHouse нет одной серебряной пули, за счёт которой система работает так быстро. \n",
      "\n",
      "Функциональность должна быть достаточной для инструментов веб-аналитики. База поддерживает диалект языка SQL, подзапросы и JOIN-ы (локальные и распределённые). Присутствуют многочисленные расширения SQL: функции для веб-аналитики, массивы и вложенные структуры данных, функции высшего порядка, агрегатные функции для приближённых вычислений с помощью sketching и т. п. При работе с ClickHouse вы получаете удобство реляционной СУБД.\n",
      "\n",
      "ClickHouse разработана в команде Яндекс.Метрики. При этом систему удалось сделать достаточно гибкой и расширяемой для того, чтобы она могла успешно использоваться для разных задач. Хотя база способна работать на кластерах большого размера, она может быть установлена на один сервер или даже на виртуальную машину. Сейчас имеется более десятка применений ClickHouse внутри компании.\n",
      "\n",
      "ClickHouse хорошо подходит для создания всевозможных аналитических инструментов. Действительно, если система успешно справляется с задачами большой Яндекс.Метрики, то можно быть уверенным, что с другими задачами ClickHouse справится с многократным запасом по производительности.\n",
      "\n",
      "В этом смысле особенно повезло Appmetrica — когда она находилась в разработке, ClickHouse уже был готов. Для обработки данных аналитики приложений мы просто сделали одну программу, которая берёт входящие данные и после небольшой обработки записывает их в ClickHouse. Любая функциональность, доступная в интерфейсе Appmetrica, представляет собой просто запрос SELECT.\n",
      "\n",
      "ClickHouse используется для хранения и анализа логов различных сервисов в Яндексе. Типичным решением было бы использовать Logstash и ElasticSearch, но оно не работает на более-менее приличном потоке данных.\n",
      "\n",
      "ClickHouse подходит в качестве базы данных для временных рядов — так, в Яндексе она используется в качестве бэкенда для Graphite вместо Ceres/Whisper. Это позволяет работать более чем с триллионом метрик на одном сервере.\n",
      "\n",
      "ClickHouse используют аналитики для внутренних задач. По опыту использования внутри компании, эффективность работы ClickHouse по сравнению с традиционными методами обработки данных (скрипты на MR) выше примерно на три порядка. Это нельзя рассматривать как просто количественное отличие. Дело в том, что имея такую высокую скорость расчёта, можно позволить себе принципиально другие методы решения задач.\n",
      "\n",
      "Если аналитик получил задачу сделать отчёт, и если это хороший аналитик, то он не будет делать один отчёт. Вместо этого он сначала получит десяток других отчётов, чтобы лучше изучить природу данных и проверить возникающие при этом гипотезы. Зачастую имеет смысл смотреть на данные под разными углами, даже не имея при этом какой либо чёткой цели, — для того чтобы находить новые гипотезы и проверять их.\n",
      "\n",
      "Это возможно лишь в том случае, если скорость анализа данных позволяет проводить исследования в интерактивном режиме. Чем быстрее выполняются запросы, тем больше гипотез можно проверить. При работе с ClickHouse возникает такое ощущение, как будто у вас увеличилась скорость мышления.\n",
      "\n",
      "В традиционных системах данные, образно выражаясь, лежат мёртвым грузом на дне болота. С ними можно сделать что угодно, но это займёт много времени и будет очень неудобно. А если ваши данные лежат в ClickHouse, то это «живые» данные: вы можете изучать их в любых срезах и «сверлить» до каждой отдельной строчки.\n",
      "\n",
      "Выводы\n",
      "Так уж получилось, что Яндекс.Метрика является второй по величине системой веб-аналитики в мире. Объём поступающих в Метрику данных вырос с 200 млн событий в сутки в начале 2009 года до чуть более 20 млрд в 2015 году. Чтобы дать пользователям достаточно богатые возможности, но при этом не перестать работать под возрастающей нагрузкой, нам приходилось постоянно менять подход к организации хранения данных.\n",
      "\n",
      "Для нас очень важна эффективность использования железа. По нашему опыту, при большом объёме данных стоит беспокоиться не о том, насколько система хорошо масштабируется, а о том, насколько эффективно используется каждая единица ресурсов: каждое процессорное ядро, диск и SSD, оперативка, сеть. Ведь если ваша система уже использует сотни серверов, а вам нужно работать в десять раз эффективнее, то вряд ли вы сможете легко установить тысячи серверов, как бы хорошо система не масштабировалась.\n",
      "\n",
      "Для достижения максимальной эффективности важна специализация под конкретный класс задач. Не существует структуры данных, которая хорошо справляется с совершенно разными сценариями работы. Например, очевидно, что key-value база не подойдёт для аналитических запросов. Чем больше нагрузка на систему, тем большая специализация будет требоваться, и не стоит бояться использовать для разных задач принципиально разные структуры данных.\n",
      "\n",
      "Нам удалось сделать так, что Яндекс.Метрика является относительно дешёвой по железу. Это позволяет предоставлять бесплатный сервис даже для самых крупных сайтов и мобильных приложений. На этом поле у Яндекс.Метрики нет конкурентов. Для примера, если у вас есть популярное мобильное приложение, то вы можете бесплатно использовать Яндекс.Метрику для приложений, даже если ваше приложение популярнее, чем Яндекс.Карты.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Привет Хабр! Если вам были интересны публикации из нашего блога, то наверняка вам будет интересно принять участие во встрече экспертов в области Data Science и машинного обучения, которая пройдёт 31 августа (среда) в DI Telegraph (Москва, Тверская 7). На встрече будет обсуждаться широкий круг вопросов, связанных с применением алгоритмов машинного обучения для решения задач анализа больших данных, тематического моделирования и генеративных алгоритмов. \n",
      "\n",
      "\n",
      "Партнером и соорганизатором мероприятия выступаем мы — российская технологическая компания DCA (Data-Centric Alliance), специализирующаяся на работе с большими данными и высоконагруженными системами. Информация про формат и спикеров под катом.\n",
      "\n",
      "В митапе примут участие успешные аналитики, разработчики, исследователи — представители технологических компаний, заинтересованные в повышении интереса к большим данным и инструментам их обработки и анализа для решения актуальных проблем и готовые поделиться своими знаниями и опытом в этой области.\n",
      "\n",
      "Формат встречи: серия небольших выступлений экспертов, ответы экспертов на вопросы аудитории, неформальное общение участников.\n",
      "\n",
      " Спикеры:\n",
      "\n",
      "Андрей Селиванов ⬝ Data Scientist, DCA (Data-Centric Alliance)\n",
      "Тема доклада: Практическое применение латентного размещения Дирихле (LDA) (тематическое моделирование)Евгений Цацорин ⬝ Data Scientist, DCA (Data-Centric Alliance)\n",
      "Тема доклада: Vowpal Wabbit, оффлайн и онлайн-обучение на примере простейших моделей и больших данныхРостислав Яворский ⬝ Доцент департамента анализа данных и искусственного интеллекта факультета компьютерных наук НИУ ВШЭ. Кандидат физико-математических наук\n",
      "Тема доклада: уточняетсяАртём Семьянов ⬝ Data Scientist, Find Attractions\n",
      "Тема доклада: Применение генерирующих нейросетей на практике; использование generative adversarial nets (GANs) для генерации текста для описания изображения\n",
      " Сбор участников: 18.30 ⬝ Записать в календарь\n",
      "\n",
      "Участие бесплатное, но для того, чтобы попасть на мероприятие, пожалуйста, пройдите предварительную регистрацию: \n",
      "\n",
      "\n",
      "Всем спасибо за внимание, ждём! \n",
      "Хороших выходных.    \n",
      " \n",
      "\n",
      "На протяжении нескольких лет активисты и любители открытых данных в заранее согласованный день проводят более сотни мероприятий различных форматов в разных городах и странах. В этом году таким днем стало 4 марта, на которое уже запланировано 145 мероприятий в десятке стран. Одним из них будет День открытых данных в Москве (для участия нужно зарегистрироваться), для которого мы подготовили много интересного: от новых массивов данных, которые можно использовать на хакатоне, до мастер-классов и лекций не только по привычным для нас госфинансам, но и по бизнес-моделям проектов, использующих открытые данные, или по этике использования больших данных.\n",
      "\n",
      "Программу двухдневного мероприятия (4-5 марта) можно условно разделить на два формата: «лекторий» с мастер-классами, дискуссиями и выступлениями по нескольким направлениям («Бизнес», «Город», «Этика», «Медиа», «НКО») и хакатон.\n",
      "\n",
      "Секция «Бизнес»\n",
      "\n",
      "«Как зарабатывать на открытых данных?» — сложный вопрос, над которым рано или поздно задумываются практически все активисты открытых данных. Иван Бегтин, директор АНО «Информационная культура» и куратор этой секции, уже делился своими идеями на сайте roem.ru и в посте на Хабре. О монетизации проектов на основе открытых данных пишет и Open Data Institute, и лаборатория GovLab, и проект OpenData500, исследующий кейсы конкретных компаний в США, Канаде, Мексике и других странах. О российских примерах расскажут приглашенные представители компаний.\n",
      "\n",
      "Секция «Город»\n",
      "\n",
      "Помогают ли открытые данные повысить качество нашей жизни или препятствуют этому? Какие проблемы городской среды можно решить с помощью открытых данных? — эксперты, во главе с куратором секции Андреем Кармацким, автором канала в Телеграмме про городские данные и руководителем компании Urbica, специализирующейся на анализе и визуализации сложных массивов данных, поделятся своими знаниями по анализу данных, урбанистике и созданию городских сервисов. Кстати, за прошедшую неделю опубликовано несколько интересных исследований на эту тему: например, исследование Яндекса о том, какие районы Москвы пригодны для жизни, а какие для развлечений, исследование оператора общественного транспорта Лондона о передвижениях пассажиров на основе данных их смартфонов, или как анализировать посещаемость парка с помощью скамейки, разработанной в Бостоне.\n",
      "\n",
      "Секция «Этика»\n",
      "\n",
      "Артур Хачуян, руководитель компании SocialDataHub, занимающейся разработкой интеллектуальных систем анализа данных, будет модерировать обсуждение темной стороны больших данных, жизни в эпоху «Большого брата» и границы морально-этического аспекта использования технологий, собирающих огромный объем личной информации о гражданах из открытого доступа для неизвестных целей. По этой теме недавно вышло исследование о том, как дата-капиталисты наживаются на нашей приватности.\n",
      "\n",
      "Секция «Медиа»\n",
      "\n",
      "На этой секции участники обсудят, как открытые данные меняют СМИ, какие тренды есть в визуализации и дата-журналистике, и попробуют ответить на вопрос: Открытые данные в медиа — это диалог людей, «общества и государства» или «бизнеса и власти»? Курировать секцию будет Мария Пильгун, руководитель магистерской программы \"Журналистика данных\" в НИУ ВШЭ.\n",
      "\n",
      "Секция «НКО»\n",
      "\n",
      "Иван Засурский, президент Ассоциации интернет-издателей, будет модерировать обсуждение открытых лицензий и важности их использования для некоммерческих организаций, а спикеры секции расскажут о движении OpenAccess и поговорят об открытом доступе к результатам проектов НКО. Мало кто знает, что некоммерческие организации на сегодняшний день более закрытые, чем коммерческие компании или государственные госорганы, — мы надеемся, что эта секция станет первым шагом на пути к их открытости.\n",
      "\n",
      "Хакатон\n",
      "\n",
      "Почти ни одно мероприятие Инфокультуры не обходится без хакатона. День открытых данных — не исключение. В этот раз мы не просто приглашаем разработчиков, журналистов, аналитиков, дизайнеров, активистов и студентов и предлагаем общеизвестные наборы данных (например, данные криминальной статистики и ДТП, открытые данные по культуре, полезные ссылки по дата-журналистике, ресурсы по открытым бюджетам и госфинансам, всегда доступные данные по гос. контрактам проекта Госзатраты (проект КГИ)), но и работаем над получением новых датасетов. Среди «новинок» будут данные о жилых помещениях от ЦИАНа — это база из 117 тысяч объектов с адресами, координатами, ценами и кучей других показателей (на указанный набор данных есть два важных ограничения, ознакомиться с которыми можно тут), данные Института проблем правоприменения о десяти тысячах решений арбитражных судов первой инстанции, готовим также массивы данных о финансах СМИ и НКО.\n",
      "\n",
      "День открытых данных, на наш взгляд, — это важное событие для сообщества, поэтому мы запустили русскоязычный сайт, на котором с удовольствием разместим информацию о мероприятиях других российских городов. Предложения, пожелания, вопросы и даже запросы данных ждем в комментариях.    \n",
      " Введение\n",
      "В стеке технологий InterSystems есть технология для разработки аналитических решений DeepSee. Это встраиваемая аналитическая технология и набор инструментов для создания систем поддержки принятия эффективных решений, в том числе, и с применением прогнозных моделей. DeepSee работает со структурированными и неструктурированными данными. Она предназначена для создания OLAP-решений для баз данных Caché и любых реляционных СУБД. InterSystems DeepSee предоставляет разработчикам средства для внедрения в свои приложения аналитической OLAP-функциональности, которая способна работать на оперативных базах данных приложений без создания отдельной инфраструктуры для решения аналитических задач.\n",
      "В статье рассматривается пример создания в OLAP-куба, работа со средствами аналитики и построение пользовательского интерфейса на примере анализа котировок акций торгуемых на Московской Бирже. \n",
      "Этапы\n",
      "\n",
      "Получение данных\n",
      "ETL\n",
      "Построение куба\n",
      "Построение сводной таблицы\n",
      "Построение дашборда\n",
      "Визуализация\n",
      "\n",
      "Получение данных\n",
      "Для визуализации данных о котировках акций необходимо их сначала загрузить. У Московской Биржи есть публичное задокументированное API, которое предоставляет информацию о торговле акциями в форматах HTML, XML, JSON, CSV. \n",
      "Вот, к примеру, XML данные за 27 мая 2013 года. Создадим XML-Enabled класс Ticker.Data в платформе InterSystems:\n",
      "Ticker.DataClass Ticker.Data Extends (%Persistent, %XML.Adaptor)\n",
      "{\n",
      "\n",
      "/// Дата торгов\n",
      "Property Date As %Date(FORMAT = 3, XMLNAME = \"TRADEDATE\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Краткое название компании\n",
      "Property Name As %String(XMLNAME = \"SHORTNAME\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Тикер\n",
      "Property Ticker As %String(XMLNAME = \"SECID\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Количество сделок\n",
      "Property Trades As %Integer(XMLNAME = \"NUMTRADES\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Общая сумма сделок\n",
      "Property Value As %Decimal(XMLNAME = \"VALUE\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Цена открытия\n",
      "Property Open As %Decimal(XMLNAME = \"OPEN\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Цена закрытия\n",
      "Property Close As %Decimal(XMLNAME = \"CLOSE\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Цена закрытия официальная\n",
      "Property CloseLegal As %Decimal(XMLNAME = \"LEGALCLOSEPRICE\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Минимальная цена акции\n",
      "Property Low As %Decimal(XMLNAME = \"LOW\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Максимальная цена акции\n",
      "Property High As %Decimal(XMLNAME = \"HIGH\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Средневзвешенная цена акции http://www.moex.com/s1194\n",
      "/// Может считаться как за день так и не за период.\n",
      "Property Average As %Decimal(XMLNAME = \"WAPRICE\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "/// Количество акций участвовавших в сделках\n",
      "Property Volume As %Integer(XMLNAME = \"VOLUME\", XMLPROJECTION = \"attribute\");\n",
      "\n",
      "}\n",
      "И напишем загрузчик данных в формате XML. Так как класс у нас XML-Enabled то конвертация из XML в объекты класса Ticker.Data происходит автоматически. Аналогичного поведения можно достичь для данных в форматах JSON (через динамические объекты) и CSV (используя %SQL.Util.Procedures). Так как API отдаёт данные за определённую дату (день) то нам надо итерировать по дням и сохранять поступающие данные. Кроме того данные о котировках акций приходят страницами по 100 записей. Загрузчик может выглядеть так:\n",
      "Загрузчик данных/// Загрузить информацию об акциях начиная с From и заканчивая To. Purge - удалить все записи перед началом загрузки\n",
      "/// Формат From, To - YYYY-MM-DD\n",
      "/// Write $System.Status.GetErrorText(##class(Ticker.Loader).Populate())\n",
      "ClassMethod Populate(From As %Date(DISPLAY=3) = \"2013-03-25\", To As %Date(DISPLAY=3) = {$ZDate($Horolog,3)}, Purge As %Boolean = {$$$YES})\n",
      "{\n",
      "    #Dim Status As %Status = $$$OK\n",
      "    // Переводим даты во внутренний формат для простоты итерации\n",
      "    Set FromH = $ZDateH(From, 3)\n",
      "    Set ToH = $ZDateH(To, 3)\n",
      "\n",
      "    Do:Purge ..Purge()\n",
      "\n",
      "    For DateH = FromH:1:ToH {\n",
      "        Write $c(13), \"Populating \", $ZDate(DateH, 3)\n",
      "        Set Status = ..PopulateDay(DateH)\n",
      "        Quit:$$$ISERR(Status)\n",
      "    }\n",
      "\n",
      "    Quit Status\n",
      "}\n",
      "\n",
      "/// Загрузить данные за день. Данные загружаются страницами по 100 записей. \n",
      "/// Write $System.Status.GetErrorText(##class(Ticker.Loader).PopulateDay($Horolog))\n",
      "ClassMethod PopulateDay(DateH As %Date) As %Status\n",
      "{\n",
      "    #Dim Status As %Status = $$$OK\n",
      "\n",
      "    Set Reader = ##class(%XML.Reader).%New()\n",
      "    Set Date = $ZDate(DateH, 3) // Преобразовать дату из внутреннего формата в YYYY-MM-DD\n",
      "    Set Count = 0 // Число загруженных записей\n",
      "\n",
      "    While Count '= $G(CountOld) {\n",
      "        Set CountOld = Count\n",
      "        Set Status = Reader.OpenURL(..GetURL(Date, Count)) // Получаем следующую страницу данных\n",
      "        Quit:$$$ISERR(Status)\n",
      "\n",
      "        // Устанавливаем соответствие нода row == объект класса Ticker.Data \n",
      "        Do Reader.Correlate(\"row\", \"Ticker.Data\")\n",
      "\n",
      "        // Десериализуем каждую ноду row в объект класса Ticker.Data\n",
      "        While Reader.Next(.Object, .Status) {\n",
      "            #Dim Object As Ticker.Data\n",
      "\n",
      "            // Сохраняем объект\n",
      "            If Object.Ticker '=\"\" {\n",
      "                Set Status = Object.%Save()\n",
      "                Quit:$$$ISERR(Status)\n",
      "                Set Count = Count + 1\n",
      "            }\n",
      "        }\n",
      "        Quit:(Count-CountOld)<100 // На текущей странице меньше 100 записей => эта страница - последняя\n",
      "    }\n",
      "    Quit Status\n",
      "}\n",
      "\n",
      "/// Получить URL с информацией о котировках акций за дату Date, пропустить первые Start записей\n",
      "ClassMethod GetURL(Date, Start As %Integer = 0) [ CodeMode = expression ]\n",
      "{\n",
      "$$$FormatText(\"http://iss.moex.com/iss/history/engines/stock/markets/shares/boards/tqbr/securities.xml?date=%1&start=%2\", Date, Start)\n",
      "}\n",
      "\n",
      "Теперь загрузим данные командой: Write $System.Status.GetErrorText(##class(Ticker.Loader).Populate())\n",
      "Весь код доступен в репозитории.\n",
      "ETL\n",
      "Как известно, для построения OLAP-куба в первую очередь необходимо сформировать таблицу фактов: таблицу операций, записи которой требуется группировать и фильтровать. Таблица фактов может быть связана с другими таблицами по схеме звезда или снежинка. \n",
      "Таблица фактов для куба обычно является результатом работы аналитиков и разработчиков по процессу, который называется ETL (extract, transform, load). Т.е. из данных предметной области делается “выжимка” необходимых для анализа данных, и переносится в удобную для хранилища структуру \"звезда\"/\"снежинка\": факты и справочники фактов. \n",
      "В нашем случае этап ETL пропустим т.к. наш класс Ticker.Data уже находятся во вполне удобном для создания куба состоянии.\n",
      "Построение куба\n",
      "DeepSee Architect — это веб-приложение для создания OLAP-куба. Для перехода к DeepSee Architect откроем Портал Управления Системой → DeepSee → Выбор области → Architect. Открывается рабочее окно Архитектора.\n",
      "Возможно нужно будет выбрать область, которая поддерживает DeepSee. В том случае если вы не видите вашей области в списке областей DeepSee перейдите в Портал Управления Системой → Меню → Управление веб-приложениями → /csp/область, и там в поле Включен поставьте галочку DeepSee и нажмите кнопку сохранить. После этого выбранная область должна появиться в списке областей DeepSee. \n",
      "Создаем новый куб.\n",
      "Нажав на кнопку \"Создать\" попадаем на экран создания нового куба, там необходимо установить следующие параметры:\n",
      "\n",
      "Имя куба — название куба используемое в запросах к нему\n",
      "Отображаемое Имя — локализуемое название куба (перевод осуществляется стандартными механизмами InterSystems)\n",
      "Источник Cube — использовать таблицу фактов или другой куб в качестве источника данных\n",
      "Исходный класс — если на предыдущем шаге был выбран класс, то указываем в качестве таблицы фактов класс Ticker.Data.\n",
      "Имя класса для куба — имя класса, в котором будет храниться определение куба. Создаётся автоматически\n",
      "Описание класса — произвольное описание\n",
      "\n",
      "Вот как выглядит наш новый куб:\n",
      "\n",
      "Определяем свойства куба\n",
      "После нажатия кнопки OK будет создан новый куб:\n",
      "\n",
      "Слева выводятся свойства базового и связанных с ним по “снежинке” классов, которые можно использовать при построении куба.\n",
      "Центральная часть экрана — это скелет куба. Его можно наполнить свойствами класса с помощью drag-n-drop из области базового класса, либо добавляя элементы вручную. Основными элементами куба являются измерения, показатели и списки.\n",
      "Измерения (Dimensions)\n",
      "Измерения — это элементы куба, которые группируют записи таблицы фактов. В измерения обычно относят “качественные” атрибуты базового класса, которые разбивают все записи таблицы фактов по тем или иным срезам. Например нам бы хотелось группировать все факты по названиям инструментов и по датам.\n",
      "Для разбиения фактов по тикерам прекрасно подойдет свойство Ticker. Перетянем Ticker на область измерений — в результате Архитектор добавит в куб измерение Ticker с одной иерархией H1 и одним уровнем Ticker. Укажем отображаемые названия в подписях к измерению и уровню.\n",
      "\n",
      "Измерения помимо группировки позволяют строить иерархии вложенности фактов от общего к частному. Типичным примером является измерение по дате, которое обычно часто требуется представить в виде иерархии Год-Месяц-День.\n",
      "Для свойств типа дата (например как у свойства Date тип %Date) в DeepSee есть специальный тип измерения time, в котором уже предусмотрены часто используемые функции для создания иерархий по дате. Воспользуемся этим и построим трехуровневую иерархию Год-месяц-день с помощью свойства Date.\n",
      "\n",
      "Заметим, что в измерении есть элементы: собственно измерение, иерархия и уровни этой иерархии (Level). Любое измерение куба состоит как минимум из одной иерархии в котором в простейшем случае всего один уровень. \n",
      "Показатели (Measures)\n",
      "Показатели или метрики это такие элементы куба, куда относят какие-либо \"количественные\" данные, которые необходимо посчитать для \"качественных\" измерений куба (Dimensions).\n",
      "Например в таблице фактов такими показателями могут быть свойства Volume (количество акций) и Average (Средняя цена). Перетянем свойство Volume на область показателей и создадим показатель \"Количество\" с функцией SUM, которая будет считать общее количество акций в текущем срезе. \n",
      "Добавим также в показатели свойство Average и укажем в качестве функции расчета MAX — расчет максимального значения. С целью использования цены для визуализации изменения максимальной цены акции во времени. \n",
      "\n",
      "Списки (Listings)\n",
      "Списки — это элементы куба, описывающие способ доступа к исходным данным куба, позволяя перейти от агрегированных к исходным данным куба. Как правило при работе с кубом, аналитик просматривает агрегированную информацию в различных срезах. Однако, часто возникает необходимость посмотреть на исходные факты, которые вошли в текущий срез. Для этого и создаются листинги — они перечисляют набор полей таблицы фактов, который нужно отобразить при переходе к просмотру фактов Drillthrough. Создадим простой листинг нажав кнопку \"Добавить элемент\":\n",
      "\n",
      "Теперь зададим поля таблицы фактов, которые надо выводить. Например выведем информацию о тикерах и колебаних их цены за день (Name, Ticker, \"Open\", CloseLegal, Low, Average, High):\n",
      "\n",
      "Компиляция куба\n",
      "Итак мы добавили в куб два показателя, два измерения и один листинг — этого вполне достаточно и уже можно посмотреть, что получилось. \n",
      "Скомпилируем класс куба (Кнопка \"Компилировать\"). Если ошибок компиляции нет, значит куб создан правильно и можно наполнить его данными. \n",
      "Для этого нужно нажать \"Построить куб\" — в результате DeepSee загрузит данные из таблицы фактов в хранилище данных куба. \n",
      "Для работы с данными куба нам пригодится другое веб-приложение — DeepSee Analyzer.\n",
      "Построение сводной таблицы (Pivot)\n",
      "DeepSee Analyzer — визуальное средство для непосредственного анализа данных кубов и подготовки источников данных для дальнейшей визуализации. Для перехода к DeepSee Analyzer откроем Портал Управления Системой → DeepSee → Выбор области → Analyzer. Открывается рабочее окно Аналайзера.\n",
      "\n",
      "В рабочем окне Аналайзера слева мы видим элементы созданного куба: показатели и измерения. Комбинируя их мы строим запросы к кубу на языке MDX — аналоге языка SQL для многомерных OLAP кубов.\n",
      "Рассмотрим интерфейс Аналайзера. Справа — поле сводной таблицы. В поле сводной таблицы Аналайзера всегда показывается результат выполнения MDX-запроса. Посмотреть текущий MDX-запрос можно если нажать кнопку . При первом открытии куба в поле сводной таблицы по умолчанию показывается количество записей в таблице фактов — в нашем случае это количество записей в классе Ticker.Data. Этому соответствует MDX: SELECT FROM [TICKER].\n",
      "Чтобы создать сводную таблицу перетянем в поле колонок измерение “Год”. Показателем выберем \"Объём\". В результате получим таблицу количества проданных акций по годам. \n",
      "\n",
      "Далее перетянем измерение “Тикер” в поле колонок и получим уже сводную таблицу количества акций по инструментам, с разбиением по годам:\n",
      "\n",
      "Сейчас для каждой ячейки полученной таблицы рассчитывается одна величина — суммарное количество акций участвовавших в сделках (в случае если не выбран ни один показатель, считается количество фактов — в данном случае это можно интерпретировать как количество дней торговли инструмента). Это можно изменить. Добавим показатель \"Средняя цена\". В результате можно видеть уже более интересную картину: сводная таблица отображает среднюю максимум цены по каждому инструменту за год.\n",
      "\n",
      "Как мы помним, в определении куба у нас заложена иерархия по датам. Это значит что по измерению Дата возможна операция DrillDown (переход по иерархии измерения от общего к частному). В Аналайзере двойной щелчок по заголовку измерения приводит переходу к следующему по иерархии измерению (DrillDown). В данном случае двойной клик по году приведет к переходу к месяцам этого года, а двойной клик на месяце — к переходу на уровень дней. В итоге можно посмотреть как менялась средняя цена акции для дней или месяцев.\n",
      "\n",
      "На предыдущем этапе мы создали листинг — инструмент перехода от агрегированных данных к исходным фактам. Выберем любую строку сводной таблицы и нажмём кнопку  для перехода к листингу:\n",
      "\n",
      "Следующий этап — визуализация. Перед сохранением упростим сводную таблицу и сохраним её под именем TickersByYears.\n",
      "\n",
      "Построение дашборда (Dashboard)\n",
      "Портал Пользователя — это веб-приложение для создания и использования дашбордов (панелей индикаторов). Дашборды содержат виждеты: таблицы, графики и карты на основе сводных таблиц, созданных аналитиками в Аналайзере.\n",
      "Для перехода к Порталу Пользователя DeepSee откроем Портал Управления Системой → DeepSee → Выбор области → Портал Пользователя. \n",
      "\n",
      "Создадим новый дашборд нажав на стрелку справа → добавить → Добавить индикаторную панель\n",
      "\n",
      "Создадим виджет нажав на стрелку справа → Виджеты → \"+\" → Линейная диаграмма с маркерами. В качестве источника данных выберем TickersByYears:\n",
      "\n",
      "Однако читатель возразит — это же средняя температура по больнице. И будет прав. Добавим фильтрацию по инструменту. Для этого нажмём стрелку справа → Виджеты → Виджет 1 → Элементы управления → \"+\". Форма создания нового фильтра выглядит следующим образом:\n",
      "\n",
      "А вот так выглядит наш виджет с фильтром. Пользователь может изменить значение фильтра на любое другое.\n",
      "\n",
      "После этого сохраним дашборд.\n",
      "Установка MDX2JSON и DeepSeeWeb\n",
      "Для визуализации созданного дашборда можно использовать следующие OpenSource решения:\n",
      "\n",
      "MDX2JSON — REST API предоставляет информацию о кубах, пивотах, дашбордах и многих других элементах DeepSee, в частности — результатах исполнения MDX запросов, что позволяет встраивать пользовательский интерфейс аналитического решения на DeepSee в любое современное Web или мобильное приложение.\n",
      "DeepSeeWeb — AngularJS приложение, предоставляющее альтернативную реализацию портала пользователя DeepSee. Может быть легко кастомизирован. Использует MDX2JSON в качестве бэкэнда. Вот пример дашборда визуализированного в DeepSeeWeb:\n",
      "\n",
      "\n",
      "Установка MDX2JSON\n",
      "Для установки MDX2JSON надо:\n",
      "\n",
      "Загрузить Installer.xml и импортировать его в любую область с помощью Studio, Портала Управления Системой или Do $System.OBJ.Load(file).\n",
      "Выполнить в терминале (пользователем с ролью %ALL): Do ##class(MDX2JSON.Installer).setup()\n",
      "\n",
      "Для проверки установки надо открыть в браузере страницу http://server:port/MDX2JSON/Test?Debug. Возможно потребуется ввести логин и пароль (в зависимости от настроек безопасности сервера). Должна открыться страница с информацией о сервере. В случае получения ошибки, можно почитать на Readme и Wiki.\n",
      "Установка DeepSeeWeb\n",
      "Для установки DeepSeeWeb надо:\n",
      "\n",
      "Загрузить установщик и импортировать его в любую область с помощью Studio, Портала Управления Системой или Do $System.OBJ.Load(file).\n",
      "Выполнить в терминале (пользователем с ролью %ALL): Do ##class(DSW.Installer).setup()\n",
      "\n",
      "Для проверки установки надо открыть в браузере страницу http://server:port/dsw/index.html. Должна открыться станица авторизации. В области SAMPLES представлено множество уже готовых дашбордов и все они автоматически отображаются в DeepSeeWeb.\n",
      "Визуализация\n",
      "Откроем http://server:port/dsw/index.html и авторизируемся, также нужно указать область с кубом. Откроется список дашбордов, в нашем случае есть только один созданный дашборд \"Акции\". Откроем его:\n",
      "\n",
      "Отображается наш созданный виджет. Для него поддерживается Drilldown и фильтр созданный в Портале Пользователя DeepSee:\n",
      "\n",
      "Выводы\n",
      "InterSystems DeepSee является мощным инструментом создания OLAP-решений, предоставляя разработчикам средства для создания и внедрения в свои приложения аналитической OLAP-функциональности, которая способна работать на оперативных базах данных приложений без создания отдельной инфраструктуры для решения аналитических задач. \n",
      "Ссылки\n",
      "\n",
      "DeepSee\n",
      "Вводный вебинар по DeepSee\n",
      "Документация\n",
      "Репозиторий\n",
      "MDX2JSON\n",
      "DeepSeeWeb\n",
      "API Московской Биржи\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Термин «большие данные» давно уже звучит привычно, и многие даже понимают, что это такое на самом деле и как его использовать. В то же время, специалисты по анализу данных придумали множество других градаций собираемой информации, в зависимости от размера, востребованности, актуальности и так далее. Удивительно, но данные могут быть «быстрыми», «горячими», «длинными» и «медленными», даже «грязными». Хотя весь этот аналитический зоопарк и не помог многочисленным аналитикам правильно предсказать решение британцев выйти из ЕС и победу Трампа. \n",
      "\r\n",
      "Большие данные — это не просто очень большие массивы информации, но совокупность подходов, методов и инструментов обработки различных данных колоссальных объёмов. \r\n",
      "Большие данные — не просто сведения, это социально-экономический феномен, который обязан своим появлением необходимости анализировать огромные массивы информации в мировом масштабе. \n",
      "\r\n",
      "Big Data опираются на три V: volume (объём), variety (разнообразие) и velocity (скорость). С объёмом всё понятно. Разнообразие зависит от широты спектра источников, питающих базы данных. А скорость вообще главный показатель современного мира, который не останавливается ни на секунду. \n",
      "\r\n",
      "А можно ли, к примеру, считать «большими данными» соцопросы, пусть даже охватывающие тысячи человек? Объём информации, которую можно получить из всевозможных опросов достаточно велик, но всё же не настолько, поэтому её можно отнести скорее к «средним данным». Наверное, если бы предвыборная аналитика охватывала миллионы респондентов, то это уже были бы «большие данные». Также Big Data может складываться из кирпичиков «маленьких данных». \n",
      "\r\n",
      "Одним из трендов сегодня являются «быстрые данные». В современном мире всё происходит молниеносно. В приложениях и социальных сетях информация, которой 1-2 часа, уже не актуальна, на кону каждая секунда. Быстрые данные важны и для банковских приложений, и для приложений социальных сетей, и особенно для мессенджеров. Каждую секунду пользователи получают новые уведомления, на основе которых принимают важные для себя решения. \n",
      "\r\n",
      "Для того, чтобы накопить «медленные данные», потребуется достаточно много времени. В отличие от быстрых данных, которые можно получить с помощью моментального опроса, медленные накапливаются буквально по крупице. Например, вы проводите опрос участников конференции по разработке. Каждый участник опрашивается до, во время и после мероприятия. Затем вся информация очень тщательно обрабатывается и суммируется.\n",
      "\r\n",
      "А когда длительность накопления начнёт измеряться веками, медленные данные превратятся в «длинные». Так как эпоха Big Data началась сравнительно недавно, то сегодня длинные данные нужно искать не в интернете, а в книгах, манускриптах, на стенах памятников архитектуры и при археологических раскопках. Исторический аспект может оказаться очень важным для конкретного исследования! \n",
      "\r\n",
      "Хотя данные и не пирожки, они могут быть «горячими» и «холодными». Здесь работает принцип «свежести»: более «свежие» — горячие — данные представляют бо̒льшую ценность. Для простого пользователя долгожданный комментарий в мессенджере «свежестью» в 10 секунд более важен, чем уже «холодный» комментарий, созданный 2 часа назад. Конечно, он ещё может быть полезен, например, чтобы уточнить какой-то факт из переписки: вспомнить название предложенной другом книги или фильма, уточнить время встречи, и так далее. Доступ к горячим данным должен быть постоянным. Холодные данные нужны нам не так часто, поэтому и постоянный доступ к ним — отнюдь не первая необходимость. \n",
      "\r\n",
      "Помимо характеристики размера, скорости или температуры, данные могут классифицироваться и по их чистоте. «Грязными» называют такие данные, которые либо ошибочны, либо содержат неполную или непоследовательную информацию, и обычно они практически бесполезны. Грязные данные составляют большую часть информации, накопленной во многих компаниях. В то же время здесь могут попадаться настоящие информационные сокровища — ценные долгосрочные идеи. Но от грязных данных хватает и неприятностей. Как утверждают в компании GovTechWorks, такая неструктурированная и нерелевантная информация обходится американским компаниям в $6 миллиардов ежегодно! \n",
      "\n",
      "\n",
      "\r\n",
      "Термин «ответственные данные», описывает ситуацию, когда накапливается только достоверная информация, которая берётся из проверенных источников, хранится и передаётся с соблюдением строгих мер безопасности. \n",
      "\r\n",
      "«Толстые данные» — это следующий шаг после того, как мы наиграемся с big data: помимо количественных характеристик здесь учитываются и качественные. То есть одних только сухих цифр в гигантских объёмах уже недостаточно для глубокого понимания тенденций и протекающих процессов, для полноты анализа необходимо принимать во внимание такие вещи, как, например, человеческие эмоции.\n",
      "\n",
      "Большие данные правят миром\r\n",
      "При таком разнообразии определений возникает вопрос: какие же они на самом деле, эти данные? В первую очередь, большие, гигантские! Big Data собираются рядом с нами, вокруг нас и даже о каждом из нас. Маленькие песчинки медленно и верно формируют их. \n",
      "\r\n",
      "На память сразу приходит популярная фраза «Большой брат следит за тобой». Из собираемых повсеместно обрывков информации складываются определенные базы данных, используемые для тех или иных исследований и манипулирования общественным мнением. Впоследствии вся полученная информация анализируется, и происходит так называемое гадание об исходе важных событий. Это гадание порождает всевозможные прогнозы по поводу побед на выборах, изменениям политической обстановки в стране, или колебаниях популярности какой-либо музыкальной группы среди молодежи. \n",
      "\n",
      "\n",
      "\r\n",
      "Звание чемпионов по сбору Big Data заслужили такие три кита, как Google, Facebook и Amazon. Эти корпорации фиксируют малейший щелчок мышки каждого пользователя их порталов. И всё это ради глобального сбора информации. На большие данные возлагаются серьёзные надежды. Исследователи предрекают их огромное влияние на все отрасли человеческой жизни и деятельности. Не обошла эта участь и медицину, и науку.\n",
      "\r\n",
      "Чем же Big Data могут быть полезны в медицине? Дело тут даже не в размере накопления информации, а в методиках её обработки и анализа. Объём медицинских данных в ряде сфер уже давно достиг размеров, что их проблематично не то что обрабатывать, а даже хранить. Самый яркий пример — расшифровка человеческого генома, состоящего более чем из 3 миллиардов знаков. На эту работу под эгидой Национальной организации здравоохранения США ушло 13 лет (с 1990 года по 2003-й). В 2017-м же, благодаря росту мощности компьютеров и развитию теоретического и программного инструментария, на подобную задачу потребуются недели, а то и дни.\n",
      "\r\n",
      "Основной задачей больших данных в медицине является создание максимально полных и удобных реестров медицинской информации с возможностью взаимного обмена, что позволит повсеместно ввести полные электронные карты пациентов, содержащих всю медицинскую историю с момента рождения. Это позволит значительно оптимизировать работу учреждений здравоохранения.\n",
      "\r\n",
      "Но давайте вернёмся к последним нашумевшим событиям, которые в прямом смысле этого слова перевернули мировой интернет — победа Дональда Трампа на выборах. Хотя его победа оказалась неожиданностью для множества людей, в том числе аналитиков и политтехнологов, вероятно, во многом это всё же закономерный результат грамотного использования больших данных.\n",
      "\r\n",
      "Швейцарский журнал Das Magazin утверждает, что эту победу обеспечили пара ученых, Big Data и современные технологии. Некто Михаль Косински разработал уникальную систему, которая позволяет выяснить максимум информации о человеке только лишь по его лайкам в соцсетях — так называемый «микротаргетинг». Позднее разработка Косински, против его желания, начала использоваться в крупных политических играх. Позже та же система сработала в предвыборной кампании американского бизнесмена. Никто и не догадывался о связи политика с аналитической компанией, ведь на столе Дональда нет даже компьютера. Но нынешний президент США выдал себя сам. Он написал в своём аккаунте Твиттер, что скоро его будут называть Mr. Brexit.\n",
      "\r\n",
      "В своей предвыборной кампании Хиллари Клинтон действовала традиционно — обращалась к разным группам населения страны, составляя отдельные обращения к чернокожему населению и женщинам. Cambridge Analytica действовали по-другому. Закупив базы данных совершеннолетних жителей США, они изучали каждого из них по методу OCEAN, учитывая личные предпочтения и интересы. В зависимости от своего характера и склада ума, каждому человеку из баз данных отправлялись послания с призывом проголосовать за клиента Cambridge Analytica, причём обоснование было подобрано в зависимости от ранее построенного индивидуального профиля адресата. Некоторые из сообщений даже были построены на принципе противоречия, и предлагали голосовать за Хиллари.\n",
      "\r\n",
      "Косински, учёный, придумавший систему микротаргетинга, пока лишь наблюдает за таким использованием его разработки со стороны. По словам Михаля, не его вина в том, что изобретение стало бомбой в чужих руках. Надо подчеркнуть, что публикация швейцарского журнала подверглась критике со стороны многочисленных европейских СМИ, которые заявляют о бездоказательности приведённой информации.\n",
      "\r\n",
      "Пока обсуждается вопрос о том, действительно ли большие данные повлияли на выборы в США, эти данные продолжают изучаться и систематизироваться. Берегитесь социальных сетей — кто знает, за кого вы ещё проголосуете или что побежите покупать, испытав на себе воздействие больших данных?    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      "(с)\n",
      "\r\n",
      "Гитхаб — это не просто площадка для хостинга и совместной разработки IT-проектов, но и огромная база знаний, составленная сотнями экспертов. К счастью, сервис предоставляет не просто инструменты для работы с открытым исходным кодом, но и качественные материалы для обучения. Мы выбрали некоторые популярные репозитории и отсортировали их по количеству звезд в порядке убывания.\n",
      "\r\n",
      "Эта подборка поможет разобраться, на какие именно репозитории стоит обратить внимание, если вас интересует работа с данными и сфера глубокого обучения.\n",
      "\n",
      "Data Science\n",
      "The Open Source Data Science Masters\r\n",
      "Звезды: 11 227, форки: 4 737\n",
      "\r\n",
      "Официальный репозиторий учебной программы Data Science Masters, разработанной в качестве альтернативы с открытым исходным кодом формального образования в области Data Science. Репозиторий представляет собой сборник обучающих материалов, собранных за несколько лет.\n",
      "\n",
      "Awesome Data Science\r\n",
      "Звезды: 9 240, форки: 2 761\n",
      "\r\n",
      "Мощная подборка, отвечающая на вопросы: «что такое Data Science?» и «что нужно знать, чтобы хорошо разбираться в этой науке?». Удобно разбита на категории. Например, есть список книг по Data Science, подборка инфографик и даже тематические группы в Фейсбук.\n",
      "\n",
      "Jupyter Interactive Notebook\r\n",
      "Звезды: 5 242, форки: 2 313\n",
      "\r\n",
      "Прародитель этого репозитория — платформа для работы со скриптами на 40 языках программирования Data Science iPython Notebooks, набравшая более 14 000 звезд и 4 000 форков. Специалисты по обработке данных и машинному обучению активно её использовали для научных вычислений.\n",
      "\r\n",
      "Сегодня Jupyter Notebook — это удобный набор файлов-блокнотов, состоящих из параграфов, в которых пишутся и исполняются запросы. С помощью встроенных визуализаторов блокнот с набором запросов превращается в полноценный дашборд с данными.\n",
      "\n",
      "Data Science Blogs\r\n",
      "Звезды: 4 510, форки: 1 178\n",
      "\r\n",
      "Простой, но обширный список обучающих материалов, отсортированный в алфавитном порядке. Здесь вы найдете все популярные блоги, а также множество небольших сайтов с полезной информацией (всего перечислен 251 ресурс).\n",
      "\n",
      "Data Science Specialization\r\n",
      "Звезды: 3 114, форки: 27 184\n",
      "\r\n",
      "Репозиторий образовательного курса по Data Science Университета Джонса Хопкинса — очень популярный курс, подготовленный Роджером Пеном, Джеффом Ликом и Брайаном Каффо. Если быть точнее, то программа обучения по специальности «Наука о данных» на Coursera включает несколько взаимосвязанных курсов по разным темам (например, R Programming), касающимся всевозможных аспектов анализа данных, а представленный в подборке репозиторий объединяет информацию, используемую во всех курсах.\n",
      "\n",
      "Spark Notebook\r\n",
      "Звезды: 2 677, форки: 587\n",
      "\r\n",
      "Spark Notebook — это блокнот с открытым исходным кодом, предоставляющий интерактивный веб-редактор, который может объединять код Scala, SQL-запросы, Markup и JavaScript для совместного анализа и изучения данных.\n",
      "\n",
      "Learn Data Science\r\n",
      "Звезды: 2 129, форки: 1 210\n",
      "\r\n",
      "Коллекция блокнотов iPython, ориентированных на фундаментальные концепции машинного обучения для новичков.\n",
      "\n",
      "Data Science at the Command Line\r\n",
      "Звезды: 2 057, форки: 503\n",
      "\r\n",
      "Репозиторий содержит тексты, данные, сценарии и пользовательские инструменты консоли, используемые в книге «Data Science at the Command Line». Это практическое руководство демонстрирует, как комбинировать небольшие, но мощные инструменты командной строки для быстрого получения, очистки, исследования и моделирования данных.\n",
      "\n",
      "Data Science Specialization Community Site\r\n",
      "Звезды: 1 395, форки: 2 661\n",
      "\r\n",
      "Несколько студентов, проходивших курс в Университете Джонса Хопкинса, создали настолько качественный контент, что сотрудники университета разместили его в общем доступе, а также сделали каталог для всего интересного контента, созданного сообществом.\n",
      "\n",
      "Визуализация данных для веба\n",
      "D3\r\n",
      "Звезды: 81 837, форки: 20 282\n",
      "\r\n",
      "D3 — это библиотека визуализации данных JavaScript для HTML и SVG. В D3 акцент сделан на веб-стандартах, благодаря чему вы можете использовать все возможности современных браузеров, не привязывая себя к проприетарной структуре, сочетая мощные компоненты визуализации, управляемый подход и взаимодействие с Document Object Model (DOM). Это самый популярный проект визуализации данных на GitHub.\n",
      "\n",
      "Chart.js\r\n",
      "Звезды: 41 393, форки: 9 294\n",
      "\r\n",
      "Chart.js — библиотека HTML5, создающая визуализацию через элемент <cаnvas>. Chart.js позиционирует себя как простой и гибкий инструмент, интерактивный, поддерживающий шесть различных типов диаграмм.\n",
      "\n",
      "ECharts\r\n",
      "Звезды: 32 204, форки: 9 369\n",
      "\r\n",
      "ECharts — браузерная библиотека для построения графиков и визуализации. Проста в использовании, интуитивно понятна и легко настраивается.\n",
      "\n",
      "Leaflet\r\n",
      "Звезды: 23 810, форки: 3 937\n",
      "\r\n",
      "Библиотека JavaScript для создания интерактивных карт, ориентированных на мобильное применение. Код библиотеки невероятно мал — она разработана для простого, быстрого и удобного использования. Функции Leaflet могут быть расширены через набор плагинов.\n",
      "\n",
      "Sigma.js\r\n",
      "Звезды: 8 348, форки: 1 305\n",
      "\r\n",
      "JS-библиотека, ориентированная на рисование графов. Sigma позволяет разрабатывать представления графов на веб-страницах и интегрировать их в веб-приложения.\n",
      "\n",
      "Vega\r\n",
      "Звезды: 6 559, форки: 702\n",
      "\r\n",
      "Vega — декларативный язык для создания, сохранения и обмена интерактивными проектами визуализации. С его помощью можно описать внешний вид и интерактивное поведение визуализации в формате JSON, а также создавать веб-представления с использованием Canvas или SVG. Vega предоставляет базовые строительные блоки для широкого спектра проектов визуализации: загрузка и преобразование данных, масштабирование, проекции карты, условные обозначения, графические метки и т.д.\n",
      "\n",
      "DC.js\r\n",
      "Звезды: 6 458, форки: 1 734\n",
      "\r\n",
      "DC.js — многомерная диаграмма, построенная на D3.js для работы с кроссфильтром. DC.js рендерит в формате SVG, совместимом с CSS. Предназначена для мощного анализа данных как в браузере, так и на мобильных устройствах.\n",
      "\n",
      "Epoch\r\n",
      "Звезды: 4 949, форки: 290\n",
      "\r\n",
      "Универсальная библиотека визуализации в реальном времени. Фокусируется на двух различных аспектах: базовые диаграммы для создания исторических отчетов и диаграммы в реальном времени для отображения часто обновляемых данных временных рядов.\n",
      "\n",
      "Глубокое обучение\n",
      "Keras\r\n",
      "Звезды: 37 611, форки: 14 344\n",
      "\r\n",
      "Keras — библиотека глубокого обучения на Python, которая используется как в TensorFlow, так и в Theano (да, вы можете запускать её поверх библиотек TensorFlow, Theano и CNTK). Keras разработана для быстрого экспериментирования, так как ключом к проведению хороших исследований является способность переходить от идеи к результату с наименьшей задержкой. Благодаря основательной и доступной документации Keras по праву занимает место в нашей подборке.\n",
      "\n",
      "Caffe\r\n",
      "Звезды: 26 892, форки: 16 276\n",
      "\r\n",
      "Caffe (Convolution Architecture For Feature Extraction) — библиотека глубокого обучения, связывающая Python и MATLAB. По сути, это библиотека общего назначения, предназначенная для развёртывания свёрточных сетей и для распознавания изображений, речи или мультимедиа.\n",
      "\r\n",
      "Также существует проект Caffe2, который включает в себя новые возможности, в частности, рекуррентные нейронные сети. В мае 2018 г. команды Caffe2 и PyTorch объединились, код Caffe2 был перенесен в репозиторий PyTorch (звезд: 24 075, форки: 5 707).\n",
      "\n",
      "MXNet\r\n",
      "Звезды: 16 157, форки: 5 824\n",
      "\r\n",
      "Легкая, компактная, гибко распределенная среда глубокого обучения для Python, R, Julia, Scala, Go, JavaScript и др. Для большей производительности MXNet позволяет смешивать императивные и символические методы программирования. Проект также содержит руководства по созданию других систем глубокого обучения.\n",
      "\n",
      "Data Science IPython Notebooks\r\n",
      "Звезды: 14 747, форки: 4 410\n",
      "\r\n",
      "Коллекция блокнотов iPython, включающая большие данные, Hadoop, scikit-learn, библиотеки, предназначенные для научных вычислений, и др. Если говорить о глубоком обучении, то охватываются TensorFlow, Theano, Caffe и другие инструменты.\n",
      "\n",
      "ConvNetJS\r\n",
      "Звезды: 9 510, форки: 1 982\n",
      "\r\n",
      "ConvNetJS представляет собой реализацию нейронных сетей и их общих модулей на JavaScript. Проект на данный момент не поддерживаемый, но всё ещё заслуживающий внимания. Позволяет обучать свёрточные (или обычные) сети прямо в браузере.\n",
      "\n",
      "Deeplearning4j\r\n",
      "Звезды: 10 227, форки: 4 570\n",
      "\r\n",
      "Библиотека глубокого обучения для Java и Scala. Интегрируется с Hadoop и Spark. Deeplearning4j также позволяет проводить вычисления на графических процессорах с поддержкой CUDA. Кроме того, имеются средства для работы с библиотекой на Python. Репозиторий содержит всю необходимую документацию и учебники.\n",
      "\n",
      "LISA Lab Deep Learning Tutorials\r\n",
      "Звезды: 3 673, форки: 2 045\n",
      "\r\n",
      "Сборник учебников Университета Монреаля. Представленные здесь материалы знакомят с некоторыми наиболее важными алгоритмами глубокого обучения, а также демонстрируют принцип работы с Theano. Theano — это Python-библиотека, которая упрощает запись моделей глубокого обучения и дает возможность обучать их на GPU.\n",
      "\r\n",
      "Этим списком количество интересностей на Гитхабе не исчерпывается. В следующий раз поговорим о проектах для машинного обучения и открытых датасетах. Если у вас есть свои примеры интересных репозиториев, поделитесь ими в комментариях.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "2021-й год стал первым годом, когда крупные компании начали понимать на практике, зачем же они следят за пользователями. До этого бигдата была инструментом поиска вещей, которые можно было сделать и статичными правилами, а вот сейчас наконец-то стала полезной принципиально иначе.\n",
      "\r\n",
      "Теперь можно получать обработку информации в реальном времени (и реагировать гибко и сразу на те же действия на сайте или в приложении), в агрегации данных (теперь банк знает, где вы живёте, какие у вас были диагнозы по чекам из аптеки и какие интернет-магазины вы предпочитаете по адресам посещаемых сайтов, от стратегического альянса с провайдером или сотовым оператором) и так далее. \n",
      "\r\n",
      "В ближайшее время будет происходить примерно следующее:\n",
      "\n",
      "\n",
      "Нам обещают, что к 2025 году с помощью ИИ на решение бизнес-задач, которое сейчас занимает месяцы и даже годы, будут уходить недели, дни, а то и вовсе часы. Ручная обработка информации практически перестанет существовать.\n",
      "Кроме того, если сейчас у бигдаты частенько нет никакого конкретного владельца, который бы регулярно обновлял и подготавливал к использованию информацию, то в самое ближайшее время эти данные будут структурированы в готовые продукты.\n",
      "Всё идёт к тому, что отдел, отвечающий за работу с данными, перестанет быть центром затрат и станет отдельной бизнес-единицей, отвечающей за свою долю прибыли.\n",
      "Но зато и нормативные требования по защите информации существенно ужесточатся, как и общая осведомлённость о правах на личные данные.\n",
      "Инструменты ИИ станут доступнее, базы данных разнообразнее. А ещё появятся специальные платформы для обмена информацией, как внутри организаций, так и за их пределами.\n",
      "\n",
      "Что происходит в целом\r\n",
      "Объём данных растёт с огромной скоростью. К 2010 году вся информация, сформированная человечеством за долгие тысячи лет, составляла всего около 2 зеттабайтов (2 миллиона терабайтов). В 2021 году это уже 65 зеттабайтов, а к 2025 году объём данных будет составлять уже 175 зеттабайтов.\n",
      "\r\n",
      "Рынок аналитики бигдаты, понятное дело, тоже растёт. По данным Research and Markets, примерно на 11,9% в год. Главная его проблема сейчас в том, что ничтожно малое количество крупных технологических компаний (в основном американских и китайских) хранит значительную часть мировых данных.\n",
      "\r\n",
      "Россия от общемировых трендов немного отстаёт. Большие данные применимы ко всем отраслям экономики — мы пока копим опыт и тестируем возможности.\n",
      "\r\n",
      "В июле 2021 года у нас только-только утвердили первый национальный стандарт в области бигдаты ГОСТ Р ИСО/МЭК 20546-2021 «Информационные технологии. Большие данные. Обзор и словарь». Но в ближайшее время обещают ещё 8 ГОСТов про эталонную архитектуру, безопасность, методы анализа, сценарии использования и т.д.\n",
      "\n",
      "Внутренняя монетизация данных\r\n",
      "Информация сама по себе ничего не даёт. Чтобы извлечь выгоду, данные надо правильно обработать. \n",
      "\r\n",
      "Монетизация бывает внешняя, т.е. прямая продажа информации, и внутренняя (использование данных для повышения эффективности работы). Недавние исследования Gartner показали, что 61% компаний активно использует бигдату для улучшения внутренних бизнес-процессов и только 10% организаций эти данные продают.\n",
      "\n",
      "Data-Driven — это подход к управлению компанией, основанный на больших данных. Хорош тем, что исключает человеческий фактор, например, возможность ошибки из-за привязок к тому, что сработало в прошлом, излишней уверенности в себе или каких-то интриг. При принятии решения значение имеют только цифры.\n",
      "\r\n",
      "С помощью массивов данных создаются три типа процессов: транзакционный (понимание и выполнение бизнес-транзакций), информационный (выводы, на основе совершённых действий) и аналитический (автоматизация действий, принятие решений и прогнозирование результатов).\n",
      "\r\n",
      "Данные можно использовать для: \n",
      "\n",
      "\n",
      "поддержания отношений с клиентами;\n",
      "улучшения внутренних бизнес-процессов; \n",
      "оптимизации продуктов или услуг;\n",
      "расширения ассортимента и т.д.\n",
      "\r\n",
      "По мнению Gartner, современная платформа, отвечающая за обработку данных, должна быть максимально простой в использовании и поддерживающей весь процесс, от подготовки информации до её аналитики.\n",
      "\r\n",
      "Главные мировые тренды в этой области: эволюция в сторону систем самообслуживания, low-code подход, возможность доступа с мобильных устройств, data storytelling, т.е. составление историй на основании данных, глубокая интеграция с искусственным интеллектом (AI) и машинным обучением (ML) и бизнес-аналитика по облачной модели (SaaS).\n",
      "\r\n",
      "Российский рынок идёт против некоторых из этих трендов. Например, спрос на облачные сервисы у нас пока невелик: большинство компаний предпочитает решения на своей площадке. Отчасти это, правда, продиктовано нормативными ограничениями.\n",
      "\r\n",
      "Одна из больших проблем в работе с данными — то, что необходимая информация может существовать в виде разрозненных фрагментов в разных бизнес-группах или компаниях. Не хватает связей между ними. Поэтому важный шаг для того, чтобы стать data-driven предприятием, — демократизация данных, т.е. перемещение их из разрозненных хранилищ на общую платформу.\n",
      "\r\n",
      "Существуют разные подходы к использованию информации:\n",
      "\n",
      "\n",
      "оптимизировать бизнес-процессы, основываясь на уже имеющихся данных;\n",
      "изучить, что имеет ценность для аудитории, и только потом добывать нужную информацию.\n",
      "\n",
      "Обработка личных данных. Мировые тренды\r\n",
      "Это очень щепетильный вопрос. Нужен баланс между выгодой и приватностью. Во многих странах либо уже приняты, либо обсуждаются стандарты об обработке персональных данных.\n",
      "\r\n",
      "Общие тенденции таковы: облегчение доступа к государственным данным, вовлечение компаний частного сектора, ужесточение требований к обработке персональных данных, но в то же время расширение оснований для неё (например, использование подразумеваемого согласия), гибкий подход к форме и процедуре получения данных и т.д.).\n",
      "\n",
      "Ситуация с личными данными в России\r\n",
      "Россия придерживается мировых трендов.\n",
      "\r\n",
      "В конце 2021 года президент поручил правительству и Госдуме подготовить поправки, которые позволят гражданам распоряжаться информацией о своих хозяйственных операциях, а также ускорить передачу бизнесу данных министерств и ведомств. А ЦБ предложил обезличить кредитные истории клиентов, чтобы можно было использовать для развития искусственного интеллекта или передавать сторонним компаниям.\n",
      "\r\n",
      "К середине 2022 года должен появиться госоператор больших данных, который будет обрабатывать и предоставлять компаниям информацию, накопленную разными ведомствами. Кроме того, государство попросит бизнес безвозмездно предоставлять обезличенные данные по некоторым направлениям. Уже сейчас идёт ряд пилотных проектов, которые позволят дать доступ коммерческих компаний к ЕСИА.\n",
      "\r\n",
      "Кроме того, вскоре должна появиться единая государственная система, в которой будут храниться первичные биометрические данные. А все коммерческие операторы будут работать с их векторами.\n",
      "\r\n",
      "Министерство экономического развития разработало проект федерального закона о Национальной системе управления данными. Документ закрепляет права граждан, юридических лиц, органов публичной власти и управления государственными внебюджетными фондами на доступ к информации, которая содержится в НСУД.\n",
      "\r\n",
      "Один из важнейших вопросов, который сейчас поднимается, — право человека на управление своими данными. Например, чтобы можно было перенести финансовую историю из одного банка в другой.\n",
      "\n",
      "Рыночные сегменты\r\n",
      "Бигдата успешно применяется в самых разных отраслях:\n",
      "\n",
      "\n",
      "Операторы связи.  Китайской компании China Unicom удалось удержать более 200 тыс. абонентов, уже готовых уйти от оператора. «Ростелеком» запустил «Платформу управления данными» (изначально создана Arenadata DB, но уже несколько лет работает на собственной разработке Ростелекома — RT.Warehouse), которая помогает бизнесам переходить к data-driven модели, а так же собственные продукты: решения по транспортировке информации, управлению мастер-данными (MDM) и справочными данными (RDM), решению методологических задач (RT DataGoverage). Самой компании эти системы помогают экономить более 1 миллиарда рублей в год.\n",
      "Банки. Уральский банк реконструкции и развития, изучая информацию по клиентской базе для создания кредитных предложений, вкладов и т.д., увеличил кредитный портфель на 55%). 3 февраля 2022 Сбер сообщил о внедрении электронного взаимодействия с Росреестром по аккредитивам в сделках недвижимости. Уралсиб сформировал единую службу управления данными в составе блока стратегического развития.\n",
      "Страхование.\n",
      "Здравоохранение (большие данные можно использовать для борьбы с болезнями, предотвращения и предсказания эпидемий).\n",
      "Автомобилестроение. Mercedes-Benz внедряет технологию, которая позволит лучше взаимодействовать с данными на основе блокчейн-платформы Ocean Protocol. И вообще, в автомобилестроении сейчас действуют три основные тенденции: встроенное подключение к интернету, автопилоты и курс на экологичность.\n",
      "Электронная коммерция.\n",
      "Розничная торговля. В онлайн-коммерции на цифровой информации построен вообще весь механизм продаж, а в оффлайн-торговле с её помощью, например, проектируют маршруты покупателей по торговому залу, чтобы правильно расставить товары. Компания «Перекрёсток» оцифровывает клиентский опыт, благодаря чему расширила сервис быстрой доставки на всю Москву, запустила онлайн-мониторинг загруженности супермаркетов и сервис по онлайн-контролю количества человек у касс. Разработки тестируются в специальном магазине-лаборатории.\n",
      "Медиаиндустрия.\n",
      "Маркетинг. Почта России заработала около 1,2 млрд рублей на линейке продуктов «Директ-Мейл» (Direct Mail), удачно использовав накопленную информацию о домохозяйствах. Учитывалось всё: пол, возраст, увлечения и даже религиозные убеждения.\n",
      "Гражданская авиация. Бигдата помогает повысить надёжность и эффективность авиакомпаний.\n",
      "\r\n",
      "В конце ноября 2021 года начала работу Шанхайская биржа данных. На торги было представлено примерно 20 торговых продуктов из нескольких категорий. В частности, данные о рейсах авиакомпаний и информация от операторов телекоммуникационных сетей.\n",
      "\n",
      "Аудиторные данные\r\n",
      "На рынок аудиторных данных вышли компании, традиционно далёкие от рекламы и маркетинга: телекомы, банки, операторы фискальных данных и т.д.\n",
      "\n",
      "Розничная торговля\r\n",
      "Три важных момента для розничной торговли:\n",
      "\n",
      "\n",
      "использование данных для оптимизации и прогнозирования продаж;\n",
      "предложение новых услуг. Причём данные, на основе которых делаются выводы, частенько выходят за пределы традиционной бизнес-модели;\n",
      "сотрудничество с брендами для создания продуманной целевой рекламной платформы.\n",
      "\n",
      "\n",
      "Электронная коммерция\r\n",
      "В электронной коммерции большие данные используют для воздействия на клиентов на эмоциональном уровне. Дают понять человеку, что он особенный и между ним и брендом есть определённая кармическая связь. Будущее электронной коммерции в объединении персональных целей с теорией и продажами. Идеальное приложение даст человеку общие рекомендации, например, по здоровью, и подскажет, какие товары купить, а продавец сможет предложить персональные скидки и программы лояльности.\n",
      "\n",
      "Платёжные системы\r\n",
      "Поставщики платежей уже умеют генерировать информацию о клиентах на основе данных. Пожалуй, самый большой потенциал монетизации данных при платежах заключается в объединении информации о держателях карт, которые получают шлюзы и эмитенты, с данными о покупках, которые приходят от продавцов.\n",
      "\n",
      "Банки\r\n",
      "Банки скоро начнут обмениваться обезличенной информацией о кредитах, счетах, доходах, транзакционной активности и т.д.\n",
      "\n",
      "Облачные сервисы могут монетизировать анонимные данные клиентов или данные, которые не позволяют идентифицировать личность, сотрудничая с рекламой, маркетингом, финансовыми услугами, розничной торговлей, электронной коммерцией и т.д.\n",
      "\n",
      "Операторы связи\r\n",
      "У операторов связи есть уникальная информация о клиентах, которую они пока довольно мало используют: голосовые данные, SMS-сообщения, мобильный трафик, геолокация, сведения о загруженных приложениях, платежах и т.д. Данные эти по своей ценности сопоставимы с теми, которые собирает Google. Их можно вполне успешно монетизировать с помощью банков, страховых компаний, ОТТ-компаний и т.д. Первые успешные примеры таких союзов уже существуют на азиатских рынках. Данные операторов уникальны, т.к. дают целостное представление о рынке, которого нет ни у клиентов, ни у поставщиков. Но есть проблемы с безопасностью, конфиденциальностью и конкуренцией, которые необходимо решить в ближайшее время.\n",
      "\n",
      "Городская инфраструктура\r\n",
      "За последние годы в России реализован целый ряд инфраструктурных проектов. Например, московские власти уже несколько лет закупают у мобильных операторов данные о перемещениях горожан. На их основе мэрия меняет транспорт и инфраструктуру в городе. А датчиками, которые передают данные о работе конкретного участка системы, оснащают транспорт, водоснабжение, электросеть и т.д.\n",
      "\n",
      "Умный город, умный регион\r\n",
      "Важные компоненты, без которых не обойдётся ни один умный город: видеонаблюдение и видеоаналитика, ситуационные центры, система 112, интеллектуальные транспортные системы, интернет вещей, биометрия, технологии поддержки и принятия решений, геоинформационные технологии и навигация, машинное обучение, облачные вычисления и др.\n",
      "\r\n",
      "В 2014–2015 гг. разработаны международные стандарты ISO 37120:2014 и 37151:2015.\n",
      "\r\n",
      "Все регионы РФ утвердили стратегии цифровой трансформации. Выше всего цифровая зрелость в Белгородской, Липецкой, Московской, Нижегородской областях, Республике Татарстан, Ханты-Мансийском и Ямало-Ненецком автономных округах, а также Москве и Санкт-Петербурге.\n",
      "\n",
      "Здравоохранение\r\n",
      "Вопреки распространённому мнению, медицинские учреждения не потребители, а владельцы данных о здоровье. У нас в стране госучреждениям запрещено заниматься коммерческой деятельностью, а вот за рубежом их продают учёным и аналитикам для ускорения инноваций в медицине. Она становится всё более персонализированной, поэтому информация об опыте реальных пациентов приобретает особенную ценность.\n",
      "\r\n",
      "Например, компания Roche приобрела платформу данных на основе информации об онкологических больных Flatiron, чтобы использовать её для НИОКР и разработки лекарств, компания Western Maryland Health System смогла снизить затраты на 78%, или 112 000 долларов США, за 6 месяцев, а компания Medopad разработала приложение, которое собирает данные с носимых устройств, мобильных устройств и из медицинских учреждений, а потом анализирует эти данные, чтобы предотвратить хронические заболевания.\n",
      "\r\n",
      "Для фармкомпаний главную ценность представляют данные, собранные вне рамок традиционных рандомизированных клинических испытаний. С внедрением носимой электроники и социальных сетей для этого появляются новые возможности. \n",
      "\r\n",
      "Совместное исследование, проведённое IDC и Seagate, показало, что совокупный годовой темп роста (CAGR) данных в сфере здравоохранения прогнозируется на уровне 1,36% до 2025 года, что быстрее, чем в производстве, финансовых услугах или средствах массовой информации.\n",
      "\n",
      "Гражданская авиация\r\n",
      "В авиации данные нужны для создания цифровых двойников самолётов, которые помогают понять, когда пора делать предиктивный ремонт и начинать техобслуживание. На самолётах стоят датчики, которые передают данные в облако или на наземные серверы, где их могут проанализировать и выявить неисправности заранее.\n",
      "\r\n",
      "Данные также помогают экономить топливо, оптимизировать операционную деятельность (включая прогнозирование задержек рейсов), формировать персональные предложения для пассажиров и даже экономить на организации питания на борту.\n",
      "\n",
      "Использование космических аппаратов\n",
      "Система ТРИС ДЗЗ — это все ресурсы дистанционного зондирования Земли (ДЗЗ) на территории страны, интегрированные в единое геоинформационное пространство. Эта система нужна для решения некоторых государственных вопросов, а ещё для науки и безопасности.\n",
      "\n",
      "Другие успешные кейсы\r\n",
      "Компания CleverData подключила к своей независимой бирже данных Getintent, международного разработчика programmatic-решений для брендов, рекламных агентств и паблишеров, а также стала партнёром Rambler Group, получив тем самым возможность размещать в его сети свою таргетинговую рекламу.\n",
      "\r\n",
      "Благодаря пандемии госорганы во всём мире стали быстрее внедрять инновации, используя коммерческие технологии для решения важных задач, а ИТ-продукты и данные в качестве контроля за пандемией и реакции на неё.\n",
      "\n",
      "ТЭК активно использует умные приборы для мониторинга оборудования и учёта ресурсов, цифровые двойники, ИИ, системы компьютерного зрения и платформы интернета вещей.\n",
      "\n",
      "Прогнозы\r\n",
      "Ожидается, что мировой рынок монетизации данных будет расти со среднегодовым темпом 6%. Стимулируют его увеличение покрытия сетей и рост облачных вычислений, а также рост возможностей для обработки данных, применение искусственного интеллекта, внедрение data-driven подхода и достижения в области большой аналитики. Сдерживать же будут нормативные документы о неприкосновенности частной жизни.\n",
      "\r\n",
      "Доминирующее положение на рынке продолжит занимать Северная Америка. Зато Азиатско-Тихоокеанский регион будет иметь самые высокие темпы роста.\n",
      "\r\n",
      "Ключевые тенденции: увеличение числа М2М-приложений (это интеллектуальные счётчики, видеонаблюдение, мониторинг здравоохранения), увеличение потребительских данных, развитие технологий искусственного интеллекта и машинного обучения. \n",
      "\r\n",
      "В ближайшие 2–3 года 50% компаний будут использовать автоматизацию и искусственный интеллект при выборе поставщиков и размещении заказов, 25% компаний будут использовать инструменты машинного обучения, 50% поставщиков программных решений будут использовать анонимные данные, собранные на их платформах, 25% организаций будут использовать данные, полученные с устройств IoT, для внедрения новых бизнес-моделей и т.д. \n",
      "\r\n",
      "Основные сегменты рынка расположились в таком порядке: банковские и финансовые услуги, правительство и оборона, логистика, СМИ и развлечения, розничная торговля, всё остальное.\r\n",
      "Ожидается, что банковские и финансовые услуги будут занимать первое место, благодаря огромному объёму накопленных данных, а прогресс в технологиях поможет им снизить затраты и повышать эффективность.\n",
      "\n",
      "Что ещё можно использовать на пути к успеху\r\n",
      "Очень выросла популярность модели «всё как услуга» (Everything-as-a-Service, XaaS).\r\n",
      "Adobe, Workday, LinkedIn и Salesforce на протяжении многих лет позволяют клиентам использовать приложения в облаках. Заменяют новой моделью традиционные способы ведения бизнеса, существенно снизив затраты на привлечение клиентов и повысив их лояльность.\n",
      "\r\n",
      "Текстовая аналитика — инструмент, который анализирует весь текст на странице, скажем, в соцсети, по определённым критериям.\n",
      "\r\n",
      "Социальные сети дают огромный массив информации для маркетингового анализа. А также для управления удовлетворённостью клиентов. Собирать данные никогда не было так дёшево и просто. Размещая информацию в открытом доступе, люди не чувствуют, что за ними наблюдают, и поэтому ведут себя как ни в чём ни бывало.\n",
      "\r\n",
      "Данные интернета вещей (IoT) стали стратегическим активом, который можно продавать и обменивать. Благодаря им появились сложные сетчатые экосистемы сотрудничества между организациями.\n",
      "\r\n",
      "Опрос, проведённый Cisco в 2017 г. среди лиц, принимающих решения в области ИТ и бизнеса в США, Великобритании и Индии, показал, что наиболее распространённые приложения IoT ориентированы на повышение качества продукции или производительности (47%), улучшение процесса принятия решений (46%) и снижение эксплуатационных расходов (45%).\n",
      "\r\n",
      "Россия, кстати, пока заметно отстаёт по монетизации интернета вещей (IoT). Но всё-таки количество подключенных устройств и межмашинных коммуникаций (М2М) продолжает расти.\n",
      "\r\n",
      "Крупнейший сегмент IoT/M2M — это устройства, которые используются на транспорте. И, конечно, главный лидер — «ЭРА-ГЛОНАСС».\n",
      "\n",
      "Web 2.0 — такое проектирование систем, при котором сетевое взаимодействие тем лучше, чем больше людей в нём участвует.\n",
      "\r\n",
      "Dark data – это информационные активы, которые компания собирает, обрабатывает и хранит на постоянной основе, но обычно не использует. По аналогии с тёмной материей в физике dark data — зачастую самая большая часть данных, которые хранятся в компании.\n",
      "\n",
      "Стратегическое планирование\r\n",
      "Собственно, монетизировать данные можно одним из четырёх путей:\n",
      "\n",
      "\n",
      "использовать их для принятия своих стратегических решений;\n",
      "собирать, сортировать и продавать информацию;\n",
      "предоставлять рекламную платформу;\n",
      "продавать программное обеспечение для сбора, хранения и анализа данных.\n",
      "\r\n",
      "Искусственный интеллект и машинное обучение, во-первых, помогают выявлять тенденции рынка на ранней стадии, во-вторых, находить новые возможности роста и, в-третьих, они снижают предвзятость при принятии решений.\n",
      "\r\n",
      "Чтобы разработать выигрышную стратегию при производстве ПО, важно понять, что нужно покупателю: улучшить производительность или спрогнозировать какие-то действия.\r\n",
      "Эффективная стратегия монетизации данных рассматривает информацию как живой актив, растущий в рамках действующего предприятия.\n",
      "\n",
      "Итого\r\n",
      "Как бы кто из нас ни относился к Большому Брату, который за всеми наблюдает, цифровизация и использование огромного количества данных — это уже наша реальность. Использование бигдаты даёт огромные возможности абсолютно для любого бизнеса, и было бы глупо их упускать.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Я Павел Свиридонов, гуманитарий, который вместо известной сети быстрого питания попал в IT-компанию. У меня нет технического образования, и я всё ещё не до конца понимаю, как работает интернет. Но как выяснилось, не только я хлопаю ресницами и пытаюсь улететь с совещаний, где речь заходит про…\n",
      "\n",
      "Впрочем, вот краткая предыстория. Однажды мой приятель — проджект в одной компании и бывший учитель английского, пожаловался: «На созвонах, где речь про бигдату заходит, я ничего не понимаю!» И это прям эхом отозвалось в моей душе: увы, но у меня с пониманием больших данных дела обстояли не лучше. Да что там: признаюсь, когда наши дата-инженеры начинали говорить, мне казалось, что беседа вдруг перешла на эльфийский.\n",
      "\n",
      "Обидно!\n",
      "\n",
      "И вот, отчасти чтобы помочь другим гуманитариям, отчасти чтобы разобраться в теме самому, я задумал маленькую контентную революцию: составить Словарь больших данных! Но такой, чтобы собранные в нём основные термины по бигдате, объяснялись просто, на бытовых примерах, понятных любому человеку, независимо от образования и профессии.\n",
      "\n",
      "На что стоит обратить внимание для погружение в тему больших данных? Какие термины предстоит изучить? И насколько глубоко нужно копнуть, чтобы понять, как работают платформы по обработке и анализу бигдаты?\n",
      "Об этом и о том, в каких муках рождался обозначенный выше Словарь, и написана эта статья.\n",
      "\n",
      "Вниз по кроличьей норе\n",
      "После того, как я с коллегами составил список основных и самых важных терминов, пришло время наполнять Словарь контентом. Я не рассчитывал, конечно, что работа над этим материалом будет легкой прогулкой, но после прочтения первого же абзаца в Википедии почувствовал себя героем известного мема. 👇🏻\n",
      "\n",
      "\n",
      "\n",
      "Структурированные и неструктурированные данные, формула 3V, горизонтально масштабируемые программные инструменты — новая информация погрузила меня в бесконечную матрицу, где за большим массивом знаний шел следующий…\n",
      "\n",
      "Словом, оказалось, что нужно было начать с понимания того, что вообще такое «большие данные». До этого казалось, что это некий “эфир”, незримая субстанция, витающая в воздухе и наполненная знаниями. На деле они обладали определенными признаками, а их описание было похоже на формулировку физического закона. Например, они характеризуются скоростью обновления, объёмом, достоверностью, тем, что эти данные можно визуализировать, или изменчивостью, т.е возможностью менять своё значение в зависимости от контекста.\n",
      "\n",
      "Но… это были ещё цветочки; ягодки появились, когда дело дошло до терминов, которые описывают работу платформы Big data. Так в мою жизнь вошли знания о реляционных базах данных, наборах данных как способе хранить информацию и SQL — языке программирования, с помощью которого можно управлять данными в таких базах.\n",
      "\n",
      "Всё чудесатее и чудесатее\n",
      "Три дня я изучал все доступные достоверные источники. От новых знаний мозг мой опух, но список терминов наполнился-таки определениями. Пришла пора проверить мои наработки с экспертом. Им выступил наш руководитель отдела систем обработки данных Иван Хозяинов.\n",
      "\n",
      "Первое, что я сказал ему, было:\n",
      "\n",
      "— Ваня, мой мозг уже напоминает плавленный сырок. Я ничего не понимаю! Вот, казалось бы, данные — они и есть данные. Но оказывается, у них есть ещё и скорость, и достоверность, и ещё какие-то признаки. И вообще, почему «большие данные»? Что, есть и маленькие???\n",
      "\n",
      "Ответ Ивана меня окончательно… удивил:\n",
      "\n",
      "— Вообще, да, данные бывают маленькими. Когда они поступают небольшими объемами и очень редко, и это какая-то скудная информация, которую никак нельзя применить. Например, показатель температуры, который приходит к нам раз в год и состоит из пары замеров, и даже непонятно, где эти измерения были сделаны.\n",
      "\n",
      "Мы пришли к выводу, что проще всего объяснить суть больших данных на примере круговорота воды в природе.\n",
      "\n",
      "Например, объём. Вода может собраться и в лужу, и в океан. Так же и данные — из источника их может поступать очень много, а может и совсем по чуть-чуть. Воду в природе можно увидеть в разных состояниях: в виде пара или льда. Данные тоже бывают в разных состояниях — их нужно распаковать, расшифровать или собрать, чтобы работать с ними. Они, как и вода, могут течь с разной интенсивностью — быстро и бурно, как горная река, или тоненькой струйкой, словно ручей. Чтобы добыть пользу из данных, их необходимо обработать специальными инструментами — почти как с водой, которую нужно прокипятить и отфильтровать, чтобы пить, или закинуть в неё сети или удочку, чтобы поймать рыбу.\n",
      "\n",
      "Не бойся Бармаглота, сын!\n",
      "С Иваном мы прошли весь список терминов, который к тому моменту состоял из пары десятков позиций, и добавили примеры. Но Словарь должен был получить ещё одобрение главного по пиару в ITSumma — Глеба Русина. И тут вышла заминка — примерно как в том меме про «Всё фигня, переделывай». Старший товарищ сказал, что мы сильно перемудрили: определения были сложными, а примеры ничего толком не объясняли. Большая часть Словаря попросту не выполняла свою функцию и была непонятной.\n",
      "\n",
      "И я пошёл переделывать.\n",
      "\n",
      "Убирал заумные определения, а примеры постарался привести бытовые.\n",
      "\n",
      "Вот как мы описали ETL/ELT-процессы с помощью того, что происходит в обычном магазине у дома.\n",
      "\n",
      "Если бы такие процессы проходили в супермаркете “Надопакет”, они выглядели бы так. Извлечение данных — extract — буква E в ETL, аналогично тому, когда грузчик достает товары из кузова грузовика и несет их на склад. При этом в магазин поступили самые разные продукты — от газет и журналов до яиц и молока.\n",
      "\n",
      "Все товары, как и данные, проходят обработку и преобразование. Это буква T (transform) в ETL. В случае супермаркета завскладом проверяет срок годности, состояние упаковки и заносит их в систему товарооборота. Данные же оценивают на качество, т.е. соответствие определенным критериям, отмечают, что это за данные, и для чего они могут понадобиться.\n",
      "\n",
      "После этого товары попадают на соответствующие полки в торговом зале. Как бы “загружаются” в магазин. Что соответствует букве L — load, загрузка/запись данных в хранилище для дальнейшего использования.\n",
      "Это лишь один удачный пример, который нам удалось придумать. Возможно, он не самый точный, но зато понятно описывает сложные процессы, происходящие на платформе по обработке и анализу данных.\n",
      "\n",
      "И ещё пара моих любимых примеров из Словаря.\n",
      "\n",
      "Что такое потоковая и пакетная обработка данных, мы объясняем через грязную посуду:\n",
      "Если вы целый день копите грязную посуду в раковине, складываете туда тарелки, кружки и кастрюли и моете только вечером — это пакетная обработка посуды.\n",
      "\n",
      "А если вы моете посуду сразу, как поели — это потоковая обработка посуды.\n",
      "Кое-что про брокер сообщений на платформе по анализу и обработке данных:\n",
      "Представьте себе логиста, который принимает грузы и направляет их туда, куда нужно. Он беглым взглядом распознает, что за посылка перед ним, сверяется со списками адресов, пишет на ней пункт назначения и отправляет груз. Примерно такую же функцию выполняет на платформе брокер сообщений.\n",
      "\n",
      "К чему это всё?\n",
      "К тому, что большие данные и всё, что с ними связано, — это не так уж и страшно. Да, на первый взгляд кажется, что это дремучий лес, в котором живут одни лишь датасатанисты (извините, коллеги). Но стоит погрузиться в тему, и становится понятно — big data везде и во всем. В каком-то смысле она — это весь мир, что нас окружает, всё, что мы видим и ощущаем.\n",
      "\n",
      "Забавно, что после работы над Словарем я вижу данные везде. Так и хочется всё оцифровать, посмотреть на результат, покопаться в нем и вытащить оттуда каких-нибудь полезных инсайтов…\n",
      "\n",
      "Конечно, для меня как гуманитария и прирожденного зазывалы на свободную кассу, это был некий челлендж. И тут больше спасибо коллегам за то, что нам удалось с помощью креативной жилки превратить научные определения в понятные термины. Надеюсь, наш Словарь и для вас будет полезен!    \n",
      " Привет, Хабр! На связи Юрий Кацер, эксперт по ML и анализу данных в промышленности, а также руководитель направления предиктивной аналитики в компании «Цифрум» Госкорпорации “Росатом”. В рамках рабочих обязанностей я решаю задачи в промышленности с помощью машинного обучения. Большую часть работы по созданию моделей составляет работа с промышленными данными.В условиях стремительного роста объема информации, собираемой на производственных предприятиях в связи с развитием интернета вещей (сбор и хранение данных), важным аспектом становится качество таких данных. В то же время проблемы и ошибки в них становятся препятствием для применения методов машинного обучения и построения моделей на основе законов физики или предметной области. Такие проблемы, как выбросы, пропуски, изменение частоты дискретизации, шум, искажают результаты или делают невозможным практическое использование данных для машинного обучения.В этой статье мы посмотрим на часто встречающиеся проблемы в промышленных данных типа временных рядов. О том, что такое временной ряд, и о других особенностях задач в промышленности я рассказываю в других статьях на хабре, рекомендую познакомиться, а мы пока перейдем к сути! На схеме ниже приведен большой список проблем в данных, о которых мы поговорим в статье.Обзор проблемПропущенные значения (потеря данных): пропуски в последовательности точек во временном ряду с регулярной частотой дискретизации.Внезапные сдвиги: изменения в статистической модели, из которой генерируются данные (изменение технологического процесса, изменение режима эксплуатации, замена или перекалибровка датчика).Изменения диапазона: аналогичны Внезапным сдвигам.Чередование сигналов: сигналы «меняются местами».Отсутствие или изменение частоты дискретизации: при отсутствии или изменении частоты дискретизации становится невозможным применить какой-либо способ анализа временных рядов, где требуется регулярность временной сетки.Зашумленные данные и меняющийся уровень шума: слишком высокий или изменяющийся во времени уровень шума в данных.Недостаточная уникальность измерений: значения становятся неточными из-за округления, высокой апертуры либо других факторов.Выбросы и невозможные значения: единичные отклонения от ожидаемого поведения данных или значения вне допустимого диапазона доменной области.Несбалансированность классов: Дисбаланс нормального и аномального классов данных ограничивает возможности применения моделей машинного обучения. Также важно помнить, что такая проблема также может возникать из-за смещения в выборке данных, а не всей генеральной совокупности.Отсутствие значений в классе: Отсутствие значений, например, в аномальном классе, делает невозможным использование методов машинного обучения с учителем (supervised) или частичного обучения с учителем (semi-supervised).Краткая история данных: История записи данных слишком коротка для их анализа и обучения моделей.Единицы измерения: единицы измерения не одинаковы для всех сигналов или источников данных, например, сантиметры и дюймы.Синхронизация времени: временные метки измерений, поступающих из разных источников, могут немного отличаться, например, UTC+0 и UTC+3.Типы данных: различные типы данных, например, float и string.ЗаключениеТаким образом, этап предварительной обработки данных в пайплайне решения или в процессе проекта становится одним из самых важных для обеспечения качественных результатов решения задач и даже применимости некоторых методов машинного обучения. Подробности, касающиеся части предварительной обработки, были представлены в этой статье и ссылках в ней.Больше информации по теме можно почитать в этих статьях:О качестве данных для машинного обученияО проблемах в промышленных данныхМетоды предварительной обработки данных для машинного обученияМогут быть полезны следующие научные статьи по теме проблем в данных:Gitzel, Ralf. “Data Quality in Time Series Data: An Experience Report.” CBI (Industrial Track). 2016.Pastorello, Gilberto, et al. “Observational data patterns for time series data quality assessment.” 2014 IEEE 10th International Conference on e-Science. Vol. 1. IEEE, 2014.Hubauer, Thomas, et al. “Analysis of data quality issues in real-world industrial data.” Annual Conference of the PHM Society. Vol. 5. №1. 2013.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Начало детективной истории\n",
      "Что если наши умные телефоны следят за нами? Оказывается, за действиями пользователя могут следить практически любые телефоны. Это не очередной сюжет фильма про “большого брата”, это наша реальность.\n",
      "Случайно я наткнулся в сети на интересный топик на xda-developers. Тревор Экхарт (Trevor Eckhart), девелопер, копался в программных кодах Андроид-телефона от HTC и нашел интересную вещь. В прошивке телефона содержалась программа, которая работала в скрытом от пользователя режиме.\n",
      "Оказывается, компания HTC встраивает в свои устройства программы, которые позволяют собирать разнообразную статистику. Есть такая американская фирма, Carrier IQ (CIQ), которая предоставляет инструменты операторам для сбора и анализа данных с сотовых телефонов абонентов (согласно счетчику на сайте: 141 миллион устройств уже обладают этой опцией). Вот, что написано у них на сайте: \n",
      "\n",
      "Carrier IQ — лидер рынка мобильных интеллектуальных технологий, который создал революционную технологию, позволяющую мобильным операторам и производителям собирать и обрабатывать информацию от конечных пользователей. Телефон является частью сети и используется в качестве инструмента для измерения ключевых параметров качества сервиса и использования. CIQ предоставляет уникальную возможность анализировать сценарии использования и ошибки по типу, расположению, приложению и сети, одновременно с этим, предоставляя детальные данные, непосредственно с самого устройства, а не общую информацию о состоянии сети.\n",
      "\n",
      "Разбираемся в вопросеЗвучит заманчиво, не так ли? По словам Тревора Экхарта, он смог получить от этой компании дистрибутив их программы. Она напоминает собой нечто вроде интерактивного опроса для пользователя:\n",
      "  \n",
      "Программа может собирать данные разнообразного характера: уровень сигнала, местоположение аппарата, нажатие клавиш, модель устройства, вообще все, с чем пользователь взаимодействует. (Согласно патенту о сборе данных с беспроводных сетей).\n",
      "\n",
      "Сбор информации происходит так: есть триггеры (triggers) и есть метрика (metrics). Необходимая информация о телефоне собирается и отправляется тогда, когда был задействован определенный триггер. Судя по описанию на сайте Тревора, количество разнообразных метрик и триггеров огромно. Вот, например, список триггеров для телефонов HTC:\n",
      "\n",
      "Нажата клавиша на диалере или на клавиатуре:\n",
      "Intent – com.htc.android.iqagent.action.ui01\n",
      "\n",
      "Открыто рекламное объявление:\n",
      "Intent – com.htc.android.iqagent.action.ui15\n",
      "\n",
      "Получено СМС:\n",
      "Intent – com.htc.android.iqagent.action.smsnotify\n",
      "\n",
      "Экран включен/выключен: \n",
      "Intent – com.htc.android.iqagent.action.ui02\n",
      "\n",
      "Получен звонок: \n",
      "Intent – com.htc.android.iqagent.action.ui15\n",
      "\n",
      "Статистика медиа данных:\n",
      "Intent – com.htc.android.iqagent.action.mp03\n",
      "\n",
      "Статистика местоположения:\n",
      "Intent – com.htc.android.iqagent.action.lc30\n",
      "\n",
      "Программа состоит из двух частей: первая — встроена в устройство, вторая — сервер, собирающий и анализирующий данные (связанные с сетью: голосовые услуги и данные, не связанные с сетью: музыкальный проигрыватель, камера, различные загруженные данные и прочее). Взято из рекламных материалов\n",
      "Пользователь со стороны оператора/производителя получает удобный интерфейс для работы с данными (с сайта CIQ):\n",
      "\n",
      "\n",
      "Все было бы нормально, если пользователь мог добровольно предоставлять данные для анализа, участвовать в опросах. Однако, в реальности все происходит иначе. Производитель оборудования получает исходники программы от CIQ, пишет свой интерфейс и оставляет требуемые функции, скрывая программу от конечного пользователя, включает в состав прошивки устройства (с одобрения оператора). Элементы кода программы были найдены в Sense UI, Touch Wiz. Даже в своем HTC Desire S с прошивкой MIUI я нашел остатки HTC'ного логгера. \n",
      "Таким образом оператор может собирать любые данные о телефоне и их анализировать, а пользователь даже об этом и не узнает. Тревор классифицировал программу как руткит из-за своей способности скрывать себя от пользователя и при этом нести вред.\n",
      "\n",
      "К слову сказать, подобное возможно не только на смартфонах, этот код может быть встроен в любые виды телефонов. Блоггер утверждает, что этим сервисом пользуются американские операторы Sprint, Verizon, а руткит установлен на андроид устройствах, Blakberry, Nokia, планшетных компьютерах и прочих. За телефоном можно следить, даже если он никогда не был активирован в сети оператора.\n",
      "\n",
      "Есть и хорошая новость: владельцы телефонов на Андроид могут скачать программу с форума Xda-developers и проверить свой телефон на наличие руткита, а если есть права root, то и попытатсья удалить его.\n",
      "\n",
      "История получает неожиданное продолжениеТревору пришло письмо от Carrier IQ с угрозами начать судебное разбирательство, если он публично не принесет извинения компании и не сообщит всем, что он ошибся в своих выводах.\n",
      "\n",
      "Дополнительная информацияОригинал статьи Тревора Экхарта (у него на сайте собрано очень много информации по этому вопросу, в том числе и технической).\n",
      "Статья на Xda-developers об угрозах со стороны CIQ.\n",
      "CIQ опубликовали пресс-релиз, где опровергают слова девелопера.\n",
      "\n",
      "UPD. Добавляю еще ссылку на Xda, здесь описана выжимка информации из блога девелопера (спасибо ilyuxa).\n",
      "\n",
      "UPD2. Ответ EFF на притязания CIQ. EFF — Electronic Frontier Organization, занимается защитой прав потребителей в цифровой сфере, они взяли на себя защиту прав Тревора. Кратко: притазания CIQ беспочвенны. (спасибо Joes).\n",
      "\n",
      "UPD3. Юридический «наезд» кончился. Вести с полей принес Joes. CIQ уведомило об отзыве своих претензий и уважает EFF за готовность защищать свободу слова :) \n",
      "Однако главный вопрос остается все еще в силе.    \n",
      " Для всех кто работает с открытыми данными, делает свои проекты для себя, людей, для участия в конкурсах всегда бывает бывает полезен исходный код примеров того как данные собирались и что с ними делалось.\n",
      "\n",
      "Я предлагаю составить список открытых репозиториев кода нацеленного именно на решение задач по работе с открытыми данными и примеры проектов которые на них основаны.\n",
      "\n",
      "Вот список репозиториев зарубежных инициатив которые мне известны:\n",
      " — огромное количество проектов Open Knowledge Foundation — CKAN, расширения к нему, утилиты по преобразованию данных\n",
      " — репозитории исходного кода Sunlight Labs — очень много примеров того как распарсены многие американские сайты и базы данных, а также библиотеки по работе с данными разного вида\n",
      " — репозитории OpenGovernment.org — много примеров проектов на Ruby\n",
      " — репозитории департамента ИТ Оксфорда — их проекты по открытым данным такие как DataVerse;\n",
      " — открытый код Белого Дома (США) — проекты по петициям, стандартам API и всяким штукам для Drupal\n",
      " — репозиторий AlphaGov (UK) — материалы проекта www.gov.uk много кода самого проекта и подпроектов\n",
      "\n",
      "Мы в Информационной Культуре работаем с открытыми данными постоянно и часть того кода что мы производим вполне может быть полезна и другим. Да что уж там, точно будет полезна.\n",
      "\n",
      "Поэтому мы выложили много материалов в виде публичных репозиториев в которых каждый может найти что-то для себя и свободно использовать.\n",
      "\n",
      "А вот и то что есть:\n",
      "\n",
      " — исходный код OpenGovData.ru - он не очень актуален и сыроват, поскольку мы постепенно мигрируем на CKAN, но вполне пригоден для тех кто хочет быстро и без чьей-либо помощи развернуть портал открытых данных в своем городе не спрашивая никого.\n",
      " — Открытые данные Мосгорздрава - данные и скрипты для их извлечения с сайта Московского департамента здравоохранения mosgorzdrav.ru. Данные собраны и обработаны по ежедневным сводкам и вполне любопытны для визуализации. Конечно же важно не забыть их обновить.\n",
      " — Открытые данные ЦБ РФ - проект по преобразованию данных из API веб-сервисов Центробанка в базу данных. Довольно старое API, для работы с веб-сервисом тогда приходилось испольховать утилиту на .NET, но возможно у кого-то найдется время и желание поработать над этими данными.\n",
      " — Госонтологии - набор RDF/OWL данных по различным областям деятельности государства. Незавершенные онтологии, но вполне пригодные для работы.\n",
      "- Открытая госдума - скрипты и данные полученные при работе с данными депутатов. Как я и говорил ранее — ими не очень интересно заниматься, там сплошная политика.\n",
      " — Монитор сайтов - небольшой Django проект по мониторингу сайтов на доступность. На нем работает сайт http://sitemon.opengovdata.ru/ мониторящий zakupki.gov.ru\n",
      " — Открытые данные Москвы - парсер данных с data.mos.ru (Официального Московского портала) и загрузчик их в CKAN hub.opengovdata.ru\n",
      " — Открытые данные МЧС — скрипты по извлечению открытых данных и сами данные МЧС. Собирает информацию о телефонах, сводках и адресах подразделений.\n",
      " — Открытые данные МИД - скрипты и данные по послам. Недоделанное так как там много ошибок при парсинге. Поэтому приложены дампы анализа из Google Refine / Open Refine\n",
      " — Открытый Кремль — данные с сайта kremlin.ru по Президенту и Администрации. В основном исторические в виде дампа блога Медведева и всех комментариев к нему. Там же есть идеи по тому как использовать их данные вот тут они собраны в виде Excel таблички https://github.com/infoculture/openkremlin/blob/master/docs/kremlin_ru_opendata.xls\n",
      "\n",
      "Есть много и других полезных проектов по анализу данных, их сбору с сайтов, преобразованию данных и многому другому. Что-то мы будем выкладывать по мере готовности  (в первую очередь код надо описывать, делать README и тд), но наверняка есть и другие полезные репозитории. Если знаете такие — присылайте. Будем составлять их список.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Введение\n",
      "\r\n",
      "(клик по картинке ведёт внутрь публикации)\n",
      "\r\n",
      "Развиваясь, организации внедряют всё больше и больше информационных систем совершенно различных направлений: бухгалтерский учет, управление персоналом, управление складом etc. Системы живут и развиваются независимо друг от друга до того самого момента, как компании не потребуется взглянуть на свои данные целиком. Объемы данных уже достигают критической точки и выясняется, что сопоставить и сравнить данные вручную становится просто невозможно. Решения основанные на противоречивых и невыверенных данных ведут к управленческим ошибкам, а дубли и неактуальность данных к неверным бизнес решениям.\n",
      "\r\n",
      "Конечно же проблема описанная выше не нова и сегодня мы обсудим классический способ решения — систему управления мастер-данными.\n",
      "\n",
      "Оглавление\n",
      "\n",
      "Введение\n",
      "Что такое MDM\n",
      "Типы корпоративных данных: что такое справочные и транзакционные данные\n",
      "Зачем оно нужно?\n",
      "\n",
      "Зоопарк ИТ-систем и консолидированная отчетность\n",
      "Интеграция систем\n",
      "Единая база контрагентов\n",
      "Очистка и нормализация данных\n",
      "Случаи из жизни\n",
      "\n",
      "Методы решения\n",
      "\n",
      "Административное решение\n",
      "Внедрение MDM-системы\n",
      "\n",
      "Типы MDM-систем\n",
      "\n",
      "Централизованная система\n",
      "Аналитическая система\n",
      "Гармонизированная система\n",
      "Примеры реализации MDM-систем\n",
      "\n",
      "Индикаторы необходимости внедрения СУ НСИ\n",
      "Выводы\n",
      "\n",
      "\n",
      "Что такое MDM\r\n",
      "Master Data Management (сокращенно: MDM, МДМ, НСИ; варианты перевода: управление мастер-данными, нормативно-справочная информация) система — комплекс процессов, систем управления, стандартов и программ позволяющих единообразно работать с данными. Проще говоря, МДМ-система предоставляет целостный взгляд на все составляющие бизнеса, в том числе на источники данных, авторство, качество, полноту и на потенциальное использование данных. (Подробнее: Задачи управления мастер-данными) \n",
      "\r\n",
      "(кликабельно)\n",
      "\n",
      "Типы корпоративных данных: что такое справочные и транзакционные данные\r\n",
      "Чтобы разобраться, чем являются и не являются мастер-данные разберем основные типы корпоративных данных.\n",
      "\r\n",
      "(взято отсюда)\n",
      "\n",
      "Неструктурированные данные — текст, почта, и другие данные, у которых нет формально определенной и описанной структуры.\n",
      "\n",
      "Полуструктурированные — данные не имеющие определенной схемы (или имеющие переменную структуру), но тем не менее имеющие формальное описание в виде тегов и\\или определенных маркеров. XML — пример, полуструктурированных данных.\n",
      "\n",
      "Структурированные (транзакционные) данные — данные имеющие формально определенную схему.\n",
      "\n",
      "Метаданные — это данные описывающие другие данные, например, схема базы данных клиентов, конфигурационный файл или шаблон отчета.\n",
      "\n",
      "Мастер-данные — это данные, содержащие ключевую информацию о бизнесе, в том числе о клиентах, о продуктах, о работниках, о технологиях и материалах. Каждая из этих групп может разделяться на несколько предметных областей: в категорию люди входят клиент, продавец, поставщик. Так же может иметь набор правил валидации, которым должны удовлетворять данные. \n",
      "\r\n",
      "Иногда в отдельную категорию выделяют иерархические данные — это данные, в которых хранятся отношения и взаимодействия между данными. Подробнее.\n",
      "\r\n",
      "Пример, общей структуры мастер-данных и валидационных правил (кликабельно)\n",
      "\n",
      "\n",
      "Зачем оно нужно?\n",
      "\n",
      "\r\n",
      "Исторически многие системы хранения, анализа и визуализации данных развивались параллельно и не совместимы между собой. По мере роста компании интеграция данных становится всё более важной и во многих случаях критической задачей, согласно Microsoft уже компании среднего размера ощущают на себе последствия работы с разнородными данными.\r\n",
      "Таким образом одной из задач МДМ-систем является синхронизация данных, что упрощает решение сопутствующих задач, как подготовка финансовой отчетности.\n",
      "\r\n",
      "МДМ-система — это один из краеугольных камней в архитектуре бизнеса вместе с ERP и BI системами, позволяющий системам аналитики и ведения бизнеса иметь единое преставление о данных, независимо от источника и формы.\n",
      "\r\n",
      "Рассмотрим несколько классических случаев, где необходимо использовать и внедрять систему управления мастер-данными. \n",
      "\n",
      "Зоопарк ИТ-систем и консолидированная отчетность\r\n",
      "Пусть в компании больше трех систем хранения-анализа данных. Заполняются они и развиваются независимо друг от друга. В какой-то момент появляется необходимость собрать консолидированную отчетность и необходимо синхронизировать нормативно-справочную информацию. Например, существуют компания Ромашка с оборотом в 1М и имеются две записи «Общ.огр. Ромашка» и «ООО Ромашка» в разных системах с оборотом 400к и 600к, без инструментов синхронизации, система создания отчетности не сумеет объединить записи.\n",
      "\n",
      "Интеграция систем\r\n",
      "Пусть имеется несколько 1С систем в отделениях компании и счета, выставленные ООО «Ромашка» необходимо выгрузить и проанализировать в CRM. Если в CRM заведены несколько дублей, например Ромашка и Общ. Огр. Ромашка, то встает вопрос к какой Ромашке в CRM эти счета привязать и есть ли среди этих Ромашек нужная?\n",
      "\n",
      "Единая база контрагентов\r\n",
      "Прежде всего создание единой базы необходимо, для качественной и достоверной информацию о контрагентах. Если клиент, уже подписавший контракт, получает дополнительные N звонков о необходимости выслать уже отправленные документы (т.к. «Общ.огр. Ромашка» и «ООО Ромашка» — синтаксически разные компании), то это негативно отражается на отношениях компании. \n",
      "\n",
      "Очистка и нормализации данных\r\n",
      "Описанные выше случаи — это задачи по очистке и нормализации данных (data cleaning and data quality).\n",
      "\r\n",
      "Очистка и нормализация данных — это безусловно инструменты, цель — это повышение лояльности клиента (e.g. избегаем повторных звонков), создание отчетности (уверенность в корректности аналитики) и увеличение скорости выполнения задач (быстрее проходим цикл продаж).\n",
      "\r\n",
      "Как правило, клиент приходит к необходимости внедрения системы управления НСИ. Например необходимость оперативного контроля над деятельностью предприятия может потребовать сбора консолидированной отчетности, что в свою очередь приведет к необходимости синхронизации НСИ в ИТ-система, что в свою очередь потребует внедрения системы управления НСИ.\n",
      "\n",
      "Случаи из жизни\n",
      "Четырнадцать 1С-ок\r\n",
      "У одной компании N было четырнадцать 1С систем в филиалах и вот однажды им пришлось срочно предоставить отчетность о своей деятельности в какую-то там палату. Отсутствие единой отчетности грозило существенными проблемами и вот M сотрудников несколько недель вместе сводили и выверяли данные. А могли бы просто физически не успеть.\n",
      "\n",
      "Фуры\r\n",
      "Клиент из Астрахани отправил фуры заказчику в другой регион, а обеспечение в пути оказывала компания Х, у которой не было МДМ-системы и единой базы контрагентов. Во время путешествия фуры проходили обслуживание в двух регионах — и по окончанию поездки компания Х выставила счет клиенту по этим регионам по стандартному прейскуранту без положенной скидки за объем, так как клиент был записан в этих двух регионах под чуть-чуть по-разному и система не сопоставила имена. Итог — дополнительные разбирательства и ухудшение деловых отношений.\n",
      "\n",
      "Повторные звонки\r\n",
      "Однажды клиенту позвонили шесть (!) раз после того, как контракт был подписан. Из-за подобной некомпетентности лояльность клиента и контракт были под угрозой.\n",
      "\n",
      "Методы решения\r\n",
      "Рассмотрим два наиболее популярных метода решения проблем, описанных выше.\n",
      "\n",
      "Административное решение\r\n",
      "Административный подход — сначала вычистить уже имеющиеся дубли в ИТ-системах, разработать систему кодировок, по которым можно сопоставить записи в справочниках разных ИТ-систем, и регламенты. Такой метод относительно прост, но имеет ряд недостатков – он не предотвратит рассинхронизацию НСИ в разных системах, а регламенты всегда можно обойти.\n",
      "\n",
      "Внедрение MDM-системы\r\n",
      "Технологический подход — использование системы обеспечивающей синхронизацию и единое представление данных. Как правило большинство крупных компаний внедряют различные версии MDM, когда ручная консолидация справочной информации и отчетности становится невозможной, а внедрение любой новой системы вынуждает изменять регламент и кодировки, только усиливая хаос.\n",
      "\r\n",
      "Безусловно, единовременное введение МДМ-системы не решит все проблемы и по мере развития бизнеса, должна развиваться и МДМ-система, может даже измениться и сам тип МДМ системы (основные типы освещены ниже), однако, как показывает практика MDM является оптимальным бизнес решением в подобных случаях.\n",
      "\n",
      "Типы МДМ-систем\r\n",
      "Мы рассмотрим три основных типа MDM-систем — подробнее можно прочитать тут.\n",
      "\n",
      "Централизованная система\n",
      "\r\n",
      "Выбирается одна IT система, это может быть как уже имеющаяся IT-система, так и отдельная система управления НСИ. Справочные данные в этой системе будут считаться эталонными, вестись в ней и рассылаться в другие системы. При этом создание и редактирование справочных данных в других IT системах запрещается. Преимуществами такого подхода являются:\n",
      "\n",
      "\n",
      " Простота внедрения;\n",
      " Простота поддержки актуальности и чистоты справочных данных во всех IT – системах, простота администрирования и разграничения прав;\n",
      " Актуальные и чистые справочные данные во всех IT –системах, что позволяет строить чистую локальную отчетность в IT системах.\n",
      "\r\n",
      "Но данный метод имеет ряд недостатков — в других системах невозможно создавать и редактировать записи определенные в центральной системе. То есть изменяются внутренние бизнес-процессы компании, что часто нежелательно, а иногда и недопустимо. Так же система неустойчива к обрывам связи и работоспособность критически зависит от текущей доступности центральной системы.\n",
      "\n",
      "Аналитическая система\n",
      "\r\n",
      "В аналитической системе НСИ все элементы НСИ создаются в клиентских системах, откуда отправляются в систему НСИ, где из этих элементов формируется запись справочника НСИ. Это позволяет быстро внедрять систему, внося минимальные изменения в клиентские системы. \n",
      "\r\n",
      "Но так как НСИ в отдельно взятой IT-системе ни с чем не синхронизируется, то в самой IT-системе могут быть дубли и отчетность может расплыться, поэтому построение оперативной отчетности затруднено (про локальную отчетность также говорят, что она «грязная» — локальные записи НСИ могут не соответствовать записям в системе НСИ).\n",
      "\n",
      "Гармонизированная система\n",
      "\r\n",
      "Эта система вобрала в себя лучшее из централизованной и аналитической систем. Она позволяет заводить данные в IT-системах, и затем сопоставлять с уже заведенными, умеет искать потенциальные дубли, разрешать конфликты, связанные с одновременным изменением одних и тех же данных в разных IT-системах, синхронизировать НСИ в IT-системах. Таким образом не меняются и не нарушаются бизнес-процессы, минимизируются ручная работа по подготовке отчетности — то есть просто строиться локальная отчетность. Однако данные подход является наиболее дорогим, трудоёмким и требуют серьезной экспертизы для построения, а так же может потребовать модификации клиентских приложений.\n",
      "\n",
      " Примеры реализации MDM-систем \r\n",
      "Примером аналитической системы управления НСИ является Navicon SalesOut, а примером централизованной и гармонизированной – разные конфигурации Navicon MDM.\n",
      "\n",
      "Индикаторы необходимости внедрения МДМ-систем\r\n",
      "Ключевые: необходима интеграция различных систем и единая отчетность на основе этих данных.\n",
      "\r\n",
      "Частные предпосылки внедрения на примере с одним из клиентов\n",
      "\n",
      "\r\n",
      "Общие индикаторы, при которых стоит задуматься о необходимости упорядочения НСИ, настройки процессов MDM:\n",
      "\n",
      "В первую очередь это наличие или планы по внедрению нескольких ИТ-систем;\n",
      " Потребности в автоматизации сквозных бизнес-процессов (т.е. процессов, в которые вовлечено несколько ИТ-систем) – потребность в интеграции;\n",
      " Потребность в консолидированной отчетности (т.е. в отчетности, использующей данные из нескольких ИТ-систем);\n",
      " Разработка ИТ-стратегии. Многие компании предпочитают решать проблемы с НСИ до их появления. Чем дольше справочные данные велись в ИТ-системах независимо друг от друга, тем сложнее будет в будущем их сверять, чистить, синхронизировать.\n",
      "\n",
      "\n",
      "Выводы\r\n",
      "Основные тезисы и выводы: синхронизация НСИ облегчает 1) внедрение новых информационных систем в IT инфраструктуру компании; 2) интеграцию имеющихся систем; 3) обработку корпоративных данных; 4) сокращает трудозатраты на актуализацию данных; 5) минимизирует риски, связанные с некорректными данными. Внедрение выделенной системы управления НСИ не всегда является обязательным, но о проблемах, которые могут возникнуть из-за рассинхронизации НСИ всегда стоит помнить при развитии IT инфраструктуры.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "4 июня в Одессе, наша команда FlyElephant совместно с GeeksLab будет проводить третью ежегодную техническую конференцию по искусственному интеллекту и большим данным — AI&BigData Lab. \n",
      "\n",
      "На конференции разработчики обсудят вопросы реализации и применения различных алгоритмов, инструментов и новых технологий для работы с большими данными и искусственным интеллектом. Будут представлены воплощенные в жизнь проекты, рассказано о функционале и принципах их работы.\n",
      "\n",
      "Программа конференции AI&BigData Lab уже частично сформирована. Среди принятых докладов можно отметить:\n",
      "\n",
      "\n",
      " Как мы научили думать digital. (Диана Лиманская, координатор отдела аналитиков в VertaMedia)\n",
      "Когда вы заходите на сайт, что бы посмотреть любимый фильм или сериал, вы ждете несколько секунд прежде, чем вам будет показана реклама. В эти несколько секунд до показа рекламы происходят сотни сложных процессов. Я хочу рассказать как мы создали самообучающуюся систему для digital-маркетинг платформы на основе математических методик, какие этапы мы прошли в ее развитие и с какими проблемами столкнулись.\n",
      "\n",
      " Перевод с «плохого» английского на «хороший». (Анатолий Востряков, исследователь в Grammarly)\n",
      "В начале я хочу дать небольшой исторический экскурс в способы автоматической коррекции ошибок в тестах. В основной части доклада я хотел бы осветить самые последние методы коррекции ошибок с использование, то что сейчас называется neural machine translation. То есть мы, переводим английский текст в английский, но с исправленными ошибками на выходе. К сожалению я ограничен в конкретных примерах из практики Грэммэрли, поэтому доклад будет в форме обзора алгоритмов, которые есть уже сейчас или появятся к моменту доклада.\n",
      "\n",
      " Эффективное вычисление k средних величин для распределенного потока больших данных. (Артем Баргер, Research Engineer в IBM​​, Израиль)\n",
      "На докладе я предоставлю детерминированный алгоритм позволяющий эффективного вычислять k средних велечин (k-means) в непрерывном потоке данных в режиме реального времени. Сублинейный алгоритм использует только logn*k^O(1) памяти, также легко адаптируется под распределенные системы вычеслений позволя с уменьшить время вычисление прямопропорционально доступным вычислительным мощностям. В конце будут предсталены эмпирические результаты для популярных наборов данных.\n",
      "\n",
      " #DataForGood — как изменить мир к лучшему с помощью анализа данных. (Максим Терещенко, Product Owner в Zoomdata​)\n",
      "О применении Big Data для оптимизации и улучшения эффективности принятия решений в бизнесе говорится уже не мало. Практически каждая крупная корпорация имеет в своем арсенале Big Data платформу. Но в рамках доклада хотелось бы отойти от бизнеса и рассмотреть тему применения AI и Big Data для социальных проектов. Сотни и тысячи аналитиков, Data Scientists, Big Data инженеров объединяются и реализуют проекты, которые меняют жизнь простых людей во всем мире и, особенно, помогают жителям слабо-развитых стран. Здесь идет речь о совсем другом уровне мотивации и командной работы. В рамках доклада, хотелось бы обсудить, что движет этими людьми, какие реальные проекты с какими технологиями были реализованы и как они поменяли жизнь людей. \n",
      "\n",
      " Методология Data Science проектов. (Сергей Шельпук, Head of Data Science в V.I.Tech)\n",
      "Проекты в области анализа данных — вызов не только для инженеров, но и для менеджеров. Доклад будет посвящён особенностям таких проектов по сравнению с обычной разработкой, ролям в команде и построению взаимодействия с заказчиком в условиях неопределённости R&D.\n",
      "\n",
      " Обучение глубоких, очень глубоких и рекуррентных сетей. (Артем Чернодуб, н.с. в ИПММС НАНУ)\n",
      "В докладе представлен обзор новых подходов к обучению глубоких и рекуррентных нейросетей. Обсуждаются ортогональная инициализация весов для сверточных и рекуррентных нейросетей и ее влияние на проблему исчезновения градиентов (vanishing gradient effect), нормализацию мини-пакетов (batch normalization),  разностное обучение (residual learning).\n",
      "\n",
      " MOLAP: Новые границы возможного. (Константин Герасименко, CEO в Easy MOLAP​, Германия)    \n",
      "Рассказ о том что такое MOLAP. Сравнение с традиционными подходами. Преимущества и недостатки. \n",
      "\n",
      " Спайковые и бионические нейронные сети: проблемы и перспекитвы. (Дмитрий Новицкий, ст. научный сотрудник, доцент в Институт Кибернетики НАНУ)\n",
      "В мире машинного обучения многие годы доминируют нейронные сети прямого распространения (feed-forward), которые почти ничего общего не имеют с нейронами и сетями нашего мозга. В этом докладе мы познакомимся с бионическими (biologically plausible) нейронными сетями. В большинстве из них нейроны испускают и принимают импульсы (спайки). Какие возникают проблемы и сложности обучения таких сетей? В каких традиционно нерешаемых (или плохо решаемых) задачах они могут быть эффективны, как эффективен в них мозг человека и животных? Как можно реализовать такие сети аппаратно, и что такое нейроморфный компьютинг? — Вот вопросы, которым посвящена данная презентация.\n",
      "\n",
      "\n",
      "Регистрация и все подробности на сайте конференции. Для читателей нашего блога действует скидочный промокод на 15%: FlyElephantHabrahabr.    \n",
      " \n",
      "источник картинки: southriverrestoration.com/wp-content/uploads/2015/04/Power-of-Communication-STOCK.jpg\n",
      "\n",
      "Как известно, качество открытых данных (в частности данных о госфинансах) часто оставляет желать лучшего. В некоторых случаях это не мешает их использовать, в других — требуются комментарии источников данных или дополнительные сведения.\n",
      "\n",
      "Источниками открытых данных в основном являются госорганы, при взаимодействии с которыми есть, как минимум, одна большая проблема — 30 дней на ответ. Не все программисты обладают достаточным терпением, да и для представителей коммерческих компаний такое ожидание неприемлемо. Но, даже если вы дождались ответа, радоваться приходиться далеко не всегда — иногда вы получаете ответы не на все заданные вопросы, иногда вам предлагают обратиться в другие госорганы, что требует дополнительного ожидания. Попробуем систематизировать, какие еще есть способы «достучаться» до госорганов по вопросам, связанным с их открытыми данными. \n",
      "\n",
      "Нулевой способ, доступный только для «избранных» — найти знакомого или знакомого знакомого в интересующем госоргане. Да, этот способ нельзя назвать официальным, но, к сожалению, он самый эффективный (могу предположить, что это связано с «менталитетом» или «традициями в госуправлении»). В некоторых случаях это единственный вариант относительно быстро получить интересующие данные или комментарии, поэтому нетворкинг и посещение мероприятий жизненно необходимы (как истинный интроверт пишу об этом с болью).\n",
      "\n",
      "Первый и самый традиционный способ, — это официальные обращения. Чаще всего их можно отправить через электронную форму на сайте или на официальную электронную почту (хорошо, что не нужно пользоваться Почтой России или приносить обращения лично). Несомненный плюс этого способа — это регулирование законом 59-ФЗ, согласно которому ответить вам обязаны. Минусы — ответ придется подождать месяц. Образцы обращений по запросу информации можно посмотреть на сайте ИРСИ, но основные правила просты: будьте вежливы и конкретны, полезным будет сослаться на нормативно-правовую базу.\n",
      "\n",
      "Пример 1. Обращение через электронную форму в Федеральное Казначейство. При использовании данных по госконтрактам аналитиками Инфокультуры было замечено, что в выборке по контрактам, где 'ИНН поставщика = ИНН Сбербанка' встречаются контракты с физ. лицами и компаниями, которые явно не относятся к Сбербанку. Не найдя объяснения этому явлению, был отправлен соответствующий запрос в Федеральное Казначейство, которое отвечает за данные портала ГосЗакупки. Ответ пришел через неделю (это очень быстро для госоргана), но он оказался бесполезным: обращение рассмотрено, персональную ответственность несет лицо, заполняющее документы от имени заказчика, в госзакупках реализован предупреждающий контроль. Что делать с ответственностью заказчика и почему предупреждающий контроль не работает осталось не ясным.\n",
      "\n",
      "Ответ ФК\n",
      "\n",
      "\n",
      "После беглого анализа данных выяснилось, что чаще всего неправильные данные указывают муниципальные организации (особенно детские сады), поэтому ошибки при заполнении документов можно списать на низкую ИТ-грамотность (имхо).\n",
      "\n",
      "Пример 2. Обращение через электронную почту в Муниципалитет Санкт-Петербурга. Это мой любимый запрос, потому что в нем госорган сделал все ошибки, которые только можно было сделать, а все эксперты, посвященные в проблему, считали, что муниципалитет прав. Проблема была в том, что в бюджете Дворцового муниципального образования использовался один и тот же код для обозначения двух разных муниципальных программ (что недопустимо согласно Бюджетному кодексу). Для меня этот вопрос был принципиален, потому что при анализе бюджетов Санкт-Петербурга мы брали за аксиому уникальность сочетания «код муниципальной программы» — «наименование муниципальной программы» и наличие данной ошибки влияло на обработку данных. Первое электронное письмо в Дворцовый осталось без ответа. Перезвонила я им через четыре месяца, так и не получив ответа. Ответившая на звонок девушка перестала дышать, когда услышала, что молчат они 4 месяца (а 59-ФЗ никто не отменял). Она перевела меня на секретаря, тот на бухгалтера, а по версии «бухгалтера» на сайте «были ошибочные данные, в ее данных все в порядке и они опубликуют правильные файлы на сайте». Конечно же они ничего не опубликовали. И, после повторного звонка, заявили, что у них все в порядке и они отправят разъяснения по почте, потому что я не понимаю бюджетную классификацию.\n",
      "\n",
      "Разъяснения Дворцового МО\n",
      "\n",
      "Муниципальная программа (согласно Бюджетному Кодексу) кодируется 7 знаками. В ответе говорится, что у Дворцового МО она кодируется 11 знаками (включая код раздела и подраздела). Чтобы доказать наличие ошибки, потребовалось обратиться в Комитет финансов СПб, после пересылки ответа которого Дворцовый признал ошибку (дополнительно проконсультировавшись с Комитетом финансов СПб) и пообещал ее исправить в 2016 году. Попутно они согласились возобновить публикацию своего бюджета в XLS (считаю это дополнительным бонусом).\n",
      "\n",
      "Интересно, что в Комитете финансов СПб мнения разделились и параллельно с официальным ответом, согласно которому коды повторяться не могут, я получила неофициальный, согласно которому коды повторятся могут.\n",
      "\n",
      "Выводов два. Первый: в бюджетах муниципалитетов действительно много ошибок (и опечаток). Второй: есть трудности с правильным пониманием бюджетной классификации муниципальными органами. \n",
      "\n",
      "Пример 3. Запрос данных о муниципальных бюджетах Ленобласти. На примере Санкт-Петербурга выяснилось, что бюджеты 100 муниципалитетов собирать по их официальным сайтам очень долго и утомительно. Появилось предположение, что их можно запросить у регионального комитета финансов. Предположение оказалось провальным: муниципальная власть не является третьим уровнем власти и не подчиняется региональной, поэтому муниципальные данные есть только у муниципалитетов (запрашивать их у регионов и Минфина бесполезно). Но мне показался необычным процесс коммуникаций с Ленинградской областью. Все обращения отправляются с сайта Ленобласти (а не напрямую с сайта интересующего госоргана) и попадают в «центр обращений», из которого их отправляют в нужные госорганы. Весь следующий день до меня дозванивалась представительница Ленобласти, которой для регистрации обращения требовалось узнать, в каком городе я живу (понятия не имею, зачем это нужно и почему этого вопроса не было в электронной форме), но после этого ответ был получен за 4 (!) дня.\n",
      "\n",
      "На встрече Минфина с разработчиками, которая прошла 16 июня, выяснилось, что при наличии «проблем» с муниципалитетами обращаться нужно или в Генеральную прокуратуру (или писать письмо В.В. Путину). Предвкушаю, как и тот и другой адресаты будут рады получать кучу писем от Инфокультуры с такими важными в масштабах государства проблемами, как опечатки в муниципальных бюджетах :).\n",
      "\n",
      "Второй способ коммуникаций — написать на почту, предназначенную для вопросов по открытым данным (например, opendata@minfin.ru у Минфина), администратору или технической поддержке сайта, оставить вопрос на специальном форуме (например, форум Казначейства на budget.gov.ru). В этом случае есть шанс, что ответ вы получите быстрее и ваш вопрос попадет напрямую к отвечающим за открытые данные людям, а при использовании форумов ответ будет доступен не только вам, но и всем заинтересованным. Правда, такие письма не всегда приравнены к официальным обращениям, и вероятность не получить никакого ответа все же есть.\n",
      "\n",
      "Третий способ, набирающий особенную популярность в июне, — участие в хакатонах и конкурсах. один из хакатонов пройдет на этих выходных в Петербурге и будет посвящен финансовым данным, другой — в Москве в июле, ну а всем известный конкурс Минфина BudgetApps в представлении не нуждается. Госорганы не только начинают участвовать в подобных мероприятиях, но и сами их организовывают. А это значит, что вы не только можете получить прямой доступ к представителям госорганов на самом мероприятии (поверьте, отвертеться от неожиданных вопросов, заданных в живую намного сложнее, чем придумать отписку за месяц), но и получить дополнительный канал связи (форму для запросов или специальную почту). Ценность этого способ в том, что не только вам нужны госорганы, но и они нуждаются в вас — как минимум, отчетность и пиар никто не отменял, а в идеальном случае это сопровождается еще и искренней заинтересованностью госорганов в получении содержательного результата. И, даже если на мероприятии представители госорганов не присутствуют, вы всегда можете обратиться к менторам и экспертам, которые могут связать вас с ними или что-то подсказать.\n",
      "\n",
      "Есть еще один, эксклюзивный, способ общения с Минфином — это встречи с разработчиками, которые проходят второй год (не уверена, что о них всегда можно узнать заранее, но будем считать, что это так). Одна из первых встреч прошла год назад, самая последняя — на прошлой неделе. И, по-моему, они начинают “работать” — докладов больше и они содержательнее, госорганы немного вникли в предметную область (открытые данные) и начинают понимать, о чем говорят, представители коммерческих компаний готовят содержательные доклады. Единственный минус — “информационное сообщение” Минфина об этих встречах пока страдает: встреча длилась два часа, докладов было много, ответы на вопросы звучали интересные, а информационное сообщение только о 20-минутном докладе Минфина (да еще и написано так, что даже заинтересованным в открытых данных читать скучно).\n",
      "\n",
      "Если ни один из описанных способов не помог — не отчаивайтесь, обращайтесь не к первоисточнику, а к сообществу, экспертам и проектам. Например, обо всем, что относится к госконтрактам и госфинансам (да и в целом к открытым данным), вы можете спросить у нас ;-).    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Команда FlyElephant приглашает всех 13 мая в Одессу на IV конференцию по практическому применению науки о данных Data Science Lab (exAI&BigDataLab).\n",
      "\n",
      "Data Science Lab — это ежегодная техническая конференция, которая объединяет исследователей, инженеров и разработчиков, связанных с Data Science для обмена опытом и обсуждения актуальных тем в области машинного обучения, обработки естественного языка, распознавания образов и других аспектов анализа данных. Темы докладов раскрывают вопросы от практического внедрения результатов исследований до самых последних теоретических разработок.\n",
      "\n",
      "Среди докладов, можно отметить следующие:\n",
      "\n",
      "\n",
      "From bag of texts to bag of clusters (Терпиль Евгений / Павел Худан, Data Scientists / NLP Engineer at YouScan).\n",
      "Сходство пациентов: вычистка дубликатов и предсказание пропущенных диагнозов (Виктор Сарапин, CEO at V.I.Tech)\n",
      "Cервинг моделей, построенных на больших данных с помощью Apache Spark (Степан Пушкарев, GM (Kazan) at Provectus / CTO at Hydrosphere.io).\n",
      "Графические вероятностные модели для принятия решений в проектном управлении (Ольга Татаринцева, Data Scientist at Eleks).\n",
      "BioVec: Word2Vec в задачах анализа геномных данных и биоинформатики (Дмитрий Новицкий, Старший научный сотрудник в ИПММС НАНУ).\n",
      "Optimizing ML Hyper-parameters with Bayesian Optimization (Maksym Bevza, Research Engineer at Grammarly).\n",
      "Data Sciences и Big Data в Телекоме (Александр Саенко, Software Engineer at SoftServe/CISCO).\n",
      "Обобщенное предпочтительное присоединение (Александр Крот, Head of Data Science at Sberbank).\n",
      "Мониторинг модных трендов с помощью глубокого обучения и TensorFlow (Ольга Романюк, Data Scientist at Eleks).\n",
      "Как знать всё о покупателях (или почти всё)? (Дарина Перемот, ML Engineer at SynergyOne).\n",
      "Коррекция геометрических искажений оптических спутниковых снимков (Алексей Кравченко, Senior Data Scientist at Zoral Labs).\n",
      "\n",
      "Среди блиц-докладов, можно отметить следующие:\n",
      "\n",
      "\n",
      "Энтерпрайз «из коробки» (Сергей Шельпук, Head of Data Science Office at Eleks).\n",
      "Recent deep learning approaches for speech generation (Дмитрий Белевцов, Techlead at IBDI).\n",
      "Распределенные вычисления: использование BOINC в Data Science (Виталий Кошура, Software Developer at Lohika).\n",
      "Применение машинного обучения при разработки HR продукта (Мамед Халилов, CEO & Founder at Morbax HR).\n",
      "\n",
      "Полная программа конференции, все детали и регистрация по адресу: www.datascience.in.ua.\n",
      "\n",
      "Для читателей нашего блога действует 10% скидочный промо-код: FlyElephant10.    \n",
      " Пару недель назад в Яндексе прошла встреча PyData, посвящённая анализу больших данных с использованием Python. В том числе на этой встрече выступил Василий Агапитов — руководитель группы разработки инструментов аналитики Яндекса. Он рассказал о двух наших библиотеках: для описания и запуска расчетов на MapReduce и для извлечения информации из логов.\n",
      "\n",
      "\n",
      "Под катом — расшифровка и часть слайдов.\n",
      "\n",
      "\n",
      "Меня зовут Агапитов Василий, я представляю команду интеллектуального анализа данных.\n",
      "\n",
      "В Яндексе мы выполняем расчеты по большим данным, в частности по данным, лежащим на кластерах MapReduce. В основном это анонимизированные логи сервисов и приложений. Кроме того, мы предоставляем наши инструменты по обработке больших данных другим командам. Наши основные потребители — команды аналитиков-разработчиков. Для простоты я буду называть их аналитиками.\n",
      "\n",
      "Хочу рассказать о двух инструментах, библиотеках, истории их появления, и том, как мир Hadoop оказал влияние на их появление.\n",
      "\n",
      "\n",
      "\n",
      "Давайте синхронизируем некоторое представление о терминах. Источники данных. Мы будем говорить исключительно о логах, поэтому для примера давайте рассмотрим какой-нибудь лог доступа к сервисам.\n",
      "\n",
      "\n",
      "\n",
      "У нас по горизонтали располагаются записи этого лога, каждая запись имеет поля: vhost — идентификатор хоста, yandexuid — идентификатор посетителя, iso_eventtime — дата и время обращения, request — сам запрос и многие другие поля.\n",
      "\n",
      "Данные из некоторых полей уже можно использовать в расчетах. Из других полей данные сначала надо извлечь и нормализовать. Например, в поле request содержатся параметры запроса. У каждого сервиса такие параметры свои. Для поиска наиболее используемым является параметр text. После того, как мы извлечем его из поля request, нам надо его нормализовать, поскольку он может быть очень большим или иметь какую-то странную кодировку.\n",
      "\n",
      "\n",
      "\n",
      "Во-вторых, мы будем рассматривать наши расчеты в части расчетов на кластерах MapReduсe. Как вы знаете, MapReduсe — технология изготовления сэндвичей. На самом деле нет, это технология обработки больших данных. Если вы с ней не знакомы, то для текущего доклада вам надо знать, что она предполагает обработку данных с использованием двух операций — Map и Reduce.\n",
      "\n",
      "Задача аналитика в том, чтобы построить расчет по событиям лога с учетом некоторой бизнес-логики. С какими трудности может столкнуться аналитик, решая свою задачу на кластерах MapReduсe без использования каких-либо библиотек? Во-первых, ему придется реализовывать бизнес-логику на базе операций MapReduсe. Такой подход добавляет в расчет код, не относящийся к бизнес-логике этого расчета, что существенно ухудшает его читаемость и поддержку. Во-вторых, нам надо данные из логов сначала извлечь и нормализовать — например, параметр text из поля request.\n",
      "\n",
      "Как принято решать первую проблему? Очевидно, нам нужна какая-то библиотека, которая упростит доступ пользователя к кластеру и взаимодействие с ним.\n",
      "\n",
      "В мире Hadoop к таким библиотекам можно отнести Pig, Hive, Cascading и некоторые другие.\n",
      "\n",
      "В Яндексе используется собственная реализация MapReduсe, называемая YT, о преимуществах которой вы можете почитать в статье на Хабре и которая предоставляет для обработки данных базовые операции MapReduсe. Но, к сожалению, YT не имела аналогов библиотек из мира Hadoop. Нам пришлось это исправить.\n",
      "\n",
      "В самом начале, когда мы столкнулись с этой проблемой, мы действительно при каждом расчете отдельно описывали Map-стадии, отдельно — Reduce-стадии, и отдельно — связь между этими стадиями для запуска расчета на кластере.\n",
      "\n",
      "\n",
      "\n",
      "Более того, у каждого был собственный запускатор. Поддерживать такой зоопарк очень дорого. Решением для нас стала библиотека Nile, библиотека для описания и запуска расчетов на кластере. При ее создании мы взяли идею Сascading из мира Hadoop и реализовали ее на языке Python — во многом потому, что Python использует аналитики для локальной обработки данных, а использовать один язык для расчетов на кластере и для локальной обработки данных очень удобно.\n",
      "\n",
      "\n",
      "\n",
      "Если вы знаете Cascading, то процесс обработки данных на Nile вам также покажется знакомым. Мы создаем поток из таблиц на кластере, модифицируем его, группируем, например, считаем какие-то агрегаты, разбиваем поток на несколько потоков, объединяем несколько потоков в один поток, производим другие действия, после чего полученный поток с нужными данными сохраняем обратно в таблицу на кластер.\n",
      "\n",
      "Какие у Nile есть операции модификации потока? Их очень много, здесь представлены наиболее часто используемые. Project, чтобы получить список нужных нам полей. Filter, чтобы отфильтровать, оставив только нужные нам записи. Groupby + aggregate, чтобы сгруппировать поток по заданному набору полей и посчитать некоторые агрегаты. Unique, random и take, чтобы построить выборки уникальную, случайную и с заданным числом записи. Join, чтобы объединить два потока по равенству заданного набора полей. Split, чтобы разбить поток на несколько потоков по некоторому правилу с дальнейшей индивидуальной обработкой каждого из них. Sort, чтобы отсортировать. Put, чтобы положить таблицу на кластер.\n",
      "\n",
      "Операции Map и Reduce также доступны, но требуются крайне редко, когда нужно сделать что-то действительно нестандартное и сложное. \n",
      "\n",
      "\n",
      "\n",
      "Давайте посмотрим на инициализацию Nile. Она довольно проста. После импорта мы создаем два объекта — cluster и job. Cluster требуется, чтобы указать, на каком кластере мы хотим запускаться и прочее примерное окружение. Job — чтобы описать процесс модификации потока.\n",
      "\n",
      "\n",
      "\n",
      "Как создать поток? Поток можно создать двумя путями: из таблицы на кластере или из существующих потоков. Первые два примера показывают, как создать поток из таблицы на кластере. В качестве аргумента передан путь до таблицы на кластере. Последний пример показывает, как нам создать поток из двух существующих потоков путем их слияния.\n",
      "\n",
      "Давайте рассмотрим какой-то пример реализации задачи на Nile.\n",
      "\n",
      "\n",
      "\n",
      "По логам доступа нужно посчитать число посетителей на хосте yandex.com.tr. Вспомним, как выглядят наш логи доступа. Из всего множества представленных полей нас будут интересовать поля vhost, чтобы отфильтровать и оставить только записи, относящиеся к хосту yandex.com.tr, и yandexuid, чтобы посчитать число посетителей.\n",
      "\n",
      "Сам код на Nile для этой задачи будет иметь следующий вид.\n",
      "\n",
      "\n",
      "\n",
      "Тут мы создаем поток из таблицы на кластере, получаем поля vhost и yandexuid. Оставляем только записи со значением поля vhost, равным yandex.com.tr, и считаем число уникальных значений поля yandexuid, после чего сохраняем поток в таблицу на кластере. Job.run() запустит нас расчет.\n",
      "\n",
      "\n",
      "\n",
      "Перед запуском расчета на кластере Nile переведет наш расчет в набор MapReduсe-операций. Слева граф преобразования потока в терминах Nile, справа — в терминах MapReduсe-операций. Кроме того, Nile автоматически оптимизирует наш расчет, а именно, если у нас есть несколько Map-операций, идущих подряд, то Nile может их склеить в одну Map-операцию, выполняемую на кластере. Мы видим, что так и произошло. Это довольно простая задача, и код для нее будет сравнительно просто выглядеть на любом языке программирования.\n",
      "\n",
      "Чтобы рассмотреть что-то более сложное, давайте вспомним о второй проблеме, с которой сталкиваются аналитики: данные из логов нужно извлечь и нормализовать.\n",
      "\n",
      "Как принято решать эту проблему? Обычно для решения этой проблемы используют ETL. Кто знает, что это? Процентов 30 знает. Кто не знает или знал, но забыл: ETL предполагает, что у нас есть сырые необработанные данные, мы из них производим извлечение нужных нам записей, полей и прочего, модифицируем их с учетом некоторой бизнес-логики и загружаем в хранилище. В дальнейшем мы будем производить все расчеты по данным из хранилища, то есть по нормализованным данным.\n",
      "\n",
      "Мы выбрали другой путь. Мы храним сырые данные и выполняем расчеты по ним, а процесс извлечения и нормализации данных происходит в каждом расчете.\n",
      "\n",
      "Почему мы выбрали такой путь? Предположим, в процессе извлечения и нормализации полей используется внешняя библиотека, и в этой библиотеке была бага. После того, как мы багу исправим, нам надо будет пересчитать расчеты за прошлое. В нашем подходе мы просто запускаем расчеты за прошлое и получаем верные результаты. В случае ETL — если эта библиотека использовалась в процессе — нам придется сначала данные заново извлечь, обработать этой библиотекой, положить в хранилище и только потом выполнить расчеты.\n",
      "\n",
      "Хороший вариант, если вы можете хранить как сырые данные, так и нормализованные. У нас, к сожалению, из-за большого объема данных такой возможности нет.\n",
      "\n",
      "Вначале мы для каждого расчета выполняли извлечение и нормализацию данных индивидуально. Затем мы заметили, что для одних и тех же логов мы часто достаем похожие поля примерно одним и тем же образом, и мы объединили эти правила в одну библиотеку QB2. Сами правила назвали экстракторами, наборы таких правил для логов — деревьями разбора.\n",
      "\n",
      "\n",
      "\n",
      "Итак, сейчас библиотека QB2 предоставляет абстрактный интерфейс к сырым логам и знает про деревья разбора. \n",
      "\n",
      "\n",
      "\n",
      "Давайте познакомимся с деревом разбора. Это дерево разбора для логов мобильных приложений Яндекса. Не присматривайтесь, это глобальная карта.\n",
      "\n",
      "В самом верху запись лога. Все остальные цветные прямоугольники — это виртуальные поля. Связи — это экстракторы. Таким образом, пользователь без библиотеки QB2 может обращаться только к полям лога. Пользователь, использующий библиотеку QB2, может использовать как поля лога, так и поля, предоставляемые библиотекой QB2 для данного лога.\n",
      "\n",
      "Давайте рассмотрим преимущества этой библиотеки на конкретном примере. Рассмотрим задачу. Пусть у нас есть некоторое мобильное приложение Яндекса, идентифицируемое значением поля API_key, равным 10321. Мы знаем, что оно пишет логи в поля, в частности в поле event_value, содержащее словарь в виде JSON-объекта. Нас будет интересовать значение ключа stage этого словаря.\n",
      "\n",
      "Нам надо посчитать число посетителей для каждого stage в разбивке по дате события. Какие поля нам для этого предоставляет библиотека QB2? Во-первых, API_key, идентификатор приложения. Во-вторых, device_id, идентификатор пользователя. И наконец event_date, дата события, которая получена не непосредственно из записи лога, а путем довольно большого числа преобразований. Без использования библиотеки QB2 нам бы пришлось в каждом расчете, где требуется это поле, выполнять преобразования вручную. Согласитесь, это неудобно.\n",
      "\n",
      "Кроме того, нам нужны поля event_value и значение stage. Их в нашем дереве разбора нет, и это логично, потому что они пишутся только для одного конкретного приложения.\n",
      "\n",
      "Нам придется дополнить наше дерево разбора до следующего вида путем применения экстракторов. Как поменяется инициализация для данной задачи? Мы дополнительно импортируем экстракторы и фильтры. Экстракторы потребуются, чтобы получить значения полей event_value и stage. Фильтры — чтобы отфильтровать записи, оставив только нужные.\n",
      "\n",
      "\n",
      "\n",
      "Вы можете задаться вопросом: почему в данном примере мы использовали фильтрацию из библиотеки QB2, хотя в предыдущем использовали фильтрацию из Nile? QB2, как и Nile, старается оптимизировать ваш расчет, а именно она пытается получить значения для полей, используемых в фильтрациях, как можно раньше. Раньше, чем значения для остальных виртуальных полей.\n",
      "\n",
      "Зачем это сделано? Чтобы мы не получали значения для остальных виртуальных полей, если наша запись не проходит по каким-то условиям фильтрации. Тем самым мы сильно экономим вычислительные ресурсы и ускоряем расчет на кластере.\n",
      "\n",
      "\n",
      "\n",
      "Сам код расчета будет иметь следующий вид. Мы тут точно так же создаем поток из таблицы на кластере и модифицируем его оператором QB2, который инициализируем следующими вещами: именем дерева разбора, которое в нашем случае совпадает с именем лога, а также набором полей и фильтров.\n",
      "\n",
      "В полях мы перечисляем поля API_key, device_id и event_date по их именам, потому что библиотека QB2 уже знает, как доставать эти поля.\n",
      "\n",
      "Для извлечения поля event_value воспользуемся стандартным экстрактором json_log_field. Что он делает? По переданному ему имени он получает значение из соответствующего поля лога и загружает его как JSON-объект. Это загруженное значение мы сохраняем в поле event_value. \n",
      "\n",
      "Чтобы получить значение stage, мы используем другой стандартный экстрактор — dictItem. По переданному ему имени ключа и имени поля он извлекает соответствующее значение из этого поля для этого ключа.\n",
      "\n",
      "Про фильтрации. Нас будут интересовать только записи, у которых определено значение поля device_id и которые относятся к нашему приложению, то есть значение поля api_key у них равно 10321. После применения оператора QB2 наш поток будет иметь следующие поля: api_key, device_id, event_date, event_value и stage. \n",
      "\n",
      "Модифицируем полученный поток следующим образом. Сгруппируем по паре полей event_date и stage и посчитаем число уникальных значений device_id. Это значение мы положим в поле users, после чего полученный поток сохраним на кластер. Job.run() запустит расчет.\n",
      "\n",
      "\n",
      "\n",
      "После окончания расчета на кластере будет таблица следующего вида: для каждой пары полей event_date и stage в поле users будет лежать число уникальных значений пользователей. Таким образом, интеграция библиотек QB2 и Nile решает обе проблемы, которые я озвучил. Спасибо за внимание.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "«Данные – нефть цифровой экономики» — выражение, которое уже стало афоризмом. Действительно, в современном мире пользовательские данные превратились в один из наиболее ценных и востребованных ресурсов. Так, по данным компании PwC, в 2018 году мировая выручка от использования пользовательских данных достигнет $300 млрд. Что касается России, то по данным журнала РБК в 2017 году оборот рынка продажи и покупки персональных данных в России составил не менее 3,3 млрд рублей. Более того, эксперты прогнозируют дальнейший интенсивный рост этого рынка. \n",
      "\r\n",
      "Тем не менее, использование персональных данных в бизнесе пока не имеет должного правового регулирования. Текущее законодательство оставляет открытым вопрос об оборотоспособности данных и возможности их монетизации. Также в судебной практике пока не сформированы универсальные критерии, позволяющие найти баланс между необходимостью защиты частной жизни пользователей и потребностями бизнес-сообщества в условиях цифровой экономики. \n",
      "\n",
      "Данные: Персональные, Большие или Большие пользовательские? \n",
      "\n",
      "Персональные данные\n",
      "\n",
      "\r\n",
      "Краеугольным камнем для понимания термина «пользовательские данные» является нормативно закреплённое понятие «персональные данные» (далее — ПДн). Понятие ПДн носит «каучуковый» характер, не позволяющий однозначно определить, какие конкретно данные относятся к персональным. При этом, общей тенденцией сейчас является крайне широкий подход к понятию ПДн — любая информация, относящаяся к идентифицированному или идентифицируемому  лицу. Это может быть как IP-адрес, ID, номер мобильного телефона либо номер кредитной карточки.\n",
      "\r\n",
      "В то же время ни для кого не секрет, что такая информация активно обрабатывается во всемирной паутине, становится основой успеха множества бизнес-проектов (реклама, маркетинг, скоринг). А каких-нибудь однозначных критериев, позволяющих проводить границу между ПДн и Большими данными не выработано. \n",
      "\n",
      "\n",
      "Большие данные\n",
      "\n",
      "\r\n",
      "В свою очередь понятие «большие данные» (Big data) носит ещё более дискуссионный характер и обычно раскрывается через свои характеристики: \n",
      "\n",
      "\n",
      "большой объем (Volume); \n",
      "\n",
      "большое разнообразие (Variety);\n",
      "\n",
      "высокая скорость накопления и обработки (Velocity);\n",
      "\n",
      "точность (Veracity); \n",
      "\n",
      "переменчивость (Variability);\n",
      "\n",
      "ценность (Value);\n",
      "\n",
      "визуализация (Visualization); \n",
      "\n",
      "жизнеспособность (Viability). \n",
      "\n",
      "\r\n",
      "Известный юрист и профессор А. И. Савельев пишет, что «большие данные можно определить как динамически изменяющийся массив информации, который представляет собой ценность в силу своих больших объемов и возможности эффективной и быстрой обработки автоматизированными средствами, что, в свою очередь, обеспечивает возможность его использования для аналитики, прогнозирования и автоматизации бизнес-процессов».\n",
      "\r\n",
      "Управляющий партнер Центра цифровых прав Саркис Дарбинян, выступая на форуме BIG DATA 2018,  так объясняет их природу: «Большие данные – это объемные потоки информации разнородных данных, которые постоянно генерируются пользователями электронных устройств и онлайн-сервисов либо техническими устройствами, а также обрабатываются в режиме реального времени». \n",
      "\r\n",
      "В России до настоящего времени нет единого понимания и подхода к регулированию Больших данных. Кроме того, продолжаются дискуссии о том, кому должны принадлежать Большие данные и как, используя их, не нарушать права на различные категории охраняемых законом данных (ПДн, коммерческой тайной, конфиденциальной информацией, авторского права на базу данных).\n",
      "\n",
      "\n",
      "Большие пользовательские данные\n",
      "\n",
      "\n",
      "«Пересечение» ПДн и Больших Данных иногда называют Большими пользовательскими данными. Правового закрепления данный термин не имеет, однако глава Роскомнадзора Александр Жаров определил Большие пользовательские данные, как «все данные о пользователе, которые собирают информационные системы и устройства, профили пользователей на интернет-ресурсах, геолокационные данные, данные о пользовательском поведении на сайтах».\n",
      "\n",
      "Судебные споры вокруг бизнес-проектов на персональных данных\n",
      "\r\n",
      "Бизнес-проекты с использованием Больших пользовательских данных неизбежно сталкиваются с проблемой соблюдения законодательства о защите ПДн. Так, наглядными примерами здесь являются следующие судебные дела: Роскомнадзор vs. НБКИ, Роскомнадзор vs. МГТС, «HeadHunter» vs. «Робот Вера», «HeadHunter» vs. «FriendWork» и «ВКонтакте» vs. «Double Data».\n",
      "\r\n",
      "Единого подхода по вопросу использования Больших пользовательских данных, в том числе размещённых пользователями в социальных сетях, пока не выработано, а позиции различных судебных инстанций пока имеют хаотичный характер. Кроме того, стороны не всегда оперируют законодательством о защите ПДн, а ссылаются наинтеллектуальные права на базу данных. Например, нормы о защите интеллектуальных прав на базу данных использовались в делах по искам компании «HeadHunter», а также в резонансном деле «ВКонтакте» против «Double Data». \n",
      "\r\n",
      "Компания «Double Data» собирала и использовала в коммерческих целях данные пользователей, размещённые в социальной сети (фамилии, имена, места работы и учебы). Никаких дополнительных разрешений компания не получала. «Вконтакте» отстаивает позицию, что такие действия нарушают исключительное смежное право на базу данных, возникшее у «Вконтакте» как у изготовителя такой базы данных с данными пользователей. «Double Data», напротив, настаивает на открытости данных, невозможности запретить повторное использование информации и отсутствии прав у «Вконтакте» на базу данных, сформированную самими пользователями. На сегодняшний день (октябрь 2018 г.) дело дошло до СИПа (кассационная инстанция). СИП согласился с выводом апелляционной инстанции, что «из материалов дела усматривается как наличие объекта смежного права (базы данных пользователей социальной сети), так и наличие исключительного права общества «В Контакте» на указанный объект». Довод ответчиков о базе данных как «побочном продукте» деятельности «В Контакте» не был признан судом обоснованным. Однако СИП отправил дело на новое рассмотрение в суд первой инстанции (дата заседания — 19.12.2018), а потому битва «ВКонтакте» и «Double Data» еще продолжается. Видится, что после окончательного разрешения данного дело станет практикообразующим и будет определяющей вехой для развития бизнеса на пользовательских данных в России.\n",
      "\r\n",
      "Кроме того, важным для дальнейшей судьбы бизнес-проектов на пользовательских данных является дело Роскомнадзор vs. МГТС, в котором суд попытался найти баланс между правом пользователей и интересами бизнеса. Суд привлек компанию МГТС к административной ответственности, установив, что сделки по «перепродаже» данных об абонентах без их согласия нарушают право на неприкосновенность частной жизни. \n",
      "\r\n",
      "Также интересно дело Роскомнадзор vs. НБКИ, которое хоть и закончилось мировым соглашением, но в рамках рассмотрения которого ВС РФ сделал вывод, что данные после их размещения пользователями в социальной сети не становятся общедоступными согласно смыслу ст. 8 ФЗ «О персональных данных».\n",
      "\n",
      "Как на практике реализуется бизнес на персональных данных? \n",
      "\r\n",
      "Из-за отсутствия ясных и однозначных правовых подходов («правил игры») по использованию больших данных уже на протяжение многих лет существуют «серые» сервисы по продаже персональных данных. Например, Dark Web, где продаются самые разные виды ПДн: от паспортных данных до медицинской информации и паролей от кредитных карт. По результатам исследования «Черный рынок баз данных» аналитического центра «МФИ Софт» за 2016 г, объем рынка нелегальных баз данных в России – больше 30 млн рублей. И эта цифра только растёт. \n",
      "\r\n",
      "Тем не менее, не стоит полагать, что бизнес на персональных данных — это априори нелегально. Стремительно развиваются и легальные проекты, направленные на монетизацию ПДн пользователей: Opiria, Handshake, Datacoup, GoodData, Pillar Project, Personal и другие сервисы. \n",
      "\r\n",
      "Более того, в мировой практике известны и примеры оффлайн-проектов, использующих в качестве оплаты ПДн. Например, в американском кафе Shiru расплатиться по счету можно своими ПДн (именами, номерами телефонов, электронными адресами, датами рождения и сведениями об интересах).\n",
      "\r\n",
      "Однако многие компании заинтересованы не столько в получении ПДн конкретного субъекта, сколько в получении массива данных, отражающих те или иные признаки целого ряда субъектов. Потому ценность получают имеющиеся у других компаний базы данных ПДн, Большие пользовательские данные. \n",
      "\r\n",
      "Зачастую компании включают положения о передаче данных в договоры оказания услуг или иные аналогичные. Кроме того, видятся интересными проекты по обмену ПДн, реализуемые с помощью соглашений между компаниями об информационном взаимодействие в области передачи персональных данных или иных аналогичных по содержанию. Например, такие соглашения распространены в сфере медицины. \n",
      "\r\n",
      "Также описывая проекты, работающие с Большими пользовательскими данными, нельзя не упомянуть проект Double Data и аналогичные ему (Clever Datа, Scorista, Scorto, FICO, Бюро кредитных историй Equifax, Национальное бюро кредитных историй). Эти проекты, по большей части, используют данные для последующей перепродажи, а также для скоринга и персонализации рекламы. Они позиционируют себя как работающие с открытыми данными, отстаивая в лице НБКИ и Double Data в судебном порядке свои права на обработку Больших пользовательских данных без дополнительного разрешения как и от пользователей, так и от компаний, первично обрабатывающих ПДн.\n",
      "\r\n",
      "Отдельно стоит отметить скандально известную компанию Cambridge Analytica, которая собирала для анализа политических предпочтений избирателей ПДн пользователей без их согласия, в том числе ПДн пользователей социальной сети Facebook. В результате таких действий не только разгорелся политический скандал, но и неизбежно возникли юридические последствия и для Cambridge Analytica, и для Facebook. Cambridge Analytica, Facebook объявила о своём банкротстве, а Facebook оштрафовали на 500 тысяч фунтов стерлингов (около 600 тысяч долларов) за несоблюдение прав субъектов ПДн: отсутствие должной защиты ПДн пользователей и недостаточную прозрачность их обработки.\n",
      "\r\n",
      "В целом, скандал Cambridge Analytica-Facebook имеет далеко идущие последствия, в том числе и для России. Так, компания Facebook стала блокировать доступ к «подозрительным» сервисам, деятельность которых допускает риски нарушения законодательства о ПДн. Например, недавно (в начале октября 2018 г.) Facebook заблокировала больше 66 аккаунтов, профилей, страниц и приложений российского стартапа, Social Data Hub, который раньше сравнивал себя с Cambridge Analytica, а сейчас позиционирует себя как «специализирующийся на разработке систем искусственного интеллекта». Тем не менее, согласно данным СМИ проект занимается также коммерческим анализом данных пользователей для государства. \n",
      "\r\n",
      "Интересно, что на сайте Social Data Hub можно ознакомиться с ответом Роскомнадзора о легальности функционирования такого сервиса. Однако это не помешало компании Facebook усмотреть в деятельности Social Data Hub нарушение пользовательского соглашения Facebook и признаки незаконного использования ПДн. Facebook удалила аккаунты стартапа и его сотрудников, а также направила письмо с требованиями: \n",
      "\n",
      "\n",
      "немедленно прекратить деятельность по обработке данных пользователей Facebook и уничтожить эти данные;\n",
      "\n",
      "предоставить Facebook полный перечень всех используемых компанией данных и организации, получивших к ним доступ;\n",
      "\n",
      "предоставить представителям Facebook доступ к хранилищам данных для проверки, что они действительно удалены.\n",
      "\n",
      "\r\n",
      "Представитель «Вконтакте» также отметил, что компания направила претензионный письмо в адрес Social Data Hub. В свою очередь руководители проекта отрицают какое-либо нарушение законодательство о ПДн, утверждая, что они «разрабатывают софт, а не продают данные».\n",
      "\n",
      "Правовые основы ведения бизнеса на персональных данных \n",
      "\r\n",
      "С юридической точки зрения бизнес-проекты на пользовательских данных реализуются с помощью различных правовых инструментов. Во многом такое положение дел объясняется отсутствием нормативного регулирования процессов монетизации пользовательских данных. Закон лишь предписывает обязательные требования и условия, при которых могут собираться и обрабатываться ПДн. \n",
      "\r\n",
      "Наиболее распространённое основание для обработки ПДн — это наличие согласия субъекта. Получение такого согласия, его «покупка», как раз лежит в основе большинства легальных бизнес-проектов на пользовательских данных. При этом важно понимать, что реализация такой покупки далека от классического гражданско-правового понимания договор купли-продажи. Помимо непосредственно получения согласия за определенное имущественное вознаграждение возможна обработка ПДн во исполнении заключенных с пользователями договоров, сопутствующих предоставлению каких-либо товаров и услуг (в большинстве случаев предоставления доступа к контенту в интернете). \n",
      "\r\n",
      "Кроме того, в последнее время популярность набирают проекты, в том числе и ICO-проекты, основная цель которых — это обеспечение легальной монетизации ПДн. Например, платформа Opiria. Данный проект позволяет пользователям предоставлять согласие на обработку своих ПДн в обмен на токены PDATA. Согласно утверждениям разработчиков, данная платформа является «глобальным децентрализованным рынком, где компании могут покупать персональные данные напрямую у потребителей без посредников». При этом Opiria гарантирует пользователям возможность контроля и управления своими ПДн в соответствии с требованиями законодательства о ПДн.\n",
      "\r\n",
      "В то же время посредничество в бизнесе на персональных данных не теряет свою актуальность. Множество компаний пытаются перепродать ПДн или совершить их обмен. Но такие проекты будут соответствовать законодательству, только когда получены соответствующие согласия на передачу ПДн третьим лицам и их последующую обработку.\n",
      "\r\n",
      "Показательным является английское дело сервиса DeepMind, который заключил соглашение об обмене ПДн с Национальной службой здравоохранения Великобритании. Однако стороны не предусмотрели получение согласия на передачу и обработку ПДн пациентов сервисом DeepMind, а потому было установлено нарушение законодательства о ПДн. Хотя данное дело основано на нормах иностранного законодательства, его выводы являются применимыми и в российских реалиях. Схожую позицию мы наблюдали, например, в ранее упомянутом деле о продаже МГТС данных о своих абонентах. \n",
      "\r\n",
      "В целом, в России для всех бизнес-проектов на ПДн крайне важно соблюдать общие требования законодательства о ПДн. В частности, необходимо: \n",
      "\n",
      "\n",
      "ограничивать обработку ПДн конкретными, заранее определенными целями;\n",
      "\n",
      "ограничивать объем обрабатываемых данных минимально необходимым объемом для осуществления заявленных целей их обработки;\n",
      "\n",
      "не объединять базы данных, содержащие ПДн, обработка которых осуществляется в целях, не совместимых между собой;\n",
      "\n",
      "уничтожать или обезличивать ПДн по достижении целей обработки или в случае утраты необходимости в достижении этих целей (кроме случаев прямо предусмотренных законом);\n",
      "\n",
      "определять правовое основание обработки ПДн (в большинстве случаев это будет согласие от субъекта ПДн, но также может быть договор, законодательная норма, общедоступность ПДн, иное);\n",
      "\n",
      "соблюдать требования к форме согласия на обработку ПДн;\n",
      "\n",
      "прекращать обработку или обеспечивать прекращение обработки другим лицом ПДн в случае отзыва субъектом согласия на обработку ПДн. \n",
      "\n",
      "\r\n",
      "Что касается проектов в сфере Больших пользовательских данных, то здесь спорных правовых вопросов ещё больше. \n",
      "\n",
      "\n",
      "Во-первых, отсутствует единый подход к пониманию Больших данных. \n",
      "\n",
      "Во-вторых, законодательство регулирует обработку Больших данных лишь частично.\n",
      "\n",
      "В-третьих, не выработано однозначной позиции по вопросу использования ПДн, находящихся в открытом доступе и обезличенных ПДн. \n",
      "\n",
      "В-четвертых, сложно определить реальную стоимость Больших пользовательских данных. \n",
      "\n",
      "В-пятых, агрегирование Больших пользовательских данных какой-либо компанией может порождать интеллектуальные права этой компании на базу данных. В результате возникает коллизия прав. А коммерческие отношения в данной сфере становятся по своей непредсказуемости подобны лотерее. \n",
      "\n",
      "\r\n",
      "Возможно, ситуация на российском рынке изменится после окончательного разрешения спора «Вконтакте» против «Double data» и/или принятия каких-либо из обсуждаемых ныне инициатив (например, законопроект о регулировании больших данных, законопроект об использовании и передаче другим лицам обезличенных ПДн, инициатива по созданию специальной платформы по управления согласием на обработку ПДн). \n",
      "\r\n",
      "Также в настоящее время в Госдуме находится законопроект, который направлен на то, чтобы определить правила в отношении такого нового объекта гражданских прав как цифровые права. Законопроект предлагает урегулировать использование Больших данных в договорных отношениях, а именно закрепить в ГК РФ конструкцию договора об оказании услуг по предоставлению информации (ст. 783.1). Данным договором может быть предусмотрено условие о неразглашении информации третьим лицам в течение определенного срока. Кроме того, законопроект предлагает расширить понятие «база данных» исходя из потребности защищать базы данных, основанных на Больших данных.\n",
      "\n",
      "Заключительные рекомендации\n",
      "\r\n",
      "Таким образом, очевидно, что с каждым годом становится все больше и больше бизнес-проектов, в которых пользовательские данные играют основополагающую роль. Большие пользовательские данные могут представлять из себя ценнейший нематериальный актив, а могут стать и токсичным пассивом для компании, если неправильно подходить к обороту и защите таких данных. Согласно оценкам экономистов, такие проекты — неотъемлемая составляющая цифровой экономики, и их число будет только расти. Да и уже сейчас, несмотря на возникающие правовые преграды, вполне возможно построить бизнес на пользовательских данных, если подходить к данным ответственно и следовать несложным рекомендациям: \n",
      "\n",
      "\n",
      "соблюдать предписания применимого законодательства; \n",
      "\n",
      "в случае наличия неопределённости в нормативном регулировании стараться максимально учитывать права всех заинтересованных субъектов;\n",
      "\n",
      "следить за судебной практикой и законотворческими инициативами;\n",
      "\n",
      "своевременно консультироваться с юристами в любых непонятных ситуация.\n",
      "\n",
      "\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Продукты HFLabs в промышленных объемах обрабатывают данные: адреса, ФИО, реквизиты компаний и еще вагон всего. Естественно, тестировщики ежедневно с этими данными имеют дело: обновляют тест-кейсы, изучают результаты очистки. Часто заказчики дают «живую» базу, чтобы тестировщик настроил сервис под нее.\n",
      "\r\n",
      "Первое, чему мы учим новых QA — сохранять данные в первозданном виде. Все по заветам: «Не навреди». В статье я расскажу, как аккуратно работать с CSV-файлами в Excel и Open Office. Советы помогут ничего не испортить, сохранить информацию после редактирования и в целом чувствовать себя увереннее.\n",
      "\n",
      "Материал базовый, профессионалы совершенно точно заскучают.\n",
      "\n",
      "Что такое CSV-файлы\r\n",
      "Формат CSV используют, чтобы хранить таблицы в текстовых файлах. Данные очень часто упаковывают именно в таблицы, поэтому CSV-файлы очень популярны.\n",
      "\n",
      "\n",
      "CSV-файл состоит из строк с данными и разделителей, которые обозначают границы столбцов\n",
      "\r\n",
      "CSV расшифровывается как comma-separated values — «значения, разделенные запятыми». Но пусть название вас не обманет: разделителями столбцов в CSV-файле могут служить и точки с запятой, и знаки табуляции. Это все равно будет CSV-файл.\n",
      "\r\n",
      "У CSV куча плюсов перед тем же форматом Excel: текстовые файлы просты как пуговица, открываются быстро, читаются на любом устройстве и в любой среде без дополнительных инструментов.\n",
      "\r\n",
      "Из-за своих преимуществ CSV — сверхпопулярный формат обмена данными, хотя ему уже лет 40. CSV используют прикладные промышленные программы, в него выгружают данные из баз.\n",
      "\r\n",
      "Одна беда — текстового редактора для работы с CSV мало. Еще ничего, если таблица простая: в первом поле ID одной длины, во втором дата одного формата, а в третьем какой-нибудь адрес. Но когда поля разной длины и их больше трех, начинаются мучения.\n",
      "\n",
      "\n",
      "Следить за разделителями и столбцами — глаза сломаешь\n",
      "\r\n",
      "Еще хуже с анализом данных — попробуй «Блокнотом» хотя бы сложить все числа в столбце. Я уж не говорю о красивых графиках.\n",
      "\r\n",
      "Поэтому CSV-файлы анализируют и редактируют в Excel и аналогах: Open Office, LibreOffice и прочих.\n",
      "\n",
      "Ветеранам, которые все же дочитали: ребята, мы знаем об анализе непосредственно в БД c помощью SQL, знаем о Tableau и Talend Open Studio. Это статья для начинающих, а на базовом уровне и небольшом объеме данных Excel с аналогами хватает.\n",
      "\n",
      "Как Excel портит данные: из классики\r\n",
      "Все бы ничего, но Excel, едва открыв CSV-файл, начинает свои лукавые выкрутасы. Он без спроса меняет данные так, что те приходят в негодность. Причем делает это совершенно незаметно. Из-за этого в свое время мы схватили ворох проблем.\n",
      "\r\n",
      "Большинство казусов связано с тем, что программа без спроса преобразует строки с набором цифр в числа.\n",
      "\n",
      "Округляет. Например, в исходной ячейке два телефона хранятся через запятую без пробелов: «5235834,5235835». Что сделает Excel? Лихо превратит номера́ в одно число и округлит до двух цифр после запятой: «5235834,52». Так мы потеряем второй телефон.\n",
      "\n",
      "Приводит к экспоненциальной форме. Excel заботливо преобразует «123456789012345» в число «1,2E+15». Исходное значение потеряем напрочь. \n",
      "\r\n",
      "Проблема актуальна для длинных, символов по пятнадцать, цифровых строк. Например, КЛАДР-кодов (это такой государственный идентификатор адресного объекта: го́рода, у́лицы, до́ма).\n",
      "\n",
      "Удаляет лидирующие плюсы. Excel считает, что плюс в начале строки с цифрами — совершенно лишний символ. Мол, и так ясно, что число положительное, коль перед ним не стоит минус. Поэтому лидирующий плюс в номере «+74955235834» будет отброшен за ненадобностью — получится «74955235834». (В реальности номер пострадает еще сильнее, но для наглядности обойдусь плюсом).\n",
      "\r\n",
      "Потеря плюса критична, например, если данные пойдут в стороннюю систему, а та при импорте жестко проверяет формат.\n",
      "\n",
      "Разбивает по три цифры. Цифровую строку длиннее трех символов Excel, добрая душа, аккуратно разберет. Например, «8 495 5235834» превратит в «84 955 235 834».\n",
      "\r\n",
      "Форматирование важно как минимум для телефонных номеров: пробелы отделяют коды страны и города от остального номера и друг от друга. Excel запросто нарушает правильное членение телефона.\n",
      "\n",
      "Удаляет лидирующие нули. Строку «00523446» Excel превратит в «523446». \r\n",
      "А в ИНН, например, первые две цифры — это код региона. Для Республики Алтай он начинается с нуля — «04». Без нуля смысл номера исказится, а проверку формата ИНН вообще не пройдет.\n",
      "\n",
      "Меняет даты под локальные настройки. Excel с удовольствием исправит номер дома «1/2» на «01.фев». Потому что Windows подсказал, что в таком виде вам удобнее считывать даты.\n",
      "\n",
      "Побеждаем порчу данных правильным импортом\r\n",
      "Если серьезно, в бедах виноват не Excel целиком, а неочевидный способ импорта данных в программу.\n",
      "\r\n",
      "По умолчанию Excel применяет к данным в загруженном CSV-файле тип «General» — общий. Из-за него программа распознает цифровые строки как числа. Такой порядок можно победить, используя встроенный инструмент импорта.\n",
      "\n",
      "Запускаю встроенный в Excel механизм импорта. В меню это «Data → Get External Data → From Text».\n",
      "\n",
      "Выбираю CSV-файл с данными, открывается диалог. В диалоге кликаю на тип файла Delimited (с разделителями). Кодировка — та, что в файле, обычно определяется автоматом. Если первая строка файла — шапка, отмечаю «My Data Has Headers».\n",
      "\n",
      "Перехожу ко второму шагу диалога. Выбираю разделитель полей (обычно это точка с запятой — semicolon). Отключаю «Treat consecutive delimiters as one», а «Text qualifier» выставляю в «{none}». (Text qualifier — это символ начала и конца текста. Если разделитель в CSV — запятая, то text qualifier нужен, чтобы отличать запятые внутри текста от запятых-разделителей.)\n",
      "\n",
      "На третьем шаге выбираю формат полей, ради него все и затевалось. Для всех столбцов выставляю тип «Text». Кстати, если кликнуть на первую колонку, зажать шифт и кликнуть на последнюю, выделятся сразу все столбцы. Удобно.\n",
      "\r\n",
      "Дальше Excel спросит, куда вставлять данные из CSV — можно просто нажать «OK», и данные появятся в открытом листе.\n",
      "\n",
      "\n",
      "Перед импортом придется создать в Excel новый workbook\n",
      "\n",
      "Но! Если я планирую добавлять данные в CSV через Excel, придется сделать еще кое-что. \n",
      "\r\n",
      "После импорта нужно принудительно привести все-все ячейки на листе к формату «Text». Иначе новые поля приобретут все тот же тип «General».\n",
      "\n",
      "\n",
      "Нажимаю два раза Ctrl+A, Excel выбирает все ячейки на листе;\n",
      "кликаю правой кнопкой мыши;\n",
      "выбираю в контекстном меню «Format Cells»;\n",
      "в открывшемся диалоге выбираю слева тип данных «Text».\n",
      "\n",
      "\n",
      "Чтобы выделить все ячейки, нужно нажать Ctrl+A два раза. Именно два, это не шутка, попробуйте\n",
      "\r\n",
      "После этого, если повезет, Excel оставит исходные данные в покое. Но это не самая твердая гарантия, поэтому мы после сохранения обязательно проверяем файл через текстовый просмотрщик.\n",
      "\n",
      "Альтернатива: Open Office Calc\r\n",
      "Для работы с CSV-файлами я использую именно Calc. Он не то чтобы совсем не считает цифровые данные строками, но хотя бы не применяет к ним переформатирование в соответствии с региональными настройками Windows. Да и импорт попроще.\n",
      "\r\n",
      "Конечно, понадобится пакет Open Office (OO). При установке он предложит переназначить на себя файлы MS Office. Не рекомендую: хоть OO достаточно функционален, он не до конца понимает хитрое микрософтовское форматирование документов.\n",
      "\r\n",
      "А вот назначить OO программой по умолчанию для CSV-файлов — вполне разумно. Сделать это можно после установки пакета.\n",
      "\r\n",
      "Итак, запускаем импорт данных из CSV. После двойного клика на файле Open Office показывает диалог.\n",
      "\n",
      "\n",
      "Заметьте, в OO не нужно создавать новый воркбук и принудительно запускать импорт, все само\n",
      "\n",
      "\n",
      "Кодировка — как в файле.\n",
      "«Разделитель» — точка с запятой. Естественно, если в файле разделителем выступает именно она.\n",
      "«Разделитель текста» — пустой (все то же, что в Excel).\n",
      "В разделе «Поля» кликаю в левый-верхний квадрат таблицы, подсвечиваются все колонки. Указываю тип «Текст».\n",
      "\r\n",
      "Штука, которая испортила немало крови: если по ошибке выбрать несколько разделителей полей или не тот разделитесь текста, файл может правильно открыться, но неправильно сохраниться.\n",
      "\r\n",
      "Помимо Calc у нас в HFLabs популярен libreOffice, особенно под «Линуксом». И то, и другое для CSV применяют активнее, чем Excel.\n",
      "\n",
      "Бонус-трек: проблемы при сохранении из Calc в .xlsx\r\n",
      "Если сохраняете данные из Calc в экселевский формат .xlsx, имейте в виду — OO порой необъяснимо и масштабно теряет данные.\n",
      "\n",
      "\n",
      "Белая пустошь, раскинувшаяся посередине, в оригинальном CSV-файле богато заполнена данными\n",
      "\r\n",
      "Поэтому после сохранения я еще раз открываю файл и убеждаюсь, что данные на месте. \n",
      "\r\n",
      "Если что-то потерялись, лечение — пересохранить из CSV в .xlsx. Или, если установлен Windows, импортнуть из CSV в Excel и сохранить оттуда. \n",
      "\r\n",
      "После пересохранения обязательно еще раз проверяю, что все данные на месте и нет лишних пустых строк.\n",
      "\n",
      "Если интересно работать с данными, посмотрите на наши вакансии. HFLabs почти всегда нужны аналитики, тестировщики, инженеры по внедрению, разработчики. Данными обеспечим так, что мало не покажется :)    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Кажется, что сфера интернет-рекламы должна быть максимально технологичной и автоматизированной. Ещё бы, ведь там работают такие гиганты и эксперты в своём деле, как Яндекс, Mail.Ru, Google и Facebook. Но, как оказалось, нет предела совершенству и всегда есть что автоматизировать.\n",
      "\n",
      "\n",
      "Источник\n",
      "\r\n",
      "Коммуникационная группа Dentsu Aegis Network Russia — крупнейший игрок на рекламном digital рынке и активно инвестирует в технологии, пытаясь в оптимизировать и автоматизировать свои бизнес-процессы. Одной из нерешенных задач рынка интернет-рекламы стала задача сбора статистики по рекламным кампаниям с разных интернет-площадок. Решение этой задачи в итоге вылилось в создание продукта D1.Digital (читать как ДиВан), о разработке которого мы и хотим рассказать.\n",
      "\n",
      "Зачем?\r\n",
      "1. На момент старта проекта на рынке не было ни одного готового продукта, решающего задачу автоматизации сбора статистики по рекламным кампаниям. Значит, никто кроме нас самих не закроет наши потребности. \n",
      "\r\n",
      "Такие сервисы, как Improvado, Roistat, Supermetrics, SegmentStream, предлагают интеграцию с площадками, социальными сетями и Google Analitycs, а также дают возможность строить аналитические дашборды для удобного анализа и контроля рекламных кампаний. Перед тем, как начать развивать свой продукт, мы попробовали использовать в работе некоторые из этих систем для сбора данных с площадок, но, к сожалению, они не смогли решить наших задач. \n",
      "\r\n",
      "Главной проблемой стало то, что тестируемые продукты отталкивались от источников данных, отображая статистику размещений в разрезах по площадкам, и не давали возможность агрегировать статистику по рекламным кампаниям. Такой подход не позволял увидеть статистику из разных площадок в одном месте и проанализировать состояние кампании целиком. \n",
      "\r\n",
      "Другим фактором стало то, что на начальных этапах продукты были ориентированы на западный рынок и не поддерживал интеграцию с российскими площадками. А по тем площадкам, с которыми была реализована интеграция, не всегда выгружались все необходимые метрики с достаточной детализацией, а интеграция не всегда была удобной и прозрачной, особенно когда нужно было получить что-то, чего нет в интерфейсе системы.\r\n",
      "В общем, мы решили не подстраиваться под сторонние продукты, а занялись разработкой своего…\n",
      "\r\n",
      "2. Рынок интернет-рекламы растет из года в год, и в 2018 году по объему рекламных бюджетов он обогнал традиционно крупнейший рынок ТВ-рекламы. Значит, есть масштаб.\n",
      "\r\n",
      "3. В отличие от рынка ТВ-рекламы, где продажа коммерческой рекламы монополизирована, в Интернете работает масса отдельных владельцев рекламного инвентаря разной величины со своими рекламными кабинетами. Так как рекламная кампания, как правило, идёт сразу на нескольких площадках, то для понимания состояния рекламной кампании, надо собрать отчеты со всех площадок и свести их в один большой отчет, который покажет картинку целиком. Значит, есть потенциал для оптимизации.\n",
      "\r\n",
      "4. Нам казалось, что у владельцев рекламного инвентаря в интернете уже есть инфраструктура для сбора статистики и ее отображения в рекламных кабинетах, и они смогут предоставить API к этим данным. Значит, есть техническая возможность реализации. Скажем сразу, что оказалось не все так просто. \n",
      "\r\n",
      "В общем, все предпосылки для реализации проекта были для нас очевидны, и мы побежали воплощать проект в жизнь…\n",
      "\n",
      "Грандиозный план\r\n",
      "Для начала мы сформировали видение идеальной системы: \n",
      "\n",
      "\n",
      "В нее должны автоматически загружаться рекламные кампании из корпоративной системы 1С с их названиями, периодами, бюджетами и размещениями по разнообразным площадкам. \n",
      "Для каждого размещения внутри рекламной кампании должна автоматически загружаться вся возможная статистика с площадок, на которых идет размещение, такая как количество показов, кликов, просмотров и прочее. \n",
      "Некоторые рекламные кампании отслеживаются с помощью стороннего мониторинга так называемыми adserving системами, такими как Adriver, Weborama, DCM и т.д. Также есть индустриальный измеритель Интернета в России — компания Mediascope. Данные независимого и индустриального мониторингов по нашей задумке также должны автоматически подгружаться к соответствующим рекламным кампаниям. \n",
      "Большинство рекламных кампаний в Интернет нацелены на определённые целевые действия (покупка, звонок, запись на тест-драйв и т.д.), которые отслеживаются с помощью Google Analytics, и статистика по которым тоже важна для понимания состояния кампании и должна загружаться в наш инструмент.\n",
      "\n",
      "Первый блин комом\r\n",
      "Учитывая нашу приверженность гибким принципам разработки ПО (agile, все дела), мы решили сначала разработать MVP и далее двигаться к намеченной цели итеративно.\r\n",
      "MVP мы решили строить на основе нашего продукта DANBo (Dentsu Aegis Network Board), представляющего из себя web приложение с общей информацией по рекламным кампаниям наших клиентов.\n",
      "\r\n",
      "Для MVP проект был максимально упрощен с точки зрения реализации. Мы выбрали ограниченный список площадок для интеграции. Это были основные площадки, такие как Яндекс.Директ, Яндекс.Дисплей, RB.Mail, MyTarget, Adwords, DBM, VK, FB, и основные adserving системы Adriver и Weborama.\n",
      "\r\n",
      "Для доступа к статистике на площадках через API мы использовали единый аккаунт. Менеджер клиентской группы, который хотел использовать автоматический сбор статистики по рекламной кампании, должен был сначала делегировать доступ к нужным рекламным кампаниям на площадках на аккаунт платформы.\n",
      "\r\n",
      "Далее пользователь системы DANBo должен был загрузить в систему Excel файл определенного формата, в котором была прописана вся информация о размещении (рекламная кампания, площадка, формат, период размещения, плановые показатели, бюджет и т.д.) и идентификаторы соответствующих рекламных кампаний на площадках и счётчиков в adserving системах. \n",
      "\r\n",
      "Выглядело это, честно говоря, ужасающе:\n",
      "\n",
      "\n",
      "\r\n",
      "Загруженные данные сохранялись в базу данных, а потом отдельные сервисы собирали из них идентификаторы кампаний на площадках и загружали по ним статистику.\n",
      "\r\n",
      "Для каждой площадки был написан отдельный windows сервис, который раз в сутки ходил под одним служебным аккаунтом в API площадки и выгружал статистику по заданным идентификаторам кампаний. Аналогично происходило и с adserving системами. \n",
      "\r\n",
      "Загруженные данные отображались на интерфейсе в виде небольшого самописного дашборда:\n",
      "\n",
      "\n",
      "\r\n",
      "Неожиданно для нас самих MVP заработал и стал загружать актуальную статистику по рекламным кампаниям в Интернете. Мы внедрили систему на нескольких клиентах, но при попытке масштабирования наткнулись на серьезные проблемы:\n",
      "\n",
      "\n",
      "Главная проблема была в трудоёмкости подготовки данных для загрузки в систему. Также данные по размещению надо было приводить к строго фиксированному формату перед загрузкой. В файл для загрузки нужно было прописывать идентификаторы сущностей из разных площадок. Мы столкнулись с тем, что технически неподготовленным пользователям очень сложно объяснить, где найти эти идентификаторы на площадке и куда в файле их нужно проставить. Учитывая количество сотрудников в подразделениях, ведущих кампании на площадках, и текучку, это вылилось в огромный объем поддержки на нашей стороне, что нас категорически не устраивало.\n",
      "Другой проблемой было то, что не на всех рекламных площадках были механизмы делегирования доступа к рекламным кампаниям на другие аккаунты. Но даже если механизм делегирования был доступен, не все рекламодатели были готовы предоставлять доступ к своим кампаниям сторонним аккаунтам.\n",
      "Немаловажным стал фактор негодования, которое у пользователей вызывало то, что все плановые показатели и детали размещения, которые они уже вносят в нашу учётную 1С систему, они должны повторно вносить и в DANBo.\n",
      "\r\n",
      "Это натолкнуло нас на мысль, что первоисточником информации о размещении должна служить наша 1С система, в которую все данные вносятся аккуратно и в срок (тут дело в том, что на основании данных 1С формируются счета, поэтому корректное внесение данных в 1С стоит у всех в KPI). Так появилась новая концепция системы… \n",
      "\n",
      "Концепция\r\n",
      "Первое, что мы решили сделать, это выделить систему сбора статистики по рекламным кампаниям в Интернет в отдельный продукт — D1.Digital. \n",
      "\r\n",
      "В новой концепции мы решили загружать в D1.Digital информацию по рекламным кампаниям и размещениям внутри них из 1С, а потом подтягивать к этим размещениям статистику с площадок и из AdServing систем. Это должно было значительно упростить жизнь пользователям (и, как водится, добавить работы разработчикам) и уменьшить объем поддержки.\n",
      "\r\n",
      "Первая проблема, с которой мы столкнулись, была организационного характера и связана с тем, что мы не смогли найти ключ или признак, по которому можно бы было сопоставить сущности из разных систем с кампаниями и размещениями из 1С. Дело в том, что процесс в нашей компании устроен так, что рекламные кампании в разные системы заносятся разными людьми (медиапленнеры, баинг и др.). \n",
      "\r\n",
      "Чтобы решить эту проблему, нам пришлось изобрести уникальный хэшированный ключ, DANBoID, который бы связывал сущности в разных системах воедино, и который можно было бы довольно легко и однозначно идентифицировать в загружаемых наборах данных. Этот идентификатор генерируется во внутренней 1С системе для каждого отдельного размещения и прокидывается в кампании, размещения и счётчики на всех площадках и во всех AdServing системах. Внедрение практики проставления DANBoID во все размещения заняло определенное время, но мы с этим справились :)\n",
      "\r\n",
      "Дальше мы выяснили, что далеко не у всех площадок есть API для автоматического сбора статистики, и даже у тех, у которых API есть, он возвращает не все нужные данные.\n",
      "\r\n",
      "На этом этапе мы решили значительно сократить список площадок для интеграции и сосредоточиться на основных площадках, которые задействованы в подавляющем большинстве рекламных кампаний. В этот список попали все крупнейшие игроки рекламного рынка (Google, Яндекс, Mail.ru), социальные сети (VK, Facebook, Twitter), основные системы AdServing и аналитики (DCM, Adriver, Weborama, Google Analytics) и другие площадки.\n",
      "\r\n",
      "У основной массы выбранных нами площадок был API, который отдавал необходимые нам метрики. В тех случаях, когда API не было, либо в нём не было нужных данных, для загрузки данных мы использовали отчёты, ежедневно приходящие на служебную почту (в одних системах есть возможность настройки таких отчётов, в других согласовали разработку таких отчётов для нас). \n",
      "\r\n",
      "При анализе данных с разных площадок мы выяснили, что иерархия сущностей не одинакова в разных системах. Более того, из разных систем информацию необходимо загружать в разной детализации.\n",
      "\r\n",
      "Для решения этой проблемы была разработана концепция SubDANBoID. Идея SubDANBoID довольно проста, мы помечаем основную сущность кампании на площадке сгенерированным DANBoID, а все вложенные сущности мы выгружаем с уникальными идентификаторами площадки и формируем SubDANBoID по принципу DANBoID + идентификатор вложенной сущности первого уровня + идентификатор вложенной сущности второго уровня +… Такой подход позволил нам связать рекламные кампании в разных системах и выгрузить детализированную статистику по ним.\n",
      "\r\n",
      "Также нам пришлось решать проблему доступа к кампаниям на разных площадках. Как мы уже писали выше, механизм делегирования доступа к кампании на отдельный технический аккаунт не всегда применим. Поэтому нам пришлось разработать инфраструктуру для автоматической авторизации через OAuth с использованием токенов и механизмы обновления этих токенов.\n",
      "\r\n",
      "Дальше в статье попробуем более подробно описать архитектуру решения и технические детали реализации.\n",
      "\n",
      "Архитектура решения 1.0\r\n",
      "Начиная реализацию нового продукта, мы понимали, что сразу нужно предусмотреть возможность подключения новых площадок, поэтому решили пойти по пути микросервисной архитектуры. \n",
      "\r\n",
      "При проектировании архитектуры мы выделили в отдельные сервисы коннекторы ко всем внешним системам — 1С, рекламным площадкам и adserving системам.\r\n",
      "Основная идея состоит в том, что все коннекторы к площадкам имеют одинаковое API и представляют собой адаптеры, приводящие API площадок к удобному нам интерфейсу. \n",
      "\r\n",
      "В центре нашего продукта — веб-приложение, которое представляет собой монолит, который спроектирован таким образом, чтобы его можно было легко разобрать на сервисы. Это приложение отвечает за обработку загруженных данных, сопоставление статистики из разных систем и представление ее пользователям системы.\n",
      "\r\n",
      "Для общения коннекторов с веб-приложением пришлось создать дополнительный сервис, который мы назвали Connector Proxy. Он выполняет функции Service Discovery и Task Scheduler. Этот сервис каждую ночь запускает задачи сбора данных для каждого коннектора. Написать сервис-прослойку было проще, чем подключать брокер сообщений, а для нас было важно получить результат как можно быстрей.\n",
      "\r\n",
      "Для простоты и скорости разработки мы также решили, что все сервисы будут представлять собой Web API. Это позволило быстро собрать proof-of-concept и проверить, что вся конструкция работает. \n",
      "\n",
      "\n",
      "\r\n",
      "Отдельной, достаточно сложной, задачей оказалась настройка доступов для сбора данных из разных кабинетов, которая, как мы решили, должна осуществляться пользователями через веб-интерфейс. Она состоит из двух отдельных шагов: сначала пользователь через OAuth добавляет токен для доступа к аккаунту, а потом настраивает сбор данных для клиента из определенного кабинета. Получение токена через OAuth необходимо, потому что, как мы уже писали, не всегда возможно делегировать доступ к нужному кабинету на площадке. \n",
      "\r\n",
      "Чтобы создать универсальный механизм выбора кабинета с площадок, нам пришлось добавить в API коннекторов метод, отдающий JSON Schema, которая рендерится в форму при помощи модифицированного компонента JSONEditor. Так пользователи смогли выбирать аккаунты, из которых необходимо загружать данные. \n",
      "\r\n",
      "Чтобы соблюдать лимиты запросов, существующие на площадках, мы объединяем запросы по настройкам в рамках одного токена, но разные токены можем обрабатывать параллельно.\n",
      "\r\n",
      "В качестве хранилища для загружаемых данных как для веб-приложения, так и для коннекторов мы выбрали MongoDB, что позволило не сильно заморачиваться по поводу структуры данных на начальных этапах разработки, когда объектная модель приложения меняется через день. \n",
      "\r\n",
      "Скоро мы выяснили, что не все данные хорошо ложатся в MongoDB и, например, статистику по дням удобнее хранить в реляционной базе. Поэтому для коннекторов, структура данных которых больше подходит под реляционную БД, в качестве хранилища мы начали использовать PostgreSQL или MS SQL Server.\n",
      "\r\n",
      "Выбранная архитектура и технологии позволили нам относительно быстро построить и запустить продукт D1.Digital. За два года развития продукта мы разработали 23 коннектора к площадкам, получили бесценный опыт работы со сторонними API, научились обходить подводные камни разных площадок, которые у каждой оказались свои, способствовали развитию API как минимум 3 площадок, автоматически загрузили информацию по почти 15 000 кампаний и по более чем 80 000 размещений, собрали много обратной связи от пользователей по работе продукта и успели несколько раз поменять основной процесс работы продукта, основываясь на этой обратной связи.\n",
      "\n",
      "Архитектура решения 2.0\r\n",
      "Прошло два года после старта разработки D1.Digital. Постоянный рост нагрузки на систему и появление все новых источников данных постепенно вскрыли проблемы в существующей архитектуре решения.\n",
      "\r\n",
      "Первая проблема связана с объемом данных, загружаемых с площадок. Мы столкнулись с тем, что сбор и обновление всех необходимых данных с самых больших площадок стало занимать слишком много времени. Например, сбор данных по adserving системе AdRiver, с помощью которой мы отслеживаем статистику по большей части размещений, занимает около 12 часов.\n",
      "\r\n",
      "Чтобы решить эту проблему, мы начали использовать всевозможные отчеты для загрузки данных с площадок, пытаемся развивать вместе с площадками их API, чтобы скорость его работы удовлетворяла нашим потребностям, и параллелить загрузку данных, на сколько это возможно.\n",
      "\r\n",
      "Другая проблема связана с обработкой загруженных данных. Сейчас при поступлении новой статистики по размещению, запускается многоступенчатый процесс пересчета метрик, который включает в себя загрузку сырых данных, просчет агрегированных метрик по каждой площадке, сопоставление данных из разных источников между собой и расчет сводных метрик по кампании. Это вызывает большую нагрузку на веб-приложение, которое производит все расчеты. Несколько раз приложение в процессе пересчета съедало всю память на сервере, порядка 10-15 Гб, что самым пагубным образом сказывалось на работе пользователей с системой. \n",
      "\r\n",
      "Обозначенные проблемы и грандиозные планы на дальнейшее развитие продукта привели нас к необходимости пересмотреть архитектуру приложения.\n",
      "\r\n",
      "Начали мы с коннекторов. \r\n",
      "Мы заметили, что все коннекторы работают по одной модели, поэтому построили фреймворк-конвейер, в котором для создания коннектора нужно было только запрограммировать логику шагов, остальное было универсальным. Если какой-то коннектор требует доработки, то мы одновременной с доработкой коннектора сразу переводим его на новый фреймворк. \n",
      "\r\n",
      "Параллельно мы начали размещать коннекторы в докер и Kubernetes. \r\n",
      "Переезд в Kubernetes планировали довольно долго, экспериментировали с настройками CI/CD, но переезжать начали только тогда, когда один коннектор из-за ошибки стал отъедать более 20 Гб памяти на сервере, практически убивая остальные процессы. На время расследования коннектор переселили в кластер Kubernetes, где он в итоге и остался, даже когда ошибка была исправлена. \n",
      "\r\n",
      "Довольно быстро мы поняли, что Kubernetes это удобно, и за полгода перенесли в кластер продакшена 7 коннекторов и Connectors Proxy, которые потребляют больше всего ресурсов.\n",
      "\r\n",
      "Вслед за коннекторами мы решили менять архитектуру остального приложения. \r\n",
      "Основной проблемой было то, что данные приходят из коннекторов в прокси большими пачками, а потом бьются по DANBoID и отдаются в центральное web приложение для обработки. Из-за большого количества пересчетов метрик возникает большая нагрузка на приложение. \n",
      "\r\n",
      "Также оказалось довольно сложно мониторить статусы отдельных заданий на сбор данных и передавать ошибки, происходящие внутри коннекторов, в центральное web приложение, чтобы пользователи могли видеть, что происходит, и из-за чего не собираются данные.\n",
      "\r\n",
      "Для решения этих проблем мы разработали архитектуру 2.0. \n",
      "\r\n",
      "Главное отличие новой версии архитектуры состоит в том, что вместо Web API мы используем RabbitMQ и библиотеку MassTransit для обмена сообщениями между сервисами. Для этого пришлось практически полностью переписать Connectors Proxy, сделав из него Connectors Hub. Название поменяли потому, что основная роль сервиса теперь не в пробросе запросов в коннекторы и обратно, а в управлении сбором метрик с коннекторов.\n",
      "\r\n",
      "Из центрального web приложения мы выделили в отдельные сервисы информацию о размещениях и статистику с площадок, что дало возможность избавиться от лишних пересчетов и на уровне размещений хранить только уже рассчитанную и агрегированную статистику. Также мы переписали и оптимизировали логику расчета основных статистик на основе сырых данных.\n",
      "\r\n",
      "Параллельно мы переносим все сервисы и приложения в докер и Kubernetes, чтобы решение проще масштабировалось и им было удобно управлять.\n",
      "\n",
      "\n",
      "\n",
      "Где мы сейчас\r\n",
      "Proof-of-concept архитектуры 2.0 продукта D1.Digital готов и работает в тестовом окружении с ограниченным набором коннекторов. Дело за малым — переписать еще 20 коннекторов на новую платформу, протестировать, что данные корректно загружаются, а все метрики правильно считаются, и выкатить всю конструкцию в продакшн. \n",
      "\r\n",
      "На самом деле, этот процесс будет происходить постепенно и нам придется оставить обратную совместимость со старыми API, чтобы все продолжало работать.\n",
      "\r\n",
      "В ближайших наших планах стоит разработка новых коннекторов, интеграция с новыми системами и добавление дополнительных метрик в набор данных, выгружаемых из подключенных площадок и adserving систем. \n",
      "\r\n",
      "Также планируем перенести все приложения, в том числе и центральное web приложение, в докер и Kubernetes. В сочетании с новой архитектурой, это позволит заметно упростить развертывание, мониторинг и контроль потребляемых ресурсов. \n",
      "\r\n",
      "Еще есть идея поэкспериментировать с выбором БД для хранения статистики, которая сейчас хранится в MongoDB. Несколько новых коннекторов мы уже перевели на SQL-базы, но там разница почти незаметна, а для агрегированной статистики по дням, которую можно запрашивать за произвольный период, выигрыш может быть достаточно серьезным.\n",
      "\r\n",
      "В общем, планы грандиозные, двигаемся дальше :)\n",
      "\n",
      "Авторы статьи R&D Dentsu Aegis Network Russia: Георгий Остапенко (shmiigaa), Михаил Коцик (hitexx)     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Пользуетесь ли вы структурами данных и алгоритмами в повседневной работе? Я обратил внимание на то, что всё больше и больше людей считает алгоритмы чем-то таким, чем, без особой связи с реальностью, технические компании, лишь по собственной прихоти, интересуются на собеседованиях. Многие жалуются на то, что задачи на алгоритмы — это нечто из области теории, имеющей слабое отношение к настоящей работе. Такой взгляд на вещи, определённо, распространился после того, как Макс Хауэлл, автор Homebrew, опубликовал твит о том, что произошло с ним на собеседовании в Google:\n",
      "\n",
      "Google: 90% наших инженеров пользуются программой, которую вы написали (Homebrew), но вы не можете инвертировать бинарное дерево на доске, поэтому — прощайте.\n",
      "\r\n",
      "Хотя и у меня никогда не возникало нужды в инверсии бинарного дерева, я сталкивался с примерами реального использования структур данных и алгоритмов в повседневной работе, когда трудился в Skype/Microsoft, Skyscanner и Uber. Сюда входило написание кода и принятие решений, основанное на особенностях структур данных и алгоритмов. Но соответствующие знания я, по большей части, использовал для того чтобы понять то, как созданы некие системы, и то, почему они созданы именно так. Знание соответствующих концепций упрощает понимание архитектуры и реализации систем, в которых эти концепции используются.\n",
      "\n",
      "\n",
      "\r\n",
      "В эту статью я включил рассказы о ситуациях, в которых структуры данных, вроде деревьев и графов, а так же различные алгоритмы, были использованы в реальных проектах. Здесь я надеюсь показать читателю то, что базовые знания структур данных и алгоритмов — это не бесполезная теория, нужная только для собеседований, а что-то такое, что, весьма вероятно, по-настоящему понадобится тому, кто работает в быстрорастущих инновационных технологических компаниях.\n",
      "\r\n",
      "Здесь речь пойдёт о совсем небольшом количестве алгоритмов, но в том, что касается структур данных, могу отметить, что я коснусь тут практически всех из них. Никого не должно удивлять то, что я — не фанат таких вопросов, задаваемых на собеседованиях, в которых алгоритмам уделяется неоправданно много внимания, которые оторваны от практики и нацелены на экзотические структуры данных вроде красно-чёрных деревьев или АВЛ-деревьев. Я никогда такие вопросы на собеседованиях не задавал и задавать их не буду. В конце этой статьи я поделюсь своими мыслями о подобных собеседованиях. Но, несмотря на вышесказанное, знания о базовых структурах данных имеют огромную ценность. Эти знания позволяют подбирать именно то, что нужно для решения определённых практических задач.\n",
      "\r\n",
      "А теперь давайте перейдём к примерам использования структур данных и алгоритмов в реальной жизни.\n",
      "\n",
      "Деревья и обход деревьев: Skype, Uber и UI-фреймворки\r\n",
      "Когда мы разрабатывали Skype для Xbox One, код приходилось писать для чистой ОС Xbox, в которой не было необходимых нам библиотек. Мы разрабатывали одно из первых полномасштабных приложений для этой платформы. Нам нужна была система навигации по приложению, с которой можно было бы работать и пользуясь сенсорным экраном, и отдавая приложению голосовые команды.\n",
      "\r\n",
      "Мы создали базовый навигационный фреймворк, основанный на WinJS. Для того чтобы это сделать, нам понадобилось поддерживать граф, похожий на DOM, что требовалось для организации наблюдения за элементами, с которыми мог взаимодействовать пользователь. Для поиска таких элементов мы выполняли обход DOM, который сводился к обходу дерева, то есть — существующей структуры DOM-элементов. Это — классический пример BFS или DFS (breadth-first search или depth-first search — поиск в ширину или поиск в глубину).\n",
      "\r\n",
      "Если вы занимаетесь веб-разработкой — это значит, что вы уже работаете с древовидной структурой, а именно — с DOM. У всех узлов DOM могут быть дочерние узлы. Браузер выводит узлы на экран после обхода дерева DOM. Если вам надо найти конкретный элемент, то для решения этой задачи можно воспользоваться встроенными методами DOM. Например — методом getElementById. Альтернативой этого может стать разработка собственного BFS- или DFS-решения для обхода узлов и поиска того, что нужно. Например, нечто подобное сделано здесь.\n",
      "\r\n",
      "Многие фреймворки, которые рендерят элементы пользовательского интерфейса, используют, в своих недрах, древовидные структуры. Так, React поддерживает виртуальную DOM и использует интеллектуальный алгоритм согласования (сравнения). Это позволяет добиваться высокой производительности за счёт того, что повторному рендерингу подвергаются только изменённые части интерфейса. Здесь можно найти визуализацию этого процесса.\n",
      "\r\n",
      "В мобильной архитектуре Uber, RIBs, тоже используются деревья. Это роднит данную архитектуру с большинством других UI-фреймворков, которые выводят на экраны иерархические структуры элементов. В архитектуре RIBs поддерживается, с целью управления состоянием, дерево RIB. Прикрепляя к нему RIBs и открепляя их от него, управляют их рендерингом. Работая с RIBs, мы иногда делали наброски, пытаясь понять, вписывается ли RIBs в существующую иерархию, и обсуждали то, должны ли у RIBs, о котором идёт речь, быть видимые элементы. То есть — говорили о том, будет ли эта структура принимать участие в формировании визуального представления интерфейса, или она будет использоваться лишь для управления состоянием.\n",
      "\n",
      "\n",
      "Переходы между состояниями при использовании RIBs. Здесь можно найти подробности о RIBs\n",
      "\r\n",
      "Если вам когда-нибудь понадобится визуализировать иерархические элементы, то знайте, что обычно для этого используются древовидные структуры, их обход и рендеринг посещённых элементов. Я сталкивался со множеством внутренних инструментов, которые используют этот подход. Пример такого инструмента — средство для визуализации RIBs, созданное командой Mobile Platform из Uber. Вот доклад на эту тему.\n",
      "\n",
      "Взвешенные графы и поиск кратчайшего пути: Skyscanner\r\n",
      "Skyscanner — это проект, который направлен на поиск наиболее выгодных предложений по авиабилетам. Поиск таких предложений выполняется путём просмотра и анализа всех существующих в мире маршрутов и их комбинирования. Суть этой задачи больше относится к автоматическому сбору данных поисковым роботом, а не к кэшированию всех этих данных, так как авиакомпании самостоятельно вычисляют время ожидания следующего рейса. Но возможность планирования путешествия с посещением нескольких городов сводится к задаче поиска кратчайшего пути.\n",
      "\r\n",
      "Планирование путешествий с посещением нескольких городов стало одной из возможностей, на реализацию которой у Skyscanner ушло довольно много времени. При этом сложности, в основном, относились к самой разрабатываемой системе. Лучшие предложения такого рода находят, используя алгоритмы поиска кратчайшего пути — вроде алгоритма Дейкстры или A*. Маршруты полётов представляют в виде ориентированного графа. Каждому его ребру назначен вес в виде стоимости билета. При поиске наилучшего маршрута нахождение самого дешёвого пути между двумя городами выполняется с использованием реализации модифицированного алгоритма A*. Если вам интересна тема подбора авиабилетов и поиска кратчайших маршрутов, то вот хорошая статья, посвящённая использованию BFS для решения подобных задач.\n",
      "\r\n",
      "Правда, в случае со Skyscanner то, какой именно алгоритм был использован для решения задачи, было не особенно важно. Кэширование, использование поисковых роботов, организация работы с различными сайтами — всё это было куда сложнее, чем подбор алгоритма. Но при этом разные варианты задачи поиска кратчайшего пути возникают во множестве различных компаний, занимающихся планированием путешествий и оптимизирующих стоимость таких путешествий. Неудивительно то, что эта тема была предметом закулисных разговоров и в Skyscanner.\n",
      "\n",
      "Сортировка: Skype (или что-то вроде этого)\r\n",
      "У меня редко бывали причины для самостоятельной реализации алгоритмов сортировки или для глубокого изучения тонкостей их устройства. Но, несмотря на это, интересно было разобраться в том, как работают такие алгоритмы — от пузырьковой сортировки, сортировки вставками, сортировки слиянием и сортировки выбором, до самого сложного алгоритма — быстрой сортировки. Я обнаружил, что редко возникает потребность в реализации подобных алгоритмов, особенно если не нужно писать функцию сортировки, являющуюся частью какой-нибудь библиотеки.\n",
      "\r\n",
      "В Skype мне, правда, пришлось прибегнуть к практическому использованию моих знаний об алгоритмах сортировки. Один из наших программистов решил реализовать, для вывода контактов, сортировку вставками. В 2013 году, когда Skype подключался к сети, контакты загружались партиями. Для загрузки их всех требовалось некоторое время. В результате тот программист подумал, что лучше будет строить список контактов, упорядоченных по именам, используя сортировку вставками. Мы много обсуждали этот алгоритм, размышляли о том, почему бы просто не использовать что-то такое, что уже реализовано. В результате больше всего времени у нас ушло на то, чтобы как следует протестировать нашу реализацию алгоритма и проверить её производительность. Лично я не видел особого смысла в создании собственной реализации этого алгоритма. Но тогда проект был на такой стадии, на которой у нас было время на подобные дела.\n",
      "\r\n",
      "Конечно, есть реальные ситуации, в которых эффективная сортировка играет в некоем проекте очень важную роль. И если разработчик может самостоятельно, основываясь на особенностях данных, выбирать наиболее подходящий алгоритм, это может дать заметный прирост производительности решения. Сортировка вставками может оказаться очень кстати там, где работают с большими наборами данных, передаваемых куда-либо в режиме реального времени, и тут же визуализируют эти данные. Сортировка слиянием может хорошо показывать себя при использовании подходов типа «разделяй и властвуй», в том случае, если речь идёт о работе с большими объёмами данных, хранящихся в разных узлах. Мне с подобными системами работать не доводилось, поэтому я пока продолжаю считать алгоритмы сортировки чем-то таким, что имеет ограниченное применение в повседневной работе. Это, правда, не касается важности понимания того, как именно работают разные алгоритмы сортировки.\n",
      "\n",
      "Хеш-таблицы и хеширование: везде\r\n",
      "Структура данных, которой я регулярно пользовался — это хеш-таблица. Сюда же можно отнести и хеш-функции. Это — полезнейший инструмент, которым можно пользоваться для решения самых разных задач — от подсчёта количества неких сущностей, обнаружения дубликатов, кеширования, до сценариев, вроде шардинга, используемых в распределённых системах. Хеш-таблицы — это, после массивов, структура данных, которая встречается в программировании чаще всего. Я ей пользовался в бесчисленном количестве ситуаций. Она есть почти во всех языках программирования, и её, если нужно, просто реализовать самостоятельно.\n",
      "\n",
      "Стеки и очереди: время от времени\r\n",
      "Стек — это структура данных, которая знакома каждому, кто занимался отладкой кода, написанного на языке, который поддерживает трассировку стека. Если говорить о стеке как о структуре данных, то я, в ходе работы, столкнулся с несколькими задачами, для решения которых она понадобилась. Но надо отметить, что со стеками я как следует познакомился, занимаясь отладкой и профилированием производительности кода. Стеки — это, кроме того, естественный выбор структуры данных, используемой при выполнении обхода дерева в глубину.\n",
      "\r\n",
      "Мне редко приходилось прибегать к использованию очередей, но в кодовых базах различных проектов они мне встречались достаточно часто. Скажем, в очереди что-то помещали и что-то из них извлекали. Обычно очереди используются для реализации обхода деревьев в ширину, для решения этой задачи они подходят идеально. Очереди можно использовать и во многих других ситуациях. Однажды мне попался код для планирования заданий, в котором я обнаружил пример достойного применения очереди с приоритетом, реализованной средствами Python-модуля heapq, когда первыми выполнялись самые короткие задания.\n",
      "\n",
      "Криптографические алгоритмы: Uber\r\n",
      "Важные данные, которые пользователи вводят в мобильные приложения или в веб-приложения, нужно, перед передачей по сети, зашифровать. А расшифровывают их лишь там, где они нужны. Для того чтобы организовать такую схему работы, на клиентских и серверных частях приложений должны присутствовать реализации криптографических алгоритмов.\n",
      "\r\n",
      "Разбираться в криптографических алгоритмах — это очень интересно. При этом для решения неких задач не стоит предлагать собственные алгоритмы. Это — одна из худших идей, которая может прийти в голову программисту. Вместо этого берётся существующий, хорошо документированный стандарт и используются встроенные примитивы соответствующих фреймворков. Обычно в качестве стандарта, выбираемого при реализации криптографических решений, выступает AES. Он достаточно надёжен для того чтобы шифровать с его помощью секретную информацию в США. Не существует известных работающих атак на протокол. AES-192 и AES-256 обычно, для решения большинства практических задач, достаточно надёжны.\n",
      "\r\n",
      "Когда я пришёл в Uber, мобильная система шифрования и система шифрования веб-приложения уже были реализованы, в их основе лежали вышеописанные механизмы, поэтому у меня был повод для изучения подробностей об AES (Advanced Encryption Standard), о HMAC (Hashed Message Authentication Codes), об алгоритме RSA и о прочих подобных вещах. Интересно было и дойти до понимания того, как доказывают криптостойкость последовательности действий, применяемой при шифровании. Например, если говорить об аутентифицированном шифровании с присоединёнными данными, оказывается, что анализируя режимы encrypt-and-MAC, MAC-then-encrypt и encrypt-then-MAC, можно доказать криптостойкость лишь одного из них, хотя это и не означает того, что остальные не являются криптостойкими.\n",
      "\r\n",
      "Вам вряд ли когда-нибудь придётся самостоятельно реализовывать криптографические примитивы, если только вы не будете заниматься реализацией совершенно нового криптографического фреймворка. Но вы вполне можете столкнуться с необходимостью принятия решений о том, какие именно примитивы использовать, и о том, как их комбинировать. Знания в сфере криптографических алгоритмов могут вам понадобиться и для того чтобы понять, почему в некоей системе применяется определённый подход к шифрованию данных.\n",
      "\n",
      "Деревья решений: Uber\r\n",
      "В ходе работы над одним из проектов нам, в мобильном приложении, понадобилось реализовать сложную бизнес-логику. А именно, основываясь на полудюжине правил, нужно было показать один из нескольких экранов. Эти правила были очень сложными, что объяснялось тем, что в расчёт надо было принимать результаты последовательности проверок и предпочтения пользователя.\n",
      "\r\n",
      "Программист, который приступил к решению этой задачи, сначала попробовал выразить все эти правила в виде инструкций if-else. Это привело к появлению чрезвычайно запутанного кода. В итоге решено было воспользоваться деревом решений. С его помощью было несложно произвести все необходимые проверки. Его было сравнительно просто реализовать. Кроме того, если нужно, его можно было, без особых проблем, изменить. Нам нужно было создать собственную реализацию дерева решений, такого, чтобы проверка условий осуществлялась бы на его рёбрах. Это — всё, что нам было нужно от этого дерева. Хотя мы могли бы сэкономить время, потраченное на реализацию дерева, воспользовавшись другим подходом, команда решила, что особое дерево будет легче поддерживать, и приступила к работе. Выглядит это дерево примерно так: рёбра символизируют результаты проверки условий (это — бинарные результаты, или результаты, представленные диапазонами значений), а листовые узлы дерева описывают экраны, на которые нужно выполнять переходы.\n",
      "\n",
      "\n",
      "Структура созданного нами дерева решений, используемого для выяснения того, какой именно экран нужно показать. При принятии решения используется сложный набор правил\n",
      "\r\n",
      "В системе сборки мобильной версии приложения Uber, называемой SubmitQueue, тоже использовалось дерево решений, но оно формировалось динамически. Команде Developer Experience нужно было решить сложную проблему выполнения ежедневного слияния (merge) сотен веток-источников изменений с целевой веткой. При этом на выполнение каждой сборки нужно было около 30 минут. Сюда входили компиляция, выполнение модульных и интеграционных тестов, а также тестов интерфейса. Распараллеливание сборок было недостаточно хорошим решением, так как в различных сборках часто были перекрывающиеся изменения, что вызывало конфликты слияния. На практике это означало, что иногда программистам приходилось ждать 2-3 часа, прибегать к команде rebase, и снова запускать процесс слияния веток, надеясь, что в этот раз они не столкнутся с конфликтом.\n",
      "\r\n",
      "Команда Developer Experience применила инновационный подход, который заключался в прогнозировании конфликтов слияния и в соответствующем размещении сборок в очереди. Делалось это с использованием особого бинарного дерева принятия решений (speculation tree).\n",
      "\n",
      "\n",
      "В SubmitQueue используется бинарное дерево принятия решений, рёбра которого снабжены комментариями о прогнозируемых вероятностях. Этот подход позволяет определить то, какие наборы сборок можно выполнять параллельно. Делается это ради сокращения времени между получением и исполнением задач и ради повышения пропускной способности системы. В ветку master при этом должен попадать только полностью проверенный и работоспособный код\n",
      "\r\n",
      "Специалисты команды Developer Experience, создавшие эту систему, написали отличный материал о ней. А вот — ещё одна статья на ту же тему, хорошо иллюстрированная. Результатом внедрения данной системы стало создание гораздо более быстрой, чем раньше, системы сборки проектов. Она позволила оптимизировать время сборки проектов и помогла облегчить жизнь сотен мобильных программистов.\n",
      "\n",
      "Шестиугольные сетки, иерархические индексы: Uber\r\n",
      "Это — последний проект, о котором я хочу тут рассказать. Он был полностью основан на одной особенной структуре данных. Именно занимаясь им я об этой структуре данных и узнал. Речь идёт о шестиугольной сетке с иерархическими индексами.\n",
      "\r\n",
      "Одной из самых сложных и интересных проблем в Uber была оптимизация стоимости поездок и распределения заказов по партнёрам. Цены на поездки могут динамически меняться, водители постоянно находятся в движении. Инженерами Uber была создана сеточная система H3. Она предназначена для визуализации и анализа данных по городам в разных масштабах. Структура данных, которая используется для решения вышеозначенных задач, представляет собой шестиугольную сетку с иерархическими индексами. Для визуализации данных используется пара специализированных внутренних инструментов.\n",
      "\n",
      "\n",
      "Разбиение карты на шестиугольные ячейки \n",
      "\r\n",
      "Эта структура данных отличается собственной системой индексирования, обхода, собственными определениями отдельных областей сетки, собственными функциями. Детальное описание этого всего можно найти в документации к соответствующему API. Подробности об H3 можно почитать здесь. Вот — исходный код. Здесь можно найти рассказ о том, как и почему была создана эта система.\n",
      "\r\n",
      "Опыт работы над этой системой позволил мне прочувствовать то, что создание собственных специализированных структур данных может иметь смысл при решении весьма специфических задач. Существует совсем немного задач, в решении которых можно применить шестиугольную сетку, если не учитывать разбиение на фрагменты карт с сопоставлением с каждым получившимся фрагментом данных разных уровней. Но, как и в других случаях, если вы знакомы с другими структурами данных, понять эту будет гораздо проще. А человеку, который разобрался с шестиугольной сеткой, проще будет создать новую структуру данных, предназначенную для решения задач, похожих на те которые решают с помощью такой сетки.\n",
      "\n",
      "Структуры данных и алгоритмы на собеседованиях\r\n",
      "Выше я рассказал о том, с какими структурами данных и алгоритмами я столкнулся, долгое время работая в различных компаниях. Предлагаю сейчас вернуться к тому твиту Макса Хауэлла, который я упоминал в самом начале материала. Там Макс жаловался на то, что на собеседовании в Google ему предложили инвертировать бинарное дерево на доске. Он этого не сделал. Ему указали на дверь. В этой ситуации я — на стороне Макса.\n",
      "\r\n",
      "Я считаю, что знания о том, как работают популярные алгоритмы, или о том, как устроены экзотические структуры данных, это — не те знания, которые нужны для работы в технологической компании. Нужно знать о том, что такое алгоритм, нужно уметь самостоятельно придумывать простые алгоритмы, например, работающие по «жадному» принципу. Ещё нужно обладать знаниями о самых распространённых структурах данных, вроде хеш-таблиц, очередей и стеков. Но что-то достаточно специфическое, вроде алгоритма Дейкстры или A*, или чего-то ещё более сложного, это — не то, что нужно заучивать наизусть. Если вам эти алгоритмы реально понадобятся — вы легко сможете найти справочные материалы по ним. Я, например, если говорить об алгоритмах, не относящихся к алгоритмам сортировки, обычно стремился разобраться с ними в общих чертах и понять их сущность. То же касается и экзотических структур данных вроде красно-чёрных деревьев и АВЛ-деревьев. У меня никогда не возникало нужды в их использовании. А если бы они мне и понадобились — я всегда мог бы освежить знания о них, прибегнув к соответствующим публикациям. Я, проводя собеседования, как я уже говорил, никогда не задаю подобные вопросы и не планирую их задавать.\n",
      "\r\n",
      "Я за практические задачи, имеющие отношение к программированию, при решении которых можно применить множество подходов: от методов «грубой силы» и «жадных» вариантов алгоритмов до более продвинутых алгоритмических идей. Например, я полагаю, что вполне нормальной является задача, касающаяся выравнивания текста по ширине, вроде этой. Мне, например, пришлось решать эту задачу при создании компонента для рендеринга текста на Windows Phone. Решить эту задачу можно, просто воспользовавшись массивом и несколькими инструкциями if-else, не прибегая к каким-то мудрёным структурам данным.\n",
      "\r\n",
      "На самом деле, многие команды и компании преувеличивают значимость алгоритмических задач. Мне понятна привлекательность вопросов по алгоритмам. Они позволяют оценить соискателя за короткое время, вопросы легко переделывать, а значит, если вопросы, которые кому-то задавали, станут достоянием общественности, это особых проблем не вызовет. Вопросы по алгоритмам хорошо подходят для организации испытаний большого количества соискателей. Например, можно создать пул, состоящий из более чем сотни вопросов, и случайным образом раздать их соискателям. Вопросы, касающиеся динамического программирования и экзотических структур данных, встречаются всё чаще и чаще. Особенно — в Кремниевой долине. Эти вопросы могут помочь компаниям в найме сильных программистов. Но эти же вопросы способны закрыть дорогу в компании тем людям, которые преуспели в делах, где глубокие знания алгоритмов не нужны.\n",
      "\r\n",
      "Если вы — из компании, которая нанимает только тех, кто едва ли не с рождения обладает глубокими знаниями сложных алгоритмов, я предлагаю вам хорошо подумать о том, те ли это люди, что вам нужны. Я, например, нанимал прекрасные команды в Skyscanner (Лондон) и в Uber (Амстердам), не задавая соискателям хитрых вопросов по алгоритмам. Я ограничивался самыми обычными структурами данных и проверкой возможностей собеседуемых, имеющих отношение к решению задач. То есть — им нужно было знать о распространённых структурах данных и уметь придумывать простые алгоритмы для решения стоящих перед ними задач. Структуры данных и алгоритмы — это всего лишь инструменты.\n",
      "\n",
      "Итоги: структуры данных и алгоритмы — это инструменты\r\n",
      "Если вы работаете в динамичной инновационной технологической компании, то вы, наверняка, в коде продуктов этой компании, столкнётесь с реализациями самых разных структур данных и алгоритмов. Если вы занимаетесь разработкой чего-то совершенно нового, то вам часто придётся искать структуры данных, упрощающие решение встающих перед вами задач. В подобных ситуациях вам, чтобы сделать правильный выбор, пригодятся общие знания об алгоритмах и структурах данных и об их плюсах и минусах.\n",
      "\r\n",
      "Структуры данных и алгоритмы — это инструменты, которыми вы должны уверенно пользоваться, создавая программы. Зная эти инструменты, вы увидите много такого, что вам уже известно, в кодовых базах, в которых они используются. Кроме того, подобные знания позволят вам с гораздо большей уверенностью решать сложные задачи. Вы будете знать о теоретических ограничениях алгоритмов, о том, каким оптимизациям их можно подвергнуть. Это поможет вам, в итоге, выйти на решение, которое, с учётом всех необходимых компромиссов, окажется настолько хорошим, насколько это возможно.\n",
      "\r\n",
      "Если вы хотите лучше изучить структуры данных и алгоритмы — вот несколько советов и ресурсов:\n",
      "\n",
      "\n",
      "Почитайте о хеш-таблицах, связных списках, деревьях, графах, кучах, очередях, стеках. Поэкспериментируйте с ними, используя тот язык, на котором пишете. Вот хороший обзор, посвящённый структурам данных. А вот — место, где можно попрактиковаться.\n",
      "Почитайте книгу «Грокаем алгоритмы». Это, как мне кажется, лучший путеводитель по алгоритмам, предназначенный для самых разных категорий программистов. Это — доступное иллюстрированное руководство, в котором содержится всё, что большинству специалистов нужно знать по данной теме. Я уверен в том, что вам не понадобится больше знаний об алгоритмах, чем те, что вы сможете найти в этой книге.\n",
      "Вот ещё пара книг: «Алгоритмы. Руководство по разработке» и «Алгоритмы на Java, 4-е издание». Я воспользовался ими для того чтобы освежить в памяти университетские знания об алгоритмах. Я их, правда, до конца не дочитал. Они показались мне достаточно сухими и неприменимыми к моей повседневной работе.\n",
      "\r\n",
      "А с какими структурами данных и алгоритмами сталкивались на практике вы?\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Как известно, компания SAP предлагает полный спектр программного обеспечения, как для ведения транзакционных данных, так и для обработки этих данных в системах анализа и отчетности. В частности платформа SAP Business Warehouse (SAP BW) представляет собой инструментарий для хранения и анализа данных, обладающий широкими техническими возможностями. При всех своих объективных преимуществах система SAP BW обладает одним значительным недостатком. Это высокая стоимость хранения и обработки данных, особенно заметная при использовании облачной SAP BW on Hana. \n",
      "\r\n",
      "А что если в качестве хранилища начать использовать какой-нибудь non-SAP и желательно OpenSource продукт? Мы в Х5 Retail Group остановили свой выбор на GreenPlum. Это конечно решает вопрос стоимости, но при этом сразу появляются вопросы, которые при использовании SAP BW решались практически по умолчанию.\n",
      "\n",
      "\n",
      "\r\n",
      "В частности, каким образом забирать данные из систем источников, которые в большинстве своем являются решениями SAP?\n",
      "\r\n",
      "«HR-метрики» стал первым проектом, в котором необходимо было решить эту проблему. Нашей целью было создание хранилища HR-данных и построение аналитической отчетности по направлению работы с сотрудниками. При этом основным источником данных является транзакционная система SAP HCM, в которой ведутся все кадровые, организационные и зарплатные мероприятия. \n",
      "\n",
      "Экстракция данных\r\n",
      "В SAP BW для SAP-систем существуют стандартные экстракторы данных. Эти экстракторы могут автоматически собирать необходимые данные, отслеживать их целостность, определять дельты изменений. Вот, например, стандартный источник данных по атрибутам сотрудника 0EMPLOYEE_ATTR:\n",
      "\n",
      "\n",
      "\r\n",
      "Результат экстракции данных из него по одному сотруднику:\n",
      "\n",
      "\n",
      "\r\n",
      "При необходимости такой экстрактор может быть модифицирован под собственные требования или может быть создан свой собственный экстрактор. \n",
      "\r\n",
      "Первой возникла идея о возможности их переиспользования. К сожалению, это оказалось неосуществимой задачей. Большая часть логики реализована на стороне SAP BW, и безболезненно отделить экстрактор на источнике от SAP BW не получилось. \n",
      "\r\n",
      "Стало очевидно, что потребуется разработка собственного механизма извлечения данных из SAP систем. \n",
      "\n",
      "Структура хранения данных в SAP HCM\r\n",
      "Для понимания требований к такому механизму, первоначально нужно определить какие именно данные нам потребуются.\n",
      "\r\n",
      "Большинство данных в SAP HCM хранится в плоских SQL таблицах. На основании этих данных приложения SAP визуализируют пользователю оргструктуры, сотрудников и прочую HR информацию. Например, вот так в SAP HCM выглядит оргструктура:\n",
      "\n",
      "\n",
      "\r\n",
      "Физически такое дерево хранится в двух таблицах — в hrp1000 объекты и в hrp1001 связи между этими объектами. \n",
      "\r\n",
      "Объекты «Департамент 1» и «Управление 1»:\n",
      "\n",
      "\n",
      "\r\n",
      "Связь между объектами:\n",
      "\n",
      "\n",
      "\r\n",
      "Как типов объектов, так и типов связи между ними может быть огромное количество. Существуют как стандартные связи между объектами, так и кастомизированные для собственных специфичных нужд. Например, стандартная связь B012 между оргединицей и штатной должностью указывает на руководителя подразделения.\n",
      "\r\n",
      "Отображение руководителя в SAP:\n",
      "\n",
      "\n",
      "\r\n",
      "Хранение в таблице БД:\n",
      "\n",
      "\n",
      "\r\n",
      "Данные по сотрудникам хранятся в таблицах pa*. Например, данные о кадровых мероприятиях по сотруднику хранятся в таблице pa0000\n",
      "\n",
      "\n",
      "\r\n",
      "Мы приняли решение, что GreenPlum будет забирать «сырые» данные, т.е. просто копировать их из SAP таблиц. И уже непосредственно в GreenPlum они будут обрабатываться и преобразовываться в физические объекты (например, Отдел или Сотрудник) и метрики (например, среднесписочная численность).\n",
      "\r\n",
      "Было определено порядка 70 таблиц, данные из которых необходимо передавать в GreenPlum. После чего мы приступили к проработке способа передачи этих данных.\n",
      "\r\n",
      "SAP предлагает достаточно большое количество механизмов интеграции. Но самый простой способ – прямой доступ к базе данных запрещен из-за лицензионных ограничений. Таким образом, все интеграционные потоки должны быть реализованы на уровне сервера приложений. \r\n",
      "Следующей проблемой было отсутствие данных об удаленных записях в БД SAP. При удалении строки в БД, она удаляется физически. Т.е. формирование дельты изменений по времени изменения не представлялось возможным.\n",
      "\r\n",
      "Конечно, в SAP HCM есть механизмы фиксация изменений данных. Например, для последующей передачи в системы получатели существуют указатели изменений(change pointer), которые фиксируют любые изменения и на основании которых формируются Idoc (объект для передачи во внешние системы).\n",
      "\r\n",
      "Пример IDoc изменения инфотипа 0302 у сотрудника с табельным номером 1251445:\n",
      "\n",
      "\n",
      "\r\n",
      "Или ведение логов изменений данных в таблице DBTABLOG.\n",
      "\r\n",
      "Пример лога удаления записи с ключом QK53216375 из таблицы hrp1000:\n",
      "\n",
      "\n",
      "\r\n",
      "Но эти механизмы доступны не для всех необходимых данных и их обработка на уровне сервера приложений может потреблять достаточно много ресурсов. Поэтому массовое включение логирования на все необходимые таблицы может привести к заметной деградации производительности системы. \n",
      "\r\n",
      "Следующей серьезной проблемой были кластерные таблицы. Данные оценки времени и расчета зарплаты в RDBMS версии SAP HCM хранятся в виде набора логических таблиц по каждому сотруднику за каждый расчет. Эти логические таблицы в виде двоичных данных хранятся в таблице pcl2.\n",
      "\r\n",
      "Кластер расчета заработной платы:\n",
      "\n",
      "\n",
      "\r\n",
      "Данные из кластерных таблиц невозможно считать SQL командой, а требуется использование макрокоманд SAP HCM или специальных функциональных модулей. Соответственно, скорость считывания таких таблиц будет достаточно низка. С другой стороны, в таких кластерах хранятся данные, которые необходимы только раз в месяц – итоговый расчет заработной платы и оценка времени. Так что скорость в этом случае не столь критична. \n",
      "\r\n",
      "Оценивая варианты с формированием дельты изменения данных, решили так же рассмотреть вариант с полной выгрузкой. Вариант каждый день передавать гигабайты неизменных данных между системами не может выглядеть красиво. Однако он имеет и ряд преимуществ – нет необходимости как реализации дельты на стороне источника, так и реализация встраивания этой дельты на стороне приемника. Соответственно, уменьшается стоимость и сроки реализации, и повышается надежность интеграции. При этом было определено, что практически все изменения в SAP HR происходят в горизонте трех месяцев до текущей даты. Таким образом, было принято решение остановиться на ежедневной полной выгрузке данных из SAP HR за N месяцев до текущей даты и на ежемесячной полной выгрузке. Параметр N зависит от конкретной таблицы\r\n",
      " и колеблется от 1 до 15.\n",
      "\r\n",
      "Для экстракции данных была предложена следующая схема:\n",
      "\n",
      "\n",
      "\r\n",
      "Внешняя система формирует запрос и отправляет его в SAP HCM, там этот запрос проверяется на полноту данных и полномочия на доступ к таблицам. В случае успешной проверки, в SAP HCM отрабатывает программа, собирающая необходимые данные и передающая их в интеграционное решение Fuse. Fuse определяет необходимый топик в Kafka и передает данные туда. Далее данные из Kafka передаются в Stage Area GP.\n",
      "\r\n",
      "Нас в данной цепочке интересует вопрос извлечения данных из SAP HCM. Остановимся на нем подробнее. \n",
      "\r\n",
      "Схема взаимодействия SAP HCM-FUSE.\n",
      "\n",
      "\n",
      "\r\n",
      "Внешняя система определяет время последнего успешного запроса в SAP.\r\n",
      "Процесс может быть запущен по таймеру или иному событию, в том числе может быть установлен таймаут ожидания ответа с данными от SAP и инициация повторного запроса. После чего формирует запрос дельты и отправляет его в SAP.\n",
      "\r\n",
      "Данные запроса передаются в body в формате json.\r\n",
      "Метод http: POST.\r\n",
      "Пример запроса:\n",
      "\n",
      "\n",
      "\r\n",
      "Сервис SAP выполняет контроль запроса на полноту, соответствие текущей структуре SAP, наличие разрешения доступа к запрошенной таблице. \n",
      "\r\n",
      "В случае ошибок сервис возвращает ответ с соответствующим кодом и описанием. В случае успешного контроля он создает фоновый процесс для формирования выборки, генерирует и синхронно возвращает уникальный id сессии. \n",
      "\r\n",
      "Внешняя система в случае ошибки регистрирует ее в журнале. В случае успешного ответа передает id сессии и имя таблицы по которой был сделан запрос.\n",
      "\r\n",
      "Внешняя система регистрирует текущую сессию как открытую. Если есть другие сессии по данной таблице, они закрываются с регистрацией предупреждения в журнале.\n",
      "\r\n",
      "Фоновое задание SAP формирует курсор по заданным параметрам и пакет данных заданного размера. Размер пакета – максимальное количество записей, которые процесс читает из БД. По умолчанию принимается равным 2000. Если в выборке БД больше записей, чем используемый размер пакета после передачи первого пакета формируется следующий блок с соответствующим offset и инкрементированным номером пакета. Номера инкрементируются на 1 и отправляются строго последовательно. \n",
      "\r\n",
      "Далее SAP передает пакет на вход web-сервису внешней системы. А она система выполняет контроли входящего пакета. В системе должна быть зарегистрирована сессия с полученным id и она должна находиться в открытом статусе. Если номер пакета > 1 в системе должно быть зарегистрировано успешное получение предыдущего пакета (package_id-1).\n",
      "\r\n",
      "В случае успешного контроля внешняя система парсит и сохраняет данные таблицы.\n",
      "\r\n",
      "Дополнительно, если в пакете присутствует флаг final и сериализация прошла успешно, происходит уведомление модуля интеграции об успешном завершении обработки сессии и модуль обновляет статус сессии.\n",
      "\r\n",
      "В случае ошибки контролей/разбора ошибка логируется и пакеты по данной сессии будут отклоняться внешней системой.\n",
      "\r\n",
      "Так же и в обратном случае, когда внешняя система возвращает ошибку, она логируется и прекращается передача пакетов.\n",
      "\r\n",
      "Для запроса данных на стороне SAP HСM был реализован интеграционный сервис. Сервис реализован на фреймворке ICF (SAP Internet Communication Framework — help.sap.com/viewer/6da7259a6c4b1014b7d5e759cc76fd22/7.01.22/en-US/488d6e0ea6ed72d5e10000000a42189c.html). Он позволяет производить запрос данных из системы SAP HCM по определенным таблицам. При формировании запроса данных есть возможность задавать перечень конкретных полей и параметры фильтрации с целью получения необходимых данных. При этом реализация сервиса не предполагает какой-либо бизнес-логики. Алгоритмы расчёта дельты, параметров запроса, контроля целостности, и пр. также реализуются на стороне внешней системы.\n",
      "\r\n",
      "Данный механизм позволяет собирать и передавать все необходимые данные за несколько часов. Такая скорость находится на грани приемлемой, поэтому это решение рассматривается нами как временное, позволившее закрыть потребность в инструменте экстракции на проекте.\r\n",
      "В целевой картине для решения задачи экстракции данных прорабатываются варианты использования CDC систем типа Oracle Golden Gate или ETL инструментов типа SAP DS.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Сегодня, в преддверии старта нового потока курса «Python для веб-разработки», делимся с вами полезным переводом статьи о небольшой интерактивной визуализации, для исследований данных о фильмах. Автор использует не только Flask и Bokeh, но и задействуя бесплатную облачную платформу баз данных easybase.io. Все подробности и демонстрации вы найдёте под катом.\n",
      "\n",
      "\r\n",
      "Python имеет фантастическую поддержку полезных инструментов анализа: NumPy, SciPy, pandas, Dask, Scikit-Learn, OpenCV и многих других. Из библиотек визуализации данных для Python Bokeh преобладает как самая функциональная и мощная. Эта библиотека поддерживает несколько интерфейсов, охватывающих многие распространенные варианты применения.\n",
      "\r\n",
      "Одна из замечательных особенностей Bokeh — возможность экспортировать рисунок в виде сырых HTML и JavaScript. Она позволяет внедрять нарисованные программно рисунки в шаблоны приложения Flask. Когда пользователь открывает веб-приложение Flask, рисунки Bokeh создаются и встраиваются в HTML-код в реальном времени. \n",
      "\r\n",
      "Для примера создадим программу интерактивного исследования данных о фильмах. В проекте будут виджеты пользовательского интерфейса (ползунки, меню), которые обновляют данные в ответ на действия пользователя. Вот, чему научит эта статья:\n",
      "\n",
      "\n",
      "Как в Bokeh создать интерактивную визуализацию с пятью точками данных.\n",
      "Как интегрировать в проект облачную базу данных с тремя тысячами точками данных (Easybase.io).\n",
      "Как вставить рисунок Bokeh в шаблон Flask.\n",
      "Как с помощью обратных вызовов JavaScript (CustomJS) добавить виджеты Bokeh, чтобы запрашивать данные.\n",
      "\n",
      "Часть первая\r\n",
      "Первые шаги — установка: \n",
      "\n",
      "pip install bokeh \n",
      "pip install Flask\n",
      "\n",
      "\r\n",
      "Создайте файл с именем app.py и начните с такого кода:\n",
      "\n",
      "from bokeh.models import ColumnDataSource\n",
      "from bokeh.plotting import figure, output_file, show\n",
      "\n",
      "source = ColumnDataSource()\n",
      "\n",
      "fig = figure(plot_height=600, plot_width=720, tooltips=[(\"Title\", \"@title\"), (\"Released\", \"@released\")])\n",
      "fig.circle(x=\"x\", y=\"y\", source=source, size=8, color=\"color\", line_color=None)\n",
      "fig.xaxis.axis_label = \"IMDB Rating\"\n",
      "fig.yaxis.axis_label = \"Rotten Tomatoes Rating\"\n",
      "\n",
      "\r\n",
      "Переменная source используется для представления данных стандартным для элементов Bokeh способом. Данные передаются в объект, скармливаемые рисунку Bokeh. Этот объект — сопоставление ключей с массивом значений. Позже мы увидим, как получить доступ и манипулировать этим объектом напрямую с помощью CustomJS. \n",
      "\r\n",
      "Заметим, что fig представляет визуальный компонент Bokeh. Параметр tooltips задает надпись, отображаемую при наведении курсора мыши на точку в визуализации. Кортежи этого массива структурированы так: (\"NAME TO DISPLAY\", \"@COLUMN_NAME_IN_SOURCE\"). Чтобы изменить размер отдельных точек на графике, можно изменить параметр size в fig.circle(). Оставьте параметры x, y и color одинаковыми: позже будет показано, как изменить их согласно какому-то условию.\n",
      "\r\n",
      "Переменная axis_label контролирует метки осей X и Y. В нашем случае ось X измеряет рейтинг фильма в IMDB, ось Y — рейтинг Rotten Tomatoes. Позже мы увидим, что (очевидно) между ними есть положительная корреляция. Теперь можно написать такой код для передачи каких-то данных в рисунок и его отображения в нашем браузере:\n",
      "\n",
      "currMovies = [\n",
      "    {'imdbid': 'tt0099878', 'title': 'Jetsons: The Movie', 'genre': 'Animation, Comedy, Family', 'released': '07/06/1990', 'imdbrating': 5.4, 'imdbvotes': 2731, 'country': 'USA', 'numericrating': 4.3, 'usermeter': 46},\n",
      "    {'imdbid': 'tt0099892', 'title': 'Joe Versus the Volcano', 'genre': 'Comedy, Romance', 'released': '03/09/1990', 'imdbrating': 5.6, 'imdbvotes': 23680, 'country': 'USA', 'numericrating': 5.2, 'usermeter': 54},\n",
      "    {'imdbid': 'tt0099938', 'title': 'Kindergarten Cop', 'genre': 'Action, Comedy, Crime', 'released': '12/21/1990', 'imdbrating': 5.9, 'imdbvotes': 83461, 'country': 'USA', 'numericrating': 5.1, 'usermeter': 51},\n",
      "    {'imdbid': 'tt0099939', 'title': 'King of New York', 'genre': 'Crime, Thriller', 'released': '09/28/1990', 'imdbrating': 7, 'imdbvotes': 19031, 'country': 'Italy, USA, UK', 'numericrating': 6.1, 'usermeter': 79},\n",
      "    {'imdbid': 'tt0099951', 'title': 'The Krays', 'genre': 'Biography, Crime, Drama', 'released': '11/09/1990', 'imdbrating': 6.7, 'imdbvotes': 4247, 'country': 'UK', 'numericrating': 6.4, 'usermeter': 82}\n",
      "]\n",
      "\n",
      "source.data = dict(\n",
      "    x = [d['imdbrating'] for d in currMovies],\n",
      "    y = [d['numericrating'] for d in currMovies],\n",
      "    color = [\"#FF9900\" for d in currMovies],\n",
      "    title = [d['title'] for d in currMovies],\n",
      "    released = [d['released'] for d in currMovies],\n",
      "    imdbvotes = [d['imdbvotes'] for d in currMovies],\n",
      "    genre = [d['genre'] for d in currMovies]\n",
      ")\n",
      "\n",
      "output_file(\"graph.html\")\n",
      "show(fig)\r\n",
      "До второй части мы будем использовать массив из пяти примеров словарей, содержащих связанные с фильмами свойства. В конечном счете мы получим 3000 записей из EasyBase в режиме реального времени. Свойства в source.data — это ссылка Bokeh на то, где на графике должен отображаться элемент и то, как он должен выглядеть. Как уже было сказано, структура этой переменной представляет собой словарь, отображающий все наши атрибуты в соответствующий массив. По этой причине используется синтаксис построения встроенного массива для захвата и извлечения каждого свойства из наших данных в его массив.\n",
      "\r\n",
      "Здесь видно, где именно эти элементы расположены на рисунке Bokeh: x, y и color передаются в метод circle(), а title и genre передаются во всплывающую подсказку (позже воспользуемся released, imdbvotes и genre). Свободно изменяйте значения в массивах. Например, если вы хотите, чтобы цвет точек соотносился с жанром, вот код: color = [\"#FF9900\" for d in currMovies], можно сделать и так: color = [\"#008800\", if d['genre'] == \"drama\" else \"#FF9900\" for d in curMovies]. Наконец, output_file указывает на место, где вы хотите сохранить свои более поздние рисунки. show(fig) сохраняет рисунок в этом месте и открывает его в вашем браузере. Запустите файл — и вот, что вы увидите:\n",
      "\n",
      "\n",
      "\r\n",
      "Пока всё не слишком увлекательно, но наведите курсор мыши на любую точку, чтобы получить информацию о фильме, которая указана в tooltips. Кроме того, попробуйте панорамировать и масштабировать интерфейс. Эти функции пригодятся позже.\n",
      "\n",
      "Часть вторая\r\n",
      "Вот ссылка на CSV-файл с тремя тысячами записей фильмов с теми же атрибутами, что в нашем примере.\n",
      "\r\n",
      "Давайте поместим записи в базу данных, чтобы обращаться к ним асинхронно и управлять ими из подходящего источника. Я воспользуюсь easybase.io потому, что это бесплатно и не нужно ничего скачивать, подробнее об этом здесь. Кроме того, легко заполнить коллекцию содержимым файла CSV или JSON. Войдите в EasyBase и создайте таблицу по крайней мере с такими столбцами (свободно добавляйте другие атрибуты, если хотите):\n",
      "\n",
      "\n",
      "\r\n",
      "Как только эта таблица откроется, нажмите кнопку + и перейдите к экрану «upload data».\n",
      "\n",
      "\n",
      "\r\n",
      "Перетащите файл CSV в это диалоговое окно. Полученная коллекция будет выглядеть примерно так:\n",
      "\n",
      "\n",
      "\r\n",
      "Помните, что всегда можно загрузить данные в формате CSV или JSON из EasyBase, выбрав всё (в разделе +) и перейдя к разделу «share».\n",
      "\r\n",
      "Перейдите в Integrate → REST → GET. Откройте свою новую интеграцию в Get, добавьте все столбцы. Сохранитесь, а затем откройте всплывающее окно интеграции. Мое окно выглядит так:\n",
      "\n",
      "\n",
      "\r\n",
      "Обратите внимание на ваш идентификатор интеграции — именно через него мы будем извлекать данные из приложения.\n",
      "\n",
      "Часть третья\r\n",
      "Теперь давайте превратим приложение в проект Flask. Перейдите в каталог с файлом app.py. Создайте папку с именем template, добавьте файл с именем index.html.\n",
      "\n",
      "project\n",
      "├── templates\n",
      "│   └── index.html\n",
      "└── app.py\n",
      "\r\n",
      "В файле app.py посмотрим очень простую реализацию приложения Flask:\n",
      "\n",
      "\n",
      "from flask import Flask, render_template\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/')\n",
      "def index():\n",
      "    return render_template('index.html')\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "\r\n",
      "Запустите программу командой export FLASK_APP=app.py && flask run на Mac или set FLASK_APP=app.py && flask run на Windows, будет создан веб-сервер. Перейдите по адресу localhost:5000, приложение отобразит templates/index.html. Напишем в файле index.html такой код:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Document</title>\n",
      "    {{ js_resources|indent(4)|safe }}\n",
      "    {{ css_resources|indent(4)|safe }}\n",
      "    {{ plot_script|indent(4)|safe }}\n",
      "</head>\n",
      "<body>\n",
      "    <h1 style=\"text-align: center; margin-bottom: 40px;\">Flask + Bokeh + EasyBase.io</h2>\n",
      "    <div style=\"display: flex; justify-content: center;\">\n",
      "        {{ plot_div|indent(4)|safe }}\n",
      "    </div>\n",
      "</body>\n",
      "</html>\n",
      "\r\n",
      "Это очень простая веб-страница с четырьмя текучими [прим. перев. — конечно, нет никаких «текучих атрибутов», вероятно, речь идет об этом видео] атрибутами: js_resources, css_resources, plot_script и plot_div. Bokeh даст нам все передаваемые в эти атрибуты переменные. Во-первых, мы намерены совместить код из первой части с app.py. Начнем с импортирования модулей:\n",
      "\n",
      "from flask import Flask, render_template\n",
      "from easybase import get\n",
      "from bokeh.models import ColumnDataSource, Div, Select, Slider, TextInput\n",
      "from bokeh.io import curdoc\n",
      "from bokeh.resources import INLINE\n",
      "from bokeh.embed import components\n",
      "from bokeh.plotting import figure, output_file, show\n",
      "\r\n",
      "Добавьте код из первой части в метод index() перед вызовом return render_template(\"index.html\"). Я заменю захардкоженный массив фильмов вызовом get () в EasyBase. Если у вас не установлена библиотека EasyBase, установите ее так: pip easybase-python. Я заменяю массив из первой части вот таким методом:\n",
      "\n",
      "def selectedMovies():\n",
      "    res = get(\"Dt-p-a0jVTBSVQji\", 0, 3000, \"password\")\n",
      "    return res\n",
      "  \n",
      "# ...\n",
      "\n",
      "currMovies = selectedMovies()\r\n",
      "Первый параметр get() — это идентификатор интеграции из предыдущей версии, после идут offset, length и authentication.\n",
      "\n",
      "Как и в первой части, этот метод возвращает массив словарей. У этих словарей атрибуты те же, что и раньше.\n",
      "\r\n",
      "В методе index() заменим return render_template(\"index.html\") вот этим кодом:\n",
      "\n",
      "script, div = components(fig)\n",
      "return render_template(\n",
      "    'index.html',\n",
      "    plot_script=script,\n",
      "    plot_div=div,\n",
      "    js_resources=INLINE.render_js(),\n",
      "    css_resources=INLINE.render_css(),\n",
      ").encode(encoding='UTF-8')\n",
      "\r\n",
      "Ниже перечисление вводимых в шаблон переменных.\n",
      "\n",
      "\n",
      "plot_script: JavaScript рисунка\n",
      "plot_div: HTML рисунка внутри тега div\n",
      "js_resources: основной и требуемый Boken JavaScript\n",
      "css_resources: Основной и требуемый Bokeh CSS\n",
      "\r\n",
      "Теперь app.py будет выглядеть примерно так:\n",
      "\n",
      "from flask import Flask, render_template\n",
      "from easybase import get\n",
      "from bokeh.models import ColumnDataSource, Div, Select, Slider, TextInput\n",
      "from bokeh.io import curdoc\n",
      "from bokeh.resources import INLINE\n",
      "from bokeh.embed import components\n",
      "from bokeh.plotting import figure, output_file, show\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/')\n",
      "def index():\n",
      "    def selectedMovies():\n",
      "        res = get(\"Dt-p-a0jVTBSVQji\", 0, 3000, \"password\")\n",
      "        return res\n",
      "    \n",
      "    source = ColumnDataSource()\n",
      "\n",
      "    fig = figure(plot_height=600, plot_width=720, tooltips=[(\"Title\", \"@title\"), (\"Released\", \"@released\")])\n",
      "    fig.circle(x=\"x\", y=\"y\", source=source, size=5, color=\"color\", line_color=None)\n",
      "    fig.xaxis.axis_label = \"IMDB Rating\"\n",
      "    fig.yaxis.axis_label = \"Rotten Tomatoes Rating\"\n",
      "\n",
      "    currMovies = selectedMovies()\n",
      "\n",
      "    source.data = dict(\n",
      "        x = [d['imdbrating'] for d in currMovies],\n",
      "        y = [d['numericrating'] for d in currMovies],\n",
      "        color = [\"#FF9900\" for d in currMovies],\n",
      "        title = [d['title'] for d in currMovies],\n",
      "        released = [d['released'] for d in currMovies],\n",
      "        imdbvotes = [d['imdbvotes'] for d in currMovies],\n",
      "        genre = [d['genre'] for d in currMovies]\n",
      "    )\n",
      "\n",
      "    script, div = components(fig)\n",
      "    return render_template(\n",
      "        'index.html',\n",
      "        plot_script=script,\n",
      "        plot_div=div,\n",
      "        js_resources=INLINE.render_js(),\n",
      "        css_resources=INLINE.render_css(),\n",
      "    ).encode(encoding='UTF-8')\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "\r\n",
      "Выполните программу командой export FLASK_APP=app.py && flask run на Mac или set FLASK_APP=app.py && flask run на Windows. Ваш сайт на localhost:5000 должен выглядеть примерно так:\n",
      "\n",
      "\n",
      "\n",
      "Часть четвертая\r\n",
      "Итак, у нас есть отображаемая Flask модель Bokeh. Конечная цель — добавить виджеты пользовательского интерфейса, с помощью которых пользователи смогут манипулировать данными. Все изменения будут касаться метода index(). Я очень старался сделать реализацию легко расширяемой. Давайте сначала создадим словарь элементов управления:\n",
      "\n",
      "genre_list = ['All', 'Comedy', 'Sci-Fi', 'Action', 'Drama', 'War', 'Crime', 'Romance', 'Thriller', 'Music', 'Adventure', 'History', 'Fantasy', 'Documentary', 'Horror', 'Mystery', 'Family', 'Animation', 'Biography', 'Sport', 'Western', 'Short', 'Musical']\n",
      "\n",
      "controls = {\n",
      "    \"reviews\": Slider(title=\"Min # of reviews\", value=10, start=10, end=200000, step=10),\n",
      "    \"min_year\": Slider(title=\"Start Year\", start=1970, end=2021, value=1970, step=1),\n",
      "    \"max_year\": Slider(title=\"End Year\", start=1970, end=2021, value=2021, step=1),\n",
      "    \"genre\": Select(title=\"Genre\", value=\"All\", options=genre_list)\n",
      "}\n",
      "\n",
      "controls_array = controls.values()\n",
      "\r\n",
      "Мы видим три ползунка и выпадающее меню. Свободно редактируйте или добавляйте любые пользовательские виджеты в этот словарь. А нам нужно реализовать обратный вызов, выполняемый при изменении любого элемента управления. Это делается с помощью модуля Bokeh CustomJS. Четвертая часть статьи может быть сложной, но я постарался разобрать все как можно понятнее. Начнем с переменной обратного вызова:\n",
      "\n",
      "callback = CustomJS(args=dict(source=source, controls=controls), code=\"\"\"\n",
      "    if (!window.full_data_save) {\n",
      "        window.full_data_save = JSON.parse(JSON.stringify(source.data));\n",
      "    }\n",
      "    var full_data = window.full_data_save;\n",
      "    var full_data_length = full_data.x.length;\n",
      "    var new_data = { x: [], y: [], color: [], title: [], released: [], imdbvotes: [] }\n",
      "    for (var i = 0; i < full_data_length; i++) {\n",
      "        if (full_data.imdbvotes[i] === null || full_data.released[i] === null || full_data.genre[i] === null)\n",
      "            continue;\n",
      "        if (\n",
      "            full_data.imdbvotes[i] > controls.reviews.value &&\n",
      "            Number(full_data.released[i].slice(-4)) >= controls.min_year.value &&\n",
      "            Number(full_data.released[i].slice(-4)) <= controls.max_year.value &&\n",
      "            (controls.genre.value === 'All' || full_data.genre[i].split(\",\").some(ele => ele.trim() === controls.genre.value))\n",
      "        ) {\n",
      "            Object.keys(new_data).forEach(key => new_data[key].push(full_data[key][i]));\n",
      "        }\n",
      "    }\n",
      "    source.data = new_data;\n",
      "    source.change.emit();\n",
      "\"\"\")\n",
      "\r\n",
      "Функция code вызывается при любом изменении входных данных. И вызывается она с доступными аргументами: source (исходные данные) и controls (словарь controls). Первая часть code проверяет, существует ли глобальная переменная JavaScript с именем full_data_save. Поскольку эта переменная не существует при первом запуске этой функции, функция создаст глубокую копию необработанных данных и сохранит их в этой глобальной переменной.\n",
      "\r\n",
      "Теперь full_data_save не будет изменяться, поэтому у нас всегда будет ссылка на исходные данные. Затем создается новый объект с именем new_data, который принимает тот же формат, что у исходных данных. После выполняется цикл по всем исходным данным с проверкой того, удовлетворяют ли данные значению элемента управления. Видно, что доступ к значению элементов управления осуществляется через controls.*control_name*.value, аналогично тому, как исходные данные мы получили через аргументы CustomJS. Поскольку атрибут released имеет формат MM-DD-YYYY, чтобы сравнить его с min_year и max_year (строки 17-18), я воспользуюсь только последними четырьмя символами. Если отдельный элемент удовлетворяет всем запросам пользователя, он перемещается в new_data с помощью приведенной ниже строки:\n",
      "\n",
      "\n",
      "Object.keys(new_data).forEach(key => new_data[key].push(full_data[key][i]));\n",
      "\r\n",
      "Код отправляет все атрибуты full_data по индексу i в соответствующий массив new_data. После просмотра всех данных в цикле исходными данными становятся новые данные. Наконец, изменения отправляются с помощью функции source.change.emit(). Я пытался сделать код расширяемым, поэтому рабочий процесс добавления нового элемента управления выглядит так:\n",
      "\n",
      "\n",
      "Добавляем новый виджет в словарь controls.\n",
      "Внутри CustomJS, добавляем написанное нами условное выражение в строку 15.\n",
      "\r\n",
      "Наконец, воспользуемся циклом, чтобы привести в действие обратный вызов и присвоить новые значения:\n",
      "\n",
      "for single_control in controls_array:\n",
      "    single_control.js_on_change('value', callback)\r\n",
      "И последнее — добавим элементы управления в layout. Заменим предыдущую строку script, div = components(fig) вот так:\n",
      "\n",
      "    \n",
      "    inputs_column = column(*controls_array, width=320, height=1000)\n",
      "    layout_row = row([ inputs_column, fig ])\n",
      "    \n",
      "    script, div = components(layout_row)\n",
      "\r\n",
      "Код создает колонку элементов управления под названием input_controls, за которым следует строка input_controls и рисунок. Теперь передаем эту строку в метод components(), а не просто в рисунок. И запустим приложение:\n",
      "\n",
      "\n",
      "\n",
      "Заключение\r\n",
      "Наше приложение успешно интегрировало интерактивный Bokeh рисунок с пользовательскими обратными вызовами на JavaScript. Обратные вызовы выполняются при редактировании любого из наших элементов управления и позволяют выполнять интерактивные запросы из нескольких источников. Python извлекает тысячи записей из Easybase.io с помощью пакета easybase-python, и все эти технологии успешно работают на локальном экземпляре Flask. \n",
      "\r\n",
      "Теперь добавим маршрут для пользователей, чтобы они могли добавлять данные в нашу базу данных из приложения Flask. Визуализации Bokeh будут обновляться в режиме реального времени. Спасибо, что прочитали! Не стесняйтесь оставлять комментарии с любыми вопросами. Ниже я добавил весь исходный код app.py:\n",
      "\n",
      "\n",
      "Простыня исходного кода\n",
      "from flask import Flask, render_template\n",
      "from easybase import get\n",
      "from bokeh.models import ColumnDataSource, Select, Slider\n",
      "from bokeh.resources import INLINE\n",
      "from bokeh.embed import components\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.layouts import column, row\n",
      "from bokeh.models.callbacks import CustomJS\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/')\n",
      "def index():\n",
      "    \n",
      "    genre_list = ['All', 'Comedy', 'Sci-Fi', 'Action', 'Drama', 'War', 'Crime', 'Romance', 'Thriller', 'Music', 'Adventure', 'History', 'Fantasy', 'Documentary', 'Horror', 'Mystery', 'Family', 'Animation', 'Biography', 'Sport', 'Western', 'Short', 'Musical']\n",
      "\n",
      "    controls = {\n",
      "        \"reviews\": Slider(title=\"Min # of reviews\", value=10, start=10, end=200000, step=10),\n",
      "        \"min_year\": Slider(title=\"Start Year\", start=1970, end=2021, value=1970, step=1),\n",
      "        \"max_year\": Slider(title=\"End Year\", start=1970, end=2021, value=2021, step=1),\n",
      "        \"genre\": Select(title=\"Genre\", value=\"All\", options=genre_list)\n",
      "    }\n",
      "\n",
      "    controls_array = controls.values()\n",
      "\n",
      "    def selectedMovies():\n",
      "        res = get(\"Dt-p-a0jVTBSVQji\", 0, 2000, \"password\")\n",
      "        return res\n",
      "\n",
      "    source = ColumnDataSource()\n",
      "\n",
      "    callback = CustomJS(args=dict(source=source, controls=controls), code=\"\"\"\n",
      "        if (!window.full_data_save) {\n",
      "            window.full_data_save = JSON.parse(JSON.stringify(source.data));\n",
      "        }\n",
      "        var full_data = window.full_data_save;\n",
      "        var full_data_length = full_data.x.length;\n",
      "        var new_data = { x: [], y: [], color: [], title: [], released: [], imdbvotes: [] }\n",
      "        for (var i = 0; i < full_data_length; i++) {\n",
      "            if (full_data.imdbvotes[i] === null || full_data.released[i] === null || full_data.genre[i] === null)\n",
      "                continue;\n",
      "            if (\n",
      "                full_data.imdbvotes[i] > controls.reviews.value &&\n",
      "                Number(full_data.released[i].slice(-4)) >= controls.min_year.value &&\n",
      "                Number(full_data.released[i].slice(-4)) <= controls.max_year.value &&\n",
      "                (controls.genre.value === 'All' || full_data.genre[i].split(\",\").some(ele => ele.trim() === controls.genre.value))\n",
      "            ) {\n",
      "                Object.keys(new_data).forEach(key => new_data[key].push(full_data[key][i]));\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        source.data = new_data;\n",
      "        source.change.emit();\n",
      "    \"\"\")\n",
      "\n",
      "    fig = figure(plot_height=600, plot_width=720, tooltips=[(\"Title\", \"@title\"), (\"Released\", \"@released\")])\n",
      "    fig.circle(x=\"x\", y=\"y\", source=source, size=5, color=\"color\", line_color=None)\n",
      "    fig.xaxis.axis_label = \"IMDB Rating\"\n",
      "    fig.yaxis.axis_label = \"Rotten Tomatoes Rating\"\n",
      "\n",
      "    currMovies = selectedMovies()\n",
      "\n",
      "    source.data = dict(\n",
      "        x = [d['imdbrating'] for d in currMovies],\n",
      "        y = [d['numericrating'] for d in currMovies],\n",
      "        color = [\"#FF9900\" for d in currMovies],\n",
      "        title = [d['title'] for d in currMovies],\n",
      "        released = [d['released'] for d in currMovies],\n",
      "        imdbvotes = [d['imdbvotes'] for d in currMovies],\n",
      "        genre = [d['genre'] for d in currMovies]\n",
      "    )\n",
      "\n",
      "    for single_control in controls_array:\n",
      "        single_control.js_on_change('value', callback)\n",
      "\n",
      "    inputs_column = column(*controls_array, width=320, height=1000)\n",
      "    layout_row = row([ inputs_column, fig ])\n",
      "\n",
      "    script, div = components(layout_row)\n",
      "    return render_template(\n",
      "        'index.html',\n",
      "        plot_script=script,\n",
      "        plot_div=div,\n",
      "        js_resources=INLINE.render_js(),\n",
      "        css_resources=INLINE.render_css(),\n",
      "    )\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "\n",
      "\n",
      "Если ты построишь его — они придут. [прим. перев. Отсыл на фразу из фильма Кевина Костнера «Поле чудес»: «Если ты построишь его — он придет»].\n",
      "\r\n",
      "На тот случай если вы задумали сменить сферу или повысить свою квалификацию — промокод HABR даст вам дополнительные 10% к скидке указанной на баннере.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Курс «Python для веб-разработки»\n",
      "Обучение профессии Data Science\n",
      "Обучение профессии Data Analyst\n",
      "Онлайн-буткемп по Data Analytics\n",
      "\n",
      "\n",
      "Eще курсы\n",
      "\n",
      "Курс по JavaScript\n",
      "Профессия Java-разработчик\n",
      "SQL для анализа данных\n",
      "C++ разработчик\n",
      "Курс по аналитике данных\n",
      "Курс по DevOps\n",
      "Профессия Веб-разработчик\n",
      "Профессия iOS-разработчик с нуля\n",
      "Профессия Android-разработчик с нуля\n",
      "Курс по Machine Learning\n",
      "Курс «Математика и Machine Learning для Data Science»\n",
      "Продвинутый курс «Machine Learning Pro + Deep Learning»\n",
      "\n",
      "\n",
      "\n",
      "Рекомендуемые статьи\n",
      "\n",
      "Как стать Data Scientist без онлайн-курсов\n",
      "450 бесплатных курсов от Лиги Плюща\n",
      "Как изучать Machine Learning 5 дней в неделю 9 месяцев подряд\n",
      "Сколько зарабатывает аналитик данных: обзор зарплат и вакансий в России и за рубежом в 2020\n",
      "Machine Learning и Computer Vision в добывающей промышленности\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Моделирование данных ощутимо упрощает взаимодействие между разработчиками, аналитиками и маркетологами, как и сам процесс создания отчетов. Поэтому я перевела статью IBM Cloud Education о ценности моделирования и от себя добавила инфо о способах трансформации данных для моделирования.Моделирование данныхУзнайте, как моделирование данных использует абстракцию для представления и лучшего понимания природы данных в информационной системе предприятия.Что такое моделирование данныхМоделирование данных — это создание визуального представления о всей информационной системе либо ее части. Цель в том, чтобы проиллюстрировать типы данных, которые используются и хранятся в системе, отношения между этими типами данных, способы группировки и организации данных, их форматы и атрибуты.Модели данных строятся на основе бизнес-потребностей. Правила и требования к модели данных определяются заранее на основе обратной связи с бизнесом, поэтому их можно включить в разработку новой системы или адаптировать к существующей.Данные можно моделировать на различных уровнях абстракции. Процесс начинается со сбора бизнес-требований от заинтересованных сторон и конечных пользователей. Эти бизнес-правила затем преобразуются в структуры данных. Модель данных можно сравнить с дорожной картой, планом архитектора или любой формальной схемой, которая способствует более глубокому пониманию того, что разрабатывается.Моделирование данных использует стандартизированные схемы и формальные методы. Это обеспечивает последовательный и предсказуемый способ управления данными в организации или за ее пределами.В идеале модели данных — это живые документы, которые развиваются вместе с потребностями бизнеса. Они играют важную роль в поддержке бизнес-процессов и планировании ИТ-архитектуры и стратегии. Моделями данных можно делиться с поставщиками, партнерами и коллегами.Преимущества моделирования данныхМоделирование упрощает просмотр и понимание взаимосвязей между данными для разработчиков, архитекторов данных, бизнес-аналитиков и других заинтересованных лиц. Кроме того, моделирование данных помогает:Уменьшить количество ошибок при разработке программного обеспечения и баз данных.Унифицировать документацию на предприятии.Повысить производительность приложений и баз данных.Упростить отображение данных по всей организации.Улучшить взаимодействие между разработчиками и командами бизнес-аналитики.Упростить и ускорить процесс проектирования базы данных на концептуальном, логическом и физическом уровнях.Типы моделей данныхРазработка баз данных и информационных систем начинается с высокого уровня абстракции и с каждым шагом становится все точнее и конкретнее. В зависимости от степени абстракции модели данных можно разделить на три категории. Процесс начинается с концептуальной модели, переходит к логической модели и завершается физической моделью. Концептуальные модели данных. Также они называются моделями предметной области и описывают общую картину: что будет содержать система, как она будет организована и какие бизнес-правила будут задействованы. Концептуальные модели обычно создаются в процессе сбора исходных требований к проекту. Как правило, они включают классы сущностей (вещи, которые бизнесу важно представить в модели данных), их характеристики и ограничения, отношения между сущностями, требования к безопасности и целостности данных. Любые обозначения обычно просты.Логические модели данных уже не так абстрактны и предоставляют более подробную информацию о концепциях и взаимосвязях в рассматриваемой области. Они содержат атрибуты данных и показывают отношения между сущностями. Логические модели данных не определяют никаких технических требований к системе. Этот этап часто пропускается в agile или DevOps-практиках. Логические модели данных могут быть полезны для проектов, ориентированных на данные по своей природе. Например, для проектирования хранилища данных или разработки системы отчетности.Физические модели данных представляют схему того, как данные будут храниться в базе. По сути, это наименее абстрактные из всех моделей. Они предлагают окончательный дизайн, который может быть реализован как реляционная база данных, включающая ассоциативные таблицы, которые иллюстрируют отношения между сущностями, а также первичные и внешние ключи для связи данных.Процесс моделирования данныхМоделирование данных начинается с договоренности о том, какие символы используются для представления данных, как размещаются модели и как передаются бизнес-требования. Это формализованный рабочий процесс, включающий ряд задач, которые должны выполняться итеративно. Сам процесс обычно выглядят так:Определите сущности. На этом этапе идентифицируем объекты, события или концепции, представленные в наборе данных, который необходимо смоделировать. Каждая сущность должна быть целостной и логически отделенной от всех остальных.Определите ключевые свойства каждой сущности. Каждый тип сущности можно отличить от всех остальных, поскольку он имеет одно или несколько уникальных свойств, называемых атрибутами. Например, сущность «клиент» может обладать такими атрибутами, как имя, фамилия, номер телефона и т.д. Сущность «адрес» может включать название и номер улицы, город, страну и почтовый индекс.Определите связи между сущностями. Самый ранний черновик модели данных будет определять характер отношений, которые каждая сущность имеет с другими. В приведенном выше примере каждый клиент «живет по» адресу. Если бы эта модель была расширена за счет включения сущности «заказы», ​​каждый заказ также был бы отправлен на адрес. Эти отношения обычно документируются с помощью унифицированного языка моделирования (UML).Полностью сопоставьте атрибуты с сущностями. Это гарантирует, что модель отражает то, как бизнес будет использовать данные. Широко используются несколько формальных шаблонов (паттернов) моделирования данных. Объектно-ориентированные разработчики часто применяют шаблоны для анализа или шаблоны проектирования, в то время как заинтересованные стороны из других областей бизнеса могут обратиться к другим паттернам.Назначьте ключи по мере необходимости и определите степень нормализации. Нормализация — это метод организации моделей данных, в которых числовые идентификаторы (ключи) назначаются группам данных для установления связей между ними без повторения данных. Например, если каждому клиенту назначен ключ, этот ключ можно связать как с его адресом, так и с историей заказов, без необходимости повторять эту информацию в таблице с именами клиентов. Нормализация помогает уменьшить объем дискового пространства, необходимого для базы данных, но может сказываться на производительности запросов.Завершите и проверьте модель данных. Моделирование данных — это итеративный процесс, который следует повторять и совершенствовать под потребности бизнеса.Типы моделирования данныхМоделирование данных развивалось вместе с системами управления базами данных (СУБД), при этом типы моделей усложнялись по мере роста потребностей предприятий в хранении данных. Иерархические модели данных представляют отношения «один ко многим» в древовидном формате. В модели этого типа каждая запись имеет единственный корень или родительский элемент, который сопоставляется с одной или несколькими дочерними таблицами. Эта модель была реализована в IBM Information Management System (IMS) ​​в 1966 году и быстро нашла широкое применение, особенно в банковской сфере. Хотя этот подход менее эффективен, чем недавно разработанные модели баз данных, он все еще используется в системах расширяемого языка разметки (XML) и географических информационных системах (ГИС).Реляционные модели данных были предложены исследователем IBM Э. Ф. Коддом в 1970 году. Они до сих пор встречаются во многих реляционных базах данных, обычно используемых в корпоративных вычислениях. Реляционное моделирование не требует детального понимания физических свойств используемого хранилища данных. В нем сегменты данных объединяются с помощью таблиц, что упрощает базу данных.Реляционные базы данных часто используют язык структурированных запросов (SQL) для управления данными. Эти базы подходят для поддержания целостности данных и минимизации избыточности. Они часто используются в кассовых системах, а также для других типов обработки транзакций.В ER-моделях данных используют диаграммы для представления взаимосвязей между сущностями в базе данных. ER-модель представляет собой формальную конструкцию, которая не предписывает никаких графических средств её визуализации. В качестве стандартной графической нотации, с помощью которой можно визуализировать ER-модель, была предложена диаграмма «сущность-связь» (Entity-Relationship diagram). Однако для визуализации ER-моделей могут использоваться и другие графические нотации, либо визуализация может вообще не применяться (например, только текстовое описание).Объектно-ориентированные модели данных получили распространение как объектно-ориентированное программирование и стали популярными в середине 1990-х годов. Вовлеченные «объекты» — это абстракции сущностей реального мира. Объекты сгруппированы в иерархии классов и имеют связанные черты. Объектно-ориентированные базы данных могут включать таблицы, но могут также поддерживать более сложные связи. Этот подход часто используется в мультимедийных и гипертекстовых базах данных.Размерные модели данных разработал Ральф Кимбалл для быстрого поиска данных в хранилище. Реляционные и ER-модели делают упор на эффективное хранение и уменьшают избыточность данных, а размерные модели упорядочивает данные таким образом, чтобы легче было извлекать информацию и создавать отчеты. Это моделирование обычно используется в системах OLAP.Две популярные размерные модели данных — это схемы «звезда» и «снежинка». В схеме «звезда» данные организованы в факты (измеримые элементы) и измерения (справочная информация), где каждый факт окружен связанными с ним измерениями в виде звездочки. Схема «снежинка» напоминает схему «звезда», но включает дополнительные слои связанных измерений, что усложняет схему ветвления.Инструменты для моделирования данныхСегодня широко используются многочисленные коммерческие и CASE-решения с открытым исходным кодом, в том числе различные инструменты моделирования данных, построения диаграмм и визуализации. Вот несколько примеров:erwin Data Modeler — это инструмент моделирования данных, основанный на языке IDEF1X, который теперь поддерживает и другие нотации, включая нотацию для размерного моделирования.Enterprise Architect — это инструмент визуального моделирования и проектирования, который поддерживает моделирование корпоративных информационных систем и архитектур, программных приложений и баз данных. Он основан на объектно-ориентированных языках и стандартах.ER/Studio — это программа для проектирования баз данных, совместимая с некоторыми из самых популярных СУБД. Она поддерживает как реляционное, так и размерное моделирование данных.Бесплатные инструменты моделирования данных включают решения с открытым исходным кодом, такие как Open ModelSphere.Для того, чтобы преобразовать данные в структуру, которая соответствует требованиям модели, можно использовать встроенный механизм регулярных запросов, которые выполняются в Google BigQuery, Scheduled Queries и AppScript. Их легко можно освоить, потому что это привычный SQL, но проводить отладку в Scheduled Queries практически нереально. Особенно, если это какой-то сложный запрос или каскад запросов. Есть специализированные инструменты для управления SQL-запросами, например, dbt и Dataform.dbt (data build tool) — это фреймворк с открытым исходным кодом для выполнения, тестирования и документирования SQL-запросов, который позволяет привнести элемент программной инженерии в процесс анализа данных. Он помогает оптимизировать работу с SQL-запросами: использовать макросы и шаблоны JINJA, чтобы не повторять в сотый раз одни и те же фрагменты кода. Главная проблема, которую решают специализированные инструменты — это уменьшение времени, необходимого на поддержку и обновление. Это достигается за счет удобства отладки.     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Пробуем алгоритм UMAP урожая 2018 — пакет Python для впечатляющих визуализаций и кластеризации данных. Статья напомнит об этой прекрасной альтернативе t-SNE или PCA и поможет с визуализацией на флагманском курсе Data Science.ЗнакомствоНедавно мне посчастливилось познакомиться с UMAP — пакетом Python для потрясающе красивых визуализаций и кластеризации данных высокой размерности. Это было то, что нужно, чтобы вспомнить, почему два года назад я начал изучать науку о данных. Сегодня мы узнаем, как анализировать многомерные наборы данных, проецируя их в 2D с помощью пакета Uniform Manifold Approximation & Projection (UMAP). Начнём с рекомендаций:Запускайте UMAP, имея не меньше 16 Гб RAM.Сократите потребление памяти, приведя каждый столбец к наименьшему подтипу с помощью NumPy.Не забудьте преобразовать/масштабировать числовые признаки. Примеры визуализаций:Изображение из документацииИзображение из документацииИзображение из документацииЧто такое UMAP?UMAP — это алгоритм снижения размерности и мощный инструмент анализа данных. Он похож на PCA (Principal Component Analysis) в смысле скорости и напоминает t-SNE в смысле уменьшения размерности, сохраняя как можно больше информации набора данных. PCA и t-SNE в сравнении с UMAP имеют два существенных недостатка:PCA очень быстрый за счёт потери маленьких деталей в данных.t-SNE крайне медленный, хотя сохраняет базовую структуру данных.Попробуем UMAP на вкус, сразу начнём со сложного набора данных:tps = pd.read_csv(\"data/train.csv\").drop(\"id\", axis=1)\n",
      "tps.head().sample(10, axis=1)Синтетические данные, созданные для сентябрьского конкурса Kaggle TPS.>>> tps.shape\n",
      "(957919, 119)Набор данных Kaggle TPS за сентябрь содержит около 1 000 000 строк и 120 признаков с бинарной целью. Все признаки численные, и мы довольно беспомощны в смысле надлежащего разведывательного анализа данных. Печать сводной статистики и построение гистограмм по каждому признаку — вот и всё, что мы можем. Но посмотрим, что может UMAP. Перед его применением сделаем выборку из набора данных, чтобы избежать затрудняющих просмотр данных перекрытий, и заполним недостающие значения. В наборе мы прогнозируем, заявит ли клиент о своей страховке.from sklearn.impute import SimpleImputer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "# Sample the data - 100k\n",
      "tps_sample = tps.sample(100000)\n",
      "X, y = tps_sample.drop(\"claim\", axis=1), tps_sample[[\"claim\"]].values.flatten()\n",
      "\n",
      "# Preprocess\n",
      "pipe = make_pipeline(SimpleImputer(strategy=\"mean\"))\n",
      "X = pipe.fit_transform(X.copy())После установки (pip install umap-learn) и импортирования UMAP инициализируем алгоритм многообразия и обучим на X, y при помощи знакомого паттерна Sklearn:%%time\n",
      "\n",
      "import umap  # pip install umap-learn\n",
      "\n",
      "manifold = umap.UMAP().fit(X, y)\n",
      "X_reduced = manifold.transform(X)\n",
      "\n",
      "Wall time: 1min 14s>>> X_reduced.shape\n",
      "(100000, 2)По умолчанию UMAP проецирует данные на плоскость. Создадим диаграмму рассеяния, окрашенную в цвета целевых классов:>>> plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=0.5);На график интересно смотреть, но он не показывает никаких чётких закономерностей: перед установкой UMAP мы не масштабировали признаки. Чтобы группировать похожие точки данных, алгоритм использует метрику расстояния, а значит, признаки численно больше других смещают такие расчёты. Выберем квантильное преобразование для масштабирования признаков на основе их квантилей и медиан. Такое масштабирование подходит для этого набора: он содержит много признаков с перекосами в данных и бимодальных признаков:%%time\n",
      "\n",
      "from sklearn.preprocessing import QuantileTransformer\n",
      "\n",
      "# Preprocess again\n",
      "pipe = make_pipeline(SimpleImputer(strategy=\"mean\"), QuantileTransformer())\n",
      "X = pipe.fit_transform(X.copy())\n",
      "\n",
      "# Fit UMAP to processed data\n",
      "manifold = umap.UMAP().fit(X, y)\n",
      "X_reduced_2 = manifold.transform(X)\n",
      "\n",
      "Wall time: 34.4 s# Plot the results\n",
      "plt.scatter(X_reduced_2[:, 0], X_reduced_2[:, 1], c=y, s=0.5);UMAP прекрасно передал скрытое различие между целевыми классами. Точки вокруг жёлтого пятна — это выбросы. Набор данных не такой уж и сложный, но график ни в коей мере не похож на то, что я вам показал. Чтобы увидеть структурные закономерности внутри каждого кластера, придётся его перерисовать. Воспользуемся стандартным пакетом визуализации UMAP с большим количеством дополнительных функций. Понадобится и набор данных получше.Уточняем визуализациюПроанализируем набор данных конкурса Kaggle TPS за май, где на основе около 75 числовых признаков классифицировано ~200 000 объявлений электронной коммерции:tps_june = pd.read_csv(\"data/train_june.csv\").sample(15000)\n",
      "\n",
      "X, y = tps_june.drop(\"target\", axis=1), tps_june[[\"target\"]].values.flatten()\n",
      "\n",
      "X.head().sample(10, axis=1)Синтетические данные Kaggle TPS за сентябрь>>> X.shape\n",
      "(200000, 76)\n",
      "\n",
      ">>> np.unique(y)\n",
      "array(['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6',\n",
      "       'Class_7', 'Class_8', 'Class_9'], dtype=object)Снова масштабируем все признаки, теперь с помощью прямого логарифмического преобразования, а также установим UMAP:import umap\n",
      "from sklearn.preprocessing import PowerTransformer\n",
      "\n",
      "# Scale\n",
      "pipe = make_pipeline(PowerTransformer())\n",
      "X = pipe.fit_transform(X.copy())\n",
      "\n",
      "# Encode the target to numeric\n",
      "y_encoded = pd.factorize(y)[0]\n",
      "\n",
      "%%time\n",
      "manifold = umap.UMAP().fit(X, y_encoded)\n",
      "\n",
      "Wall time: 1min 38sУстановим umap.plot:pip install umap-learn[plot]После обучения импортируем этот модуль и построим облако точек:import umap.plot  # pip install umap-learn[plot]\n",
      "\n",
      "umap.plot.points(manifold, labels=y, theme=\"fire\");Разве не похоже на туманность из космоса?Вот что мы видим:Класс 8 доминирует и группируется вокруг центра. Вокруг класса 8 видим полукруг смешанных точек.Чётко отличается от остальных класс 6.Одиночные точки данных можно классифицировать как выбросы.Замечание по визуализации выше: мы просто передаём обученное многообразие (не преобразованные данные!) в points и указываем метки цветового кодирования.Чтобы проводить диагностику и лучше понимать структуру многообразия, вы можете построить графы связности в многообразии функцией umap.plot.connectivity. Это потребует много времени, вычислений и памяти.Графы связности в многообразииДокументация с подробностями по визуализации.Важнейшие параметры UMAPUMAP имеет множество параметров, которые могут существенно повлиять на многообразие, а значит, на визуальные эффекты. Вот самые важные:n_components # 2 по умолчанию\n",
      "n_neighbors  # 15 по умолчанию\n",
      "min_dist     # 0,1 по умолчанию\n",
      "metric       # euclidean по умолчанию Как вы уже догадались, n_components управляет количеством измерений проекции. В наборах с более чем сотней признаков 2D может не сохранить основную топологическую структуру данных, поэтому рекомендую попробовать значения от 2 до 20 с шагом 5 и оценить базовые модели, чтобы увидеть изменения точности.n_neighbors контролирует область локальной окрестности, которую UMAP при построении многообразия рассматривает для каждого образца данных:Меньшие значения сужают фокус до локальной структуры, принимая во внимание особенности и мелкие закономерности, но потенциально теряя общую картину.Большие значения n_neighbors дают бо́льшую гибкость и позволяют UMAP сосредоточиться на более широком «обзоре» данных в соответствующем измерении. Платить придётся потерей тонких деталей структуры.Обучение UMAP с различными n_neighbors на выборке данных TPS за июньБуквальное расстояние между точками данных — min_dist:Значения меньше 0,1 приводят к более неровным вкраплениям, позволяя быстрее увидеть отдельные кластеры. Близкие к единице значения дают точкам больше свободы, позволяя увидеть более широкую топологическую структуру.Обучение многообразия UMAP с различными min_dist на выборке данных TPS за июньПараметр metric — это название формулы вычисления расстояния между точками. Выбор широкий и включает manhattan, minkowski и chebyshev.Лучшие практики работы с UMAPПервое, на что следует обратить внимание, — это потребление памяти. Особенно много RAM UMAP потребляет в обучении и создании графов, например графов связности. Даже набор данных на 200 000 значений при построении диаграмм потреблял ~18 Гб. Исправлением в документации предлагается установить low_memory в True.QuantileTransformer рекомендую для сумасшедших распределений — бимодальных, тримодальных и т. д.PowerTransformer лучше всего подходит для перекошенных признаков. Какой бы преобразователь вы ни выбрали, цель всегда в том, чтобы привести признаки к наиболее нормальному распределению. А разобраться с визуализацией ещё лучше и научиться извлекать из неё пользу вы сможете на наших курсах:Курс «Machine Learning и Deep Learning» (6 месяцев)Профессия Data Scientist (24 месяца)Также вы можете перейти на страницы курсов из каталога, чтобы увидеть, как мы готовим к началу карьеры в других направлениях.Профессии и курсыData Science и Machine LearningПрофессия Data ScientistПрофессия Data AnalystКурс «Математика для Data Science»Курс «Математика и Machine Learning для Data Science»Курс по Data EngineeringКурс «Machine Learning и Deep Learning»Курс по Machine LearningPython, веб-разработкаПрофессия Fullstack-разработчик на PythonКурс «Python для веб-разработки»Профессия Frontend-разработчикПрофессия Веб-разработчикМобильная разработкаПрофессия iOS-разработчикПрофессия Android-разработчикJava и C#Профессия Java-разработчикПрофессия QA-инженер на JAVAПрофессия C#-разработчикПрофессия Разработчик игр на UnityОт основ — в глубинуКурс «Алгоритмы и структуры данных»Профессия C++ разработчикПрофессия Этичный хакерА также:Курс по DevOps    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Последние пару месяцев компания Facebook, переименованная в конце октября в Meta, переживает не лучшие времена — зарубежные СМИ пестрят новостями с обзорами внутренних расследований, раскрывающих печальные факты из деятельности компании, государственные ведомства США заводят дела против Meta, инвесторы подают в суд за махинации с отчётами, стоимость акций стремительно снижается. Всё это произошло после того, как бывшая сотрудница Meta Френсис Хауген перед увольнением слила в СМИ и Конгрессу США внутренние документы компании. Что такого страшного в этих документах, кто такая Хауген и что на данный момент известно — рассказываем в материале.Хотела как лучше, а получилось…Скандал со слитыми документами Meta появился благодаря бывшей сотруднице компании — менеджеру по продукту Facebook Френсис Хауген. Она устроилась на работу в Facebook в 2019 году, сразу попав в команду разработки стратегии защиты от вмешательства в выборы США. В мае этого года Френсис уволилась, прихватив с собой результаты работы внутренних исследовательских групп. Они были размещены во внутренней рабочей сети Facebook Workplace. Все эти документы сразу были переданы в СМИ и Конгресс США.Сначала Френсис передавала данные анонимно, боясь ответственности за разглашение конфиденциальной информации. Открыто заявить о себе женщина решилась только в начале октября этого года в эфире программы 60 minutes. Незадолго до этого она попросила у американского правительства защиту, сославшись на Закон о защите информаторов в США. Закон освобождает от ответственности за разглашение информации о внутренних рабочих процессах в случае, если информаторы сообщают о незаконной, неэтичной и любой противоправной деятельности компании.В интервью Френсис рассказала, что Meta намеренно вкладывает недостаточно средств на борьбу с фильтрацией негативного и дезинформирующего контента на площадке. Текущая команда сотрудников, работающая по этому направлению, физически не способна справиться с огромным объёмом работ. Людям приходится часто задерживаться и проваливать очевидно невыполнимые планы, выставляемые руководством. По словам Френсис, она поняла это с первых недель работы в компании, но до последнего старалась как-то исправить ситуацию.Френсис Хауген на интервью в программе 60 minutesФренсис решила уволиться после нескольких публичных отчётов Facebook, в которых она обнаружила прямое несоответствие между фактическим и представляемым общественности положением дел в компании. Сотрудники регулярно отправляли отчёты о загруженности и несовершенстве системы, количестве негативного контента и других важных вещах, которые либо не включались в доклады, либо значительно искажались в угоду приемлемой статистике. Френсис рассказала, что была очень расстроена и обеспокоена этим фактом. Особенно с учётом того, что компания не показывала особую озабоченность результатами внутренних исследований.Заявление об увольнении Хауген написала в апреле. Ещё месяц понадобился на то, чтобы передать дела оставшимся сотрудникам. За этот месяц она тщательно просматривала результаты работы других команд, опубликованных во внутренней сети Facebook Workplace. До инцидента Meta предоставляла сотрудникам свободный доступ к данным других проектов, называя это открытой и доверительной системой работы. Предполагалось, что работники будут вступать в группы других проектов с предложениями по поводу решения ситуаций и обсуждения проблем в целом. После скандала с Хауген эту систему отменили, но было поздно — за месяц экс-сотрудница слила достаточно данных, чтобы вокруг Meta разразился настоящий скандал. Френсис утверждает, что не хочет топить Meta. Её цель — заставить компанию предпринимать реальные действия, а не создавать видимость работы, предоставляя публике фальшивые отчёты. При этом она активно выступает в Конгрессе, утверждая, что Meta намеренно игнорирует внутренние исследования и не борется с негативным контентом. По её словам, негатив способствует росту вовлечённости аудитории, поэтому его удаление с ресурса невыгодно компании.Самое важное — детиПервые статьи со ссылкой на некие слитые документы Meta появились в американских СМИ лишь через несколько месяцев после увольнения Хауген. Впервые о них упомянуло издание Wall Street Journal (WSJ) в статье про внутреннюю систему VIP-пользователей на Facebook. Опубликованная 14 сентября новость рассказывала о тайной системе XCheck, благодаря которой пользователи, внесённые в VIP-лист, могут избегать общие ограничения соцсети. Например, «випам» дозволено публиковать оскорбления, призывы к насилию и другие материалы, за которые обычно банят аккаунты или даже привлекают к ответственности в суде.В этот же день WSJ опубликовало ещё одну статью, в котором рассматривается вред, оказываемый платформами Meta на девочек-подростков. Например, Instagram негативно влияет на самооценку детей, заставляя их чувствовать себя хуже и испытывать тревогу, вплоть до появления мыслей о суициде. Данные о вреде основаны на собственных исследованиях компании, слитых Хауген. В этой новости личность информатора также не раскрывается. На тот момент Meta не отрицала результаты исследования, признавая, что использование соцсети может наносить вред лишь «некоторой части аудитории». Здесь стоит отметить, что около 40% аудитории Instagram не достигли возраста 22 лет. Из них ежедневно соцсеть посещают не менее 22 млн человек. Говоря о «некоторой части аудитории», представители Meta существенно преуменьшают негативное влияние Instagram на детей, о чём неоднократно ещё до ситуации с Хауген заявляли американские политики, журналисты и аналитики. Вопрос о влиянии смартфонов и соцсетей на детей в последние годы в США стоит особенно остро. Вероятно, поэтому новость о скрываемом вреде Instagram на подростков вызвала такой общественный резонанс. Компании даже пришлось публиковать ответ на критику WSJ, заявив, что материалы исследования неверно интерпретированы, а сама работа не может дать полноценный взгляд на ситуацию ввиду небольшой выборки. Сам глава Instagram сравнил сидение в соцсети с ездой на автомобилях, заверив, что жертвы неизбежны. Это только усугубило конфликт и привлекло дополнительное внимание общественности к сливу документов.Глава Instagram Адам МоссериНепогрешимая Meta и святой ЦукербергЕщё до слива документов СМИ начали замечать изменившееся поведение Цукерберга. В частности, издание New York Times написало, что с начала 2021 года компания Meta и её глава Марк Цукерберг усиленно работают над восстановлением репутации. Цукерберг ведёт себя как воплощение добропорядочности, рассказывая на презентациях о позитивных достижениях компании и будущем создании метавселенной, а не действиях Meta в ответ на щекотливые новости. В новостной ленте Facebook чаще начали встречаться позитивные посты о работе в Meta и другая информация, обеляющая компанию. Кроме того, с сентября 2020 года Цукерберг усиленно продаёт принадлежащие ему акции Meta. В середине июля его доля в компании сократилась до 14%, что вдвое меньше, чем на момент IPO Facebook в 2012 году. 90% от вырученных средств он направил в благотворительный фонд, основанный им и его женой Присциллой Чан.Слитые документы свели на нет все попытки Meta и Цукерберга обелить свою репутацию. Интервью Хауген в программе 60 minutes ещё сильнее усугубило эту проблему. Излишнее внимание привлёк и произошедший практически сразу после публикации интервью глобальный сбой в работе всех платформ Meta. Он продолжался более пяти часов, задев львиную долю населения планеты.Через два дня после интервью с Хауген Цукерберг опубликовал пост, в котором опроверг все представленные обвинения. Он назвал выводы Хауген и СМИ «крайне нелогичными», поскольку ни пользователи, ни заказчики рекламы не заинтересованы в поощрении негативного контента. Кроме того, по его словам, обвинения противоречат достижениям компании в области контроля за «негативным» контентом. Марк заверил, что компания вкладывает все силы в разработку безопасной среды как для детей, так и для взрослых. По его словам, этот слив обесценивает труды сотрудников, поскольку «на самых базовых уровнях» создаёт неверное и тяжело опровержимое представление о политике Meta.Основатель Meta Марк ЦукербергВы всё неправильно поняли, это другое!Цукерберг в своём посте не рассказал ничего нового относительно позиции Meta. Ранее представители компании в ответе на запросы издания отвечали абсолютно теми же тезисами, заверяя общественность, что СМИ искусственно нагнетают ситуацию. При этом на фоне скандала компания приостановила выпуск и разработку новых продуктов, чтобы убедиться, что они не оказывают негативного влияния на детей и не испортят репутацию компании при их обнародовании.Слова Цукерберга и других представителей Meta подхватила и вице-президент по политике в области контента Facebook Моника Бикерт. Она обвинила Хауген в краже документации и неверной интерпретации исследований. По словам Бикерт, деятельность Френсис вообще никак не была связана с добытыми материалами, поэтому она неспособна понять их истинный смысл и уж тем более передавать искажённые выводы общественности. Американские СМИ заподозрили, что речь Бикерт была заранее отрепетирована. Непосредственно имени Хауген в речи не звучит, будто вице-президент пытается исключить её из повестки и вынести некое предупреждение следующему разоблачителю. Кроме того, не без участия руководства внутри компании распространился слух, что утечка целиком и полностью спланирована конкурентами компании. Это меняет отношение сотрудников как к слитым документам, так и к самой Хауген.Вице-президент по политике в области контента Facebook Моника БикертВместе с постоянными заявлениями о том, что СМИ и Хауген всё неверно поняли, Facebook ввела несколько функций для модерации контента. В частности, через несколько дней после выступления Хауген Meta ввела функцию по информированию подростков в Instagram о необходимости отвлечься от соцсети. Также компания обновила правила площадок, усилив борьбу с запугиванием, домогательствами и преследованием пользователей, и объявила о планах существенно уменьшить количество политических постов в ленте. В дополнение компания опубликовала отчёт, в котором заверила, что за последние три квартала на 50% сократила долю негативного контента на Facebook. По её словам, сейчас в соцсети на негатив приходится всего около 0,05% материалов.Вскоре после этого подтвердить слова Хауген в суде вызвалась ещё одна бывшая сотрудница Meta Софи Джан. Она работала специалистом по обработке и анализу данных, но была уволена за «низкую производительность». Чжан передала правоохранительным органам ещё одну партию документов, свидетельствующих о многочисленных нарушениях компании. Что ещё известно В слитых документах содержатся не только данные об уже упомянутом исследовании Instagram и привилегий для VIP-профилей. Например, после анализа документации стало известно, что ИИ компании не справляется с модерацией контента. Несмотря на все заверения Цукерберга и Meta об исключительных возможностях нейросети, она не способна своевременно и эффективно находить и удалять негативный и дезинформирующий контент. Например, ИИ не умеет выявлять расистские высказывания, видео со стрельбой в школах, путает петушиные бои с автокатастрофой и понижает приоритет в выдаче на материалах, со статусом которых не может определиться (вреден или нет). Кроме того, нейросеть не только не справляется с дезинформацией о коронавирусе и вакцинах, но и не имеет представления об объёмах дезинформации в соцсетях. Компания предоставляет отчёты о доле фейковых постов на платформах, в то время как на самом деле точно не знает, сколько их на самом деле.Также СМИ представили общественности и более серьёзные нарушения. Например, сотрудники Meta были осведомлены о торговле людьми на Ближнем Востоке и регулярно находили множество постов, в которых беженки из стран Азии и Африки рассказывали, что оказались в рабстве. На эти сообщения никак не реагировали и не передавали соответствующим службам, ведущим расследование. Также компания не вмешивалась в публикацию материалов от издания Breitbart. Оно освещало протесты Black Lives Matter в крайне негативном ключе, усугубляя и без того напряжённую ситуацию. Что ещё интересно — нейросеть Meta при ранжировании постов считала эмодзи за пять обычных лайков. Это также стало известно только после слива документов. Таким образом компания (осознано или нет — достоверно неизвестно) вместе с обычным планомерно продвигает негативный контент, вызывающий ярое недовольство у пользователей. Чем чаще люди ставили негативный эмодзи, тем сильнее система продвигала такие материалы. Также стало известно, что Meta планировала таргетировать рекламу на детей от шести лет, что противоречит текущему законодательству США. В слитых материалах представлен готовый план по реализации таргетинга. Вместе с тем неизвестно, как компания планировала обходить законодательство.Случайность или хорошо спланированная операция?Американские СМИ регулярно отмечают, что слив Хауген получил известность во многом благодаря грамотно спланированной пиар-акции. После выступления в эфире 60 minutes к Хауген в очень короткие сроки присоединились организации по юридической поддержке информаторов и те, кто когда-либо имел претензии к Meta. Одним из таких противников стал фонд Omidyar Network. Его основатель и глава eBay Пьер Омидьяр через фонд выделил дополнительные средства Хауген, чтобы она могла оплачивать судебные расходы и адвокатов. Вместе с тем некоторые юристы согласились предоставлять услуги Хауген бесплатно и даже организовали сбор средств на финансирование процесса.Omidyar Network регулярно участвует в агитации против крупных технологических компаний, в том числе против Meta. Он пообещал долгосрочные инвестиции в дело Хауген, чтобы поддержать общественный резонанс и разговор о проблемах, поднятых вследствие утечки. Хауген поддерживают организации Whistleblower Aid и Center for Humane Technology, регулярно получающие финансирование от Omidyar Network. Главный представитель Хауген по связям с общественностью в США и бывший пресс-секретарь Обамы Билл Бёртон как раз руководит Center for Humane Technology.Неизвестно, было ли привлечение данных организаций и лиц спланировано и оговорено ещё до выступления Хауген. Тем не менее, постоянная публикация материалов в СМИ и выступления экс-сотрудницы в Конгрессе привели не только к репутационным, но и значительным финансовым потерям Meta. В частности, Сенат США потребовал прекратить внедрение криптокошелька Novi и отказаться от разработки цифровой валюты Diem. По словам сенаторов, слитые документы показали, что Meta нельзя доверять такое серьёзное направление, поскольку компания не способна обеспечить достаточный уровень надёжности и безопасности. Вместе с тем акционеры Meta подали в суд коллективный иск за намеренное искажение информации о текущем положении дел в компании. На фоне скандала за два месяца стоимость ценных бумаг упала с $377 до $312,2 за штуку (на текущий момент выросли до $332,54 на фоне ребрендинга). Если бы акционеры знали о внутренних исследованиях и репутационных рисках, они не стали бы покупать акции. Вместе с иском Федеральная торговая комиссия США начала своё расследование на предмет мошенничества.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Признаюсь честно, у меня как у программиста, хоть и не настоящего, есть недоверие к «no-code» решениям. То есть тем, которые не требуют программирования, где всё можно делать через drag-and-drop и клики мышкой. Разве можно сделать на них что-то серьёзное? Как говорила ещё моя прабабушка: чтобы у кого-то появился «no-code», кто-то другой должен написать «a lot of code», ибо булки не растут на деревьях. Однако, с некоторого момента начинаешь ценить своё время и фокусироваться больше на процессе поиска ответов на вопросы, а не на подготовке инструментария для этого. Поэтому я полюбил готовые и простые способы работы с данными, в частности, те, которые позволяют выгружать и подготавливать данные для последующего анализа (та самая аббревиатура «ETL»). Причём, на процесс подготовки данных, включая загрузку, преобразование и визуализацию данных, хочется тратить минимум времени.Сейчас нет дефицита в ETL-инструментах. Их точно десятки, а если сосчитать все мелкие стартапы, то может и сотня наберётся. Одни импортируют данные из SQL баз, другие умеют подключаться к разным сервисам, наподобие Google Analytics или 1С, третьи — работают со статическими файлами. Есть и универсальные, которые «всеядны», причём часть из них умеют всё делать без программирования, то есть «no code». Проверенных сейчас найдётся, наверное, с дюжину. А точнее — «дюжина + 1», потому что я написал ещё один (да, в изобретении велосипедов мне нет равных).TABLUM.IO — это многофункциональный сервис для загрузки и анализа данных, со встроенной визуализацией и автоматизацией. Сервис умеет работать и с базами данных, и со статическими файлами. Но в этой заметке я покажу, как с помощью него можно парсить данные из сервисов, работающих по API, и приведу несколько примеров загрузки и парсинга данных по URL. А в конце расскажу, какие фокусы можно делать с импортированными данными: в частности, как по ним строить графики и отправлять результат по расписанию в Телеграм, Slack или на email. Причём, это будет почти «no-code», т.к. из программирования нам потребуется только щепотка SQL команд для фильтрации лишних данных, а остальное выполняется кликами мышкой, drag-and-drop'ом файлов и магией кода, который Дедушка Мороз напрограммировал вам на Новый Год (старался с самого лета). Чтобы было интереснее, попробуем реализовать небольшую, но вполне конкретную задачку. На базе TABLUM.IO соберём информер с ежедневной отправкой графиков и таблиц в Telegram, Slack и на email. Например, это будет информер курсов обмена валют. Хотя по аналогии можно настроить мониторинг продуктовых метрик, состояния сервера, динамики оплат из биллинга, статистики коммитов из гита и многого другого. Было бы откуда загружать данные. Кстати, если вам удобнее смотреть видео вместо чтения статьи, есть и такая опция. Хотя в статье примеров будет больше. Итак, мы собираем свой информер курса обмена валют, и начинаем с выбора источника данных. Для нашей задачи подойдёт фид с floatrates.com. Он возвращает массив обменных курсов для указанной валюты (в нашем случае — рубля) в формате JSON. Фрагмент возвращаемой структуры приведён ниже:Для загрузки данных по указанному URL нужно выбрать коннектор URL Downloader и вставить адрес источника данных в форму: TABLUM.IO загрузит данные, распарсит их и превратит в SQL таблицу (что немного похоже на магию). Сущность, в которую запишутся данные, будем называть «View».Небольшое лирическое отступление: каждый столбец при загрузке неструктурированных и нетипизированных данных автоматически получает свой тип. В интерфейсе сервиса тип данных кодируется цветным буллетом рядом с заголовком столбца: зелёный — числовой тип, оранжевый — дата и время, фиолетовый — финансовый числовой. В зависимости от типов данных в столбце доступны разные функции сортировки, фильтрации, генерации запросов и т.п. Но вернёмся к нашей исходной задаче создания информера. Текущая таблица слишком объёмная для отправки в мессенджер, поэтому было бы неплохо сократить число валют до нескольких популярных (пусть это будут USD, EUR, GBP), а также убрать лишние столбцы. Для этого нужно сгенерировать новый «View» из текущего. Это можно сделать через SQL-запрос ко «View» со списком курсов валют, выбрав в качестве источника данных «Saved Datasets». Подзапросы возможны благодаря тому, что все данные, которые загружаются в TABLUM.IO, превращаются в SQL таблицу. И с ними можно работать, как с обычной БД (синтаксис SQLite3). Для адресации к конкретной таблице нужно указать имя «View» в формате <Dataset_X>.<View_Y>. Например, если текущий датасет имеет идентификатор DS_1247, а вью — V_1001, то общий запрос к данным будет выглядеть так:SELECT * FROM DS_1247.V_1001Следующей SQL-командой можно получить значения курсов валют для нашего случая:SELECT\n",
      "\t  `date` as Date,\n",
      "\t  `alphacode` as Currency,\n",
      "\t  `inverserate` as RUB\n",
      "\tFROM\n",
      "\t  DS_1247.V_1001\n",
      "\tWHERE `alphacode` in (’USD’, ‘EUR’, ‘GBP’)Теперь таблица компактная, в ней только три столбца и три строки. Можно настраивать периодическую отправку данных в мессенджеры или на email:Выбираем обновление раз в сутки и в целях демонстрации все доступные каналы (Slack, Telegram и Email). Чтобы убедиться, что всё работает не дожидаясь следующего дня, можно нажать на «Run Test».В Телеграм-бот и Slack придут примерно такие сообщения:А на почту:Как вы наверняка заметили, данные из табличного вида сконвертировались в текстовое представление. Но данные можно отправлять и в виде картинки с диаграммой или графиком. Для примера №2 построим график, используя другой набор данных — динамику курса доллара за последние 30 дней с сайта ЦБ, и настроим его отправку в Telegram и Slack. Здесь данные также загружаются через URL Downloader, но в формате XML. В данном запросе появляется дополнительный параметр root=”Record”, который определяет корневой элемент XML данных, с которого URL Downloader начнёт парсить массив. Итак, данные загружены, сконвертированы сервисом в таблицу, остаётся построить график. Это три клика:Результат:Теперь выбираем мессенджер для отправки результата:Вот в таком виде будет приходить ежедневный апдейт в Telegram:URL Downloader — это универсальный загрузчик данных в форматах XML, JSON, CSV, TSV. Для многомерных массивов или древовидных структур ему можно задавать корневой элемент через параметр root в форматеroot=”node1>node2>node3”А также указывать дополнительные HTTP заголовки. Например, мой запрос к сервису JIRA выглядит следующим образом:(в примере ключ от Basic авторизации я, конечно, немножко поменял)С помощью TABLUM.IO в SQL таблицу можно перевести результат любого API вызова, при условии, что он вернёт данные в форматах JSON, XML или CSV. И, в общем случае, можно загружать данные по URL и с желаемой периодичностью. Вот ещё несколько примеров:Можно распарсить что-нибудь с GIT, построить аналитику по коммитам и получать апдейты в мессенджер:Аналогичным образом парсятся RSS/Atom-фиды или погода:Учитывая то, что современные сервисы работают по HTTP POST, авторизуются по токену и возвращают данные в JSON или XML форматах (всё это сервис умеет делать), TABLUM.IO становится своеобразным «карманным» Zapier, на котором можно реализовать даже несложные пайплайны данных (через API). Буду рад, если и вы также найдёте для себя сценарии работы с сервисом. Кстати, если у вас возникли идеи, как ещё можно использовать возможности TABLUM.IO, напишите в комментариях.А если вам интересно следить за судьбой проекта, подписывайтесь на Telegram-канал.    \n",
      " Использование интерфейса визуализации данных, также известное как создание дашбордов, является неотъемлемой частью набора навыков аналитиков данных. Дашборды и приложения для работы с данными используются сейчас повсеместно: от представления результатов анализа с помощью ряда визуальных образов до демонстрации ваших приложений машинного обучения.Если речь идет о данных, значит, это касается и Python. В частности, мы говорим о библиотеке Dash, которая построена на базе одной из самых популярных библиотек для построения графиков - Plotly.Dash позволяет легко создавать и делиться результатами анализа данных с помощью интерактивных дашбордов, используя только код Python. Нет необходимости изучать HTML, CSS или сложные JavaScript-фреймворки, такие как React.js.В этом руководстве вы получите представление о том, на что способен Dash, и как интегрировать его в свой рабочий процесс.Установка Dash и PlotlyВы можете установить Dash с помощью pip. Также необходимо инсталлировать библиотеку pandas для работы с наборами данных:pip install dash pandasПриведенная выше команда также выполняет установку plotly. Plotly известен своими интерактивными графиками, и Plotly и Dash созданы Plotly Software Foundation, поэтому библиотеки довольно хорошо работают вместе.Требования по использованию DashУ такого мощного фреймворка, как Dash, есть несколько требований. Во-первых, вы должны знать Plotly Python, поскольку Dash может отображать только интерактивные графики Plotly.Далее, вам необходимо базовое понимание HTML и CSS. Dash похож на React, но на языке Python. Это шаблонный фреймворк, на котором вы можете создать сайт без JavaScript.Дашборд содержит множество графики, и пользователю самому решать, как отобразить все на одной странице. Plotly обрабатывает изображения, но аспект компоновки зависит от Dash и его HTML-компонентов.Создание Dash-приложения Давайте создадим наше Dash-приложение. После инсталляции импортируем следующие библиотеки:import dash\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "import plotly.express as px\n",
      "import pandas as pddash – это глобальная библиотека, содержащая все основные функции. dash_core_components и dash_html_components – это библиотеки, которые устанавливаются с dash по умолчанию. Они включают специфические для Dash фичи и Python-представление HTML-компонентов (тегов). Подробнее о них позже.Любое приложение Dash запускается с помощью следующей команды:app = dash.Dash(name=\"my_first_dash_app\")\n",
      "\n",
      ">>> app\n",
      "<dash.dash.Dash at 0x1ee6af51af0>Вышеприведенный код полностью создает бойлерплейт для чистого веб-сайта. Нам не нужна пустая страница, поэтому давайте заполним ее.Сначала мы загрузим встроенный датасет из Plotly и создадим простую диаграмму рассеяния:# Load dataset using Plotly\n",
      "tips = px.data.tips()\n",
      "\n",
      "fig = px.scatter(tips, x=\"total_bill\", y=\"tip\") # Create a scatterplotЗатем мы добавляем это изображение в атрибут layout нашего приложения внутри тега div с небольшими текстами:app.layout = html.Div(children=[\n",
      "   html.H1(children='Hello Dash'),  # Create a title with H1 tag\n",
      "\n",
      "   html.Div(children='''\n",
      "       Dash: A web application framework for your data.\n",
      "   '''),  # Display some text\n",
      "\n",
      "   dcc.Graph(\n",
      "       id='example-graph',\n",
      "       figure=fig\n",
      "   )  # Display the Plotly figure\n",
      "])\n",
      "\n",
      "if __name__ == '__main__':\n",
      "   app.run_server(debug=True) # Run the Dash appТеперь мы создаем HTML-теги с помощью библиотеки dash_html_components (html), а изображение – используя основные компоненты (dcc) library .За тегом заголовка H1 следует div, содержащий обычный текст, затем сам график с использованием функции Graph библиотеки dcc. Все это находится внутри атрибута children одного тега DIV.В конце мы также добавляем команду, которая запускает наше приложение в режиме отладки, то есть изменения вступают в силу по мере внесения поправок в скрипт. Вот полный код на данный момент:import dash\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "\n",
      "# Create the app\n",
      "app = dash.Dash(__name__)\n",
      "\n",
      "# Load dataset using Plotly\n",
      "tips = px.data.tips()\n",
      "\n",
      "fig = px.scatter(tips, x=\"total_bill\", y=\"tip\") # Create a scatterplot\n",
      "\n",
      "app.layout = html.Div(children=[\n",
      "   html.H1(children='Hello Dash'),  # Create a title with H1 tag\n",
      "\n",
      "   html.Div(children='''\n",
      "       Dash: A web application framework for your data.\n",
      "   '''),  # Display some text\n",
      "\n",
      "   dcc.Graph(\n",
      "       id='example-graph',\n",
      "       figure=fig\n",
      "   )  # Display the Plotly figure\n",
      "])\n",
      "\n",
      "if __name__ == '__main__':\n",
      "   app.run_server(debug=True) # Run the Dash appПоместите его в скрипт Python и запустите. В терминале вы получите сообщение о том, что необходимо перейти по этой ссылке: http://127.0.0.1:8050/.Итак, поехали:В следующих разделах мы подробно расскажем о том, что здесь произошло.Создание app.layoutДавайте начнем с атрибута layout. Это единственный атрибут, который содержит все ваши HTML-компоненты и изображения. Вы должны передать ему всю вашу графику и HTML-теги в конечном теге DIV.В зависимости от размера вашего проекта, данный атрибут может стать довольно большим, поэтому я рекомендую создавать все ваши HTML-теги и фигуры в отдельных переменных, а затем передавать их в layout.Например, вот как будет выглядеть приведенное выше приложение:app = dash.Dash(name=\"app\")\n",
      "\n",
      "# Load dataset using Plotly\n",
      "tips = px.data.tips()\n",
      "\n",
      "fig = px.scatter(tips, x=\"total_bill\", y=\"tip\")  # Create a scatterplot\n",
      "\n",
      "title = html.H1(\"Hello Dash!\")\n",
      "text_div = html.Div(\"Dash: A web application framework for your data.\")\n",
      "graph_to_display = dcc.Graph(id=\"scatter\", figure=fig)\n",
      "\n",
      "app.layout = html.Div(children=[title, text_div, graph_to_display])Это гораздо аккуратнее и компактнее, и это одна из тех вещей, о которых вы не узнаете в документации Dash. Там куча вложенного кода вместо того, чтобы сделать вышеописанное.Компоненты HTML и CSS в DashДавайте обсудим, как HTML и CSS работают в Dash. Подбиблиотека dash_html_components содержит наиболее распространенные теги HTML, такие как divs, кнопки, текстовые поля, названия, теги заголовков (H1-6) и т. д.Они реализованы в коде Python под соответствующими именами как представление их HTML-аналогов. Итак, код, подобный приведенному, ниже:import dash_html_components as html\n",
      "\n",
      "html.Div([\n",
      "   html.H1('Hello Dash'),\n",
      "   html.Div([\n",
      "       html.P('Dash converts Python classes into HTML'),\n",
      "       html.P(\"This conversion happens behind the scenes by Dash's JavaScript frontend\")\n",
      "   ])\n",
      "])Будет интерпретироваться вашим браузером следующим образом:<div>\n",
      "   <h1>Hello Dash</h1>\n",
      "   <div>\n",
      "       <p>Dash converts Python classes into HTML</p>\n",
      "       <p>This conversion happens behind the scenes by Dash's JavaScript front-end</p>\n",
      "   </div>\n",
      "</div>Все HTML-теги этой подбиблиотеки содержат следующие основные аргументы:id: то же, что и атрибут id HTML-теговclassName: то же самое, что и атрибут class тегов HTMLstyle: такой же, как атрибут style тегов HTML, но принимает только словарь стилей CSSchildren: первый аргумент большинства компонентов HTMLВот пример div с некоторыми кастомизированными настройками:app = dash.Dash(name=\"app\")\n",
      "\n",
      "app.layout = html.Div(\n",
      "   children=html.H3(\"Simple Div\"),\n",
      "   id=\"sample_div\",\n",
      "   className=\"red_div\",\n",
      "   style={\"backgroundColor\": \"red\"},\n",
      ")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   app.run_server(debug=True)Аргумент children уникален - он может принимать числа и строки, но чаще всего вы передаете другие компоненты HTML списком, если это тег-контейнер, например такой как div.Замечание о стилях CSS: большинство атрибутов стиля CSS используют дефисы для разделения слов. Когда вы передаете их в аргумент style Dash, они должны применять camelCase, например backgroundColor вместо background-color.Я настоятельно рекомендую вам изучить данные HTML-теги, поскольку это единственное, что сохраняет целостность макета (лейаута) вашего приложения. Вот полный список HTML-тегов, поддерживаемых Dash.Основные компоненты DashДругой важной частью Dash являются его основные компоненты. Библиотека dash_core_components содержит несколько других HTML-тегов, но в нее уже встроено немного CSS и JavaScript.Некоторые примеры включают выпадающие списки, слайдеры, функционалы загрузки и выгрузки, а также компонент для отображения графиков Plotly.Вот несколько образцов этих компонентов, начиная с выпадающего списка:import dash\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "\n",
      "app = dash.Dash(__name__)\n",
      "\n",
      "app.layout = html.Div([\n",
      "   dcc.Dropdown(\n",
      "       options=[\n",
      "           {'label': 'FC Barcelona', 'value': 'FCB'},\n",
      "           {'label': 'Real Madrid', 'value': 'RM'},\n",
      "           {'label': 'Manchester United', 'value': 'MU'}\n",
      "       ],\n",
      "       value='FCB' # The default value to display\n",
      "   )\n",
      "])Выпадающий список с мультивыбором:app = dash.Dash(__name__)\n",
      "\n",
      "app.layout = html.Div([\n",
      "   dcc.Dropdown(\n",
      "       options=[\n",
      "           {'label': 'FC Barcelona', 'value': 'FCB'},\n",
      "           {'label': 'Real Madrid', 'value': 'RM'},\n",
      "           {'label': 'Manchester United', 'value': 'MU'}\n",
      "       ],\n",
      "       multi=True,\n",
      "       value=\"FCB\"\n",
      "   )\n",
      "], style={\"width\": 200})\n",
      "\n",
      "if __name__ == '__main__':\n",
      "   app.run_server(debug=True)Слайдер с помеченными точками останова:import dash\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "\n",
      "app = dash.Dash(__name__)\n",
      "\n",
      "app.layout = html.Div([\n",
      "   dcc.Slider(\n",
      "       min=0,\n",
      "       max=9,\n",
      "       marks={i: 'Label{}'.format(i) for i in range(10)},\n",
      "       value=5,\n",
      "   )\n",
      "])\n",
      "\n",
      "if __name__ == '__main__':\n",
      "   app.run_server(debug=True)Существует универсальный атрибут value, который представляет дефолтное значение при первом рендеринге компонента.Полный список основных компонентов можно посмотреть здесь.Создание финального интерфейса визуализации данных с помощью Python DashВ качестве завершающего примера посмотрите на приведенное ниже приложение:import seaborn as sns\n",
      "\n",
      "app = dash.Dash(__name__)\n",
      "\n",
      "diamonds = sns.load_dataset(\"diamonds\")\n",
      "\n",
      "scatter = px.scatter(\n",
      "   data_frame=diamonds,\n",
      "   x=\"price\",\n",
      "   y=\"carat\",\n",
      "   color=\"cut\",\n",
      "   title=\"Carat vs. Price of Diamonds\",\n",
      "   width=600,\n",
      "   height=400,\n",
      ")\n",
      "histogram = px.histogram(\n",
      "   data_frame=diamonds,\n",
      "   x=\"price\",\n",
      "   title=\"Histogram of Diamond prices\",\n",
      "   width=600,\n",
      "   height=400,\n",
      ")\n",
      "violin = px.violin(\n",
      "   data_frame=diamonds,\n",
      "   x=\"cut\",\n",
      "   y=\"price\",\n",
      "   title=\"Violin Plot of Cut vs. Price\",\n",
      "   width=600,\n",
      "   height=400,\n",
      ")\n",
      "\n",
      "left_fig = html.Div(children=dcc.Graph(figure=scatter))\n",
      "right_fig = html.Div(children=dcc.Graph(figure=histogram))\n",
      "\n",
      "upper_div = html.Div([left_fig, right_fig], style={\"display\": \"flex\"})\n",
      "central_div = html.Div(\n",
      "   children=dcc.Graph(figure=violin),\n",
      "   style={\"display\": \"flex\", \"justify-content\": \"center\"},\n",
      ")\n",
      "app.layout = html.Div([upper_div, central_div])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   app.run_server(debug=True)Мы импортируем датасет Diamonds из Seaborn и создаем три графика: диаграмму рассеяния, гистограмму и скрипичную диаграмму. Мы хотим отобразить график рассеяния и гистограмму рядом друг с другом и поместить скрипичную диаграмму прямо под ними в центре.Для этого мы создаем два div'а, содержащих график рассеяния и гистограмму, left_figure и right_figure. Затем для простоты эти два div помещаем в еще один div - upper_div.Мы устанавливаем CSS-стиль flex-box для этого div, который размещает фигуры бок о бок.Затем мы создаем центральный div, содержащий скрипичную диаграмму, и выравниваем его по центру с помощью flex-box и его атрибута justify-content.Наконец, мы помещаем все в лейаут внутри последнего DIV и запускаем скрипт. Вот конечный результат:ЗаключениеВот краткое описание шагов по созданию базового Dash-приложения:Создайте приложение с помощью dash.Dash и дайте ему любое название.Набросайте лейаут своих графиков в дашборде, прежде чем писать код.Создайте графики, которые войдут в ваш дашбордСоздайте шаблонный лейаут с помощью HTML-компонентов DashДобавьте свои изображеня в соответствующие контейнерыНаконец, добавьте все HTML-компоненты к атрибуту лейаутаНесмотря на то, что мы рассмотрели многие базовые аспекты, такие как HTML, основные компоненты и лейаут приложения, это лишь поверхностное знакомство с возможностями Dash.Я показал вам множество примеров интерактивных HTML-компонентов, но не рассказал, как интегрировать их в ваше приложение. Как вы можете обновлять графики на основе вводимых пользователем данных, таких как слайдеры, текст или что-то подобное?Вот здесь-то и приходят на помощь коллбеки. Это очень мощная и основная фича Dash. Чтобы использовать коллбеки, вы определяете в Dash функции, которые выстреливают при изменении компонента пользователем, и функция модифицирует новый компонент на основе этого события.Dash посвящает большой раздел в документации только для объяснения коллбеков, поскольку поначалу в них трудно разобраться. Я предлагаю вам обратиться к этой теме на следующем этапе.Загляните в галерею примеров, где вы сможете увидеть и насладиться некоторыми очень крутыми проектами, созданными с помощью Dash. Самое приятное, что многие из них с открытым исходным кодом, а это значит, что вы можете многое для себя почерпнуть и вдохновиться на новые идеи. Спасибо за прочтение!В заключение приглашаем на открытое занятие «Работа с Power BI. Построение дашборда. Настройка и взаимодействие визуальных элементов». На уроке загрузим данные в Power BI Desktop из источника, построим небольшой дашборд, посмотрим на назначение и варианты настройки базовых визуальных элементов. Записаться можно по ссылке.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Это были душераздирающие видео кадры 2003 года, когда Шаттл Колумбия (the Columbia Space Shuttle) с семью астронавтами на борту загорелся и развалился в воздухе за несколько минут до посадки. Авария произошла примерно в 500 милях от космодрома на мысе Канаверал во Флориде.\n",
      "\n",
      "\n",
      "Среди фрагментов Колумбии, которые были тщательно собраны, был и винчестер Seagate ёмкостью 400MB или скорее то что от него осталось, так как после пожара и падения на землю с большой высоты, мало что могло уцелеть. На этом винчестере была информация необходимая для завершения физического эксперимента — CVX-2, который имел место на обречённой миссии Шаттла.\n",
      "\n",
      "Сотрудники космического центра Джонсона (The Johnson Space Centre), анализирующие крушение шаттла передали эти фрагменты винчестера инженерам эксперимента CVX-2 (Critical Viscosity of Xenon) (Критическая Вязкость Ксенона), которые в свою очередь передали их в «Kroll Ontrack» в Миннеаполисе, штат Миннесота, чтобы узнать, возможно ли восстановить хоть какие-то данные. Для исследователя Роберта Берга (Robert Berg) и его команды это была единственная, но очень слабая надежда, спасения существенных данных от эксперимента который исследовал газовые потоки Ксеона в условиях микрогравитации.\n",
      "\n",
      "Люди «Kroll Ontrack» сумели восстановить около 90 процентов из 400 МБ данных винчестера с обугленным и сломанным корпусом. Спустя несколько лет, Роберт Берг и его команда проанализировали данные и опубликовали результаты эксперимента в апрельском выпуске журнала (Physical Review E journal).\n",
      "These showed that, rather liked whipped cream which changes from a fluid to a near-solid after being whipped or stirred vigorously, the gas Xenon change its viscosity from gas to liquid when similarly treated in very low gravity. The phenomenon of a sudden change in viscosity is called shear thickening. (Inducing or introducing shear to a fluid (gas or liquid) increases its viscosity and the substance behaves as a 'non-Newtonian' fluid.)Этот эксперимент показал, что также как в случае со взбитыми сливками, которые переходят из жидкого состояния в состояние близкое к твердому, после их взбивания или сильного взбалтывания — газ Ксенон переходит в жидкое состояние, если с ним проделать теже манипуляции при малой гравитации. Феномен внезапных изменений в вязкости вещества называется загустевания при сдвиге. (Индуцирование или совершение сдвига над текучей средой (газом или жидкостью) увеличивает его вязкость и субстанция ведет себя как неньютоновская жидкость).\n",
      "\n",
      "Это был очень сложный эксперимент нуждающийся в продолжении и детализированном анализе данных с винчестера, чтобы открыть «утончающий» эффект, который в конечном итоге был найден, как и винчестер. Так заканчивается двадцатилетняя научно-исследовательская работа и при этом ставит точку в ужасной истории миссии Шаттла Колумбии.\n",
      "\n",
      "Источник (на английском): blocksandfiles.com/article/5056\n",
      "\n",
      "PS: Физические исследования оставляю без перевода, так как с физикой у меня и по русски слабовато.\n",
      "\n",
      "UPD: Спасибо sylvio и Kaberc за помощь в переводе.    \n",
      " Примечание переводчика: хоть статья довольно старая (опубликована 2 года назад) и носит громкое название, в ней все же дается хорошее представление о различиях реляционных БД и NoSQL БД, их преимуществах и недостатках, а также приводится краткий обзор нереляционных хранилищ.\n",
      "\n",
      "В последнее время появилось много нереляционных баз данных. Это говорит о том, что если вам нужна практически неограниченная масштабируемость по требованию, вам нужна нереляционная БД.\n",
      "\n",
      "Если это правда, значит ли это, что могучие реляционные БД стали уязвимы? Значит ли это, что дни реляционных БД проходят и скоро совсем пройдут? В этой статье мы рассмотрим популярное течение нереляционных баз данных применительно к различным ситуациям и посмотрим, повлияет ли это на будущее реляционных БД.\n",
      "\n",
      "Реляционные базы данных существуют уже около 30 лет. За это время вспыхивало несколько революций, которые должны были положить конец реляционным хранилищам. Конечно, ни одна из этих революций не состоялась, и одна из них ни на йоту не поколебала позиции реляционных БД.\n",
      "\n",
      "Начнем с основ\n",
      "Реляционная база данных представляет собой набор таблиц (сущностей). Таблицы состоят из колонок и строк (кортежей). Внутри таблиц могут быть определены ограничения, между таблицами существуют отношения. При помощи SQL можно выполнять запросы, которые возвращают наборы данных, получаемых из одной или нескольких таблиц. В рамках одного запроса данные получаются из нескольких таблиц путем их соединения (JOIN), чаще всего для соединения используются те же колонки, которые определяют отношения между таблицами. Нормализация — это процесс структурирования модели данных, обеспечивающий связность и отсутствие избыточности в данных.\n",
      "\n",
      "Доступ к реляционным базам данных осуществляется через реляционные системы управления базами данных (РСУБД). Почти все системы баз данных, которые мы используем, являются реляционными, такие как Oracle, SQL Server, MySQL, Sybase, DB2, TeraData и так далее.\n",
      "\n",
      "Причины такого доминирования неочевидны. На протяжении всего существования реляционных БД они постоянно предлагали наилучшую смесь простоты, устойчивости, гибкости, производительности, масштабируемости и совместимости в сфере управлении данными.\n",
      "\n",
      "Однако чтобы обеспечить все эти особенности, реляционные хранилища невероятно сложны внутри. Например, простой SELECT запрос может иметь сотни потенциальных путей выполнения, которые оптимизатор оценит непосредственно во время выполнения запроса. Все это скрыто от пользователей, однако внутри РСУБД создает план выполнения, основывающийся на вещах вроде алгоритмов оценки стоимости и наилучшим образом отвечающий запросу.\n",
      "\n",
      "Проблемы реляционных БД\n",
      "Хотя реляционные хранилища и обеспечивают наилучшую смесь простоты, устойчивости, гибкости, производительности, масштабируемости и совместимости, их показатели по каждому из этих пунктов не обязательно выше, чем у аналогичных систем, ориентированных на какую-то одну особенность. Это не являлось большой проблемой, поскольку всеобщее доминирование реляционных СУБД перевешивало какие-либо недочеты. Тем не менее, если обычные РБД не отвечали потребностям, всегда существовали альтернативы.\n",
      "\n",
      "Сегодня ситуация немного другая. Разнообразие приложений растет, а с ним растет и важность перечисленных особенностей. И с ростом количества баз данных, одна особенность начинает затмевать все другие. Это масштабируемость. Поскольку все больше приложений работают в условиях высокой нагрузки, например, таких как веб-сервисы, их требования к масштабируемости могут очень быстро меняться и сильно расти. Первую проблему может быть очень сложно разрешить, если у вас есть реляционная БД, расположенная на собственном сервере. Предположим, нагрузка на сервер за ночь увеличилась втрое. Как быстро вы сможете проапгрейдить железо? Решение второй проблемы также вызывает трудности в случае использования реляционных БД.\n",
      "\n",
      "Реляционные БД хорошо масштабируются только в том случае, если располагаются на единственном сервере. Когда ресурсы этого сервера закончатся, вам необходимо будет добавить больше машин и распределить нагрузку между ними. И вот тут сложность реляционных БД начинает играть против масштабируемости. Если вы попробуете увеличить количество серверов не до нескольких штук, а до сотни или тысячи, сложность возрастет на порядок, и характеристики, которые делают реляционные БД такими привлекательными, стремительно снижают к нулю шансы использовать их в качестве платформы для больших распределенных систем.\n",
      "\n",
      "Чтобы оставаться конкурентоспособными, вендорам облачных сервисов приходится как-то бороться с этим ограничением, потому что какая ж это облачная платформа без масштабируемого хранилища данных. Поэтому у вендоров остается только один вариант, если они хотят предоставлять пользователям масштабируемое место для хранения данных. Нужно применять другие типы баз данных, которые обладают более высокой способностью к масштабированию, пусть и ценой других возможностей, доступных в реляционных БД.\n",
      "\n",
      "Эти преимущества, а также существующий спрос на них, привел к волне новых систем управления базами данных.\n",
      "\n",
      "Новая волна\n",
      "Такой тип баз данных принято называть хранилище типа ключ-значение (key-value store). Фактически, никакого официального названия не существует, поэтому вы можете встретить его в контексте документо-ориентированных, атрибутно-ориентированных, распределенных баз данных (хотя они также могут быть реляционными), шардированных упорядоченных массивов (sharded sorted arrays), распределенных хэш-таблиц и хранилищ типа ключ-значения. И хотя каждое из этих названий указывает на конкретные особенности системы, все они являются вариациями на тему, которую мы будем назвать хранилище типа ключ-значение.\n",
      "\n",
      "Впрочем, как бы вы его не называли, этот «новый» тип баз данных не такой уж новый и всегда применялся в основном для приложений, для которых использование реляционных БД было бы непригодно. Однако без потребности веба и «облака» в масштабируемости, эти системы оставались не сильно востребованными. Теперь же задача состоит в том, чтобы определить, какой тип хранилища больше подходит для конкретной системы.\n",
      "Реляционные БД и хранилища типа ключ-значение отличаются коренным образом и предназначены для решения разных задач. Сравнение характеристик позволит всего лишь понять разницу между ними, однако начнем с этого:\n",
      "\n",
      "Характеристики хранилищ\n",
      "\n",
      "\n",
      "Реляционная БД\n",
      "Хранилище типа ключ-значение\n",
      "\n",
      "\n",
      "База данных состоит из таблиц, таблицы содержат колонки и строки, а строки состоят из значений колонок. Все строки одной таблицы имеют единую структуру.\n",
      "\n",
      "Для доменов можно провести аналогию с таблицами, однако в отличие от таблиц для доменов не определяется структура данных. Домен – это такая коробка, в которую вы можете складывать все что угодно. Записи внутри одного домена могут иметь разную структуру.\n",
      "\n",
      "\n",
      "\n",
      "Модель данных1 определена заранее. Является строго типизированной, содержит ограничения и отношения для обеспечения целостности данных.\n",
      "\n",
      "Записи идентифицируются по ключу, при этом каждая запись имеет динамический набор атрибутов, связанных с ней.\n",
      "\n",
      "\n",
      "\n",
      "Модель данных основана на естественном представлении содержащихся данных, а не на функциональности приложения.\n",
      "\n",
      "В некоторых реализация атрибуты могут быть только строковыми. В других реализациях атрибуты имеют простые типы данных, которые отражают типы, использующиеся в программировании: целые числа, массива строк и списки.\n",
      "\n",
      "\n",
      "\n",
      "Модель данных подвергается нормализации, чтобы избежать дублирования данных. Нормализация порождает отношения между таблицами. Отношения связывают данные разных таблиц.\n",
      "\n",
      "Между доменами, также как и внутри одного домена, отношения явно не определены.\n",
      "\n",
      "\n",
      "\n",
      "Никаких join’ов\n",
      "Хранилища типа ключ-значение ориентированы на работу с записями. Это значит, что вся информация, относящаяся к данной записи, хранится вместе с ней. Домен (о котором вы можете думать как о таблице) может содержать бессчетное количество различных записей. Например, домен может содержать информацию о клиентах и о заказах. Это означает, что данные, как правило, дублируются между разными доменами. Это приемлемый подход, поскольку дисковое пространство дешево. Главное, что он позволяет все связанные данные хранить в одном месте, что улучшает масштабируемость, поскольку исчезает необходимость соединять данные из различных таблиц. При использовании реляционной БД, потребовалось бы использовать соединения, чтобы сгруппировать в одном месте нужную информацию.\n",
      "\n",
      "Хотя для хранения пар ключ-значение потребность в отношения резко падает, отношения все же нужны. Такие отношения обычно существуют между основными сущностями. Например, система заказов имела бы записи, которые содержат данные о покупателях, товарах и заказах. При этом неважно, находятся ли эти данные в одном домене или в нескольких. Суть в том, что когда покупатель размещает заказ, вам скорее всего не захочется хранить информацию о покупателе и о заказе в одной записи.\n",
      "Вместо этого, запись о заказе должна содержать ключи, которые указывают на соответствующие записи о покупателе и товаре. Поскольку в записях можно хранить любую информацию, а отношения не определены в самой модели данных, система управления базой данных не сможет проконтролировать целостность отношений. Это значит, что вы можете удалять покупателей и товары, которые они заказывали. Обеспечение целостности данных целиком ложится на приложение.\n",
      "\n",
      "Доступ к данным\n",
      "\n",
      "\n",
      "Реляционная БД\n",
      "Хранилище типа ключ-значение\n",
      "\n",
      "\n",
      "Данные создаются, обновляются, удаляются и запрашиваются с использованием языка структурированных запросов (SQL).\n",
      "\n",
      "Данные создаются, обновляются, удаляются и запрашиваются с использованием вызова API методов.\n",
      "\n",
      "\n",
      "\n",
      "SQL-запросы могут извлекать данные как из одиночной таблица, так и из нескольких таблиц, используя при этом соединения (join’ы).\n",
      "\n",
      "Некоторые реализации предоставляют SQL-подобный синтаксис для задания условий фильтрации.\n",
      "\n",
      "\n",
      "\n",
      "SQL-запросы могут включать агрегации и сложные фильтры.\n",
      "\n",
      "Зачастую можно использовать только базовые операторы сравнений (=, !=, <, >, <= и =>).\n",
      "\n",
      "\n",
      "\n",
      "Реляционная БД обычно содержит встроенную логику, такую как триггеры, хранимые процедуры и функции.\n",
      "\n",
      "Вся бизнес-логика и логика для поддержки целостности данных содержится в коде приложений.\n",
      "\n",
      "\n",
      "\n",
      "Взаимодействие с приложениями\n",
      "\n",
      "\n",
      "Реляционная БД\n",
      "Хранилище типа ключ-значение\n",
      "\n",
      "\n",
      "Чаще всего используются собственные API, или обобщенные, такие как OLE DB или ODBC.\n",
      "\n",
      "Чаще всего используются SOAP и/или REST API, с помощью которых осуществляется доступ к данным.\n",
      "\n",
      "\n",
      "\n",
      "Данные хранятся в формате, который отображает их натуральную структуру, поэтому необходим маппинг структур приложения и реляционных структур базы.\n",
      "\n",
      "Данные могут более эффективно отображаться в структуры приложения, нужен только код для записи данных в объекты.\n",
      "\n",
      "\n",
      "\n",
      "Хранилища типа ключ-значение: преимущества\n",
      "Есть два четких преимущества таких систем перед реляционными хранилищами.\n",
      "\n",
      "Подходят для облачных сервисов\n",
      "Первое преимущество хранилищ типа ключ-значение состоит в том, что они проще, а значит обладают большей масштабируемостью, чем реляционные БД. Если вы размещаете вместе собственную систему, и планируете разместить дюжину или сотню серверов, которым потребуется справляться с возрастающей нагрузкой, за вашим хранилищем данных, тогда ваш выбор – хранилища типа ключ-значение.\n",
      "\n",
      "Благодаря тому, что такие хранилища легко и динамически расширяются, они также пригодятся вендорам, которые предоставляют многопользовательскую веб-платформу хранения данных. Такая база представляет относительно дешевое средство хранения данных с большим потенциалом к масштабируемости. Пользователи обычно платят только за то, что они используют, однако их потребности могут вырасти. Вендор сможет динамически и практически без ограничений увеличить размер платформы, исходя из нагрузки.\n",
      "\n",
      "Более естественная интеграция с кодом\n",
      "Реляционная модель данных и объектная модель кода обычно строятся по-разному, что ведет к некоторой несовместимости. Разработчики решают эту проблему при помощи написания кода, который отображает реляционную модель в объектную модель. Этот процесс не имеет четкой и быстро достижимой ценности и может занять довольно значительное время, которое могло быть потрачено на разработку самого приложения. Тем временем многие хранилища типа ключ-значение хранят данные в такой структуре, которая отображается в объекты более естественно. Это может существенно уменьшить время разработки.\n",
      "\n",
      "Другие аргументы в пользу использования хранилищ типа ключ-значение, наподобие «Реляционные базы могут стать неуклюжими» (кстати, я без понятия, что это значит), являются менее убедительными. Но прежде чем стать сторонником таких хранилищ, ознакомьтесь со следующим разделом.\n",
      "\n",
      "Хранилища типа ключ-значение: недостатки\n",
      "Ограничения в реляционных БД гарантируют целостность данных на самом низком уровне. Данные, которые не удовлетворяют ограничениям, физически не могут попасть в базу. В хранилищах типа ключ-значение таких ограничений нет, поэтому контроль целостности данных полностью лежит на приложениях. Однако в любом коде есть ошибки. Если ошибки в правильно спроектированной реляционной БД обычно не ведут к проблемам целостности данных, то ошибки в хранилищах типа ключ-значение обычно приводят к таким проблемам.\n",
      "\n",
      "Другое преимущество реляционных БД заключается в том, что они вынуждают вас пройти через процесс разработки модели данных. Если вы хорошо спроектировали модель, то база данных будет содержать логическую структуру, которая полностью отражает структуру хранимых данных, однако расходится со структурой приложения. Таким образом, данные становятся независимы от приложения. Это значит, что другое приложение сможет использовать те же самые данные и логика приложения может быть изменена без каких-либо изменений в модели базы. Чтобы проделать то же самое с хранилищем типа ключ-значение, попробуйте заменить процесс проектирования реляционной модели проектированием классов, при котором создаются общие классы, основанные на естественной структуре данных.\n",
      "\n",
      "И не забудьте о совместимости. В отличие от реляционных БД, хранилища, ориентированные на использование в «облаке», имеют гораздо меньше общих стандартов. Хоть концептуально они и не отличаются, они все имеют разные API, интерфейсы запросов и свою специфику. Поэтому вам лучше доверять вашему вендору, потому что в случае чего, вы не сможете легко переключиться на другого поставщика услуг. А учитывая тот факт, что почти все современные хранилища типа ключ-значение находятся в стадии бета-версий2, доверять становится еще рискованнее, чем в случае использования реляционных БД.\n",
      "\n",
      "Ограниченная аналитика данных\n",
      "Обычно все облачные хранилища строятся по типу множественной аренды, что означает, что одну и ту же систему использует большое количество пользователей и приложений. Чтобы предотвратить «захват» общей системы, вендоры обычно каким-то образом ограничивают выполнение запросов. Например, в SimpleDB запрос не может выполняться дольше 5 секунд. В Google AppEngine Datastore за один запрос нельзя получить больше, чем 1000 записей3. \n",
      "\n",
      "Эти ограничения не страшны для простой логики (создание, обновление, удаление и извлечение небольшого количества записей). Но что если ваше приложение становится популярным? Вы получили много новых пользователей и много новых данных, и теперь хотите сделать новые возможности для пользователей или каким-то образом извлечь выгоду из данных. Тут вы можете жестко обломаться с выполнением даже простых запросов для анализа данных. Фичи наподобие отслеживания шаблонов использования приложения или системы рекомендаций, основанной на истории пользователя, в лучшем случае могут оказаться сложны в реализации. А в худшем — просто невозможны.\n",
      "\n",
      "В таком случае для аналитики лучше сделать отдельную базу данных, которая будет заполняться данными из вашего хранилища типа ключ-значение. Продумайте заранее, каким образом это можно будет сделать. Будете ли вы размещать сервер в облаке или у себя? Не будет ли проблем из-за задержек сигнала между вами и вашим провайдером? Поддерживает ли ваше хранилище такой перенос данных? Если у вас 100 миллионов записей, а за один раз вы можете взять 1000 записей, сколько потребуется на перенос всех данных?\n",
      "\n",
      "Однако не ставьте масштабируемость превыше всего. Она будет бесполезна, если ваши пользователи решат пользоваться услугами другого сервиса, потому что тот предоставляет больше возможностей и настроек.\n",
      "\n",
      "Облачные хранилища\n",
      "Множество поставщиков веб-сервисов предлагают многопользовательские хранилища типа ключ-значение. Большинство из них удовлетворяют критериям, перечисленным выше, однако каждое обладает своими отличительными фичами и отличается от стандартов, описанных выше. Давайте взглянем на конкретные пример хранилищ, такие как SimpleDB, Google AppEngine Datastore и SQL Data Services.\n",
      "\n",
      "Amazon: SimpleDB\n",
      "SimpleDB — это атрибутно-ориентированное хранилище типа ключ-значение, входящее в состав Amazon WebServices. SimpleDB находится в стадии бета-версии; пользователи могут пользовать ей бесплатно — до тех пор пока их потребности не превысят определенный предел.\n",
      "\n",
      "У SimpleDB есть несколько ограничений. Первое — время выполнения запроса ограничено 5-ю секундами. Второе — нет никаких типов данных, кроме строк. Все хранится, извлекается и сравнивается как строка, поэтому для того, чтобы сравнить даты, вам нужно будет преобразовать их в формат ISO8601. Третье — максимальные размер любой строки составляет 1024 байта, что ограничивает размер текста (например, описание товара), который вы можете хранить в качестве атрибута. Однако поскольку структура данных гибкая, вы можете обойти это ограничения, добавляя атрибуты «ОписаниеТовара1», «Описание товара2» и т.д. Но количество атрибутов также ограничено — максимум 256 атрибутов. Пока SimpleDB находится в стадии бета-версии, размер домена ограничен 10-ю гигабайтами, а вся база не может занимать больше 1-го терабайта.\n",
      "\n",
      "Одной из ключевых особенностей SimpleDB является использование модели конечной констистенции (eventual consistency model). Эта модель подходит для многопоточной работы, однако следует иметь в виду, что после того, как вы изменили значение атрибута в какой-то записи, при последующих операциях чтения эти изменения могут быть не видны. Вероятность такого развития событий достаточно низкая, тем не менее, о ней нужно помнить. Вы же не хотите продать последний билет пяти покупателям только потому, что ваши данные были неконсистентны в момент продажи.\n",
      "\n",
      "Google AppEngine Data Store\n",
      "Google's AppEngine Datastore построен на основе BigTable, внутренней системе хранения структурированных данных от Google. AppEngine Datastore не предоставляет прямой доступ к BigTable, но может восприниматься как упрощенный интерфейс взаимодействия с BigTable.\n",
      "\n",
      "AppEngine Datastore поддерживает большее число типов данных внутри одной записи, нежели SimpleDB. Например, списки, которые могут содержать коллекции внутри записи.\n",
      "\n",
      "Скорее всего вы будете использовать именно это хранилище данных при разработке с помощью Google AppEngine. Однако в отличии от SimpleDB, вы не сможете использовать AppEngine Datastore (или BigTable) вне веб-сервисов Google.\n",
      "\n",
      "Microsoft: SQL Data Services\n",
      "\n",
      "SQL Data Services является частью платформы Microsoft Azure. SQL Data Services является бесплатной, находится в стадии бета-версии и имеет ограничения на размер базы. SQL Data Services представляет собой отдельное приложение — надстройку над множеством SQL серверов, которые и хранят данные. Эти хранилища могут быть реляционными, однако для вас SDS является хранилищем типа ключ-значение, как и описанные выше продукты.\n",
      "\n",
      "Необлачные хранилища\n",
      "Существует также ряд хранилищ, которыми вы можете воспользоваться вне облака, установив их у себя. Почти все эти проекты являются молодыми, находятся в стадии альфа- или бета-версии, и имеют открытый код. С открытыми исходниками вы, возможно, будете больше осведомлены о возможных проблемах и ограничениях, нежели в случае использования закрытых продуктов.\n",
      "\n",
      "CouchDB\n",
      "CouchDB — это свободно распространяемая документо-ориентированная БД с открытым исходным кодом. В качестве формата хранения данных используется JSON. CouchDB призвана заполнить пробел между документо-ориентированными и реляционными базами данных с помощью «представлений». Такие представления содержат данные из документов в виде, схожим с табличным, и позволяют строить индексы и выполнять запросы.\n",
      "\n",
      "В настоящее время CouchDB не является по-настоящему распределенной БД. В ней есть функции репликации, позволяющие синхронизировать данные между серверами, однако это не та распределенность, которая нужна для построения высокомасштабируемого окружения. Однако разработчики CouchDB работают над этим.\n",
      "\n",
      "Проект Voldemort\n",
      "Проект Voldemort — это распределенная база данных типа ключ-значение, предназначенная для горизонтального масштабирования на большом количестве серверов. Он родилась в процессе разработки LinkedIn и использовалась для нескольких систем, имеющих высокие требования к масштабируемости. В проекте Voldemort также используется модель конечной консистенции.\n",
      "\n",
      "Mongo\n",
      "\n",
      "Mongo — это база данных, разрабатываемая в 10gen Гейром Магнуссоном и Дуайтом Меррименом (которого вы можете знать по DoubleClick). Как и CouchDB, Mongo — это документо-ориентированная база данных, хранящая данные в JSON формате. Однако Mongo скорее является объектной базой, нежели чистым хранилищем типа ключ-значение.\n",
      "\n",
      "Drizzle\n",
      "\n",
      "Drizzle представляет совсем другой подход к решению проблем, с которыми призваны бороться хранилища типа ключ-значение. Drizzle начинался как одна из веток MySQL 6.0. Позже разработчики удалили ряд функций (включая представления, триггеры, скомпилированные выражения, хранимые процедуры, кэш запросов, ACL, и часть типов данных), с целью создания более простой и быстрой СУБД. Тем не менее, Drizzle все еще можно использовать для хранения реляционных данных. Цель разработчиков — построить полуреляционную платформу, предназначенную для веб-приложений и облачных приложений, работающих на системах с 16-ю и более ядрами.\n",
      "\n",
      "Решение\n",
      "В конечном счете, есть четыре причины, по которым вы можете выбрать нереляционное хранилище типа ключ-значение для своего приложения:\n",
      "\n",
      "Ваши данные сильно документо-ориентированны, и больше подходят для модели данных ключ-значение, чем для реляционной модели.\n",
      "Ваша доменная модель сильно объектно-ориентированна, поэтому использования хранилища типа ключ-значение уменьшит размер дополнительного кода для преобразования данных.\n",
      "Хранилище данных дешево и легко интегрируется с веб-сервисами вашего вендора.\n",
      "Ваша главная проблема — высокая масштабируемость по запросу.\n",
      "\n",
      "Однако принимая решение, помните об ограничениях конкретных БД и о рисках, которые вы встретите, пойдя по пути использования нереляционных БД.\n",
      "\n",
      "Для всех остальных требований лучше выбрать старые добрые реляционные СУБД. Так обречены ли они? Конечно, нет. По крайней мере, пока.\n",
      "\n",
      "\n",
      "1 — по моему мнению, здесь больше подходит термин «структура данных», однако оставил оригинальное data model.\n",
      "2 — скорее всего, автор имел в виду, что по своим возможностям нереляционные БД уступают реляционным.\n",
      "3 — возможно, данные уже устарели, статья датируется февралем 2009 года.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Tarantool это замечательное высокопроизводительное no-Sql решение, разработка компании Mail.Ru. Исходники\n",
      "\n",
      "Данное решение позволяет использовать как режим key/value, так и выборку множества записей в рекордсет по одному или нескольким критериям (полям поиска). Аналогов в рунете и не только, я пока не встречал. С натяжкой можно сравнить редис. Но в редисе — списковые данные и их нельзя выбирать по ключу. Судя до утверждениям разработчиков, скорость доступа по ключу превосходит memcache, при этом еще в бэдграунде осуществляется постоянное сохранение данных на диск. Но к сожалению, данная разработка имеет единственный perl клиент для доступа к данным, из-за чего не имеет такой популярности, как например у redis или memcache. \n",
      "\n",
      "В doc/box-protocol источников есть описание Протокола, которое я в настоящее время переработал для написания клиента на Си и PHP. Изучив Протокол, вы можете реализоать нативный клиент на любимом Вам языке. Надеюсь, данная статья в этом Вам пригодится. \n",
      "\n",
      "\n",
      "Все данные в этой базе разделены на namespace, скорее всего это аналог БД в MySQL. Нумерация всех неймспейсов — цифровая ( 0, 1, 2 и тд). На каждый namespace можно наложить определенный индекс. Нумерация Индексов — так же цифровая. Индексы накладываются на одно или несколько полей. Индекс может иметь тип HASH или TREE\n",
      "\n",
      "Все Индесы и неймспейсы прописываются в Конфиге. Ниже показан пример, где прописано два индекса, цифровой и символьный. При том, второй индекс составной:namespace[0].enabled = 1\n",
      "namespace[0].index[0].type = \"HASH\"\n",
      "namespace[0].index[0].unique = 1\n",
      "namespace[0].index[0].key_field[0].fieldno = 0\n",
      "namespace[0].index[0].key_field[0].type = \"NUM\"\n",
      "namespace[0].index[0].key_field[1].fieldno = 1\n",
      "namespace[0].index[1].type = \"TREE\"\n",
      "namespace[0].index[1].key_field[0].fieldno = 0\n",
      "namespace[0].index[1].key_field[0].type = \"STR\"\n",
      "namespace[0].index[1].key_field[1].fieldno = 1\n",
      "\n",
      "Необходимо отметить про ключи. Ключи могут быть цифровыми (1,2,3… 6.2 *10^9 ) или символьными.\n",
      "\n",
      "Все данные в неймспейсе хранятся в ввиде кортежей. Картеж представляет собойй набор полей. Поле может быть либо цифровым, либо символьным. \n",
      "\n",
      "Обмен между клиентом и сервером осущестляется Сообщениями. Все Сообщения в tarantool Протокол делятся на Запрос Request и ответ Responce. Каждое Сообщение имеет обязательный Заголовок Header и так же может иметь Tело. \n",
      "\n",
      "Заголовок Сообщения включает: тип Сообщения, длину тела и идентификатор запроса. \n",
      "\n",
      "Структура Заголовка:\n",
      "\n",
      "<blockquote>typedef  struct {\n",
      "\t\tuint32_t type;\n",
      "\t\tuint32_t len;\n",
      "\t\tuint32_t request_id;\n",
      "} Header;\n",
      "</blockquote>\n",
      "Определены следующие типы Сообщений:\n",
      "INSERT 0xd (13) \n",
      "SELECT 0x11 (17)\n",
      "UPDATE 0x13 (19)\n",
      "DELETE 0x14 (29)\n",
      "PING 0xff 0xff 0x0 0x0 (65280)\n",
      "\n",
      "Идентификатор запроса устанавливается клиентом и может быть нулевой.\n",
      "\n",
      "Общая структура запроса:\n",
      "\n",
      "typedef\tstruct {\n",
      "\tHeader header;\n",
      "\tunion {\n",
      "\t\tInsertRequest insert;\n",
      "\t\tSelectRequest select;\n",
      "\t\tUpdateRequest insert;\n",
      "\t\tDeleteRequest insert;\n",
      "\t};\n",
      "} Request;\n",
      "\n",
      "Команда PING не имеет тела, по этому отсутствует PingRequest ;)\n",
      "\n",
      "Тело команды INSERT состоит из номера namespace, над которым будет осуществляться операция, флага и кортежа. \n",
      "\n",
      "namespace — Это пространство, в котором хранятся кортежи. Нумерация неймспейсов цифроая. В каждом неймспейсе могут быть определены индексы. Первичный индекс (PRIMARY) должен \n",
      "присутствовать обязательно. Индексы определены в конфигурационном файле.\n",
      "\n",
      "В настоящее время определен только единственный флаг BOX_RETURN_TUPLE (0x01) который показывает, вернуть ли данные в теле ответа.\n",
      "Структура INSERT запроса:\n",
      "\n",
      "typedef\tstruct {\n",
      "\tuint32_t namespaceNo;\n",
      "\tuint32_t flag;\n",
      "\tTuple tuple;\n",
      "} InsertRequest;\n",
      "\n",
      "Все данные описываются с помощью кортежей Tuple. Кортеж состоит из поля cardinality, которое представляет собой размерность кортежа (кол-во полей) и массива полей. В общем виде это будет выглядеть так:\n",
      "\n",
      "typedef\tstruct {\n",
      "\tuint32_t card;\n",
      "\tField field[];\n",
      "} Tuple;\n",
      "\n",
      "Каждое поле представлено массивом байт. Поле может иметь: int32, int64 или потоком байт.\n",
      "В настоящее время я пока определил так: \n",
      "typedef Field u_char * data;\n",
      "\n",
      "Все данные полей упакованы с помощью LED128 en.wikipedia.org/wiki/LEB128\n",
      "\n",
      "тело SELECT запроса включает: номер namespace, номер индекса, по которому происходит выборка, смещение offset и размер вывода limit и кол-во кортежей и сами кортежи. Параметры offset и limit аналогичны выборке: SELECT * FROM… LIMIT \n",
      "\n",
      "typedef\tstruct {\n",
      "\tuint32_t namespaceNo;\n",
      "\tuint32_t indexNo;\n",
      "\tuint32_t offset;\n",
      "\tuint32_t limit;\n",
      "\tuint32_t count;\n",
      "\tTuple tuples[];\n",
      "} SelectRequest;\n",
      "\n",
      "Если мы указываем SELECT * FROM t0 WHERE k0=1, то кол-во кортежей=1 и значение Tuple должно соответствовать 1. Если определен вторичный составной индекс k1 (цифровое поле и символьное), то на запрос \n",
      "SELECT * FROM t0 WHERE k1 = (21, 'USSR') \n",
      "кол-во кортежей=2 и должно быть представлено два значения Tuple. Необходимо сделать пояснение, что представленный sql схематичный, и не соответствует стандарту SQL'92. Дело в том, что данные в tarantool/box представлены кортежами, а не таблицами (столбцы и строки). На кортеж может содержать любое кол-во полей. Все кортежи хранятся в немспейсе. Однако на мемспейс можно налажить HASH или rbTREE индекс по которому будет осуществлен поиск.\n",
      "\n",
      "Тело UPDATE запроса включает: номер неймспейса, флаг, кортеж, кол-во операций и сами операции. Поля флаг и кортеж аналогичны операции UPDATE. Кол-во операций может быть равно нолю. Структура будет выглядеть следующим образом:\n",
      "\n",
      "typedef\tstruct {\n",
      "\tuint32_t namespaceNo;\n",
      "\tuint32_t flag;\n",
      "\tTuple tuple;\n",
      "\tint32_t count;\n",
      "\tOperation operation[];\n",
      "} UpdateRequest;\n",
      "\n",
      "Каждая Операция представляет собой структуру, содержащей номер поля по которому будет проведена операция, код операции и аргумент. \n",
      "\n",
      "Используются коды операций:\n",
      " 0 — присвоение аргумента данному полю.\n",
      "Если аргументом является тип int32, то так же возможны следующие действия: \n",
      " 1 — добавить аргумент к существующему полю\n",
      " 2 — выполнить AND с существующем полем \n",
      " 3 — выполнить XOR с существующем полем\n",
      " 4 — выполнить OR с существующем полем\n",
      "\n",
      "typedef\tstruct {\n",
      "\tint32_t fieldNo;\n",
      "\tint8_t  opcode;\n",
      "\tField   arg;\n",
      "} Operation;\n",
      "\n",
      "Операция DELETE всегда выполняется по первичному ключу и содержит номер неймспейса и кортеж. Структура операции DELETE представлена ниже:\n",
      "\n",
      "typedef\tstruct {\n",
      "\tuint32_t namespaceNo;\n",
      "\tTuple tuple;\n",
      "} SelectRequest;\n",
      "\n",
      "Каждый ответ сервера содержит: Заголовок Header, код ответа и при необходимости тело ответа. Заголовок ответа — аналогичен заголовку запроса. Код возрата 0 — успех, либо смотрим ошибки в include/iproto.h \n",
      "\n",
      "В общем виде получается следующая структура ответа:\n",
      "\n",
      "typedef\tstruct {\t\t\t\t\n",
      "\t\tHeader header;\n",
      "\t\tint32_t code;\n",
      "\t\tunion {\n",
      "\t\t\tSelectResponce selectBody;\n",
      "\t\t\tinsertResponce insertBody;\n",
      "\t\t\tuint8_t * data;\n",
      "\t\t\tint32_t count;\n",
      "\t\t};\t\t\t\n",
      "} Responce;\n",
      "\n",
      "\n",
      "Тело ответа на запрос SELECT стостоит из поля содержащего общее кол-во кортежей и самого множества возвращаемых кортежей. Если результат запроса пустой, то кортежи не возвращаются и поле количества содержит ноль. \n",
      "\n",
      "typedef\tstruct {\n",
      "\tint32_t count;\n",
      "\tFqTuple tuples[];\n",
      "} SelectResponce;\n",
      "\n",
      "Каждый возвращаемый кортеж (FqTuple) содержит размер кортежа, некий идентификаро cardinality, который выступает как разделитель (boundaries) и самого кортежа.\n",
      "\n",
      "typedef\tstruct {\n",
      "\tint32_t size;\n",
      "\tuint32_t card;\n",
      "\tTuple tuple;\n",
      "} FqTuple;\n",
      "\n",
      "Если в запросе InsertRequest установлен флаг BOX_RETURN_TUPLE, то ответ может содержать тело:\n",
      "\n",
      "typedef\tstruct {\n",
      "\tint32_t count;\n",
      "\tFqTuple tuple;\n",
      "} InsertResponce;\n",
      "\n",
      "Аналогичным является и ответ на запрос UPDATE.\n",
      "\n",
      "Запрос Delete возвращает количество удаленый записей. Так как при удалении используется только первичный индекс, то мы можем удалить только одну запись, соответственно вернется 0 или 1. Это поле count структуры Responce. Еще в структуре выделен массив байт data для анализа данных.\n",
      "\n",
      "Возможны в тексте есть некоторые неточности, где-то можно использоать int32_t вместо uint32_t. \n",
      "Возможно что-то я недопонял, так что буду рад делововой критики от авторов этого замечательного проекта.    \n",
      " Приветствую уважаемых читателей.\n",
      "Данный материал прольет свет на проблему удобства работы с РСУБД, которой я посвятил много лет, но никак не находил времени рассказать.\n",
      "\n",
      "Если вы не занимаетесь поиском, просмотром и анализом данных или же делаете это, но полностью удобно и не имеете ни в чем нужды, смело бросайте чтение данного текста.\n",
      "\n",
      "Проблематика\n",
      "Итак, вы – пользователь, имеющий право на чтение в некой СУБД. Вероятно, перед вами стоит набор типовых подзадач:\n",
      "\n",
      "\n",
      "Разобраться со структурой данных\n",
      "Найти в ней нужные сущности\n",
      "Найти в них нужные поля\n",
      "Найти связи между сущностями\n",
      "Найти интересующие значения\n",
      "Отобрать набор значений\n",
      "Выбрать нужные данные\n",
      "Убедиться, что это действительно ТЕ САМЫЕ данные, которые вы искали\n",
      "Сохранить результаты\n",
      "Подготовить из них отчеты\n",
      "\n",
      "\n",
      "Наконец, весьма вероятно, что эти задачи вам надо решать регулярно.\n",
      "\n",
      "На рынке инструментов обработки данных представлено огромное количество средств, посвященных построению запросов, кубов и отчетов. Прискорбно, но большая часть из них не видит наличия у пользователя вышеперечисленных задач во всей их полноте. Перечислим типичные проблемы, в обратном порядке относительно предыдущего списка:\n",
      "\n",
      "Редко где встретишь возможность зафиксировать найденные знания, каждый новый запрос – как с чистого листа\n",
      "Ранее подготовленные запросы можно поднять только в\n",
      "чистом SQL\n",
      "Структура запроса (поля, фильтры, группировки) тяжело изменяема. Как правило, цена быстрым изменениям – потеря целостности или остаточная избыточность\n",
      "Подготовка фильтров производится вслепую\n",
      "Связи между сущностями подлежат ручному указанию\n",
      "На пользователя вываливается вся доступная ему схема БД\n",
      "\n",
      "\n",
      "Все это на фоне отсутствия документации по структуре. Несмотря на то, что она находится вне инструмента, проблема касается инструментов напрямую.\n",
      "\n",
      "Если вам недостаточно понятно, в чем же реальность перечисленных проблем, то я надеюсь, что вы поймете это, узнав, как я пытаюсь их решать. Если, на ваш взгляд, указанные проблемы надуманны, то нам будет легче, если вы не станете читать дальше. \n",
      "\n",
      "Решение\n",
      "Комплексное решение обозначенных проблем в программе «Свобода Выборки» ведется по нескольким направлениям одновременно. Текст этого раздела детально описывает примененные концепции и нюансы реализации\n",
      "\n",
      "Понятная и удобная структура данных\n",
      "Только самая простейшая база данных содержит сущности, в которых невозможно потеряться. В хорошо проработанной прикладной области таблицы и поля измеряются соответственно десятками и сотнями, а если говорить о хранилищах, то это сотни и тысячи. Кроме того, у разработчика или аналитика зачастую по несколько соединений (тест, продуктив, архив) на каждую предметную область, которых тоже, как правило, довольно много.\n",
      "\n",
      "Отличить необходимое от полностью лишнего – одна из первых задач, решаемых нашим инструментом.\n",
      "\n",
      "Базы данных и таблицы\n",
      "В рамках выбранного «движка» доступа, можно создавать и запоминать соединения, что впрочем, стандартно.\n",
      "\n",
      "В полном списке таблиц можно найти интересующие поля по подстроке, например:\n",
      "\n",
      "Контекст\n",
      "Контекст сущности/таблицы – понятие, близкое юниверсам Business Objects, но более широкое. Естественно исходить из предположения, что пользователя не интересуют таблицы, не связанные с выбранной. При этом было бы ошибкой думать, что в контекст входят только непосредственно связанные таблицы – поэтому в контексте мы можем видеть вложенность любого уровня:\n",
      "\n",
      "\n",
      "\n",
      "Узлы – это сущности, листья – это поля. Связанные сущности показываются рядом с полями той сущности, с которой у них связь. Множественные связи на одну и ту же сущность различаются полями, через которые указана ссылка:\n",
      "\n",
      "\n",
      "\n",
      "По умолчанию показываются только сущности, на которые ссылаются поля, но можно увидеть и связи с другой стороны.\n",
      "\n",
      "\n",
      "\n",
      "Эта операция производится либо разово, индивидуально для каждой таблицы, либо включается режим для всех таблиц автоматически.\n",
      "\n",
      "\n",
      "Распознавание\n",
      "Вам повезло, если архитектор системы предусмотрел все внешние ключи и прописал их в схеме, тогда связи будут определены автоматически и вам почти не нужно будет об этом заботиться. Я обожаю смотреть такие базы данных; примеры на скриншотах выше сделаны в одной из них – спасибо моим учителям на одном из проектов.\n",
      "\n",
      "Вам не повезло не только в случае, если архитектор не предусмотрел этого, но и если субдрайвер компоненты доступа не поддерживает получения данной информации. В ADO и ODBC часто встречается такая ситуация, но эти движки – не единственные, с которыми умеет работать наш инструмент.\n",
      "\n",
      "Определение\n",
      "Пусть, по каким-либо причинам, мы все-таки столкнулись с тем, что данных о связях взять из схемы нельзя. Но ведь ими каждый раз так удобно пользоваться.\n",
      "\n",
      "Мы можем их определить самостоятельно. Для этого есть несколько способов.\n",
      "\n",
      "Простейший – это разовая связка по одному полю, через контекстное меню и диалог (в отдельных случаях это можно сделать через drag'n'drop полей друг на друга):\n",
      "\n",
      "\n",
      "\n",
      "Более сложный это связка по нескольким полям:\n",
      "\n",
      "\n",
      "\n",
      "Наконец, высший пилотаж – это многократная типовая связь всех нужных таблиц:\n",
      "\n",
      "\n",
      "\n",
      "Последнюю задачу мне пришлось выполнять, анализируя схему одного популярного Open Source портала, где очевидных связей – несколько десятков групп на каждую из справочных таблиц, в которых, в свою очередь, число связей доходит до таких же порядков.\n",
      "\n",
      "Все эти связи будут сохранены в локальной «схеме», существующей как дополнение к автоматически распознаваемым ссылкам через драйвер к БД, и впредь будут определять контексты наравне с ними.\n",
      "\n",
      "Прочие возможности\n",
      "Для отдельных частных случаев предусмотрены отдельные вкусности, делающие контекст более гибким. Подробно останавливаться не будем, просто снимок контекстного меню:\n",
      "\n",
      "Прекрасно, но для чего все это?!\n",
      "Определение и отображение контекста сущности, с которой работает пользователь, сделано не только для того, чтобы он мог сосредоточиться на выборе нужных ему данных, вложенных в ссылки. Пользователь не думает об их связывании, так как построитель запросов учитывает связывающие поля и характер связи прозрачно для пользователя.\n",
      "\n",
      "Удобство построения запроса\n",
      "Мы сделали первый шаг для удобной работы – отделили важное от второстепенного. Теперь наша задача – упростить отбор насущного из доступного.\n",
      "\n",
      "На ходу\n",
      "Наша задача – раскрывать контекст на нужную глубину и выбирать требуемые поля. Таблицы – это лишь  способ навигации к ним, их указание не требуется совсем. Типовые операции с полями в запросе – добавление в выборку, добавление в фильтр и добавление в выражение (в выборке или фильтре).\n",
      "\n",
      "На рисунке выделены три кнопки-иконки типовых действий и показан результат одного из них (добавление в фильтр) на примере поля Name таблицы ServiceSubGroup, на которую ссылается таблица Service:\n",
      "\n",
      "\n",
      "\n",
      "При выборе таблицы в инструменте есть возможность автоматического добавления всех полей в выборку. Однако нам не нужен лишний мусор в результатах, поэтому мы хотим избавиться от некоторых полей (на рисунке все та же таблица Service):\n",
      "\n",
      "\n",
      "\n",
      "…И добавить поле из глубины контекста (выполнено через drag'n'drop):\n",
      "\n",
      "\n",
      "\n",
      "…найти максимальное значение поля в группах и количество строк:\n",
      "\n",
      "\n",
      "\n",
      "…затем перегруппировать поля:\n",
      "\n",
      "\n",
      "\n",
      "…сделать запрос (доопределение фильтра опущено, о нем позже):\n",
      "\n",
      "\n",
      "\n",
      "…временно убрать из выборки одно поле и разгруппировать другое:\n",
      "\n",
      "\n",
      "\n",
      "…сделать новый запрос и увидеть более интересный результат:\n",
      "\n",
      "\n",
      "\n",
      "Все это и многое другое теперь мы может делать практически минимумом нажатий.\n",
      "\n",
      "Объектная модель\n",
      "Важнейшая особенность инструмента «Свобода выборки» – это то, что объекты базы данных везде трактуются целостно: показываются, добавляются, удаляются, прячутся и так далее. Это позволяет пользователю расслабиться, оперируя ими, а инструменту не потерять и не\n",
      "испортить их в диалоге с пользователем.\n",
      "\n",
      "Объектная модель четко контролирует необходимость и достаточность соединений, учитывает поля, которые и по которым надо группировать запрос. Например:\n",
      "\n",
      "\n",
      "\n",
      "Делаем группировочный запрос суммы всех услуг в группе, разбитый по подгруппе и названию услуги, в середине рисунка показан сгенерированный запрос.\n",
      "\n",
      "Затем мы хотим сгруппировать только по подгруппе, для этого «прячем» название услуги:\n",
      "\n",
      "\n",
      "\n",
      "В запросе Shortname исчезло из обеих частей (select, group by).\n",
      "\n",
      "Наконец, уберем условие отбора (опять же, временно):\n",
      "\n",
      "\n",
      "\n",
      "В запросе не только полностью исчезла секция where, но и стало на один join меньше, поскольку поля таблицы ServiceGroup нам больше не нужны.\n",
      "\n",
      "Естественно, в те же несколько кликов можно вернуть запрос в обратное состояние.\n",
      "\n",
      "Редактор выражений\n",
      "Пользование одними только полями и связями достаточно лишь до некоторого предела. Рано или поздно наступает необходимость вычислять из них новые значения, не представленные в базе данных напрямую. Как совместить уже известные нам удобства и ручное создание, а тем более изменение конструкций?\n",
      "\n",
      "Выход достаточно простой: надо продолжать использовать поля как объекты, а весь остальной богатейший синтаксис (кроме стандартных агрегатных функций) всех различных СУБД отдать на откуп пользователю. То есть все, что не является полями, он напишет текстом, плюс есть возможность сослаться или не сослаться на поля в «правильном стиле».\n",
      "\n",
      "Редактор выражений устроен так, что в текст выражения в любом месте можно вставить ссылку на любое доступное поле или удалить любую из ранее указанных ссылок:\n",
      "\n",
      "\n",
      "\n",
      "На рисунке можно видеть, что мы составили разность между суммой сессии и суммой сумма операций (в надежде, что она даст 0, как и должна) и поделили все это на 100 (суммы изначально в копейках). В данном выражении шесть управляемых и различаемых инструментом объекта: открывающая скобка, два поля, агрегатная функция и две закрывающие скобки. Скобки вынесены в управляемые объекты для упрощения контроля их парности (при удалении скобки с одной стороны автоматически удаляется скобка/функция с другой) и возможности проверки вхождения поля в агрегатное выражение. Все остальные символы: -,/,1,0,0 будут переданы на сервер «как есть» и для инструмента они не значат ничего.\n",
      "\n",
      "Это может показаться не очень удобным – переключаться между созданием ссылок и записью остальной части выражения. Однако даже если сравнивать одну лишь скорость набора среднего количества символов в поле, мы уже получаем выигрыш. Приплюсуем сюда возможную глубину контекста. А в качестве платы за удобство манипуляций, рассмотренных как ранее, так и позднее в этом тексте, эту разницу вообще можно не рассматривать.\n",
      "\n",
      "Любое заданное в выборке или фильтре поле уже является готовым к изменению выражением, например:\n",
      "\n",
      "Максимально удобная фильтрация\n",
      "Понятно, что при наличии прекрасных средств для манипуляции выборкой работа с ограничителями должна быть не менее простой. Рассмотрим, какие возможности есть в нашем распоряжении.\n",
      "\n",
      "Задание фильтров в несколько кликов\n",
      "Панель работы с фильтрами изначально содержит пустой контейнер для них:\n",
      "\n",
      "\n",
      "\n",
      "В контейнер есть несколько способов добавить фильтр:\n",
      "\n",
      "\n",
      "Создать новый, пустой\n",
      "Нажать на кнопку «в фильтр» в схеме\n",
      "Перетащить поле мышью из схемы\n",
      "Перетащить в контейнер выражение из выборки (будет создан фильтр с копией выражения)\n",
      "\n",
      "\n",
      "В итоге мы получим что-то вроде (фильтры лишь набросаны, но все не-доопределены):\n",
      "\n",
      "\n",
      "\n",
      "Доопределить фильтры можно, выбрав любой из них и затем указать его разновидность и значение:\n",
      "\n",
      "Предиктивный выбор значений\n",
      "Независимо от сложности выражения в левой части, у нас есть возможность быстро разобраться во множестве возможных значений, которые мы собираемся фильтровать. Сделаем наше выражение чуть более броским, и посмотрим, что нам предложит «справочник»:\n",
      "\n",
      "\n",
      "\n",
      "Справочник старательно выдал нам все комбинации для данного выражения.\n",
      "\n",
      "Однако это был простейший случай, поскольку были выбраны все комбинации полей таблиц, которым они принадлежат. Зачастую их слишком много, и нет никакой гарантии, что они вообще окажутся в выборке. Для таких случаев есть два дополнительных режима отбора:\n",
      "\n",
      "\n",
      "\n",
      "Они расположены по возрастанию длительности запроса.\n",
      "\n",
      "«Ожидаемые без фильтра» означает, что будут отобраны только те значения, на которые есть ссылки из таблиц отобранных полей.\n",
      "\n",
      "«Ожидаемые с фильтром» учитывают наложенные фильтры, тем самым дополнительно сужая варианты. Естественно, текущий фильтр при этом отбрасывается.\n",
      "\n",
      "Удалив из предыдущего примера пустой фильтр, и поставив отбор «Ожидаемые с фильтром», мы увидим следующий результат:\n",
      "\n",
      "\n",
      "\n",
      "Отбирать значения мы могли долго и мучительно, каждый раз получая пустую выборку,  т.к. фильтр по данному выражению либо избыточен, либо «несовместим с жизнью».\n",
      "\n",
      "Группировка, перегруппировка и отключение фильтров\n",
      "Мы пока рассмотрели простейший набор из нескольких простых фильтров. Однако панель позволяет группировать их в любые комбинации, выполнять над ними операции объединения по признаку «и», «или», отрицать. Это делается при помощи жеста drag'n'drop и кнопок на панели  инструментов. Каждый фильтр можно отключить, даже корневой. Можно, скажем, создать такую композицию:\n",
      "\n",
      "Добротно и на века\n",
      "То, что единожды было сделано, не хочется повторять снова, по крайней мере если мы говорим о труде. Перечислим же, на какие результаты можно положиться, однажды произведя их.\n",
      "\n",
      "Сохранение конфигурации связей\n",
      "Одна из основ удобства инструмента – контексты таблиц. Если вам пришлось работать над ними вручную, то они сохраняются в файл, который в том числе можно редактировать вручную или носить с собой на другое рабочее место. Однако всем лучше, если эта информация прописана архитектором.\n",
      "\n",
      "Восстанавливаемая структура запроса\n",
      "Созданный запрос можно сохранить и перенести как полноценный «документ»: при открытии в другое время или в другом месте он будет восстановлен во всех мелочах. Единственное естественное ограничение – соединение с базой данных должно быть доступно и ее структура должна совпадать с точностью до использованных полей. Перенос настроек контекста не требуется, копии нужных связей прописаны в документе.\n",
      "\n",
      "Каждый выполненный запрос сохраняется в исторический пул\n",
      "Как только вы выполнили запрос, он добавляется в список ранее выполненных. Это сделано для того, чтобы можно было сравнить результаты между итерациями уточнения полей, условий, группировок и прочего. Кроме набора данных сохраняется структура запроса (как документ) – ее можно восстановить, и оригинальный SQL запрос (он мог быть изменен вручную).\n",
      "\n",
      "Автосохранение и восстановление сессии\n",
      "Наконец, вся история запросов сохраняется на случай аварийного прерывания программы, после перезапуска она целиком восстанавливается.\n",
      "\n",
      "Направления развития\n",
      "В инструменте есть много мелочей, которые здесь не упоминались, поскольку они не являются отличительными особенностями. Кроме того, многие из этих мелочей, возможно, отмерли или отомрут из-за нечастого использования, либо побочных результатов рефакторинга.\n",
      "\n",
      "Пока инструмент малоизвестен, он содержит только то, с чем сталкивается разработчик. При наличии интереса и спроса многое можно починить и доделать.\n",
      "\n",
      "Изначально инструмент развивался как настольный, однако недавно в порядке эксперимента был реализован упрощенный веб-интерфейс к нему. Сам десктопный интерфейс довольно-таки устарел.\n",
      "\n",
      "Ваша реакция, уважаемые читатели, надеюсь, и определит, стоит ли инструменту жить динамичной жизнью, или он лишь иллюстрация\n",
      "еще одной тупиковой ветви развития. Спасибо за внимание!\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\r\n",
      "Привет, Хабр. Сегодня мы хотим предложить вашему вниманию перевод одной очень интересной статьи, в которой поднимается весьма неоднозначная тема. Ни для кого не секрет, что всевозможные корпорации и социальные сети постоянно собирают различную информацию о своих пользователях и посетителях, включая логи их поведения в сети. \n",
      "\r\n",
      "Неустанно протоколируется каждое наше действие, как онлайн, так и, зачастую, оффлайн. Впоследствии эти данные анализируются и используются всеми заинтересованными лицами, в том числе с получением прибыли. На нас зарабатывают социальные сети и рекламные агентства, но мы не имеем ни малейшей выгоды от этого. \n",
      "\r\n",
      "К сожалению, интернет ничего не забывает. Всё, что попало в сеть, почти наверняка останется в ней навсегда. Даже не надейтесь на «забывчивость» социальных сетей и поисковиков, они не уничтожают данные, они их только накапливают. Многие оффлайновые компании распространяют личные данные своих клиентов: стоит оставить в какой-то анкете свой номер телефона, как через некоторое время посыплются рекламные SMS и звонки от совершенно других контор. Увы, в современном мире не осталось места таким вещам, как конфиденциальность и элементарные правила приличия в отношении распространения персональной информации. Это привело к тому, что в мире становится всё больше сторонников законодательного закрепления «права быть забытым». Этот термин означает процедуру принудительного удаления какой-либо информации о человеке из поисковой выдачи, если эти данные перестали быть актуальными и наносят ущерб репутации. Представьте, что вы, например, лет 15-20 назад обанкротились и распродавали своё имущество. С тех пор дела у вас наладились, вы встали на ноги и начали жизнь с чистого листа. Но и по сей день при вводе вашего имени в поисковик одним из первых будет выдаваться сообщение о том, что вы пошли по миру. Неприятно, правда?\n",
      "\r\n",
      "К борьбе за «право быть забытым» недавно активно присоединились европейские политики, и теперь на Google начинают оказывать давление, требуя удалить некоторые данные из поисковой выдачи. Пока что это единичные случаи, но лиха беда начало. Пожалуй, это один из первых в истории случаев, когда совпали устремления политиков и простых граждан. В данном случае, чиновниками движет желание ограничить распространение информации об их неблаговидных, с точки зрения потенциальных избирателей, деяниях. Но суть та же — никто не хочет, чтобы неприятные подробности его жизни всегда вылезали на первую страницу Гугла или Яндекса. \n",
      "\r\n",
      "По другому пути пошли отечественные законодатели. Скандальный закон, требующий хранить персональные данные россиян исключительно на российских серверах, воспринят общественностью в штыки.\n",
      "\n",
      "\n",
      "\r\n",
      "Поскольку на нас всё равно кто-то наживается, то почему бы не отщипнуть кусочек от этого финансового пирога? И первые шаги в этом направлении уже сделаны: стартап Citizenme разработал приложение, которое устанавливается на смартфон и собирает различную информацию о своём владельце, которую потом… можно продать. \n",
      "\r\n",
      "Авторы проекта поставили своей долгосрочной целью создать удобный механизм для продажи информации о нашей собственной активности напрямую тем компаниям, которые мы выберем самостоятельно. Инструмент для сбора и анализа данных представляет собой мобильное приложение, которое подключено к аккаунтам владельца в различных социальных сетях. При этом авторы проекта, в первую очередь, стараются таким образом предупредить людей, помочь им понять, какие данные о них собираются и как потом используются.\n",
      "\r\n",
      "Несмотря на раздражающее обилие рекламы, встречающейся на просторах сети, именно она является питательной средой для интернета, благодаря рекламе становится возможным финансирование огромного количества онлайн-проектов. Однако политика агрессивного сбора информации о пользователях сделала людей недоверчивыми. И авторы Citizenme надеются, что их детище позволит пользователям взять под контроль процесс сбора информации о них самих.\n",
      "\n",
      "Как это работает\n",
      " Для начала необходимо прописать в приложении все ваши аккаунты в соцсетях. Эти данные будут храниться только на смартфоне, а не на сервере Citizenme. На текущий момент приложение поддерживает Facebook, LinkedIn и Twitter, другие соцсети будут добавлены позднее.\n",
      "\r\n",
      "Здесь наглядно отражается информация о вас, попадающая в общий доступ, поэтому вы сможете точнее настроить параметры безопасности в каждой из соцсетей. Более того, в приложении содержится информация, в которой подробно описываются нюансы политики предоставления услуг каждой из поддерживаемых систем. Наиболее важные настройки, например, правила использования Фейсбуком ваших фотографий и прочего контента, выделены красным цветом. И когда компании изменяют условия пользования, Citizenme предупреждает вас об этом и даёт возможность дать свою оценку. Правда, это совершенно символическая функция.\n",
      "\r\n",
      "Авторы проекта совместно с Кембриджским Университетом разработали серию тестов, которые позволяют понять, как разные соцсети анализируют ваши данные. Например, приложение способно предсказать ваши политические предпочтения с точки зрения того или иного ресурса. Если это и не слишком полезно, то как минимум любопытно, и позволяет понять, почему вам показывают ту или иную рекламу.\n",
      "\n",
      "Планы на будущее\r\n",
      "Сейчас приложение разработано только под iOS, но в следующем месяце авторы планируют выпустить версию под Android. Разработчики намерены охватить все операционные сети и, с вашего согласия, собирать гораздо больше информации, включая местоположение, статистику с фитнес-трекеров и многое другое. Это, как минимум, позволит вам самим многое узнать о своём цифровом портрете, на который ориентируются рекламодатели. Ну, и заодно позволит вам продавать больше данных о себе.\n",
      "\r\n",
      "Дабы построить прозрачные отношения со своими пользователями, разработчики собираются выложить полные коды клиентского приложения и, возможно, серверного. Также планируется создать независимую некоммерческую организацию, которая будет контролировать все изменения в условиях использования приложения. Этот шаг также будет направлен на завоевание доверия пользователей, которые смогут голосовать за каждое нововведение.\r\n",
      "Финансирование проекта будет осуществляться за счёт комиссии с прибыли, получаемой пользователями от продажи своих данных. Если вы не захотите «торговать собой», то нужно будет оплачивать подписку. \n",
      "\n",
      "Большой Вопрос\n",
      "\n",
      "\r\n",
      "Наверное, вы уже на раз задали его себе, читая этот материал: а захочет ли кто-нибудь вообще продавать информацию о себе? Авторы проекта уверены, что желающие найдутся. По мнению основателя проекта, пользователей можно разделить на две большие группы. В первую входят люди старших возрастов, которым нужен полный контроль над информацией о себе в сети, и которые с радостью будут оплачивать подписку. Вторую группу составляет молодёжь, которая не придаёт значения распространению информации о себе и предпочтёт заработать на этом.\n",
      "\r\n",
      "Также разработчики подчёркивают, что получение прибыли и отслеживание своих данных в сети не является единственными видами применения их приложения. Например, вы хотите приобрести машину, и опубликовав соцсетях своё намерение, вы вскоре начнёте получать рекламные предложения и информацию о скидках на автомобили. И Citizenme позволит точно контролировать подобную активность и дозировать информацию. При этом все стороны будут довольны: рекламодатели получат точечные проверенные данные о потенциальном клиенте, а пользователь сэкономит на приобретении. Экосистема в выигрыше со всех сторон.\n",
      "\n",
      "\n",
      "\r\n",
      "Надо сказать, что Citizenme не является первой попыткой такого рода. Некоммерческая организация Attention Trust, уже прекратившая своё существование, пыталась разработать инструмента для сбора и продажи данных ещё в 2005 году. Но, похоже, эта попытка несколько опередила время. Сегодня смартфоны и соцсети стали вездесущими, а объём данных, собираемых в интернете, возрос многократно. Поэтому авторы Citizenme оптимистично оценивают будущее своего приложения.\n",
      "\r\n",
      "Кто знает, может быть, скоро появятся целые рынки и биржи для самостоятельной торговли данными о себе. И если сейчас речь идёт о продаже и контроле над вашими данными, опубликованными в сети, то следующим логичным шагом будет добавление функции «информационного трекера». Вполне возможно, скоро подобные инфотрекеры станут не менее популярны, чем фитнес-трекеры.\n",
      "\r\n",
      "Однако ряд пользователей не захотят ничего продавать и предпочтут максимально ограничить сбор данных о себе. И им необходимо будет предложить инструменты для подобной защиты. Такие механизмы прорабатываются в Yota Devices в рамках так называемой концепции «безопасного смартфона». Об этом мы расскажем в одной из наших следующих публикаций.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. А как вы относитесь к идее самостоятельной продажи данных о себе, о своей активности в сети? \n",
      "            19.17%\n",
      "           Хорошо отношусь, можно подзаработать. \n",
      "            116\n",
      "           \n",
      "            55.21%\n",
      "           Плохо отношусь, не хочу, чтобы обо мне слишком много знали посторонние. \n",
      "            334\n",
      "           \n",
      "            25.62%\n",
      "           Затрудняюсь, пока сам не знаю, как этому относиться. \n",
      "            155\n",
      "            \n",
      "       Проголосовали 605 пользователей. \n",
      "\n",
      "       Воздержались 99 пользователей. \n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Сегодня лекция одного из самых известных в России специалистов по машинному обучению Дмитрия Ветрова, который руководит департаментом больших данных и информационного поиска на факультете компьютерных наук, работающим во ВШЭ при поддержке Яндекса. \n",
      "\n",
      "Как можно хранить и обрабатывать многомерные массивы в линейных по памяти структурах? Что дает обучение нейронных сетей из триллионов триллионов нейронов и как можно осуществить его без переобучения? Можно ли обрабатывать информацию «на лету», не сохраняя поступающие последовательно данные? Как оптимизировать функцию за время меньшее чем уходит на ее вычисление в одной точке? Что дает обучение по слаборазмеченным данным? И почему для решения всех перечисленных выше задач надо хорошо знать математику? И другое дальше.\n",
      "\n",
      "\n",
      "\n",
      "Люди и их устройства стали генерировать такое количество данных, что за их ростом не успевают даже вычислительные мощности крупных компаний. И хотя без таких ресурсов работа с данными невозможна, полезными их делают люди. Сейчас мы находимся на этапе, когда информации так много, что традиционные математические методы и модели становятся неприменимы. Из лекции Дмитрия Петровича вы узнаете, почему вам надо хорошо знать математику для работы с машинным обучением и обработкой данных. И какая «новая математика» понадобится вам для этого. Слайды презентации — под катом.\n",
      "\n",
      "\n",
      "\n",
      "Дмитрий Ветров окончил ВМиК МГУ, кандидат физико-математчиеских наук. Автор более 120 научных публикаций. Дмитрий Петрович разработал курсы «Байесовские методы машинного обучения» и «Графические модели», которые читает в МГУ и в Школе анализа данных Яндекса. Принимал участие в нескольких междисциплинарных исследовательских проектах по разработке новых методов машинного обучения и вероятностного вывода (когнитивные науки, медицина, неорганическая химия, и др.). Руководит исследовательской группой байесовских методов.    \n",
      "  Если вашей организации нужно хранить и обрабатывать данные, то, независимо от их объёма, без облачной или локальной СУБД не обойтись. Сегодня мы поговорим о ведущих представителях рынка корпоративных баз данных, о тех разработках, на которые стоит обратить внимание в 2016-м году.\n",
      "\n",
      "Рынок корпоративных СУБД существует уже несколько десятилетий. Полагаем, оценивая ту или иную систему, нелишним будет, кроме прочего, учитывать и её историю. Но зрелость рынка не значит, что в наши дни он – место тихое и спокойное. Уровень конкуренции здесь очень высок.\n",
      "\n",
      "Требования бизнеса постоянно растут, меняются подходы к созданию IT-инфраструктур. Разработчики СУБД либо успевают дать рынку то, чего он хочет, либо оказываются на периферии. А пользователи баз данных, благодаря возможности выбора, от подобного положения дел лишь выигрывают.\n",
      "\n",
      "Нельзя сказать, что выбрать корпоративную СУБД просто, но мы уверены, что среди ведущих систем, о которых мы расскажем, вы сможете найти ту, которая вас заинтересует.\n",
      "\n",
      "Oracle Database\n",
      "\n",
      "Oracle выпустила свою первую реляционную СУБД в 1979-м году. За годы присутствия компании на рынке слово «Oracle» стало синонимом понятий «корпоративная СУБД» и «надёжная работа с данными». Oracle, как, кстати, и некоторые другие компании, разработки которых присутствуют в нашем обзоре, входит в список Fortune 500. Главное направление деятельности Oracle – мощное и довольно сложное решение в области баз данных.\n",
      "\n",
      "Текущая версия СУБД компании называется Oracle 12c. Буква «c» означает «cloud». Это отражает движение Oracle в сторону облачных технологий, которые позволяют организациям консолидировать базы данных и управлять ими как облачными службами. Среди особенностей СУБД Oracle – многоарендная архитектура, быстрое развёртывание решений, возможности по обработке данных в памяти.\n",
      "\n",
      "Microsoft SQL Server\n",
      "\n",
      "О Microsoft можно говорить всё, что угодно, но это – одна из самых прибыльных технологических компаний в мире. Её настольные операционные системы буквально повсюду, однако, не последнюю роль в успехе Microsoft сыграл SQL Server. Трудно представить себе сервер, на котором установлена ОС от Microsoft, без SQL Server.\n",
      "\n",
      "Простота использования SQL Server, его доступность и тесная интеграция с ОС семейства Windows, делают его очевидным выбором для компаний, пользующихся продуктами Microsoft для корпоративных целей. Microsoft говорит о свежем выпуске SQL Server 2016 как о платформе для локальных и облачных баз данных, а так же для систем бизнес-аналитики.\n",
      "\n",
      "Кроме того, Microsoft занимается продвижением SQL Server 2016 как решения, способного помочь организациям в построении критически важных приложений для оперативной обработки транзакций (OLTP). Такие решения характеризуются высокой производительностью, возможностями по обработке данных в памяти и по защите данных при их хранении и перемещении. На базе SQL Server 2016 можно создавать хранилища большого объема, системы анализа данных. \n",
      "\n",
      "Есть различные варианты аренды MS SQL Server. Можно приобрести лицензию на одного пользователя, можно лицензировать ядра сервера, без ограничения количества пользователей.\n",
      "\n",
      "Можно также скачать бесплатный выпуск начального уровня SQL Server 2016 Express, который оптимально подходит для развертывания небольших баз данных в рабочих средах и установить его на недорогой VPS, этого вполне достаточно для создания небольших серверных приложений для обработки данных, занимающих до 10 ГБ места на диске.\n",
      "\n",
      "IBM DB2\n",
      "\n",
      "Говорят, что компания IBM получила прозвище «Голубой гигант» из-за своих мейнфреймов 50-х 60-х годов, которые занимали целую комнату и были выкрашены в соответствующий цвет. Усилия компании по продвижению СУБД DB2, её безжалостная конкурентная борьба с Oracle, позволяют предположить, что IBM хочет заполнить компьютерами, на которых установлена её СУБД, многие и многие серверные комнаты. Есть исследования, посвящённые сравнению сопоставимых решений от IBM и Oracle. Например, здесь можно взглянуть на данные отчёта, который подготовила ITG. Отчёт говорит о серьёзной экономии при использовании DB2 и сопутствующих технологий IBM.\n",
      "\n",
      "В апреле сего года вышла свежая версия СУБД – DB2 11.1. Она может работать на многих системах, в частности, на Linux, Unix и Windows, на мейнфреймах IBM z Systems, поддерживает аппаратное ускорение на процессорах Power 8.\n",
      "\n",
      "SAP ASE\n",
      "\n",
      "Первый выпуск СУБД Sybase увидел свет в 1987 году под именем SQL Server. Он вырос в Adaptive Server Enterprise – самый известный и успешный продукт компании. В 2010-м Sybase была поглощена SAP, в итоге сегодня база данных называется SAP Adaptive Server Enterprise (SAP ASE). Хотя Sybase и стала частью другой компании, её СУБД всё ещё является одним из основных игроков корпоративного рынка. Кроме того, Sybase известна тем, что направляла значительные усилия на мобильные корпоративные решения, как правило, покупая разработчиков перспективных продуктов. Судя по всему, SAP продолжает эту традицию.\n",
      "\n",
      "SAP ASE 16, самый свежий выпуск СУБД, поддерживает технологию блокировок на уровне разделов, интеграцию с SAP HANA и SAP Business Suite. База данных отличается экономичностью, возможностями эффективного масштабирования и системного аудита, поддерживает динамическое назначение потоков, оптимизацию плана запросов с соединениями типа «звезда» и многие другие возможности.\n",
      "\n",
      "PostgreSQL\n",
      "\n",
      "PostgreSQL, наследник Postgres, разработка которой началась в 1986-м году, – это бесплатная объектно-реляционная СУБД с открытым исходным кодом. Она находит применение в весьма интересных местах, как интернет-казино, системы автоматизации дата-центров, реестры доменов. Кроме того, её используют в высоконагруженных задачах Yahoo! и Skype. PostgreSQL можно обнаружить в таком количестве скрытых от постороннего взгляда систем, что она вполне может называться «Секретной корпоративной базой данных».\n",
      "\n",
      "Текущий стабильный релиз PostgreSQL – 9.6. Он выпущен 29 сентября этого года.\n",
      "\n",
      "PostgreSQL поддерживает множество операционных систем. Среди них – Linux, Windows, FreeBSD, Solaris. PostgreSQL используется как стандартная база данных в ОС от Apple, начиная с Mac OS X Lion. Возможности этой СУБД сравнимы с разработками Oracle и IBM. В частности, она характеризуется полным соответствием требованиям ACID по надёжности транзакций, способна поддерживать высокие параллельные нагрузки.\n",
      "\n",
      "MariaDB Enterprise\n",
      "\n",
      "MariaDB Enterprise – это СУБД с полностью открытым исходным кодом, выпущенным под лицензиями GPL, LGPL или BSD. Путь MariaDB начался в 2009-м году. Эта система была форком MySQL, над которым работало сообщество разработчиков под руководством создателей MySQL. Они начали новый проект, обеспокоенные политикой лицензирования Oracle, которая приобрела MySQL.\n",
      "\n",
      "Популярной MariaDB стала за счёт MySQL. В частности, после того, как MariaDB заняла место MySQL в известных дистрибутивах Linux. Так, только в 2013 году, разработчики Red Hat Enterprise Linux отказались от MySQL в пользу MariaDB, то же самое было сделано в Fedora 19, на MariaDB перешли openSUSE и Slackware Linux. Кроме того, MariaDB стала использоваться в качестве серверной базы данных в проекте Wikipedia.\n",
      "\n",
      "Ещё один важнейший фактор, повлиявший на то, что MariaDB оказалась впереди MySQL, заключается в наличии расширенного оптимизатора запросов и других улучшений, касающихся скорости работы.\n",
      "\n",
      "Самый свежий релиз MariaDB Enterprise Server – 10.1, известный так же как MariaDB Enterprise Spring 2016. Этот выпуск улучшает защиту данных от атак на уровне приложений и сетей и способствует разработке новых, высокопроизводительных приложений.\n",
      "\n",
      "MySQL\n",
      "\n",
      "MySQL начинала как узкоспециализированное решение для разработчиков, но выросла в одного из ключевых игроков рынка корпоративных СУБД. Сначала, в 2008-м, она была продана Sun Microsystems, позже, в 2009-м, стала частью империи Oracle. Уже много лет MySQL – это нечто гораздо большее, нежели нишевое решение. На MySQL работают сотни тысяч коммерческих веб-сайтов, она служит в качестве серверной СУБД для огромного количества внутренних корпоративных приложений.\n",
      "\n",
      "Сегодня MySQL остаётся весьма популярным вариантом для веб-решений, она продолжает служить центральным компонентом стека LAMP. В то же самое время, MySQL, из-за поглощения Oracle, испытывает ослабление поддержки со стороны пользователей и независимых разработчиков.\n",
      "\n",
      "Падение популярности MySQL позволило ускорить адаптацию других СУБД с открытым исходным кодом и её собственных форков, наподобие Percona или вышеупомянутой MariaDB с полностью открытым исходным кодом, в которой нет закрытых модулей, появившихся в новых версиях MySQL Enterprise Edition.\n",
      "\n",
      "Самый свежий выпуск этой СУБД, MySQL Community Server 5.7.15, увидел свет в начале августа 2016-го.\n",
      "\n",
      "Teradata Database\n",
      "\n",
      "Слышали о компании Teradata? Если вы занимались созданием большого хранилища данных, то, скорее всего, слышали. Teradata ведёт историю с конца 1970-х. Именно тогда она начала работу над решениями, которые позже стали называться «хранилищами данных». В 1992 году Teradata построила первую терабайтную БД для Wal-Mart. С тех времён слово «Teradata» постоянно упоминается в разговорах экспертов по корпоративным системам хранения данных.\n",
      "\n",
      "Возможности Teradata Database относятся к сфере очень больших баз данных. Эта система отлично подходит для поддержки таких популярных в последнее время течений, как исследование больших данных, бизнес-аналитика, интернет вещей. Teradata выпустила версию 15.10 своей реляционной СУБД в начале 2015-го.\n",
      "\n",
      "IBM Informix\n",
      "\n",
      "\n",
      "Как видите, в нашем списке имеется ещё одна разработка IBM. Компания предлагает широкий диапазон вариантов объектно-реляционной СУБД Informix. Её продвигают как интеллектуальную базу данных, которую можно развернуть на множестве платформ.\n",
      "\n",
      "Часто ассоциируемая с учебными заведениями, Informix пришла в корпоративный мир и заняла первое место по удовлетворённости клиентов. Пользователи этой СУБД обычно весьма лестно выражаются о её низкой стоимости, о необходимости небольшого объёма технического обслуживания и высокой надёжности.\n",
      "\n",
      "Ingres\n",
      "\n",
      "Реляционная СУБД Ingres весьма актуальна на корпоративном рынке, кроме того, она – один из старожилов мира баз данных. Работа над ней началась в начале 1970-х.\n",
      "\n",
      "Ingres имеет привлекательную модель формирования цены, что может означать уменьшение общей стоимости владения системой. Эта система может похвастаться продвинутыми возможностями по переходу на неё с более дорогих СУБД. Кроме того, она отличается высокими характеристиками безопасности, необходимыми для соответствия требованиям HIPAA и закону Сарбейнза – Оксли.\n",
      "Самая свежая корпоративная версия Ingres – это 10.2. Она вышла в 2015-м, представив поддержку геопространственных типов данных, удалённых GCA-клиентов, транслитерации UTF-8, DBMS-аутентификации и других новых возможностей.\n",
      "\n",
      "В июне 2016 года была выпущена для тестирования Ingres 11 Technical Preview.\n",
      "\n",
      "Amazon SimpleDB\n",
      "\n",
      "Это – уже одиннадцатый пункт в нашем списке. Можете считать его десертом к основным десяти блюдам.\n",
      "На первый взгляд понятия «Amazon» и «СУБД» могут показаться не вполне совместимыми, но на самом деле это не так, особенно сегодня, в эру облачных вычислений. SimpleDB (Simple Database Service) предлагает организациям простую, гибкую и недорогую альтернативу традиционным СУБД.\n",
      "\n",
      "SimpleDB позволяет пользователям хранить данные и работать с ними посредством запросов к веб-сервисам. Она отличается масштабируемостью, высокой скоростью, минимальными требованиями по обслуживанию и интеграцией с другими службами Amazon. Начать работу с SimpleDB можно бесплатно.\n",
      "\n",
      "Итоги\n",
      "Нет единственно правильного решения для всех задач по работе с данными. Не существует и идеальной корпоративной СУБД. У каждой из них есть свои плюсы и минусы, которые, к тому же, очень сильно зависят от специфики бизнеса.\n",
      "\n",
      "Лучшее, что можно сделать для того, чтобы выбрать подходящую СУБД – проанализировать всё, что предлагает рынок, с учётом особенностей конкретной организации.\n",
      "\n",
      "Надеемся, наш рассказ о ведущих корпоративных СУБД 2016-го года внёс посильный вклад в дело выбора системы, которая вам подойдёт.      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Какая база данных используется в вашем бизнесе? \n",
      "            25.77%\n",
      "           Oracle \n",
      "            201\n",
      "           \n",
      "            43.08%\n",
      "           Microsoft SQL Server \n",
      "            336\n",
      "           \n",
      "            3.21%\n",
      "           IBM DB2 \n",
      "            25\n",
      "           \n",
      "            2.44%\n",
      "           SAP ASE \n",
      "            19\n",
      "           \n",
      "            41.79%\n",
      "           PostgreSQL \n",
      "            326\n",
      "           \n",
      "            8.97%\n",
      "           MariaDB Enterprise \n",
      "            70\n",
      "           \n",
      "            34.87%\n",
      "           MySQL \n",
      "            272\n",
      "           \n",
      "            1.54%\n",
      "           Teradata Database \n",
      "            12\n",
      "           \n",
      "            0.9%\n",
      "           IBM Informix \n",
      "            7\n",
      "           \n",
      "            0.38%\n",
      "           Ingres \n",
      "            3\n",
      "           \n",
      "            0.51%\n",
      "           Amazon SimpleDB \n",
      "            4\n",
      "           \n",
      "            9.1%\n",
      "           Другая \n",
      "            71\n",
      "            \n",
      "       Проголосовали 780 пользователей. \n",
      "\n",
      "       Воздержались 172 пользователя. \n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      "На днях произошло, в какой-то степени, знаменательное событие и одна из крупнейших компаний России заявила о том что теперь публикует открытые данные на своем сайте. Этой компанией является Сбербанк и соответствующий раздел на их сайте. Открытие раздела удостоилось пресс-релиза на их сайте и о нем, как о важном событии, написали десятки финансовых и не финансовых СМИ.\n",
      "Действительно ли Сбербанк совершил нечто невероятное? Рядовое ли это явление и является ли то что сделал Сбербанк сейчас открытыми данными? Вот о чем далее пойдет речь.\n",
      "В качестве вступления\n",
      "Прежде чем продолжить о Сбербанке, давайте вернемся к термину открытые данные. \n",
      "1-е официальное определение из закона 112-ФЗ (это поправки к 8-ФЗ) \n",
      "Информация, размещаемая ее обладателями в сети \"Интернет\" в формате, допускающем автоматизированную обработку без предварительных изменений человеком в целях повторного ее использования, является общедоступной информацией, размещаемой в форме открытых данных.\n",
      "2-е определение из Википедии\n",
      "Открытые данные (англ. open data) — концепция, отражающая идею о том, что определённые данные должны быть свободно доступны для машиночитаемого использования и дальнейшей републикации без ограничений авторского права, патентов и других механизмов контроля. Освободить данные от ограничений авторского права можно с помощью свободных лицензий, таких как лицензий Creative Commons. Если какой-либо набор данных не является общественным достоянием, либо не связан лицензией, дающей права на свободное повторное использование, то такой набор данных не считается открытым, даже если он выложен в машиночитаемом виде в Интернет.\n",
      "3-е из хартии открытых данных\n",
      "Open data is digital data that is made available with the technical and legal characteristics necessary for it to be freely used, reused, and redistributed by anyone, anytime, anywhere.\r\n",
      "Или на сумбурном русском:\n",
      "Открытые данные — это цифровые данные сделанные общедоступными с техническими и юридическими характеристиками обязательными для того чтобы они свободно использовались, использовались повторно и распространялись кем угодно, когда угодно и где угодно\n",
      "Также, у открытых данных есть четко сформулированные принципы их публикации, отраженные как раз в хартии открытых данных.\r\n",
      "Эти принципы:\n",
      "\n",
      "Открытость по умолчанию\n",
      "Своевременно и полно\n",
      "Доступно и удобно\n",
      "Сравнимо и интегрируемо\n",
      "Для улучшения управления и вовлечения граждан\n",
      "Для развития и инноваций\n",
      "\n",
      "За те 7 лет что я лично занимаюсь темой открытых данных в России я слышал и видел как открытыми данными называли очень и очень многое что ими не является. Самый выдающийся по глупости вопрос был в том что, когда определение дается через описание \"свободно доступных машиночитаемых данных\", то вопрос \"А машиночитаемые данные — это те которые я могу в машине прочитать?\".\n",
      "Но во всех определениях важно помнить одно — открытые данные ориентированны на технологически квалифицированного потребителя. Государство не производит само новых информационных продуктов, оно дает возможность это делать стартапам, ИТ компаниям и общественникам.\n",
      "Почему публикуют открытые данные?\n",
      "Чтобы разобрать этот конкретный случай важно знать зачем вообще владельцы данных их публикуют? Особенно компании и госорганы — иногда это может показаться совершенно странным. \n",
      "Пиар. Обязательства или Выгода\n",
      "Это три главные причины почему кто-либо данные публикует (вопросы фана и тщеславия я сознательно оставляю за скобками). \n",
      "И если Вы видите активность какой-либо организации в открытых данных, да и в вообще в вопросах открытости и прозрачности, то ищите ответ в одной из этих трех причин. \n",
      "Пиар\n",
      "Например, как устроен пиар на открытых данных. Главная его отличительная способность ориентация на массового потребителя, массового избирателя, массового гражданина. \n",
      "Вопросы технологий и данных остаются в стороне. Вопросы посещаемости, медийного охвата, число статей с упоминанием — выходят на первое место. \n",
      "Живой пример — это портал открытых данных Москвы — власти города распространяют новости о публикациях даже если там размещен какой-нибудь бессмысленный набор данных из 28 строк. \n",
      "Обязательства\n",
      "Обязательства или принуждение — это когда открытые данные публикуются потому что закон требует их публикации. Владелец данных не всегда может быть заинтересован в открытости, но он соблюдает требования закона и их публикует. \n",
      "Например, Центробанк собирает с банков формы отчетности и раскрывает в специальном разделе на сайте — это нормативно закрепленные обязательство банков и ЦБ.\n",
      "Другой пример — упоминавшийся выше 112-ФЗ и 8-ФЗ. Органы власти обязаны раскрывать базовые наборы данных и публикуют их именно как их обязательства за неисполнение которых они несут ответственность перед законом. \n",
      "Обязательство — это фундамент открытости. Именно по этой причине многие из тех кто обязаны раскрывать данные не предпринимают дополнительные действия по их доступности. Они только соблюдают обязательные требования, но не пишет об этом рекламных пресс-релизов. \n",
      "Например, если Правительство Москвы публикует набор данных с адресами 28 военторгов и распространяет это по новостным сайтам, то совершенно не факт что, например, декларации о доходах чиновников города они опубликуют как открытые данные и также распространят по СМИ. \n",
      "Иначе говоря — обязательство исполняются тихо и незаметно, настолько, насколько это возможно\n",
      "Выгода\n",
      "Зачем кому-то может быть выгодна публикация собственных данных? Казалось бы — владей и молчи, кому-то еще знать совершенно необязательно. \n",
      "Тем не менее есть причины почему открытые данные публикуются государственными и коммерческими структурами. Например, раздел Datasets в Kaggle заполняется в поисках новых находок, решений и инсайтов для которых нужны тысячи data scientist'ов.\n",
      "Или почему Федеральное Казначейство распространяет вот уже много лет данные с портала госзакупок через FTP сервер (еще до историй с открытыми данными) — потому что это проще и дешевле при распространении базы данных необходимой сотням контрагентов в субъектах федерации. \n",
      "Какие-то компании организуют хакатоны и ищут себе сотрудников. Другие публикуют открытые данные для поддержания репутации в сообществе, как это делает Google в их Transparency Report\n",
      "Так что же Сбербанк?\n",
      "Если Вы снова посмотрите на раздел открытых данных Сбербанка, то обнаружите следующие особенности:\n",
      "Нет свободных лицензий\n",
      "Вместо свободы использования и распространения там только отказ от ответственности звучащий как \n",
      "Представленная информация — результат анализа данных ПАО Сбербанк, 4 квартал, 2016 год. Данные не являются управленческой, бухгалтерской, финансовой отчетностью. При использовании ссылок на указанную информацию упоминание ПАО Сбербанк обязательно. Не является рекламой.\n",
      "Что не имеет даже близкого отношения к свободным лицензиям\n",
      "Нет наборов данных\n",
      "Чтобы скачать данные нужно на графике найти специальную кнопку и там в меню еще найти раздел выгрузки в XLSX, CSV или JSON. Особенность в том что все эти выгрузки — это выгрузки из Javascript файлов выполняемые на стороне клиентов. \n",
      "Все данные, по факту, хранятся в 13 Javascript файлах начиная с http://www.rdatascience.ru/opendata/data1.js и до http://www.rdatascience.ru/opendata/data13.js\n",
      "А выгрузка в CSV и тд делается с помощью Javascript кода. И выкачать какой либо набор данных напрямую невозможно. Акцент сделан на визуализацию, а не на работу с данными аналитиками.\n",
      "Отсутствует описание наборов\n",
      "Несмотря на то что на сайте даже используют термин \"Паспорт датасета\" который активно используется в реальных паспортах наборов данных на государственных порталах, конечно же ничего такого там нет. Ни информации об ответственных, ни описания структуры наборов — ничего нет\n",
      "Продажа услуг и смешение с большими данными\n",
      "Раздел заканчивается продажей исследований Сбербанка и тем что все это сделано на больших данных. А сам формат подачи больше похож на лонгрид какого-то инфобизнеса, а не раздел открытых данных.\n",
      "Выводы\n",
      "Из всего этого можно сделать лишь один вывод — целью Сбербанка для этого раздела был только пиар и ничего более. Хочется только надеяться что когда-нибудь Сбербанк найдет форму работы с открытыми данными которая приносила бы выгоду и им и сообществу. Потому как пока это более похоже на попытку воспользоваться популярным термином для раскрутки своих коммерческих услуг    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Визуализация данных стала неотъемлемой частью жизни практически каждого веб-разработчика. Если построение графиков, диаграмм, карт и дашбордов до сих пор никогда не были вашей головной болью, просто немного подождите: наверняка и вы скоро вступите в наш «клуб».\n",
      "\r\n",
      "Данная статья дает общее, но вовлеченное представление о восьми самых интересных, на мой взгляд, JavaScript-библиотеках для построения интерактивных геовизуализаций. В целом, таких решений сейчас много, и выбрать оптимальное под тот или иной конкретный проект – задача порой непростая как минимум по времени. Этой публикацией я попытаюсь хотя бы немного упростить жизнь тем, кто только начинает разбираться в данной теме. Кстати, это слегка модифицированный перевод моей недавней статьи на Onextrapixel (оригинал на английском).\n",
      "\n",
      "\n",
      "\r\n",
      "На всякий случай сразу уточню: вообще, JavaScript-библиотеки для создания карт можно разделить на два типа. Одни просто позволяют отображать географическое положение одного или нескольких объектов. Для подобной задачи можно использовать карты типа Google Maps или OpenStreetMaps в качестве источника. В принципе, таких решений достаточно, и результат их работы примерно таков:\n",
      "\n",
      "\n",
      "\r\n",
      "Другой тип – и о нем я хочу сегодня поговорить – JavaScript-библиотеки, с помощью которых можно делать именно визуализацию данных посредством создания красивых интерактивных карт. Они показывают либо связи между величинами в разных географических регионах, либо перемещение каких-либо объектов относительно их местоположения, и т.п. – всё то, что крайне важно в плане анализа данных и business intelligence.\n",
      "\n",
      "JavaScript-библиотеки для интерактивных карт-визуализаций\n",
      "amMap (от amCharts)\n",
      "\n",
      "\n",
      "amMap – специальная JavaScript (HTML5) библиотека для построения карт, разработанная командой amCharts. Она не нуждается в каких-либо внешних зависимостях и позволяет довольно просто делать красивые choropleth, bubble, dot (point), connector и flow карты, поддерживая много полезных интерактивных фич.\n",
      "\r\n",
      "В частности, amMap дает возможность легко «погружаться» в тот или иной выбранный участок карты с помощью drill down или, например, использовать любые картинки в качестве маркеров – довольно любопытная вещь.\n",
      "\r\n",
      "Для начала работы с amMap нужно скачать ZIP-файл с бинарниками. Кстати, в нем уже лежит по 455 карт в каждом из поддерживаемых форматов (JavaScript/SVG). Документация у amMap не очень большая и сводится по большей части к набору вопросов и ответов.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: нет.\n",
      "Лицензия: бесплатно (брендированные карты) или платно (от $140).\n",
      "\n",
      "AnyMap (от AnyChart)\n",
      "\n",
      "\n",
      "AnyMap – одна из популярных JavaScript (HTML5) библиотек для визуализации данных, созданных компанией AnyChart. Как и в случае с amMap, для создания карты с ее помощью не требуется больших усилий, равно как и подключения каких-либо внешних ресурсов или сторонних библиотек (jQuery и т.д.). AnyMap также поддерживает все базовые типы серий, который только могут понадобиться: choropleth, bubble, dot, connector, flow.\n",
      "\r\n",
      "В число основных интерактивных фич AnyMap входят drill down для более плотного изучения отдельных районов, событийная модель для обработки событий (пользовательской интерактивности), цветовая шкала для автоматического раскрашивания карты по заданным настройкам.\n",
      "\r\n",
      "Забегая немного вперед, скажу, что, на мой взгляд, AnyMap больше похожа на D3, нежели на amMap или Highmaps. Она как бы сочетает в себе возможности «коробочных» решений и в то же время является хорошо расширяемой и контролируемой со стороны разработчика, позволяет полностью кастомизировать визаулизации путем добавления или изменения различных элементов и легко создать собственные карты.\n",
      "\r\n",
      "Бинарники AnyMap находятся в ZIP-файле и содержат множество примеров, а на CDN у AnyChart доступны сотни карт в самых разных форматах: GeoJSON, TopoJSON, SHP, SVG. Также стоит заметить, что у этой библиотеки богатая документация и обширное описание API, кстати, довольно легкое для понимания и – опять же – с большим количеством сэмплов.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: нет.\n",
      "Лицензия: бесплатно (брендированные карты) или платно (от $79).\n",
      "\n",
      "D3.js (от Mike Bostock)\n",
      "\n",
      "\n",
      "D3.js – мощная в плане возможных результатов библиотека для визуализации данных с открытым исходным кодом. В отличие от других упомянутых JavaScript-библиотек, D3 – это, скорее, фреймворк, который дает максимальную свободу творчества. Хотя создание интерактивных карт здесь не так очевидно, как, скажем, в решениях от amCharts, AnyChart или Highcharts.\n",
      "\r\n",
      "В частности, чтобы сделать такую замечательную интерактивную карту, как на иллюстрации, потребуется изучить достаточно большое количество материалов. Хотя, надо сказать, это стоит того, потому что в итоге можно сделать реально потрясающие, оригинальные визуализации.\n",
      "\r\n",
      "Фактически в D3 доступно множество фич, среди которых полная кастомизация поведения, событийная модель и т.д. В целом, я бы сказал, что это – идеальное решение для веб-приложений. Хотя, к сожалению, D3 не предоставляет собственных карт. Это значит – их придется самостоятельно искать в открытых источниках, но вряд ли сейчас это большая проблема.\n",
      "\r\n",
      "Документации как таковой у D3 нет, однако есть хорошо расписанное API и множество различных туториалов, примеров и прочих полезных материалов буквально по всему Интернету.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: нет.\n",
      "Лицензия: бесплатно.\n",
      "\n",
      "Google GeoCharts (от Google)\n",
      "\n",
      "\r\n",
      "У гиганта Google есть своя JavaScript-библиотека для визуализации данных – Google Charts. Она содержит возможности и для построения интерактивных карт-визуализаций, называющихся здесь geocharts, видимо, чтобы их не путали с обычными картами, которые нужны только для отображения местоположения объектов.\n",
      "\r\n",
      "Карта в Google Geocharts только одна, и это – карта мира. Однако можно выбрать регион, на который при рисовании будет сделан фокус с помощью зума.\n",
      "\r\n",
      "К сожалению, здешний API не поддерживает прокрутку (scrolling), изменение масштаба (zoom), перетаскивание (drag). Также Google не предлагает иных типов серий для карт, кроме choropleth и bubble. Однажды данные недостатки довольно сильно меня, мягко скажем, опечалили, но в этом был и положительный момент: в конечном счете мне пришлось начать искать и изучать всяческие другие библиотеки, и вот я сейчас делюсь результатами этого микроисследования с вами.\n",
      "\r\n",
      "Так или иначе, GeoCharts – отличное решение для тех людей и проектов, которым не нужны большие и сложные фичи, но кто ищет прежде всего скорость и стабильность вместо какой-то особенной красоты и кастомизации.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: нет.\n",
      "Лицензия: бесплатно.\n",
      "\n",
      "Highmaps (от Highsoft)\n",
      "\n",
      "\n",
      "Highmaps – «младшая сестра» Highcharts и Highstock, популярных библиотек разработки компании Highsoft. Ее ключевые преимущества – открытый код со всеми его плюсами, небольшой размер, высокая скорость отрисовки и широкий набор интерактивных возможностей, таких как drill down и т.п.\n",
      "\r\n",
      "Среди поддерживаемых в Highmaps серий – choropleth и bubble. Кроме того, с помощью этой библиотеки можно рисовать линии для обозначения дорог, рек и коннекторов. Хотя сделать, скажем, flow-карту здесь не так просто, как в тех же amMap и AnyMap.\n",
      "\r\n",
      "Также стоит отметить, что ни Highcharts, ни Highmaps не работают без jQuery, так что для их использования эта зависимость должна быть подключена (или, конечно, есть еще вариант поискать другую библиотеку для визуализации данных — ха-ха).\n",
      "\r\n",
      "Положительный момент – Highmaps имеет большое количество карт в форматах SVG и GeoJSON. Также у нее обильная документация, которая позволяет довольно быстро и успешно разобраться во всевозможных настройках.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: jQuery.\n",
      "Лицензия: бесплатно (брендированные карты) и платно (от $390).\n",
      "\n",
      "jQuery Mapael (от Vincent Broute)\n",
      "\n",
      "\n",
      "jQuery Mapael – еще одна очень достойная, хорошо сделанная библиотека с открытым исходным кодом, предназначенная для создания динамических векторных карт. Она основана на jQuery и Raphael JS, так что эти зависимости нужно иметь в виду при построении поддерживаемых ею choropleth, bubble или connector карт.\n",
      "\r\n",
      "Набор своих, готовых для использования карт лежит в репозитории Mapael. Документация и описание API соединены в одну статью но, честно говоря, начать работу с данной библиотекой достаточно легко и с помощью вот этого подробного туториала.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: jQuery и Raphael.\n",
      "Лицензия: бесплатно.\n",
      "\n",
      "jVectorMap (от Kirill Lebedev)\n",
      "\n",
      "\r\n",
      "Библиотека для интерактивных карт jVectorMap имеет открытый исходный код, как и ряд других перечисленных в статье. Она не будет работать без jQuery. Хотя кому-то может показаться, что такая внешняя зависимость делает данную (или любую другую) библиотеку не слишком легко-универсальной, это не должно стать большой проблемой в наше время, ведь jQuery с большим отрывом занимает первое место среди JavaScript-библиотек для сайтов и по популярности, и по доле рынка.\n",
      "\r\n",
      "К сожалению, выбор доступных типов серий здесь не слишком велик: в jVectorMap предлагаются только choropleth и marker карты, но в то же время – отдадим этой библиотеке должное – они сделаны весьма добротно. Кроме того, тут можно использовать в качестве маркеров любые картинки, а также имеются drill down и ряд других интерактивных фич.\n",
      "\r\n",
      "У jVectorMap довольно небольшое описание API и нет документации в привычном по крайней мере для меня виде. Однако имеется маленький, но все равно очень полезный туториал Getting Started. Свои карты используются, но только в формате GeoJSON.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: jQuery.\n",
      "Лицензия: бесплатно и платно (от $39).\n",
      "\n",
      "Kartograph (от Gregor Aisch)\n",
      "\n",
      "\n",
      "Kartograph – еще одно открытое решение со всеми вытекающими из этого преимуществами. Данную библиотеку характеризует обилие визуальных эффектов, таких как тени, размытие и т.д.\n",
      "\r\n",
      "Интересно также, что вдобавок к традиционным choropleth и bubble типам серий здесь еще есть 3D columns (или, как их иногда называют, вертикальные 3D bars).\n",
      "\r\n",
      "Готовой коллекции карт в Kartograph нет. Но библиотека работает с любыми картами в формате SVG. Также она предлагает утилиту Kartograph.py для создания карт.\n",
      "\r\n",
      "Хотя Kartograph имеет зависимости и не будет работать без Raphael JS (используется как движок для рисования) и jQuery, эта библиотека делает процесс создания карт действительно довольно понятным и беспроблемным, тогда как документация и описание API данной библиотеки, соединенные в одну статью, позволяют вполне легко и удобно заполучить нужные интерактивные карты для ваших проектов.\n",
      "\n",
      "Описание API: есть.\n",
      "Зависимости: jQuery и Raphael.\n",
      "Лицензия: бесплатно.\n",
      "\n",
      "Заключение\r\n",
      "Все перечисленные мною JavaScript-библиотеки для визуализации данных с помощью интерактивных карт – кросс-браузерные. (Ну, в XXI веке иначе, наверное, и быть уже не может.) Некоторые из них даже поддерживают старые браузеры, такие как Internet Explorer 6. (Хотя вот это, я надеюсь, очень скоро уже перестанет быть важным, в конце 2010-х.)\n",
      "\r\n",
      "Все бесплатные библиотеки, которые я упомянул, довольно-таки неплохо справляются с созданием интерактивных карт. Но зато они не обеспечивают такую широкую поддержку, как коммерческие, да и – положа руку на сердце – не так уж сильны в кастомизации (ну, кроме D3, конечно).\n",
      "\r\n",
      "Говоря о коммерческих библиотеках – кстати, обычно ими можно пользоваться бесплатно для некоммерческих и личных целей, – я не могу не отметить AnyMap и Highmaps за их обстоятельные документации и описания API, равно как и за большие пулы всяческих примеров/демо JavaScript-карт с добротной интерактивностью.\n",
      "\r\n",
      "Спасибо за внимание. Надеюсь, эта статья окажется для кого-то полезна с той целью, чтобы быстро ухватить основную суть топовых JavaScript-библиотек, предназначенных для визуализации данных посредством интерактивных карт. Пожалуйста, делитесь в комментариях собственными мыслями, выводами, пониманиями и, конечно, названиями ваших любимых библиотек, если вдруг я незаслуженно, на ваш взгляд, не затронул их в данной статье.\n",
      "\r\n",
      "Всем хорошего дня!    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "16 марта прошло первое в этом году заседание Совета по открытым данным, повесткой которого были открытые данные Рослесхоза, Роспатента, раскрытие пространственных данных и информации о мерах государственной поддержки. Посмотрим, как это было и к чему пришли в этот раз.\n",
      "\n",
      "\n",
      "Официальный пресс-релиз опубликован на сайте Открытого Правительства. На мой взгляд, составлен он очень специфично — больше всего внимания уделено вопросу по публикации информации о мерах государственной поддержки, материалы которого были очень сырыми. Общее впечатление от мероприятия немного противоречивое — с одной стороны, доклады представителей госорганов были более подготовленными и не дублировали раздаточные материалы (как на предыдущем совете), с другой, практически в каждом представленном документе были смысловые и/или орфографические ошибки.\n",
      "\n",
      "1. Утверждение ведомственного плана по открытым данным Рослесхоза\n",
      "\n",
      "Первый пункт повестки был представлен двумя докладами: один от представителя Рослесхоза, второй от представителя Общественного совета при Рослесхозе. Интересно, что перечни актуальных наборов данных, озвученные в докладах, практически не совпадали. К приоритетным социально-значимым наборам данных на ближайшие два года Рослесхоз относит уже опубликованные 15 датасетов + данные о закупках и контрактах, данные о противодействии коррупции, информацию об обращениях граждан, сведения о доходах, расходах и имуществе госслужащих. \n",
      "\n",
      "Список актуальных наборов, предлагаемый Общественным советом, включает в себя данные о квартальной сети, инвентаризации лесов, данные лесного реестра в разрезе субъектов РФ, данные о лесопожарной обстановке и принимаемых мерах и т.д. (список частично опубликован на сайте Общественного совета).\n",
      "\n",
      "Одной из основных причин, затрудняющих работу по публикации открытых данных Рослесхозом, является отсутствие в открытом доступе сведений о квартальной сети. Сложность публикации квартальной сети связана с тем, что “данные о лесных просеках шириной более 4 м являются “информацией ограниченного доступа”. Тем не менее, в настоящее время в около четверти всех субъектов РФ данные о квартальной сети минимально пригодного для использования качества имеются в открытом доступе” (из доклада Председателя Общественного совета при Рослесхозе Е.А. Шварц). Второй причиной было отсутствие понимания того, какие данные можно раскрывать (тут нельзя не добавить, что ответ представителя Аналитического центра “Форум” заключался в том, чтобы “открывать все, что не является государственной тайной или информацией ограниченного доступа”), а третьей — передача полномочий на региональный уровень, за счет чего данные формируются не на федеральном уровне, а на региональном. \n",
      "\n",
      "В целом, от доклада осталось положительное впечатление — видно, что ведомство подходит к публикации данных не формально, а “со смыслом”, при существенной поддержке Общественного совета.\n",
      "\n",
      "2. Утверждение ведомственного плана Роспатента\n",
      "\n",
      "Доклад Роспатента включал статистику по оказанным государственным услугам, информацию об имеющихся информационных системах и ссылки на рейтинг АИС “Мониторинг госсайтов”, согласно которому Роспатент показывает хорошие результаты и занимает 11 место среди всех ФОИВ. С трудом представляю, как можно всерьез отчитываться о подготовке 10 наборов данных за 4 года, но остановимся на планах Роспатента на ближайшие два года. Мнение о них двоякое: с одной стороны, запланировано раскрытие интересной и востребованной информации из реестров изобретений, полезных моделей, промышленных образцов, товарных знаков и знаков обслуживания, программ для ЭВМ и т.д., а с другой — раскрытие 39 наборов данных из этих реестров (не реже 1 раза в месяц) будет выполнено только при условии дополнительного финансирования, размер которого сопоставим с полуторным бюджетом всего Роспатента (сумма была обоснована необходимостью преобразования информационных систем для выгрузки данных). \n",
      "\n",
      "Справедливый комментарий Павла Конотопова (руководитель проектов открытого правительства Аналитического центра “Форум”) заключался в том, что прямой зависимости между финансированием и количеством наборов данных нет, и текущие возможности информационных систем позволяют программистам безболезненно осуществлять выгрузку данных уже сейчас. Еще один комментарий был о том, что в уже опубликованных данных Роспатент исключает атрибуты, не относящиеся к государственной тайне, за счет чего уменьшается практическая польза датасетов.\n",
      "\n",
      "В любом случае, данные из реестров, на которые ведомство запрашивает дополнительное финансирование, будут востребованы всеми гражданами, осуществляющими патентные исследования.\n",
      "\n",
      "3. Раскрытие информации, включенной в перечень пространственных данных, сведений, в форме открытых данных (проект приказа Минэкономразвития РФ № 431-ФЗ)\n",
      "\n",
      "Это самый краткий пункт повестки, причем проект обсуждаемого приказа не был включен в материалы к заседанию. На соответствующее замечание об отсутствии материалов представитель Минэкономразвития ответил, что все проекты приказов публикуются на портале regulation.gov.ru, и, при желании, можно было все найти. Действительно, после Совета по открытым данным на указанном портале все нашла, правда название проекта приказа, указанное в материалах Минэкономразвития к заседанию, не сходится с названием документа, опубликованного на regulation.gov.ru. По словам Павла Конотопова, подробно изучившего новые правила предоставления картографической информации, получение карты России в масштабе 1:25 000 для компании будет стоить порядка 45 — 450 млн рублей, что, конечно, неприемлемо для представителей малого и среднего бизнеса. \n",
      "\n",
      "4. Регламент публикации информации о мерах государственной поддержки, предоставляемой институтами развития\n",
      "\n",
      "Этот пункт повестки подробнее всего описан в официальном пресс-релизе. Обсуждение было посвящено атрибутивному составу набора данных, разработанному Аналитическим центром при Правительстве. Идея заключается в том, что в России существует более 1700 мер государственной поддержки, получить информацию о которых из единого источника нельзя. Разработка структуры набора данных, по словам докладчика, заняла полгода и включала в себя взаимодействие с представителями институтов развития. Полностью согласна с решением Совета по открытым данным отправить документ на доработку, так как даже с точки зрения наличия ошибок и опечаток в структуре данных, документ выглядит очень сырым. Например, в наименовании атрибута “Гендерная характеристика (ДА/НЕТ)” в примере данных указано “Женщина (женское предприятие)”, финансовые показатели где-то измеряются в тысячах, где-то в миллионах, а отсутствие тезаурусов приведет к существенным затруднениям при анализе данных. О последней проблеме высказался Сергей Израйлит (директор департамента планирования, отчетности и оценки эффективности Фонда “Сколково”), заметив, что 98% полей к их мерам поддержки не применима. Представитель Российского экспортного центра заметил, что формулировка некоторых полей приведет к противоречиям нормам ВТО.\n",
      "\n",
      "Подводя итоги, это заседание, на мой взгляд, было полезным для представителей Роспатента и, особенно, для представителей Рослесхоза — оба ведомства пришли с подготовленными вопросами о проблемах, с которыми они столкнулись при подготовке планов по открытости (у Роспатента — проблема с публикацией графических изображений, у Рослесхоза — отсутствие в открытом доступе данных о квартальной сети). Доклад Общественного совета Рослесхоза, несмотря на специфичность изложения в раздаточных материалах, оставляет положительное впечатление о работе Общественного совета. Что касается картографических данных и датасета по мерам государственной поддержки, — над тем и другим нужно еще работать и работать :).    \n",
      " \n",
      "Когда я сдавал экзамены в универе, я всегда садился готовиться на ближайшую к преподу парту и никогда не шел первым, а пропускал вперед 8-10 человек. Тем самым я успевал «подслушать» дополнительные вопросы и, частично, ответы на них.\n",
      "\n",
      "И тут у меня возникла идея, как сделать подсказки по олимпиаде для тех старшеклассников, которые читают Хабр (что говорит о том, что у них действительно продвинутые мозги). Я нашел тех ребят, которые показали отличные результаты на предыдущих олимпиадах НТИ и поспрашивал их, какие дополнительные материалы могут помочь разобраться в теме лучше и дадут дополнительные преимущества на завершающем практическом этапе.\n",
      "\n",
      "Прием заявок на олимпиаду идет до 22 октября, а там есть еще время подготовиться.\n",
      "\n",
      "Коротко про олимпиаду НТИ\n",
      "Группа студентов посмотрели на то, что у нас твориться с образованием и решили приложить усилия, чтобы это исправить. Что творится? А то, что родители многих школьников не разрешают в гараже строить беспилотник или ходить на курсы по построению спутников, «потому что надо сдавать ЕГЭ». Так вот, наши инициативные товарищи решили создать движуху, что талантливые школьники смогут получать баллы за ЕГЭ и поступать в вузы не при помощи тестов, а поработав руками и головой. И теперь у школьников есть отмазка, зачем он кошке всверлил в мозг электроды — «к олимпиаде готовлюсь».\n",
      "\n",
      "Я считаю, что это важно и достойно. Да и к олимпиадам отношусь с уважением.\n",
      "\n",
      "Мои отчеты, как проходила олимпиада в 2016 году\n",
      "\n",
      "Всероссийская инженерная олимпиада для старшеклассников: Космические системы\n",
      "Всероссийская инженерная олимпиада для старшеклассников: BigData и Интеллектуальные энергетические системы\n",
      "\n",
      "Сами организаторы олимпиады вот что пишут про требования к знаниям и умениям участников: \n",
      "\n",
      "\n",
      "понимание основных концепций в области машинного обучения и нейросетей;\n",
      " программирование на языках, сочетающих скорость разработки с возможностью использовать современные инструменты для работы с данными (например Python);\n",
      " умение быстро находить решения, разбираться в них и адаптировать под свои потребности в интернете на специализированных ресурсах (stackexchange и пр.);\n",
      " умение эффективно работать с большими данными, писать алгоритмы эффективно; умение вести тестирование, отладку;\n",
      "\n",
      "А вот рекомендуемые «официальные материалы для подготовки»:\n",
      "\n",
      "\n",
      "Задачи за 2015/16 учебный год (PDF)\n",
      "Задания всех этапов профиля «Большие данные и машинное обучение» 2017 (PDF)\n",
      "Онлайн-курс на Stepik: Подготовка к Олимпиаде НТИ (на основе задач 2015/16 года)\n",
      "Дистанционный курс на Stepik: Программирование на Python \n",
      "Онлайн-курс на Stepik: Алгоритмы: теория и практика. Методы\n",
      "Онлайн-курс на Stepik: Дискретные структуры\n",
      "Онлайн-курс на Stepik: Основы статистики\n",
      "Онлайн-курс МФТИ на Coursera: Теория вероятностей для начинающих\n",
      "Онлайн-курс Яндекса и МФТИ на Coursera: «Обучение на размеченных данных»\n",
      "Онлайн-курс ВШЭ и ШАД на Coursera: «Введение в машинное обучение»\n",
      "Онлайн-курс Яндес и МФТИ на Coursera:«Поиск структуры в данных»\n",
      "\n",
      "Как происходит олимпиада по «Большим данным»:\n",
      "\n",
      "\n",
      "\n",
      "Во время проведения заключительного этапа финалисты работают над выявлением заданной информации из массивы данных, для этого участникам необходимо написать свою программу, используя набор текстов для обучение. Рабочее место — стол, ноутбук, подключенный к интернету.\n",
      "\n",
      "\n",
      "\n",
      "Команда работает в режиме “каждый за своим ноутбуком”, время от времени обсуждая что-то друг с другом. Для проверки заданий ведущий запускает программу команды-участницы с набором случайных текстов и оценивает результат анализа данных.\n",
      "\n",
      "Советы «бывалых»\n",
      "Анонимус1111 советует. \n",
      "\n",
      "Чтобы затащить конкретно последний этап в «машинке», то что-то вот такое:\n",
      "\n",
      "\n",
      "\n",
      "Но для того, чтобы понять, что мужик говорит:\n",
      "\n",
      "\n",
      "Нужно уметь в jupyter *это прямо вообще плюс*\n",
      "Очень нужно уметь в Linux, ребята страдали прямо. Вот тут за 13 минут можно хотя бы понять что да как.\n",
      "Ну и конечно, чтобы не затачиваться под конкретную задачу, ибо в след. раз точно не тоже самое будет. Нужно хорошо уметь в sklearn, numpy\n",
      "\n",
      "Анонимус2222 советует.\n",
      "\n",
      "Общие рекомендации:\n",
      "\n",
      "\n",
      "начинать не с нейронных сетей, а с более простых методов\n",
      " играть с реальными данными\n",
      " использовать Python в Jupyter Notebook\n",
      "\n",
      "Конкретные ресурсы:\n",
      "\n",
      "\n",
      "Введение\n",
      "Стандартный курс\n",
      "Стандартная библиотека\n",
      "И книга по ней\n",
      "Вводная задача\n",
      "Как работает внутри\n",
      "Сборник ссылок\n",
      "Журнал\n",
      "\n",
      "Анонимус3333 рекомендует.\n",
      "\n",
      "\n",
      "Hacker's guide to Neural Networks\n",
      "Understanding LSTM Networks\n",
      "Канал и конкретно видео:\n",
      "\n",
      "О нейронках.\n",
      "\n",
      "\n",
      "\n",
      "О tensorflow.\n",
      "\n",
      "\n",
      "\n",
      "Простой математический курс о машинном обучении с векторами, тервером и базовым алгоритмам.\n",
      "\n",
      "\n",
      "\n",
      "Совет от меня лично.\n",
      "Надо всеми силами постараться дружить с проектом GoTo, потому что там есть и качественные курсы и преподаватели, которые готовы всегда отвечать на вопросы и тусить с учениками в Яндексе по выходным. Для самых хитрых есть гранты. На крайний случай, можно вступить в группу в ВК и у всех спрашивать советы.\n",
      "\n",
      "Что бы вы посоветовали старшеклассникам, чтобы они повысили свой скилл в области Больших данных?      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Сколько старшеклассников читает Хабр? \n",
      "            13.79%\n",
      "           Я учусь в 7 классе \n",
      "            4\n",
      "           \n",
      "            3.45%\n",
      "           Я учусь в 8 классе \n",
      "            1\n",
      "           \n",
      "            3.45%\n",
      "           Я учусь в 9 классе \n",
      "            1\n",
      "           \n",
      "            41.38%\n",
      "           Я учусь в 10 классе \n",
      "            12\n",
      "           \n",
      "            37.93%\n",
      "           Я учусь в 11 классе \n",
      "            11\n",
      "            \n",
      "       Проголосовали 29 пользователей. \n",
      "\n",
      "       Воздержались 69 пользователей. \n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>like</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Юбилейный поток Python для инженеров</td>\n",
       "      <td>14  дек  2022 в 18:01</td>\n",
       "      <td>/ru/company/southbridge/news/t/705358/</td>\n",
       "      <td>Всего голосов 9: ↑7 и ↓2  +5</td>\n",
       "      <td>16 января весь Слёрм будет праздновать: старт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Астрономам порекомендовали меньше использовать...</td>\n",
       "      <td>16  сен  2020 в 16:48</td>\n",
       "      <td>/ru/news/t/519414/</td>\n",
       "      <td>Всего голосов 26: ↑24 и ↓2  +22</td>\n",
       "      <td>\\n\\r\\nАстрономы из Лейденской обсерватории оп...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Разработчик предложил устроить коммунистическу...</td>\n",
       "      <td>15  янв  2022 в 19:00</td>\n",
       "      <td>/ru/news/t/645777/</td>\n",
       "      <td>Всего голосов 27: ↑17 и ↓10  +7</td>\n",
       "      <td>\\n\\r\\n14 января 2022 года разработчик jokteur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>18  мая  2022 в 18:33</td>\n",
       "      <td>/ru/company/southbridge/news/t/666476/</td>\n",
       "      <td>Всего голосов 7: ↑6 и ↓1  +5</td>\n",
       "      <td>24-26 июня пройдёт онлайн-интенсив для инжене...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3-месячный курс по Python</td>\n",
       "      <td>12  янв   в 10:21</td>\n",
       "      <td>/ru/company/southbridge/news/t/710188/</td>\n",
       "      <td>Всего голосов 9: ↑8 и ↓1  +7</td>\n",
       "      <td>Рутинная работа уходит в прошлое: сегодня все...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24 марта Слёрм проведёт открытый урок «Первый ...</td>\n",
       "      <td>18  мар  2022 в 18:31</td>\n",
       "      <td>/ru/company/southbridge/news/t/656419/</td>\n",
       "      <td>Всего голосов 20: ↑17 и ↓3  +14</td>\n",
       "      <td>Думаете, написать свою первую программу на Py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>22  апр  2022 в 14:42</td>\n",
       "      <td>/ru/news/t/662437/</td>\n",
       "      <td>Всего голосов 2: ↑2 и ↓0  +2</td>\n",
       "      <td>27 апреля в 18:00 собираем питонистов на YouT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Последний поток Python для инженеров в этом году</td>\n",
       "      <td>3  авг  2022 в 15:23</td>\n",
       "      <td>/ru/company/southbridge/news/t/680662/</td>\n",
       "      <td>Всего голосов 8: ↑8 и ↓0  +8</td>\n",
       "      <td>29 августа мы запускаем 4 поток курса «Python...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Асинхронное программирование на Python для джу...</td>\n",
       "      <td>29  сен  2022 в 09:34</td>\n",
       "      <td>/ru/company/southbridge/news/t/690692/</td>\n",
       "      <td>Всего голосов 15: ↑8 и ↓7  +1</td>\n",
       "      <td>Как джуну выделиться на фоне таких же новичко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>«Разработчикам вход запрещен» или курс «Python...</td>\n",
       "      <td>27  сен  2021 в 09:03</td>\n",
       "      <td>/ru/company/southbridge/news/t/580100/</td>\n",
       "      <td>Всего голосов 13: ↑12 и ↓1  +11</td>\n",
       "      <td>Полгода назад мы анонсировали первый поток ку...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                   date  \\\n",
       "0               Юбилейный поток Python для инженеров  14  дек  2022 в 18:01   \n",
       "1  Астрономам порекомендовали меньше использовать...  16  сен  2020 в 16:48   \n",
       "2  Разработчик предложил устроить коммунистическу...  15  янв  2022 в 19:00   \n",
       "3  Слёрм запускает 3-дневный интенсив по Python д...  18  мая  2022 в 18:33   \n",
       "4                          3-месячный курс по Python      12  янв   в 10:21   \n",
       "5  24 марта Слёрм проведёт открытый урок «Первый ...  18  мар  2022 в 18:31   \n",
       "6  TechnoMeetsPython. Онлайн митап о Python-разра...  22  апр  2022 в 14:42   \n",
       "7   Последний поток Python для инженеров в этом году   3  авг  2022 в 15:23   \n",
       "8  Асинхронное программирование на Python для джу...  29  сен  2022 в 09:34   \n",
       "9  «Разработчикам вход запрещен» или курс «Python...  27  сен  2021 в 09:03   \n",
       "\n",
       "                                     link                             like  \\\n",
       "0  /ru/company/southbridge/news/t/705358/     Всего голосов 9: ↑7 и ↓2  +5   \n",
       "1                      /ru/news/t/519414/  Всего голосов 26: ↑24 и ↓2  +22   \n",
       "2                      /ru/news/t/645777/  Всего голосов 27: ↑17 и ↓10  +7   \n",
       "3  /ru/company/southbridge/news/t/666476/     Всего голосов 7: ↑6 и ↓1  +5   \n",
       "4  /ru/company/southbridge/news/t/710188/     Всего голосов 9: ↑8 и ↓1  +7   \n",
       "5  /ru/company/southbridge/news/t/656419/  Всего голосов 20: ↑17 и ↓3  +14   \n",
       "6                      /ru/news/t/662437/     Всего голосов 2: ↑2 и ↓0  +2   \n",
       "7  /ru/company/southbridge/news/t/680662/     Всего голосов 8: ↑8 и ↓0  +8   \n",
       "8  /ru/company/southbridge/news/t/690692/    Всего голосов 15: ↑8 и ↓7  +1   \n",
       "9  /ru/company/southbridge/news/t/580100/  Всего голосов 13: ↑12 и ↓1  +11   \n",
       "\n",
       "                                               texts  \n",
       "0   16 января весь Слёрм будет праздновать: старт...  \n",
       "1   \\n\\r\\nАстрономы из Лейденской обсерватории оп...  \n",
       "2   \\n\\r\\n14 января 2022 года разработчик jokteur...  \n",
       "3   24-26 июня пройдёт онлайн-интенсив для инжене...  \n",
       "4   Рутинная работа уходит в прошлое: сегодня все...  \n",
       "5   Думаете, написать свою первую программу на Py...  \n",
       "6   27 апреля в 18:00 собираем питонистов на YouT...  \n",
       "7   29 августа мы запускаем 4 поток курса «Python...  \n",
       "8   Как джуну выделиться на фоне таких же новичко...  \n",
       "9   Полгода назад мы анонсировали первый поток ку...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def web_scraping():\n",
    "    \n",
    "    urls = ['https://habr.com/ru/search/page' + str(x) for x in range(1,6)]\n",
    "    \n",
    "    keywords = ['python', 'анализ данных']\n",
    "    df_list = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        \n",
    "      req1 = requests.get(urls[1], params={'q':[keyword]})\n",
    "      req2 = requests.get(urls[2], params={'q':[keyword]})\n",
    "      req3 = requests.get(urls[3], params={'q':[keyword]})\n",
    "      req4 = requests.get(urls[4], params={'q':[keyword]})\n",
    "      \n",
    "      soup1 = BeautifulSoup(req1.text)\n",
    "      soup2 = BeautifulSoup(req2.text)\n",
    "      soup3 = BeautifulSoup(req3.text)\n",
    "      soup4 = BeautifulSoup(req4.text)\n",
    "      \n",
    "      articles1 = soup1.find_all('article', class_='tm-articles-list__item')\n",
    "      articles2 = soup2.find_all('article', class_='tm-articles-list__item')\n",
    "      articles3 = soup3.find_all('article', class_='tm-articles-list__item')\n",
    "      articles4 = soup4.find_all('article', class_='tm-articles-list__item')\n",
    "      \n",
    "      articles_all = articles1 + articles2 + articles3 + articles4\n",
    "      time.sleep(0.2)\n",
    "    \n",
    "      for article in articles_all:\n",
    "        title = article.find('h2', class_='tm-article-snippet__title tm-article-snippet__title_h2')\n",
    "        if title:\n",
    "          title = title.text\n",
    "          link = article.find('a', class_='tm-article-snippet__title-link').get('href')\n",
    "          dates = article.find('span', class_='tm-article-datetime-published').text\n",
    "          likes = article.find('div', class_ = \"tm-votes-meter tm-data-icons__item\").text\n",
    "          article_soup = BeautifulSoup(requests.get('https://habr.com' + link).text)\n",
    "          texts = article_soup.find_all('div', class_='tm-article-body')[0].text\n",
    "          #print(texts)\n",
    "        else:\n",
    "          title = NaN\n",
    "          link = NaN\n",
    "          dates = NaN\n",
    "          article_soup = NaN\n",
    "          texts = NaN\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df_list.append(pd.DataFrame({'title': [title],'date': [dates], 'link': [link], 'like': [likes], 'texts': [texts]}))\n",
    "      df = pd.concat(df_list).drop_duplicates()\n",
    "        \n",
    "    return df.reset_index(drop=True).dropna(subset=['title'])\n",
    "\n",
    "web_scraping().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae92b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
